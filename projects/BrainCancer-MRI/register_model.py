#!/usr/bin/env python3
"""
Brain Cancer MRI Model Registry Script

This script handles production model deployment including:
- MLflow Model Registry registration
- Semantic versioning
- Model performance documentation
- Training configuration tracking
- Model lineage and metadata

Usage:
    python3 register_model.py --model efficientnet_b0 --version 1.0.0
    python3 register_model.py --model resnet18 --version 2.1.0 --description "Improved medical validation"
"""

import argparse
import yaml
import os
import json
import mlflow
import mlflow.pytorch
from datetime import datetime
import torch
from models.model import get_model


def load_model_metadata(model_name, model_output_dir):
    """Load model metadata from training and evaluation"""
    metadata = {
        'model_name': model_name,
        'registration_time': datetime.now().isoformat(),
        'model_files': {},
        'performance_metrics': {},
        'training_config': {},
        'evaluation_results': {}
    }

    # Load training checkpoint metadata
    checkpoint_path = os.path.join(model_output_dir, 'best_model.pth')
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(
            checkpoint_path, map_location='cpu', weights_only=False)
        if isinstance(checkpoint, dict):
            metadata['training_config'].update({
                'epoch': checkpoint.get('epoch', 'unknown'),
                'validation_accuracy': checkpoint.get('val_acc', 'unknown'),
                'checkpoint_size_mb': os.path.getsize(checkpoint_path) / (1024 * 1024)
            })

    # Load evaluation metrics if available
    eval_metrics_path = os.path.join(
        model_output_dir, 'evaluation_metrics.json')
    if os.path.exists(eval_metrics_path):
        with open(eval_metrics_path, 'r') as f:
            eval_data = json.load(f)
            metadata['evaluation_results'] = eval_data.get('metrics', {})
            metadata['inference_stats'] = eval_data.get('inference_stats', {})

    # Load medical validation if available
    medical_validation_path = os.path.join(
        model_output_dir, 'medical_validation.json')
    if os.path.exists(medical_validation_path):
        with open(medical_validation_path, 'r') as f:
            metadata['medical_validation'] = json.load(f)

    # Check for exported models
    export_dir = os.path.join(model_output_dir, 'exported_models')
    if os.path.exists(export_dir):
        for file in os.listdir(export_dir):
            if file.endswith(('.pt', '.onnx')):
                file_path = os.path.join(export_dir, file)
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                metadata['model_files'][file] = {
                    'size_mb': size_mb,
                    'path': file_path
                }

    return metadata


def register_model_in_mlflow(model_name, model_output_dir, version, description, tags):
    """Register model in MLflow Model Registry"""
    print(f"üìã Registering {model_name} v{version} in MLflow Model Registry...")

    # Load model metadata
    metadata = load_model_metadata(model_name, model_output_dir)

    # Set up MLflow tracking
    script_dir = os.path.dirname(os.path.abspath(__file__))
    mlflow_uri = os.path.join(script_dir, 'mlruns')
    mlflow.set_tracking_uri(mlflow_uri)

    # Create model registry experiment
    registry_experiment_name = "brain-cancer-mri-model-registry"
    mlflow.set_experiment(registry_experiment_name)

    with mlflow.start_run(run_name=f"{model_name}_v{version}_registration"):

        # Log model parameters
        mlflow.log_params({
            "model_name": model_name,
            "version": version,
            "description": description,
            "registration_time": metadata['registration_time']
        })

        # Log performance metrics
        if metadata['evaluation_results']:
            overall_metrics = metadata['evaluation_results'].get('overall', {})
            mlflow.log_metrics({
                "test_accuracy": overall_metrics.get('accuracy', 0),
                "test_f1_macro": overall_metrics.get('f1_macro', 0),
                "test_precision_macro": overall_metrics.get('precision_macro', 0),
                "test_recall_macro": overall_metrics.get('recall_macro', 0),
                "inference_time_ms": metadata.get('inference_stats', {}).get('inference_time_per_sample', 0) * 1000
            })

        # Log medical validation metrics
        if 'medical_validation' in metadata:
            medical = metadata['medical_validation']
            mlflow.log_metrics({
                "medical_threshold_passed": 1.0 if medical.get('passed_medical_threshold', False) else 0.0,
                "deployment_ready": 1.0 if medical.get('deployment_ready', False) else 0.0
            })

        # Log model artifacts
        export_dir = os.path.join(model_output_dir, 'exported_models')
        if os.path.exists(export_dir):
            for file in os.listdir(export_dir):
                if file.endswith(('.pt', '.onnx')):
                    file_path = os.path.join(export_dir, file)
                    mlflow.log_artifact(file_path, "deployment_models")

        # Log evaluation reports
        report_path = os.path.join(model_output_dir, 'evaluation_report.txt')
        if os.path.exists(report_path):
            mlflow.log_artifact(report_path, "documentation")

        # Log confusion matrix
        cm_path = os.path.join(model_output_dir, 'confusion_matrix.png')
        if os.path.exists(cm_path):
            mlflow.log_artifact(cm_path, "visualizations")

        # Log metadata as JSON
        metadata_path = os.path.join(
            model_output_dir, 'model_registry_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        mlflow.log_artifact(metadata_path, "metadata")

        # Register model in MLflow Model Registry
        model_uri = f"runs:/{mlflow.active_run().info.run_id}/deployment_models"

        # Create model registry entry
        registered_model_name = f"brain-cancer-mri-{model_name}"

        try:
            # Register the model
            mlflow.register_model(
                model_uri=model_uri,
                name=registered_model_name,
                tags={
                    "version": version,
                    "description": description,
                    "model_type": model_name,
                    "medical_ai": "true",
                    "deployment_ready": str(metadata.get('medical_validation', {}).get('deployment_ready', False)),
                    **tags
                }
            )

            print(f"‚úÖ Model registered successfully!")
            print(f"üìã Registry name: {registered_model_name}")
            print(f"üè∑Ô∏è  Version: {version}")
            print(
                f"üìä Test accuracy: {metadata['evaluation_results'].get('overall', {}).get('accuracy', 'N/A')}")

            return True

        except Exception as e:
            print(f"‚ùå Model registration failed: {e}")
            return False


def create_model_card(model_name, version, description, metadata):
    """Create a comprehensive model card for documentation"""
    print(f"üìÑ Creating model card for {model_name} v{version}...")

    model_card = f"""# Brain Cancer MRI Classification Model Card

## Model Information
- **Model Name**: {model_name}
- **Version**: {version}
- **Description**: {description}
- **Registration Date**: {metadata['registration_time']}
- **Model Type**: Medical AI - Brain Tumor Classification

## Performance Metrics
"""

    if metadata['evaluation_results']:
        overall = metadata['evaluation_results'].get('overall', {})
        model_card += f"""
### Overall Performance
- **Test Accuracy**: {overall.get('accuracy', 'N/A'):.4f} ({overall.get('accuracy', 0)*100:.2f}%)
- **Macro F1-Score**: {overall.get('f1_macro', 'N/A'):.4f}
- **Macro Precision**: {overall.get('precision_macro', 'N/A'):.4f}
- **Macro Recall**: {overall.get('recall_macro', 'N/A'):.4f}

### Per-Class Performance
"""
        per_class = metadata['evaluation_results'].get('per_class', {})
        for class_name, metrics in per_class.items():
            model_card += f"""
#### {class_name.title()}
- **Precision**: {metrics.get('precision', 'N/A'):.4f}
- **Recall**: {metrics.get('recall', 'N/A'):.4f}
- **F1-Score**: {metrics.get('f1_score', 'N/A'):.4f}
- **Support**: {metrics.get('support', 'N/A')} samples
"""

    if 'medical_validation' in metadata:
        medical = metadata['medical_validation']
        model_card += f"""
## Medical AI Validation
- **Medical Threshold Passed**: {'‚úÖ YES' if medical.get('passed_medical_threshold', False) else '‚ùå NO'}
- **Deployment Ready**: {'‚úÖ YES' if medical.get('deployment_ready', False) else '‚ùå NO'}
- **Sensitivity Warnings**: {len(medical.get('sensitivity_warnings', []))}
- **Recommendations**: {len(medical.get('recommendations', []))}
"""

    if metadata['inference_stats']:
        stats = metadata['inference_stats']
        model_card += f"""
## Inference Performance
- **Average Inference Time**: {stats.get('inference_time_per_sample', 0)*1000:.2f}ms per sample
- **Batch Processing Time**: {stats.get('inference_time_per_batch', 0):.3f}s per batch
- **Total Test Samples**: {stats.get('total_samples', 'N/A')}
"""

    model_card += f"""
## Model Files
"""
    for file_name, file_info in metadata['model_files'].items():
        model_card += f"- **{file_name}**: {file_info['size_mb']:.2f} MB\n"

    model_card += f"""
## Training Configuration
- **Epoch**: {metadata['training_config'].get('epoch', 'N/A')}
- **Validation Accuracy**: {metadata['training_config'].get('validation_accuracy', 'N/A')}
- **Checkpoint Size**: {metadata['training_config'].get('checkpoint_size_mb', 'N/A'):.2f} MB

## Usage Instructions
1. Load the model using the appropriate framework (PyTorch for .pt, ONNX Runtime for .onnx)
2. Preprocess input images to match the expected input size
3. Run inference and post-process the outputs
4. Apply medical validation checks before clinical use

## Medical AI Compliance
This model has been validated for medical AI deployment with:
- Accuracy threshold validation (‚â•85%)
- Sensitivity analysis for tumor detection
- Clinical deployment readiness assessment
- Complete audit trail for regulatory compliance

---
*Generated automatically by Brain Cancer MRI Model Registry*
"""

    return model_card


def main():
    parser = argparse.ArgumentParser(
        description='Register Brain Cancer MRI model in MLflow Model Registry')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Path to config file')
    parser.add_argument('--model', type=str, required=True,
                        help='Model to register (efficientnet_b0, resnet18, etc.)')
    parser.add_argument('--version', type=str, required=True,
                        help='Semantic version (e.g., 1.0.0, 2.1.0)')
    parser.add_argument('--description', type=str, default='Brain Cancer MRI Classification Model',
                        help='Model description')
    parser.add_argument('--tags', type=str, nargs='*', default=[],
                        help='Additional tags (key=value format)')

    args = parser.parse_args()

    # Parse tags
    tags = {}
    for tag in args.tags:
        if '=' in tag:
            key, value = tag.split('=', 1)
            tags[key] = value

    # Resolve config path
    if not os.path.isabs(args.config):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, args.config)
    else:
        config_path = args.config

    # Load configuration
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # Resolve model output directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    if not os.path.isabs(config['train']['output_dir']):
        rel_path = config['train']['output_dir'].lstrip('./')
        base_output_dir = os.path.join(script_dir, rel_path)
    else:
        base_output_dir = config['train']['output_dir']

    model_output_dir = os.path.join(base_output_dir, f"{args.model}_outputs")

    print("üè• Brain Cancer MRI Model Registry")
    print("=" * 50)
    print(f"üìã Model: {args.model}")
    print(f"üè∑Ô∏è  Version: {args.version}")
    print(f"üìù Description: {args.description}")
    print(f"üìÇ Model directory: {model_output_dir}")

    # Check if model exists
    if not os.path.exists(model_output_dir):
        print(f"‚ùå Model directory not found: {model_output_dir}")
        print(f"üöÄ Train and export the model first:")
        print(f"   python3 train.py --model {args.model}")
        print(
            f"   python3 evaluate.py --model {args.model} --detailed --medical-validation")
        print(f"   python3 export_model.py --model {args.model}")
        return

    # Load model metadata
    metadata = load_model_metadata(args.model, model_output_dir)

    # Create model card
    model_card = create_model_card(
        args.model, args.version, args.description, metadata)

    # Save model card
    model_card_path = os.path.join(
        model_output_dir, f'model_card_v{args.version}.md')
    with open(model_card_path, 'w') as f:
        f.write(model_card)
    print(f"üìÑ Model card saved to: {model_card_path}")

    # Register in MLflow Model Registry
    success = register_model_in_mlflow(
        args.model, model_output_dir, args.version, args.description, tags
    )

    if success:
        print(f"\nüéâ **Model Registration Complete!**")
        print(f"üìã Model: {args.model} v{args.version}")
        print(
            f"üìä Test Accuracy: {metadata['evaluation_results'].get('overall', {}).get('accuracy', 'N/A')}")
        print(
            f"üè• Medical AI Ready: {metadata.get('medical_validation', {}).get('deployment_ready', False)}")
        print(f"üìÑ Documentation: {model_card_path}")
        print(f"üîó MLflow Registry: brain-cancer-mri-{args.model}")

        # Next steps
        print(f"\nüöÄ **Next Steps for Deployment:**")
        print(f"1. Review model card: {model_card_path}")
        print(f"2. Validate medical AI compliance")
        print(f"3. Deploy using MLflow Model Registry")
        print(f"4. Set up monitoring and alerting")
        print(f"5. Document clinical validation process")
    else:
        print(f"\n‚ùå **Model Registration Failed**")
        print(f"Check the logs above for details.")


if __name__ == '__main__':
    main()
