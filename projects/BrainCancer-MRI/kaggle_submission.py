#!/usr/bin/env python3
"""
Brain Tumor Classification: Xception Medical Solution for Kaggle

This script is optimized for Kaggle submission with:
- Conservative batch sizes for limited GPU memory
- Reduced number of workers for Kaggle environment
- Optimized for Tesla P100 GPU
- Automatic dataset handling from Kaggle input
- Results saved to Kaggle working directory

USAGE:
    python kaggle_submission.py

FEATURES:
    - Automatic dataset detection from Kaggle input
    - Conservative memory usage for Kaggle environment
    - Optimized for Tesla P100 GPU
    - Results automatically saved to /kaggle/working/
"""

import os
import sys
import warnings
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import transforms, models
from torchvision.datasets import ImageFolder
import kagglehub
from tqdm import tqdm
import time

# Dataset-specific normalization constants (calculated with fixed seed=42)
NORMALIZATION_MEAN = [0.152985081076622, 0.152985081076622, 0.152985081076622]
NORMALIZATION_STD = [0.16176629066467285,
                     0.16176629066467285, 0.16176629066467285]

warnings.filterwarnings('ignore')

# =============================================================================
# KAGGLE ENVIRONMENT DETECTION
# =============================================================================


def detect_kaggle_environment():
    """Detect if running in Kaggle environment and return configuration"""
    is_kaggle = os.path.exists('/kaggle/input')

    if is_kaggle:
        print("üöÄ Kaggle environment detected - applying Kaggle optimizations")

        # Kaggle-specific settings
        config = {
            'batch_size': 8,  # Very conservative for Kaggle
            'num_workers': 2,  # Reduced for Kaggle environment
            'epochs': 20,      # Reduced epochs for faster execution
            'patience': 5,     # Reduced patience for early stopping
            'gradient_accumulation_steps': 4,  # Increase effective batch size
            'compile_mode': None,  # Disable torch.compile for stability
            'pin_memory': True,
            'persistent_workers': False,  # Disable for Kaggle
            'prefetch_factor': 2,  # Reduced for memory efficiency
        }

        # Clear GPU cache for Kaggle
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("‚úÖ GPU cache cleared for Kaggle environment")

    else:
        print("üè† Local environment detected - using standard settings")
        config = {
            'batch_size': 32,
            'num_workers': 4,
            'epochs': 30,
            'patience': 10,
            'gradient_accumulation_steps': 2,
            'compile_mode': 'default',
            'pin_memory': True,
            'persistent_workers': True,
            'prefetch_factor': 4,
        }

    return config, is_kaggle

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================


class XceptionMedical(nn.Module):
    """
    Xception Medical Model for Brain Tumor Classification

    Architecture:
    - Backbone: EfficientNet-B0 (pretrained on ImageNet)
    - Input: 224√ó224 RGB images (converted from grayscale)
    - Classifier: Dropout(0.3) ‚Üí Linear(1280‚Üí128) ‚Üí ReLU ‚Üí Dropout(0.25) ‚Üí Linear(128‚Üí3)
    - Training: Full fine-tuning with Adamax optimizer
    """

    def __init__(self, num_classes=3):
        super(XceptionMedical, self).__init__()

        # EfficientNet-B0 backbone (pretrained)
        self.backbone = models.efficientnet_b0(pretrained=True)

        # Remove the original classifier
        self.backbone.classifier = nn.Identity()

        # Xception-inspired medical classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),                    # Medical dropout
            nn.Linear(1280, 128),              # Intermediate layer
            nn.ReLU(),
            nn.Dropout(0.25),                  # Second dropout
            nn.Linear(128, num_classes)        # Final classification
        )

    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)

# =============================================================================
# DATA TRANSFORMS
# =============================================================================


def get_transforms():
    """
    Get training and validation transforms for Xception Medical model

    RETURNS:
        tuple: (train_transform, val_transform)

    TRANSFORMS:
        - Resize to 224√ó224 (standard EfficientNet-B0 input size)
        - Convert grayscale to RGB (3 channels for EfficientNet-B0)
        - Random horizontal flip (50% probability)
        - Random rotation (¬±10 degrees)
        - Color jitter (brightness, contrast)
        - Normalize with dataset-specific statistics (computed with fixed seed=42)
    """
    train_transform = transforms.Compose([
        # Standard EfficientNet-B0 input size
        transforms.Resize((224, 224)),
        # Convert to RGB for EfficientNet-B0
        transforms.Grayscale(num_output_channels=3),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD)
    ])

    val_transform = transforms.Compose([
        # Standard EfficientNet-B0 input size
        transforms.Resize((224, 224)),
        # Convert to RGB for EfficientNet-B0
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD)
    ])

    return train_transform, val_transform

# =============================================================================
# DATA LOADING
# =============================================================================


def find_dataset_path():
    """Find the dataset path in Kaggle environment or download if needed"""

    # Check if we're in Kaggle environment
    if os.path.exists('/kaggle/input'):
        print("üìÅ Checking Kaggle input directory...")
        kaggle_input = '/kaggle/input'

        # Look for the brain cancer dataset
        for item in os.listdir(kaggle_input):
            item_path = os.path.join(kaggle_input, item)
            if os.path.isdir(item_path):
                # Check if this contains the brain cancer data
                for subitem in os.listdir(item_path):
                    subitem_path = os.path.join(item_path, subitem)
                    if os.path.isdir(subitem_path):
                        # Look for the specific structure
                        if 'Brain_Cancer' in subitem or 'brain-cancer' in subitem:
                            # Check if it has the right structure
                            for class_dir in os.listdir(subitem_path):
                                class_path = os.path.join(
                                    subitem_path, class_dir)
                                if os.path.isdir(class_path) and any(
                                    class_name in class_dir.lower()
                                    for class_name in ['glioma', 'menin', 'tumor']
                                ):
                                    print(
                                        f"‚úÖ Found dataset in Kaggle input: {subitem_path}")
                                    return subitem_path

        print("‚ö†Ô∏è Dataset not found in Kaggle input, downloading...")

    # Download from KaggleHub if not found
    print("üì• Downloading dataset from KaggleHub...")
    path = kagglehub.dataset_download("orvile/brain-cancer-mri-dataset")
    data_path = os.path.join(path, "Brain_Cancer raw MRI data", "Brain_Cancer")

    if os.path.exists(data_path):
        print(f"‚úÖ Dataset downloaded to: {data_path}")
        return data_path
    else:
        raise FileNotFoundError(f"Dataset not found at: {data_path}")


def create_consistent_splits(data_path, random_state=42):
    """
    Create consistent train/val/test splits using ImageFolder

    ARGUMENTS:
        data_path (str): Path to the dataset directory
        random_state (int): Random seed for reproducible splits

    RETURNS:
        tuple: (train_indices, val_indices, test_indices, classes)
    """
    print(f"üìä Loading dataset from: {data_path}")

    # Use ImageFolder - the standard PyTorch way
    try:
        full_dataset = ImageFolder(data_path, transform=None)
        print(f"‚úÖ Dataset loaded successfully with ImageFolder")
    except Exception as e:
        print(f"‚ùå Error loading dataset: {e}")
        print("Expected directory structure:")
        print("  data_path/")
        print("  ‚îú‚îÄ‚îÄ class1/")
        print("  ‚îú‚îÄ‚îÄ class2/")
        print("  ‚îî‚îÄ‚îÄ class3/")
        return None, None, None, None

    # Check dataset structure
    print(f"üìä Dataset info:")
    print(f"  - Classes: {full_dataset.classes}")
    print(f"  - Class mapping: {full_dataset.class_to_idx}")
    print(f"  - Total samples: {len(full_dataset)}")

    # Check if we have multiple classes
    if len(full_dataset.classes) < 2:
        print("‚ùå Error: Need at least 2 classes for classification!")
        print(
            f"Found only {len(full_dataset.classes)} class(es): {full_dataset.classes}")
        return None, None, None, None

    # Create stratified splits
    train_size = int(0.7 * len(full_dataset))
    val_size = int(0.15 * len(full_dataset))

    # Get labels for stratification
    labels = [full_dataset.targets[i] for i in range(len(full_dataset))]

    # Split: train (70%) + temp (30%)
    train_indices, temp_indices = train_test_split(
        range(len(full_dataset)),
        train_size=train_size,
        random_state=random_state,
        stratify=labels
    )

    # Split temp into val (15%) + test (15%)
    temp_labels = [labels[i] for i in temp_indices]
    val_indices, test_indices = train_test_split(
        temp_indices,
        train_size=val_size,
        random_state=random_state,
        stratify=temp_labels
    )

    print(f"üìä Dataset splits created:")
    print(f"  - Training: {len(train_indices)} samples")
    print(f"  - Validation: {len(val_indices)} samples")
    print(f"  - Test: {len(test_indices)} samples")

    return train_indices, val_indices, test_indices, full_dataset.classes


def load_datasets(data_path, train_indices, val_indices, test_indices):
    """
    Load datasets with appropriate transforms using pre-defined splits

    ARGUMENTS:
        data_path (str): Path to the dataset directory
        train_indices (list): Training indices
        val_indices (list): Validation indices
        test_indices (list): Test indices

    RETURNS:
        tuple: (train_dataset, val_dataset, test_dataset, classes)
    """
    print(f"üìä Loading datasets for Xception Medical model...")
    print(f"Data path: {data_path}")

    # Get transforms
    train_transform, val_transform = get_transforms()

    # Load full dataset once
    full_dataset = ImageFolder(data_path, transform=None)
    print(
        f"‚úÖ Full dataset loaded: {len(full_dataset)} samples, {len(full_dataset.classes)} classes")

    # Create subsets with appropriate transforms
    train_dataset = Subset(ImageFolder(
        data_path, transform=train_transform), train_indices)
    val_dataset = Subset(ImageFolder(
        data_path, transform=val_transform), val_indices)
    test_dataset = Subset(ImageFolder(
        data_path, transform=val_transform), test_indices)

    print(f"‚úÖ Datasets created successfully:")
    print(f"  - Train: {len(train_dataset)} samples")
    print(f"  - Val: {len(val_dataset)} samples")
    print(f"  - Test: {len(test_dataset)} samples")
    print(f"  - Classes: {full_dataset.classes}")

    return train_dataset, val_dataset, test_dataset, full_dataset.classes

# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================


def train_model(model, train_loader, val_loader, model_name, epochs=30, patience=10, gradient_accumulation_steps=2):
    """
    Train the model with early stopping and gradient accumulation

    ARGUMENTS:
        model: PyTorch model
        train_loader: Training data loader
        val_loader: Validation data loader
        model_name (str): Name of the model for logging
        epochs (int): Maximum number of epochs
        patience (int): Early stopping patience
        gradient_accumulation_steps (int): Gradient accumulation steps

    RETURNS:
        tuple: (history, best_val_acc)
    """
    device = next(model.parameters()).device
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adamax(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', patience=5, factor=0.5)

    # Training history
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    best_val_acc = 0.0
    patience_counter = 0

    print(f"\n{'='*50}")
    print(f"TRAINING {model_name.upper()}")
    print(f"{'='*50}")

    if gradient_accumulation_steps > 1:
        print(f"Using gradient accumulation:")
        print(
            f"  - Steps: {gradient_accumulation_steps} (effective batch size: {train_loader.batch_size * gradient_accumulation_steps})")

    print(f"Training {model_name} with Adamax optimizer (lr=0.001)")

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        optimizer.zero_grad()

        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')

        for batch_idx, (data, targets) in enumerate(train_pbar):
            data, targets = data.to(device), targets.to(device)

            # Forward pass
            outputs = model(data)
            loss = criterion(outputs, targets)

            # Scale loss for gradient accumulation
            loss = loss / gradient_accumulation_steps
            loss.backward()

            # Gradient accumulation
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()

            # Statistics
            train_loss += loss.item() * gradient_accumulation_steps
            _, predicted = outputs.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()

            # Update progress bar
            train_pbar.set_postfix({
                'Loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(device), targets.to(device)
                outputs = model(data)
                loss = criterion(outputs, targets)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()

        # Calculate metrics
        train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total

        # Update scheduler
        scheduler.step(val_acc)

        # Save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Print epoch results
        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), f'best_{model_name}_model.pth')
            print(f'  ‚úÖ New best model saved! (Val Acc: {val_acc:.2f}%)')
        else:
            patience_counter += 1
            print(
                f'  ‚è≥ No improvement for {patience_counter}/{patience} epochs')

        if patience_counter >= patience:
            print(f'  üõë Early stopping triggered after {epoch+1} epochs')
            break

    print(
        f'\nüèÜ Training completed! Best validation accuracy: {best_val_acc:.2f}%')
    return history, best_val_acc


def evaluate_model(model, test_loader, model_name, classes):
    """
    Evaluate the model on test set

    ARGUMENTS:
        model: PyTorch model
        test_loader: Test data loader
        model_name (str): Name of the model
        classes (list): Class names

    RETURNS:
        tuple: (test_acc, predictions, targets)
    """
    device = next(model.parameters()).device
    model.eval()

    test_correct = 0
    test_total = 0
    all_predictions = []
    all_targets = []

    print(f"\nüîç Evaluating {model_name} on test set...")

    with torch.no_grad():
        for data, targets in tqdm(test_loader, desc='Testing'):
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            _, predicted = outputs.max(1)

            test_total += targets.size(0)
            test_correct += predicted.eq(targets).sum().item()

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    test_acc = 100. * test_correct / test_total

    print(f"üìä {model_name} Test Results:")
    print(f"  - Accuracy: {test_acc:.2f}%")
    print(f"  - Correct: {test_correct}/{test_total}")

    # Print classification report
    print(f"\nüìã Classification Report:")
    print(classification_report(all_targets, all_predictions, target_names=classes))

    return test_acc, all_predictions, all_targets


def plot_results(history, test_acc, preds, targets, classes):
    """Plot training curves and confusion matrix"""

    # Create figure with subplots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # Training curves
    epochs = range(1, len(history['train_loss']) + 1)

    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')
    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)

    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')
    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)

    # Confusion matrix
    cm = confusion_matrix(targets, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes, ax=ax3)
    ax3.set_title(f'Confusion Matrix (Test Acc: {test_acc:.2f}%)')
    ax3.set_xlabel('Predicted')
    ax3.set_ylabel('Actual')

    # Class distribution
    class_counts = np.bincount(targets)
    ax4.bar(classes, class_counts, color=[
            'skyblue', 'lightgreen', 'lightcoral'])
    ax4.set_title('Test Set Class Distribution')
    ax4.set_ylabel('Number of Samples')
    ax4.tick_params(axis='x', rotation=45)

    plt.tight_layout()

    # Save plot
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    print("‚úÖ Training results plot saved as 'training_results.png'")

    # Save to Kaggle working directory if available
    if os.path.exists('/kaggle/working'):
        kaggle_plot_path = '/kaggle/working/training_results.png'
        plt.savefig(kaggle_plot_path, dpi=300, bbox_inches='tight')
        print(
            f"‚úÖ Plot also saved to Kaggle working directory: {kaggle_plot_path}")


def count_parameters(model):
    """Count the number of trainable parameters in the model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# =============================================================================
# MAIN FUNCTION
# =============================================================================


def main():
    """
    Main function for Kaggle submission

    This script is optimized for Kaggle environment with:
    1. Automatic dataset detection from Kaggle input
    2. Conservative batch sizes for limited GPU memory
    3. Reduced number of workers for Kaggle environment
    4. Optimized for Tesla P100 GPU
    5. Results automatically saved to /kaggle/working/
    """

    print("=" * 60)
    print("BRAIN TUMOR CLASSIFICATION: KAGGLE SUBMISSION")
    print("=" * 60)

    # Detect environment and get configuration
    config, is_kaggle = detect_kaggle_environment()

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(
            f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # =============================================================================
    # STEP 1: FIND OR DOWNLOAD DATASET
    # =============================================================================
    print("\nüì• STEP 1: Finding dataset...")

    try:
        data_path = find_dataset_path()
        print(f"‚úÖ Dataset path: {data_path}")
    except Exception as e:
        print(f"‚ùå Error finding dataset: {e}")
        return

    # =============================================================================
    # STEP 2: CREATE DATA SPLITS
    # =============================================================================
    print("\nüîÑ STEP 2: Creating consistent data splits...")

    train_indices, val_indices, test_indices, classes = create_consistent_splits(
        data_path)

    if train_indices is None:
        print("‚ùå Failed to create data splits")
        return

    # =============================================================================
    # STEP 3: LOAD DATASETS
    # =============================================================================
    print("\nüìä STEP 3: Loading datasets with transforms...")

    train_dataset, val_dataset, test_dataset, classes = load_datasets(
        data_path, train_indices, val_indices, test_indices
    )

    print(f"Classes: {classes}")
    print(
        f"Xception Medical - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

    # Display normalization information
    print(f"‚úÖ Using dataset-specific normalization (computed with fixed seed=42):")
    print(f"   Mean: {NORMALIZATION_MEAN}")
    print(f"   Std: {NORMALIZATION_STD}")

    # =============================================================================
    # STEP 4: CREATE DATA LOADERS
    # =============================================================================
    print("\nüìä STEP 4: Creating data loaders...")

    # Use Kaggle-optimized settings
    batch_size = config['batch_size']
    num_workers = config['num_workers']
    pin_memory = config['pin_memory']
    persistent_workers = config['persistent_workers']
    prefetch_factor = config['prefetch_factor']

    print(f"DataLoader settings (Kaggle-optimized):")
    print(f"  - Batch size: {batch_size}")
    print(f"  - Workers: {num_workers}")
    print(f"  - Pin memory: {pin_memory}")
    print(f"  - Persistent workers: {persistent_workers}")
    print(f"  - Prefetch factor: {prefetch_factor}")

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )

    # =============================================================================
    # STEP 5: CREATE MODEL
    # =============================================================================
    print("\nüèóÔ∏è STEP 5: Creating model...")

    # Check GPU memory
    if device.type == 'cuda':
        available_memory = torch.cuda.get_device_properties(
            0).total_memory - torch.cuda.memory_allocated()
        print(f"Available GPU memory: {available_memory / 1024**3:.2f} GB")
        if available_memory < 1 * 1024**3:  # Less than 1GB available
            print("‚ö†Ô∏è Low GPU memory, using conservative settings")

    model = XceptionMedical(num_classes=len(classes)).to(device)

    # Apply torch.compile if available and enabled
    if hasattr(torch, 'compile') and config['compile_mode'] is not None:
        try:
            print(
                f"üîß Applying torch.compile with mode: {config['compile_mode']}")
            model = torch.compile(model, mode=config['compile_mode'])
            print("‚úÖ Model compiled successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è torch.compile failed: {e}")
            print("Continuing without compilation...")

    print(f"Xception Medical: {count_parameters(model):,} parameters")

    # =============================================================================
    # STEP 6: TRAIN MODEL
    # =============================================================================
    print("\nüöÄ STEP 6: Training Xception Medical model...")

    history, best_val = train_model(
        model, train_loader, val_loader, 'xception_medical',
        epochs=config['epochs'],
        patience=config['patience'],
        gradient_accumulation_steps=config['gradient_accumulation_steps']
    )

    # =============================================================================
    # STEP 7: EVALUATE MODEL
    # =============================================================================
    print("\nüîç STEP 7: Loading best model and evaluating on test set...")

    # Load best model
    model.load_state_dict(torch.load('best_xception_medical_model.pth'))
    test_acc, preds, targets = evaluate_model(
        model, test_loader, 'Xception Medical', classes
    )

    # =============================================================================
    # STEP 8: VISUALIZE RESULTS
    # =============================================================================
    print("\nüìà STEP 8: Generating visualizations...")

    plot_results(history, test_acc, preds, targets, classes)

    # =============================================================================
    # STEP 9: FINAL RESULTS
    # =============================================================================
    print("\nüèÜ STEP 9: Final results...")

    # Final results
    results_df = pd.DataFrame({
        'Model': ['Xception Medical'],
        'Test Accuracy (%)': [test_acc],
        'Best Val Accuracy (%)': [best_val],
        'Parameters': [count_parameters(model)],
        'Input Size': ['224√ó224'],
        'Optimizer': ['Adamax'],
        'Learning Rate': [0.001],
        'Environment': ['Kaggle' if is_kaggle else 'Local']
    })

    print("\n" + "=" * 60)
    print("FINAL RESULTS")
    print("=" * 60)
    print(results_df.to_string(index=False))

    print(
        f"\nüèÜ Best Model: Xception Medical with {test_acc:.2f}% test accuracy")

    # Save results
    results_df.to_csv('model_results.csv', index=False)
    print("\nResults saved to 'model_results.csv'")

    # Save results to Kaggle working directory if available
    if os.path.exists('/kaggle/working'):
        kaggle_results_path = '/kaggle/working/model_results.csv'
        results_df.to_csv(kaggle_results_path, index=False)
        print(
            f"Results also saved to Kaggle working directory: {kaggle_results_path}")

        # Save model files to Kaggle working directory
        if os.path.exists('best_xception_medical_model.pth'):
            import shutil
            kaggle_model_path = '/kaggle/working/best_xception_medical_model.pth'

            # Check if we're already in the Kaggle working directory
            if os.path.abspath('best_xception_medical_model.pth') != os.path.abspath(kaggle_model_path):
                shutil.copy('best_xception_medical_model.pth',
                            kaggle_model_path)
                print("‚úÖ Xception model saved to Kaggle working directory")
            else:
                print("‚úÖ Xception model already in Kaggle working directory")

    print("\n" + "=" * 60)
    print("CONCLUSION")
    print("=" * 60)
    print("""
    Key Findings:
    1. Xception Medical achieved high accuracy through:
       - Medical-optimized classifier design
       - Full fine-tuning with Adamax optimizer
       - 224√ó224 input size (standard EfficientNet-B0 size)
       - Grayscale to RGB conversion for EfficientNet-B0 compatibility
       - Dataset-specific normalization (seed=42)
    
    2. Kaggle optimizations:
       - Conservative batch sizes for limited GPU memory
       - Reduced number of workers for Kaggle environment
       - Gradient accumulation for effective larger batch sizes
       - Automatic dataset detection from Kaggle input
    
    3. Architecture advantages:
       - EfficientNet-B0 backbone with Xception-inspired classifier
       - Intermediate 128-unit layer for feature refinement
       - Dropout layers (0.3, 0.25) for regularization
    
    This solution demonstrates the effectiveness of Xception-inspired architectures
    for medical image classification, optimized specifically for Kaggle environment.
    """)


if __name__ == "__main__":
    main()
