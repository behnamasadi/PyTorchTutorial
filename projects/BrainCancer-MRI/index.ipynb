{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b4bf29-f756-4961-8fae-a355b1e7a463",
   "metadata": {},
   "source": [
    "###  1. **Should I compute mean and std over the whole dataset or just the training set?**\n",
    "\n",
    "**‚Üí You should compute mean and std **only over the training data**.**\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Mean and std are **data-dependent statistics**, and if you include validation or test data, you're \"leaking\" information from those sets into the preprocessing of the model.\n",
    "* This violates the core principle of proper evaluation: **the model (and any preprocessing it depends on) must not have seen validation/test data**.\n",
    "\n",
    "---\n",
    "\n",
    "###  2. **What if I augment the training data ‚Äî should I recalculate the mean/std on augmented data?**\n",
    "\n",
    "**‚Üí No, you should calculate mean and std on the original (non-augmented) training data.**\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* The mean/std normalization is part of **standardization**, which should be applied *before* data augmentation.\n",
    "* Augmentations (like flipping, rotation, jitter) **introduce randomness** and are meant to expand the variability of the dataset ‚Äî but we normalize *before* that so that the network trains stably.\n",
    "\n",
    "So your transform pipeline for training should look like this:\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)  # computed from the (non-augmented) training data\n",
    "])\n",
    "```\n",
    "\n",
    "Your test/validation transforms should **not have augmentation**, but should still include the same normalization:\n",
    "\n",
    "```python\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)  # same mean/std as training\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "\n",
    "| Stage                  | Apply Augmentation? | Use Mean/Std? | Notes                                                |\n",
    "| ---------------------- | ------------------- | ------------- | ---------------------------------------------------- |\n",
    "| **Compute Mean/Std**   | ‚ùå No                | üîç Yes        | Only on original training set, after resize & tensor |\n",
    "| **Training Transform** | ‚úÖ Yes               | ‚úÖ Yes         | Add Normalize at the end of transform                |\n",
    "| **Validation/Test**    | ‚ùå No                | ‚úÖ Yes         | Just resize + tensor + normalize                     |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f6c1-2e44-4d37-9ffb-e39791bb54b5",
   "metadata": {},
   "source": [
    "if you're using `torch.utils.data.random_split(...)` **after** loading the full dataset, then you're calculating the mean and std over the entire dataset **before** the train/val/test split, which leaks information. But there's a clean and efficient workaround:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Solution: Use `ImageFolder` twice with a custom split for mean/std computation**\n",
    "\n",
    "Here‚Äôs what you can do:\n",
    "\n",
    "#### Step 1: Load the dataset without augmentation and **only for computing mean/std**\n",
    "\n",
    "```python\n",
    "# Basic transform for computing mean/std (resize + tensor only)\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # or 512x512 if you prefer\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset (no augmentation)\n",
    "dataset = datasets.ImageFolder(path, transform=basic_transform)\n",
    "```\n",
    "\n",
    "#### Step 2: Split into train/val/test **before** computing mean/std\n",
    "\n",
    "```python\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
    ")\n",
    "```\n",
    "\n",
    "#### Step 3: Compute mean/std **only on the training subset**\n",
    "\n",
    "You‚Äôll need to wrap the `train_dataset` into a `DataLoader` directly:\n",
    "\n",
    "```python\n",
    "def calculate_mean_std(subset, batch_size=64):\n",
    "    loader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total = 0\n",
    "\n",
    "    for images, _ in tqdm(loader, desc='Computing mean/std'):\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total += images.size(0)\n",
    "\n",
    "    mean /= total\n",
    "    std /= total\n",
    "    return mean, std\n",
    "\n",
    "mean, std = calculate_mean_std(train_dataset)\n",
    "```\n",
    "\n",
    "#### Step 4: Recreate train/val/test datasets with **final transforms**\n",
    "\n",
    "Now that you have `mean` and `std`, you recreate the datasets **again**, but this time with appropriate augmentation and normalization:\n",
    "\n",
    "```python\n",
    "# Define transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Reload dataset and reapply split (same seed!)\n",
    "full_dataset = datasets.ImageFolder(path, transform=None)  # delay transform\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=transform_train),\n",
    "    indices=train_dataset.indices\n",
    ")\n",
    "val_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=transform_val_test),\n",
    "    indices=val_dataset.indices\n",
    ")\n",
    "test_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=transform_val_test),\n",
    "    indices=test_dataset.indices\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Final Result\n",
    "\n",
    "Now:\n",
    "\n",
    "* You calculate `mean` and `std` **only on training data**.\n",
    "* You avoid information leakage.\n",
    "* You use the **same splits and indices** with proper transforms.\n",
    "\n",
    "Would you like me to wrap this into a single clean script for you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a92a60-6853-469e-9742-58f88a0a8c4d",
   "metadata": {},
   "source": [
    "You're *very close*, and the logic is solid ‚Äî but there are **two small corrections** you need to make:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó 1. Fix: `shuffle` is not a valid argument in `calculate_mean_std(...)`\n",
    "\n",
    "Your call here:\n",
    "\n",
    "```python\n",
    "mean, std = calculate_mean_std(train_dataset, batch_size=batch_size,\n",
    "                               shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "```\n",
    "\n",
    "You're passing `shuffle=False` to `calculate_mean_std(...)`, but your function does **not** take a `shuffle` parameter.\n",
    "\n",
    "‚úÖ **Fix**:\n",
    "Remove `shuffle=False` from the call:\n",
    "\n",
    "```python\n",
    "mean, std = calculate_mean_std(train_dataset, batch_size=batch_size,\n",
    "                               num_workers=num_workers, pin_memory=pin_memory)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó 2. Important Detail: `Subset` does **not** update `.transform` correctly\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "train_dataset.transform = training_transform\n",
    "```\n",
    "\n",
    "That only works if `train_dataset` is a regular dataset. But `random_split` returns a `torch.utils.data.Subset`, and modifying its `.transform` **doesn't work**, because it doesn't own the transform ‚Äî the underlying dataset does.\n",
    "\n",
    "‚úÖ **Fix**:\n",
    "Use `Subset` with a **new ImageFolder with the correct transform**, like this:\n",
    "\n",
    "```python\n",
    "# Reload dataset with no transform\n",
    "full_dataset = datasets.ImageFolder(path, transform=None)\n",
    "\n",
    "# Apply transforms to subset manually\n",
    "train_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=training_transform),\n",
    "    indices=train_dataset.indices\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=validation_transform),\n",
    "    indices=val_dataset.indices\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.Subset(\n",
    "    datasets.ImageFolder(path, transform=validation_transform),\n",
    "    indices=test_dataset.indices\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Optional: Add Type Safety to `mean/std` output\n",
    "\n",
    "The current function returns a `tensor`, which is fine. But for logging or saving, you might want to cast to a list:\n",
    "\n",
    "```python\n",
    "print(\"Training Dataset mean:\", mean.tolist())\n",
    "print(\"Training Dataset std:\", std.tolist())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary of Fixes\n",
    "\n",
    "1. **Remove `shuffle=False`** from `calculate_mean_std` call.\n",
    "2. **Don‚Äôt overwrite `transform` on `Subset`**; instead, recreate the datasets using `Subset` on a new `ImageFolder` instance with the desired transform.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to return a fully corrected version of the script in one go?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9a3e0-f878-4ff1-b81c-3e7a49cc08bb",
   "metadata": {},
   "source": [
    "Yes ‚Äî great question! Understanding the issue with `torch.utils.data.Subset` is key to building modular and reusable PyTorch pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó The Issue with `Subset`\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "subset = Subset(original_dataset, indices)\n",
    "subset.transform = new_transform  # ‚ùå This does nothing!\n",
    "```\n",
    "\n",
    "### ‚úÖ Why It Fails\n",
    "\n",
    "The `Subset` class is just a wrapper that holds:\n",
    "\n",
    "```python\n",
    "class Subset:\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "* `Subset` **does not expose or own** a `.transform` property.\n",
    "* Changing `subset.transform = ...` **does NOT change** the transform of `subset.dataset`.\n",
    "* So any transformation logic you expect to apply via `.transform` is silently ignored.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example\n",
    "\n",
    "```python\n",
    "# This transform won't be applied\n",
    "subset = Subset(datasets.ImageFolder(..., transform=old_transform), indices)\n",
    "subset.transform = new_transform  # This has no effect!\n",
    "```\n",
    "\n",
    "To apply a different transform, you must create a **new dataset instance with the transform**, then wrap that in a `Subset`:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Correct Way\n",
    "\n",
    "```python\n",
    "full_dataset = datasets.ImageFolder(path, transform=new_transform)\n",
    "subset = Subset(full_dataset, indices)\n",
    "```\n",
    "\n",
    "This works because now `subset.dataset.transform == new_transform`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Common Mistake\n",
    "\n",
    "```python\n",
    "# ‚ùå Incorrect\n",
    "train_dataset.transform = new_transform  # Does nothing if it's a Subset\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Action                                         | Works? | Why                                                   |\n",
    "| ---------------------------------------------- | ------ | ----------------------------------------------------- |\n",
    "| `subset.transform = ...`                       | ‚ùå No   | `Subset` has no `.transform`                          |\n",
    "| `subset.dataset.transform = ...`               | ‚úÖ Yes  | But modifies the parent dataset (not ideal if reused) |\n",
    "| `Subset(ImageFolder(..., transform), indices)` | ‚úÖ Yes  | Clean and safe                                        |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a utility function like `create_split_with_transform()` to automate this safely?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33129951-bd09-4963-85fb-b21127e52d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeaea054-3cba-4e9f-bcd6-1078ccb89f27",
   "metadata": {},
   "source": [
    "You can now run:\n",
    "```bash\n",
    "python compute_mean_std.py --config config/config.yaml\n",
    "python train.py --config config/config.yaml\n",
    "python evaluate.py --config config/config.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
