{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115c872a-84f1-4900-afa0-6437c0b6c62f",
   "metadata": {},
   "source": [
    "# Dataset Visual Odometry / SLAM Evaluation\n",
    "\n",
    "1. [Download odometry data set (grayscale, 22 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_gray.zip)\n",
    "2. [Download odometry data set (color, 65 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_color.zip)\n",
    "3. [Download odometry data set (velodyne laser data, 80 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_velodyne.zip)\n",
    "4. [Download odometry data set (calibration files, 1 MB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_calib.zip)\n",
    "5. [Download odometry ground truth poses (4 MB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_poses.zip)\n",
    "\n",
    "\n",
    "\n",
    "## Sensor setup \n",
    "<img src=\"images/setup_top_view.png\" />\n",
    "\n",
    "<img src=\"images/passat_sensors_920.png\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a408f-466e-434e-b578-ff5f33dbb0d1",
   "metadata": {},
   "source": [
    "## Calibration Files and Projection Matrices\n",
    "\n",
    "to get the calibration data run:\n",
    "```\n",
    "python kitti_calibration.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2602ef9-d67c-4f55-bc31-f410e1441dec",
   "metadata": {},
   "source": [
    "\n",
    "- $P0$: Reference camera (left of stereo pair 1), extrinsics are identity.\n",
    "- $P1$: Right camera of stereo pair 1, extrinsics include baseline offset.\n",
    "- $P2$: Left camera of stereo pair 2, extrinsics depend on setup.\n",
    "- $P3$: Right camera of stereo pair 2, extrinsics depend on setup.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Camera: $P0$:\n",
    "\n",
    "```\n",
    "Projection Matrix:\n",
    "[[707.0912   0.     601.8873   0.    ]\n",
    " [  0.     707.0912 183.1104   0.    ]\n",
    " [  0.       0.       1.       0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```\n",
    "---\n",
    "\n",
    "Camera: $P1$:\n",
    "```\n",
    "Projection Matrix:\n",
    "[[ 707.0912    0.      601.8873 -379.8145]\n",
    " [   0.      707.0912  183.1104    0.    ]\n",
    " [   0.        0.        1.        0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[ 5.37150653e-01]\n",
    " [-1.34802944e-17]\n",
    " [ 0.00000000e+00]]\n",
    "```\n",
    "\n",
    "From the above image the distance between two camera is `0.54` on $x$ axis and from decomposition we have: `5.37150653e-01`.\n",
    "\n",
    "Refs: [1](https://www.cvlibs.net/datasets/kitti/setup.php)\n",
    "[2](https://stackoverflow.com/questions/29407474/how-to-understand-the-kitti-camera-calibration-files), [3](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT), [4](https://www.cvlibs.net/datasets/kitti/eval_odometry.php), [5](https://github.com/avisingh599/mono-vo/), [6](https://github.com/alishobeiri/Monocular-Video-Odometery), [7](https://avisingh599.github.io/vision/monocular-vo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d06b7-37c4-4519-a41d-92ad42aeefb4",
   "metadata": {},
   "source": [
    "\n",
    "## Ground Truth Poses\n",
    "each row of the data has 12 columns, 12 come from flattening a `3x4` transformation matrix of the left:\n",
    "\n",
    "```\n",
    "r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e8a0f-4588-4c6e-8ffd-3107681a4b31",
   "metadata": {},
   "source": [
    "## Display Ground Truth Poses in rerun \n",
    "just run: \n",
    "\n",
    "```\n",
    "python kitti_gt_to_rerun.py\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"images/display_ground_truth_poses_rerun.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13b782-d4a9-4f72-98f8-118384f767e6",
   "metadata": {},
   "source": [
    "## Display Ground Truth Poses in rerun \n",
    "just run: \n",
    "\n",
    "```\n",
    "python kitti_gt_to_rerun.py\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"images/display_ground_truth_poses_rerun.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b566a8-f229-47b4-80cb-d843170ca64a",
   "metadata": {},
   "source": [
    "## Stereo Vision\n",
    "just run:\n",
    "```\n",
    "python kitti_stereo.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5270691-4e0b-4278-8916-39107e8ac5ae",
   "metadata": {},
   "source": [
    "## Reconstruct Sparse/Dense Model From Known Camera Poses with Colmap\n",
    "\n",
    "Your data should have the following structure: \n",
    "\n",
    "```\n",
    "├── database.db\n",
    "├── dense\n",
    "│   ├── refined\n",
    "│   │   └── model\n",
    "│   │       └── 0\n",
    "│   └── sparse\n",
    "│       └── model\n",
    "│           └── 0\n",
    "├── images\n",
    "│   ├── 00000.png\n",
    "│   ├── 00001.png\n",
    "│   ├── 00002.png\n",
    "│   └── 00003.png\n",
    "└── sparse\n",
    "    └── model\n",
    "        └── 0\n",
    "            ├── cameras.txt\n",
    "            ├── images.txt\n",
    "            └── points3D.txt\n",
    "```\n",
    "\n",
    "1. `cameras.txt`: the format is:\n",
    "\n",
    "```\n",
    "CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n",
    "```\n",
    "so for KITTI dataset the camera model is `PINHOLE`, and it has four parameters which are the focal lengths (`fx`, `fy`) and principal point coordinates (`cx`, `cy`).\n",
    "\n",
    "- `CAMERA_ID`: 1\n",
    "- `MODEL`: PINHOLE\n",
    "- `WIDTH`: 1226\n",
    "- `HEIGHT`: 370\n",
    "- `fx`: 707.0912\n",
    "- `fy`: 707.0912\n",
    "- `cx`: 601.8873\n",
    "- `cy`: 183.1104\n",
    "\n",
    "should be like this:\n",
    "\n",
    "```\n",
    "1 PINHOLE 1226 370 707.0912 707.0912 601.8873 183.1104\n",
    "```\n",
    "\n",
    "2. `images.txt`: the format is\n",
    "```\n",
    "IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "```\n",
    "\n",
    "so you data should be like this, mind the extra line after each line:\n",
    "\n",
    "```\n",
    "1 1.0 0.0 0.0 0.0 0.031831570484910754 -0.2020180259287443 -0.05988511865826446 1 000000.png\n",
    "\n",
    "2 0.9999990698095921 -0.000486454947446343 0.0008155417501438222 -0.0009790981505847082 -0.026717887515950233 -0.09385561937368328 -0.38812196090339146 1 000001.png\n",
    "\n",
    "3 0.9999976159395401 -0.0011567120445530273 0.0013793515824379724 -0.0012359294859380324 -0.23100950491953082 -0.05900910756124116 -0.9698261247623092 1 000002.png\n",
    "\n",
    "4 0.9999950283825452 -0.0017604272641239351 0.0022926784138869423 -0.0012600522730534293 0.17578254454768152 -0.014474209460539546 -1.9112790713853196 1 000003.png\n",
    "```\n",
    "and finally:\n",
    "\n",
    "3. `points3D.txt`: This file should be empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60446115-551d-4065-8082-33ce08db32d1",
   "metadata": {},
   "source": [
    "You can run the following command to convert some colmap dataset into TXT to compare with your dataset:\n",
    "\n",
    "```\n",
    "colmap model_converter --input_path $DATASET_PATH/sparse/0 --output_path $DATASET_PATH/ --output_type TXT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabbe94-a620-4082-9d79-a6122ac76b6a",
   "metadata": {},
   "source": [
    "KITTI format for ground truth poses (for instance, for the file `data/kitti/odometry/05/poses/05.txt`) is:\n",
    "\n",
    "```\n",
    "r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz\n",
    "```\n",
    "The colmap format for `images.txt` is: \n",
    "\n",
    "```\n",
    "IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "```\n",
    "\n",
    "Run the script [kitti_to_colmap.py](../scripts/kitti/kitti_to_colmap.py). It dumps the output into `images.txt` file. \n",
    "\n",
    "\n",
    "You can run the following script to add noise: [kitti_to_colmap_noise.py](../scripts/kitti/kitti_to_colmap_noise.py).\n",
    "\n",
    "\n",
    "The inside of `~/colmap_projects/kitti_noisy` create a soft link pointing to KITTI images:\n",
    "ln -s <path-to-kitti-odometry-image> images\n",
    "\n",
    "in my case:\n",
    "\n",
    "```\n",
    " ln -s /home/$USER/workspace/OpenCVProjects/data/kitti/odometry/05/image_0/ images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb55a26-17c3-4eeb-82f9-206b5b3560d6",
   "metadata": {},
   "source": [
    "## 1) Self-supervised monocular VO (Depth + Pose)\n",
    "\n",
    "\n",
    "* **DepthNet (single image → depth)**\n",
    "\n",
    "  * **Encoder**: `ResNet18` or `MobileNetV2` (tiny & fast). If you want to play with transformers, you *can* try `swin_tiny` as encoder, but start with ResNet18 first.\n",
    "  * **Decoder**: 4–5 upsampling stages with skip connections from the encoder; bilinear upsample + 3×3 convs; edge-aware smoothness.\n",
    "* **PoseNet (2 or 3 frames → 6-DoF)**\n",
    "\n",
    "  * Tiny CNN (e.g., 5 conv layers with stride) on **frame pairs/triplets concatenated along channels**; global average pool → FC(6) for relative pose (axis-angle + translation).\n",
    "\n",
    "**Losses (no GT depth/pose):**\n",
    "\n",
    "* Photometric reprojection (L1 + SSIM) between target and source, using depth + predicted pose + known intrinsics.\n",
    "* Min-reprojection across multiple sources.\n",
    "* Auto-mask static pixels; optionally per-pixel explainability mask.\n",
    "* Edge-aware smoothness on inverse depth.\n",
    "\n",
    "**Why this first?** It’s the classic “SfM-Learner / Monodepth2 family”—easy to fit, and you’ll learn view synthesis, warping, Jacobians, and all the VO basics.\n",
    "\n",
    "## 2) Supervised depth (optional add-on)\n",
    "\n",
    "If you want clean depth numbers without the VO loop, swap the photometric loss for **L1 + scale-invariant log loss** against GT depth on a dataset that has it (e.g., NYU-v2, KITTI depth). You can still keep PoseNet for pose-consistency regularization.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Shapes, sizes, and VRAM knobs\n",
    "\n",
    "* **Input res**: 128×416 or 192×640 (sweet spot).\n",
    "* **Batch**: start with **2** (or **1** + gradient accumulation).\n",
    "* **AMP**: `autocast` + `GradScaler` on.\n",
    "* **Pose frames**: use **(t, t±1)** or a triplet (t−1, t, t+1); predict T_{t→s}.\n",
    "* **Depth range**: predict **inverse depth** with sigmoid and scale to [d_min, d_max].\n",
    "\n",
    "---\n",
    "\n",
    "# Minimal architectures (proven + tiny)\n",
    "\n",
    "### DepthNet (ResNet18 encoder + UNet-style decoder)\n",
    "\n",
    "* Encoder: torchvision `resnet18` up to layer4.\n",
    "* Decoder: for each scale, `upsample ×2 → concat skip → conv(3×3)×2`.\n",
    "* Output: multi-scale disparity {1/8, 1/4, 1/2, 1}, supervise each scale.\n",
    "\n",
    "This is essentially **Monodepth2-style** and fits easily on 4 GB at 192×640 with batch 2.\n",
    "\n",
    "### PoseNet (very small CNN)\n",
    "\n",
    "* Input: concat two RGB frames → 6 channels (or three frames → 9 channels).\n",
    "* Conv(7×7, s=2) → Conv(5×5, s=2) → Conv(3×3, s=2) × 3 → GAP → FC(6).\n",
    "* Last layer init near zero; multiply by 0.01 to keep poses small at start.\n",
    "\n",
    "---\n",
    "\n",
    "# Training recipe (self-sup monocular VO, KITTI)\n",
    "\n",
    "1. **Preprocess**: resize to 192×640, keep fx, fy, cx, cy scaled accordingly.\n",
    "2. **Batching**: sample snippets (length 3) → (t−1, t, t+1).\n",
    "3. **Forward**:\n",
    "\n",
    "   * Depth_t = DepthNet(I_t).\n",
    "   * For each source s∈{t−1,t+1}: T_{t→s} = PoseNet(I_t, I_s).\n",
    "   * Warp I_s→t using Depth_t, T_{t→s}, K (pinhole). Compute photometric loss with SSIM+L1; take **min** over sources per pixel.\n",
    "4. **Regularize**: smoothness on Depth_t (edge-aware with image gradients).\n",
    "5. **Masking**: auto-mask when photometric error of identity warp < reprojection error (static scenes).\n",
    "6. **Optim**: AdamW, lr 1e-4 (Depth), 1e-4 (Pose); cosine decay; weight decay 1e-4.\n",
    "7. **Tricks**: random brightness/contrast jitter, random flips (careful with flips + poses).\n",
    "8. **Logging**: show sample depth maps, photometric error heatmaps, and train/val losses.\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "* **Pose**: ATE/RPE (TUM style) on TUM or EuRoC; KITTI Odometry sequence metrics (t_rel, r_rel).\n",
    "* **Depth** (if you evaluate on GT): AbsRel, SqRel, RMSE, RMSE_log, δ<1.25^n.\n",
    "* **Ablations**: with/without auto-mask; with/without min reprojection; ResNet18 vs Swin-Tiny encoder.\n",
    "\n",
    "---\n",
    "\n",
    "# If you want a Swin-based DepthNet (optional)\n",
    "\n",
    "You can swap the encoder:\n",
    "\n",
    "* Use `swin_tiny_patch4_window7_224` as a **feature pyramid** by tapping outputs after each stage (patch merges simulate downsampling).\n",
    "* Add simple lateral 1×1 convs to map Swin stage channels to decoder widths (e.g., 256→128→64→32).\n",
    "* Keep the same UNet decoder.\n",
    "  Expect **~+20–30% VRAM** vs ResNet18 at the same input size. Start with **128×416** and batch 1 if needed.\n",
    "\n",
    "---\n",
    "\n",
    "# Concrete baselines to run on 4 GB\n",
    "\n",
    "### Baseline A (fastest to success)\n",
    "\n",
    "* **DepthNet**: ResNet18 encoder + UNet decoder\n",
    "* **PoseNet**: tiny 6-layer CNN\n",
    "* **Data**: KITTI Odometry @ **192×640**\n",
    "* **Batch**: 2 (or 1 + accum 8) with **AMP**\n",
    "* **Expected**: training fits comfortably; you’ll see depth form in a few epochs\n",
    "\n",
    "### Baseline B (transformer-curious)\n",
    "\n",
    "* **DepthNet**: Swin-Tiny encoder + light decoder\n",
    "* **PoseNet**: same tiny CNN\n",
    "* **Data**: KITTI Odometry @ **128×416**\n",
    "* **Batch**: 1 (accum 16), **AMP**, maybe gradient checkpointing in Swin\n",
    "* **Expected**: slightly better edges; slower; tighter on VRAM\n",
    "\n",
    "---\n",
    "\n",
    "# Handy implementation tips\n",
    "\n",
    "* **Warper**: implement differentiable pinhole projection with `grid_sample` (mind align_corners, padding mode).\n",
    "* **Scale consistency**: monocular depth is scale-ambiguous—either use median-scaling for eval or add stereo pairs (if available) for scale.\n",
    "* **Camera intrinsics per-sample**: store K in your dataset class and scale with resizing.\n",
    "* **Stability**: start PoseNet outputs near zero; clamp inverse depth to [0.01, 10] (scene-dependent).\n",
    "* **Speed**: compute SSIM on 1/2 scale to save memory, but backprop to full-res photometric term.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8e389-66bb-43ee-825a-2ed4d7268905",
   "metadata": {},
   "source": [
    "####  Single-Scale Output \n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    disp = self.net(x)\n",
    "    return {\"disp_0\": disp}\n",
    "```\n",
    "\n",
    "That’s just a **single-scale** version (full resolution) — meant to keep things simple for initial testing.\n",
    "\n",
    "\n",
    "In **Monodepth2**, **SfMLearner**, and similar self-supervised VO methods,\n",
    "we **predict disparity at multiple scales** (1/8, 1/4, 1/2, 1).\n",
    "\n",
    "Those extra scales make the loss more stable and let the network learn coarse-to-fine depth structure.\n",
    "So let’s extend your `ResNetUNet` to do that.\n",
    "\n",
    "### Multi-Scale Output\n",
    "\n",
    "Directly affects how **stable your training** will be \n",
    "\n",
    "\n",
    "| Scale             | Purpose                                             |\n",
    "| ----------------- | --------------------------------------------------- |\n",
    "| Coarse (1/8, 1/4) | Capture global structure; stabilize gradients early |\n",
    "| Fine (1/2, 1)     | Recover local details; sharpen edges                |\n",
    "\n",
    "During training, we compute **photometric + smoothness losses** at *each scale* and sum them, usually with decreasing weights:\n",
    "\n",
    "\n",
    "$ L = \\sum_s \\lambda_s (L_\\text{photo}^s + \\beta L_\\text{smooth}^s) $\n",
    "\n",
    "e.g., $\\lambda = [1.0, 0.5, 0.25, 0.125]$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, encoder_weights=ResNet18_Weights.IMAGENET1K_V1,\n",
    "                 center_mult=1.0, up_mode=\"deconv\"):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Encoder(weights=encoder_weights)\n",
    "        enc_chs = self.encoder.out_channels  # [64, 64, 128, 256, 512]\n",
    "        c0, c2, c3, c4, c5 = enc_chs\n",
    "\n",
    "        center_out = int(c5 * center_mult)\n",
    "        self.center = ConvBlock(c5, center_out) if center_mult != 1.0 else nn.Identity()\n",
    "        bottom_ch = center_out if center_mult != 1.0 else c5\n",
    "\n",
    "        # Decoder blocks\n",
    "        self.dec4 = DecoderBlock(bottom_ch, c4, c4, up_mode=up_mode)\n",
    "        self.dec3 = DecoderBlock(c4, c3, c3, up_mode=up_mode)\n",
    "        self.dec2 = DecoderBlock(c3, c2, c2, up_mode=up_mode)\n",
    "        self.dec1 = DecoderBlock(c2, c0, c0, up_mode=up_mode)\n",
    "\n",
    "        # Final full-res output\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "            nn.Conv2d(c0, num_classes, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Extra prediction heads for multi-scale outputs\n",
    "        self.disp3 = nn.Conv2d(c4, num_classes, kernel_size=3, padding=1)\n",
    "        self.disp2 = nn.Conv2d(c3, num_classes, kernel_size=3, padding=1)\n",
    "        self.disp1 = nn.Conv2d(c2, num_classes, kernel_size=3, padding=1)\n",
    "        self.disp0 = nn.Conv2d(c0, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0, x2, x3, x4, x5 = self.encoder(x)\n",
    "        x5 = self.center(x5)\n",
    "\n",
    "        d4 = self.dec4(x5, x4)  # -> stride 16\n",
    "        d3 = self.dec3(d4, x3)  # -> stride 8\n",
    "        d2 = self.dec2(d3, x2)  # -> stride 4\n",
    "        d1 = self.dec1(d2, x0)  # -> stride 2\n",
    "\n",
    "        # Multi-scale disparities (upsampled to input size)\n",
    "        disp_3 = torch.sigmoid(self.disp3(d4))\n",
    "        disp_2 = torch.sigmoid(self.disp2(d3))\n",
    "        disp_1 = torch.sigmoid(self.disp1(d2))\n",
    "        disp_0 = torch.sigmoid(self.disp0(d1))\n",
    "\n",
    "        disp_3 = F.interpolate(disp_3, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        disp_2 = F.interpolate(disp_2, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        disp_1 = F.interpolate(disp_1, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        disp_0 = F.interpolate(disp_0, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return {\n",
    "            \"disp_0\": disp_0,  # full res\n",
    "            \"disp_1\": disp_1,  # 1/2\n",
    "            \"disp_2\": disp_2,  # 1/4\n",
    "            \"disp_3\": disp_3,  # 1/8\n",
    "        }\n",
    "```\n",
    "\n",
    "In the above code the network behaves exactly like **Monodepth2’s DepthNet**:\n",
    "It returns a dictionary of multi-scale disparities, all upsampled to the same size for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "###  How to use Multi-Scale Output In Training\n",
    "\n",
    "You can now iterate over the dictionary:\n",
    "\n",
    "```python\n",
    "disp_outs = depth_net(I_t)\n",
    "loss_total = 0\n",
    "for scale, disp in disp_outs.items():\n",
    "    depth = disp_to_depth(disp)\n",
    "    # downscale I_t, I_s, K accordingly\n",
    "    # compute min-reprojection + smoothness per scale\n",
    "    loss_total += scale_weight * loss_scale\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1016f6-75d6-459a-995d-e9f466d76b60",
   "metadata": {},
   "source": [
    "###  Why we upsample all scales to full resolution\n",
    "\n",
    "All four predicted disparity maps (`disp_0 … disp_3`) are **upsampled to full input resolution** before returning them.\n",
    "\n",
    "\n",
    "In **Monodepth2** and other self-supervised depth methods, each decoder scale (1/8, 1/4, 1/2, 1) predicts its own disparity map.\n",
    "However, when computing the **photometric reconstruction loss**, it’s often **simpler** to have all disparities (and warped images) at the *same resolution* (usually the original input size).\n",
    "\n",
    "That’s why we do:\n",
    "\n",
    "```python\n",
    "disp_i = F.interpolate(disp_i, size=x.shape[2:], mode=\"bilinear\")\n",
    "```\n",
    "\n",
    "so that every predicted disparity map aligns with the original target frame resolution.\n",
    "This makes it easy to:\n",
    "\n",
    "* project pixels consistently using the same camera intrinsics, and\n",
    "* visualize all disparities at the same size.\n",
    "\n",
    "---\n",
    "\n",
    "###  The “proper” alternative (multi-scale losses)\n",
    "\n",
    "In the *original* Monodepth2, they **did not upsample** everything to full size.\n",
    "Instead, for each scale $s$:\n",
    "\n",
    "1. Downsample the input images $ I_t, I_s $ and intrinsics $ K $ by $ 2^s $.\n",
    "2. Compute photometric + smoothness losses at that scale.\n",
    "\n",
    "Then they sum losses across scales:\n",
    "\n",
    "$\n",
    "L_\\text{total} = \\sum_{s=0}^3 \\lambda_s L_\\text{photo}^s + \\beta L_\\text{smooth}^s\n",
    "$\n",
    "\n",
    "This is a bit more precise, but also more code to manage.\n",
    "\n",
    "---\n",
    "\n",
    "### Upsampling vs multi-scale loss — trade-offs\n",
    "\n",
    "\n",
    "| Approach                                 | Pros                                                    | Cons                                                      |\n",
    "| ---------------------------------------- | ------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| **Upsample all to full res (your code)** | Simple, consistent intrinsics, easy visualization       | Slightly redundant compute; small interpolation smoothing |\n",
    "| **Keep native scale for each output**    | Exact geometric consistency; matches each encoder level | More complex (need per-scale K & downsampled images)      |\n",
    "\n",
    "In practice, **both work** — many modern implementations upsample all scales to simplify training.\n",
    "\n",
    "---\n",
    "\n",
    "### When to keep as-is\n",
    "\n",
    "For your **first implementation of self-supervised DepthNet/PoseNet**,\n",
    "keep the current design (upsampling to input size). It’s clean, stable, and good enough for learning.\n",
    "\n",
    "Later, if you want more geometrical precision, you can move to the multi-scale loss variant:\n",
    "\n",
    "```python\n",
    "# Example: compute photometric loss per scale\n",
    "for i, (scale, disp) in enumerate(disp_outs.items()):\n",
    "    down_factor = 2 ** i\n",
    "    I_t_s = F.interpolate(I_t, scale_factor=1/down_factor, mode=\"area\")\n",
    "    I_src_s = [F.interpolate(I_s, scale_factor=1/down_factor, mode=\"area\") for I_s in I_srcs]\n",
    "    K_s = scale_intrinsics(K, 1/down_factor)\n",
    "    loss_scale = compute_loss(I_t_s, I_src_s, disp, K_s)\n",
    "    total_loss += weight[i] * loss_scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR Summary\n",
    "\n",
    "| Setting                    | Description                              | Use when                                       |\n",
    "| -------------------------- | ---------------------------------------- | ---------------------------------------------- |\n",
    "|  **Upsample to full res** | Simplifies training and loss computation | Recommended for your current DepthNet          |\n",
    "|  **Keep multi-scale**    | Physically precise, scale-consistent     | For advanced training later (Monodepth2-style) |\n",
    "\n",
    "---\n",
    "\n",
    "So yes — currently all outputs are upsampled so you can easily apply your loss once at the original resolution.\n",
    "That’s totally valid, especially for your first self-supervised VO setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fbb47-bc93-46d6-a8b5-1cce45c20824",
   "metadata": {},
   "source": [
    "### Training PoseNet alongside DepthNet\n",
    "\n",
    "\n",
    "In **self-supervised monocular visual odometry**, the **DepthNet and PoseNet are trained jointly, end-to-end** from raw videos.\n",
    "Neither has ground-truth supervision; the photometric loss ties them together.\n",
    "\n",
    "\n",
    "* **DepthNet** alone predicts only *relative* depth up to scale.\n",
    "  It needs the camera motion to know *how to warp one frame to another* for photometric consistency.\n",
    "* **PoseNet** provides that motion — it learns to estimate the **relative pose** (T_{t→s}) between frames.\n",
    "\n",
    "Together they minimize the **photometric reconstruction loss**:\n",
    "\n",
    "$\n",
    "L_\\text{photo} = \\min_s |I_t - W(I_s, D_t, T_{t\\rightarrow s}, K)|\n",
    "$\n",
    "\n",
    "where $W(\\cdot)$ warps the source $I_s$ into the target view using the predicted **depth** and **pose**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Typical training loop\n",
    "\n",
    "```python\n",
    "for batch in loader:\n",
    "    I_t = batch[\"t\"]           # target frame\n",
    "    I_srcs = batch[\"srcs\"]     # e.g. [I_{t-1}, I_{t+1}]\n",
    "    K = batch[\"K\"]\n",
    "\n",
    "    # --- forward both networks ---\n",
    "    disp_outs = depth_net(I_t)\n",
    "    depth = disp_to_depth(disp_outs[\"disp_0\"])\n",
    "    T_list = [se3_to_SE3(pose_net(torch.cat([I_t, I_s], 1))) for I_s in I_srcs]\n",
    "\n",
    "    # --- photometric loss (min reprojection) ---\n",
    "    loss = compute_min_reproj_loss(I_t, I_srcs, depth, T_list, K)\n",
    "\n",
    "    # --- optional smoothness/regularization ---\n",
    "    loss += λ_smooth * edge_aware_smoothness(disp_outs[\"disp_0\"], I_t)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "You optimize both nets’ parameters with a shared optimizer (or separate LRs):\n",
    "\n",
    "```python\n",
    "params = list(depth_net.parameters()) + list(pose_net.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Common practice\n",
    "\n",
    "| Stage         | Encoder        | Decoder | PoseNet | Train? | Notes                             |\n",
    "| ------------- | -------------- | ------- | ------- | ------ | --------------------------------- |\n",
    "| 1️⃣ Warm-up   | frozen         | ✅       | ✅       | yes    | learn decoder & PoseNet first     |\n",
    "| 2️⃣ Fine-tune | unfrozen       | ✅       | ✅       | yes    | refine depth features jointly     |\n",
    "| 3️⃣ Optional  | freeze PoseNet | ✅       | ❌       | no     | if you only want depth refinement |\n",
    "\n",
    "---\n",
    "\n",
    "###  Variations\n",
    "\n",
    "* **Supervised depth:** train only DepthNet with GT depth → no PoseNet needed.\n",
    "* **Stereo supervision:** PoseNet replaced by known baseline → simpler.\n",
    "* **Multi-frame VO:** PoseNet takes 3 frames (t-1, t, t+1) to produce both poses jointly.\n",
    "\n",
    "---\n",
    "\n",
    " **Summary**\n",
    "\n",
    "| Network      | Input               | Output                 | Trained jointly? |\n",
    "| ------------ | ------------------- | ---------------------- | ---------------- |\n",
    "| **DepthNet** | 1 RGB frame         | per-pixel depth        | ✅                |\n",
    "| **PoseNet**  | 2 (or 3) RGB frames | relative 6-DoF pose(s) | ✅                |\n",
    "\n",
    "They cooperate through the photometric loss — that’s the essence of **self-supervised visual odometry / monocular depth learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bc9b8-c0c7-4fd9-b2c1-e975462bfa61",
   "metadata": {},
   "source": [
    "## City Scapes\n",
    "\n",
    "Refs: [1](https://github.com/mcordts/cityscapesScripts), [2](https://github.com/mcordts/cityscapesScripts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
