{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115c872a-84f1-4900-afa0-6437c0b6c62f",
   "metadata": {},
   "source": [
    "# Dataset Visual Odometry / SLAM Evaluation\n",
    "\n",
    "1. [Download odometry data set (grayscale, 22 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_gray.zip)\n",
    "2. [Download odometry data set (color, 65 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_color.zip)\n",
    "3. [Download odometry data set (velodyne laser data, 80 GB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_velodyne.zip)\n",
    "4. [Download odometry data set (calibration files, 1 MB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_calib.zip)\n",
    "5. [Download odometry ground truth poses (4 MB)](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_poses.zip)\n",
    "\n",
    "\n",
    "\n",
    "## Sensor setup \n",
    "<img src=\"images/setup_top_view.png\" />\n",
    "\n",
    "<img src=\"images/passat_sensors_920.png\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a408f-466e-434e-b578-ff5f33dbb0d1",
   "metadata": {},
   "source": [
    "## Calibration Files and Projection Matrices\n",
    "\n",
    "to get the calibration data run:\n",
    "```\n",
    "python kitti_calibration.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2602ef9-d67c-4f55-bc31-f410e1441dec",
   "metadata": {},
   "source": [
    "\n",
    "- $P0$: Reference camera (left of stereo pair 1), extrinsics are identity.\n",
    "- $P1$: Right camera of stereo pair 1, extrinsics include baseline offset.\n",
    "- $P2$: Left camera of stereo pair 2, extrinsics depend on setup.\n",
    "- $P3$: Right camera of stereo pair 2, extrinsics depend on setup.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Camera: $P0$:\n",
    "\n",
    "```\n",
    "Projection Matrix:\n",
    "[[707.0912   0.     601.8873   0.    ]\n",
    " [  0.     707.0912 183.1104   0.    ]\n",
    " [  0.       0.       1.       0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```\n",
    "---\n",
    "\n",
    "Camera: $P1$:\n",
    "```\n",
    "Projection Matrix:\n",
    "[[ 707.0912    0.      601.8873 -379.8145]\n",
    " [   0.      707.0912  183.1104    0.    ]\n",
    " [   0.        0.        1.        0.    ]]\n",
    "Intrinsic Matrix:\n",
    "[[707.0912   0.     601.8873]\n",
    " [  0.     707.0912 183.1104]\n",
    " [  0.       0.       1.    ]]\n",
    "Rotation Matrix:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "Translation Vector:\n",
    "[[ 5.37150653e-01]\n",
    " [-1.34802944e-17]\n",
    " [ 0.00000000e+00]]\n",
    "```\n",
    "\n",
    "From the above image the distance between two camera is `0.54` on $x$ axis and from decomposition we have: `5.37150653e-01`.\n",
    "\n",
    "Refs: [1](https://www.cvlibs.net/datasets/kitti/setup.php)\n",
    "[2](https://stackoverflow.com/questions/29407474/how-to-understand-the-kitti-camera-calibration-files), [3](https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT), [4](https://www.cvlibs.net/datasets/kitti/eval_odometry.php), [5](https://github.com/avisingh599/mono-vo/), [6](https://github.com/alishobeiri/Monocular-Video-Odometery), [7](https://avisingh599.github.io/vision/monocular-vo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d06b7-37c4-4519-a41d-92ad42aeefb4",
   "metadata": {},
   "source": [
    "\n",
    "## Ground Truth Poses\n",
    "each row of the data has 12 columns, 12 come from flattening a `3x4` transformation matrix of the left:\n",
    "\n",
    "```\n",
    "r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e8a0f-4588-4c6e-8ffd-3107681a4b31",
   "metadata": {},
   "source": [
    "## Display Ground Truth Poses in rerun \n",
    "just run: \n",
    "\n",
    "```\n",
    "python kitti_gt_to_rerun.py\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"images/display_ground_truth_poses_rerun.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13b782-d4a9-4f72-98f8-118384f767e6",
   "metadata": {},
   "source": [
    "## Display Ground Truth Poses in rerun \n",
    "just run: \n",
    "\n",
    "```\n",
    "python kitti_gt_to_rerun.py\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"images/display_ground_truth_poses_rerun.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b566a8-f229-47b4-80cb-d843170ca64a",
   "metadata": {},
   "source": [
    "## Stereo Vision\n",
    "just run:\n",
    "```\n",
    "python kitti_stereo.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5270691-4e0b-4278-8916-39107e8ac5ae",
   "metadata": {},
   "source": [
    "## Reconstruct Sparse/Dense Model From Known Camera Poses with Colmap\n",
    "\n",
    "Your data should have the following structure: \n",
    "\n",
    "```\n",
    "├── database.db\n",
    "├── dense\n",
    "│   ├── refined\n",
    "│   │   └── model\n",
    "│   │       └── 0\n",
    "│   └── sparse\n",
    "│       └── model\n",
    "│           └── 0\n",
    "├── images\n",
    "│   ├── 00000.png\n",
    "│   ├── 00001.png\n",
    "│   ├── 00002.png\n",
    "│   └── 00003.png\n",
    "└── sparse\n",
    "    └── model\n",
    "        └── 0\n",
    "            ├── cameras.txt\n",
    "            ├── images.txt\n",
    "            └── points3D.txt\n",
    "```\n",
    "\n",
    "1. `cameras.txt`: the format is:\n",
    "\n",
    "```\n",
    "CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\n",
    "```\n",
    "so for KITTI dataset the camera model is `PINHOLE`, and it has four parameters which are the focal lengths (`fx`, `fy`) and principal point coordinates (`cx`, `cy`).\n",
    "\n",
    "- `CAMERA_ID`: 1\n",
    "- `MODEL`: PINHOLE\n",
    "- `WIDTH`: 1226\n",
    "- `HEIGHT`: 370\n",
    "- `fx`: 707.0912\n",
    "- `fy`: 707.0912\n",
    "- `cx`: 601.8873\n",
    "- `cy`: 183.1104\n",
    "\n",
    "should be like this:\n",
    "\n",
    "```\n",
    "1 PINHOLE 1226 370 707.0912 707.0912 601.8873 183.1104\n",
    "```\n",
    "\n",
    "2. `images.txt`: the format is\n",
    "```\n",
    "IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "```\n",
    "\n",
    "so you data should be like this, mind the extra line after each line:\n",
    "\n",
    "```\n",
    "1 1.0 0.0 0.0 0.0 0.031831570484910754 -0.2020180259287443 -0.05988511865826446 1 000000.png\n",
    "\n",
    "2 0.9999990698095921 -0.000486454947446343 0.0008155417501438222 -0.0009790981505847082 -0.026717887515950233 -0.09385561937368328 -0.38812196090339146 1 000001.png\n",
    "\n",
    "3 0.9999976159395401 -0.0011567120445530273 0.0013793515824379724 -0.0012359294859380324 -0.23100950491953082 -0.05900910756124116 -0.9698261247623092 1 000002.png\n",
    "\n",
    "4 0.9999950283825452 -0.0017604272641239351 0.0022926784138869423 -0.0012600522730534293 0.17578254454768152 -0.014474209460539546 -1.9112790713853196 1 000003.png\n",
    "```\n",
    "and finally:\n",
    "\n",
    "3. `points3D.txt`: This file should be empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60446115-551d-4065-8082-33ce08db32d1",
   "metadata": {},
   "source": [
    "You can run the following command to convert some colmap dataset into TXT to compare with your dataset:\n",
    "\n",
    "```\n",
    "colmap model_converter --input_path $DATASET_PATH/sparse/0 --output_path $DATASET_PATH/ --output_type TXT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabbe94-a620-4082-9d79-a6122ac76b6a",
   "metadata": {},
   "source": [
    "KITTI format for ground truth poses (for instance, for the file `data/kitti/odometry/05/poses/05.txt`) is:\n",
    "\n",
    "```\n",
    "r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz\n",
    "```\n",
    "The colmap format for `images.txt` is: \n",
    "\n",
    "```\n",
    "IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "```\n",
    "\n",
    "Run the script [kitti_to_colmap.py](../scripts/kitti/kitti_to_colmap.py). It dumps the output into `images.txt` file. \n",
    "\n",
    "\n",
    "You can run the following script to add noise: [kitti_to_colmap_noise.py](../scripts/kitti/kitti_to_colmap_noise.py).\n",
    "\n",
    "\n",
    "The inside of `~/colmap_projects/kitti_noisy` create a soft link pointing to KITTI images:\n",
    "ln -s <path-to-kitti-odometry-image> images\n",
    "\n",
    "in my case:\n",
    "\n",
    "```\n",
    " ln -s /home/$USER/workspace/OpenCVProjects/data/kitti/odometry/05/image_0/ images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb55a26-17c3-4eeb-82f9-206b5b3560d6",
   "metadata": {},
   "source": [
    "## 1) Self-supervised monocular VO (Depth + Pose)\n",
    "\n",
    "\n",
    "* **DepthNet (single image → depth)**\n",
    "\n",
    "  * **Encoder**: `ResNet18` or `MobileNetV2` (tiny & fast). If you want to play with transformers, you *can* try `swin_tiny` as encoder, but start with ResNet18 first.\n",
    "  * **Decoder**: 4–5 upsampling stages with skip connections from the encoder; bilinear upsample + 3×3 convs; edge-aware smoothness.\n",
    "* **PoseNet (2 or 3 frames → 6-DoF)**\n",
    "\n",
    "  * Tiny CNN (e.g., 5 conv layers with stride) on **frame pairs/triplets concatenated along channels**; global average pool → FC(6) for relative pose (axis-angle + translation).\n",
    "\n",
    "**Losses (no GT depth/pose):**\n",
    "\n",
    "* Photometric reprojection (L1 + SSIM) between target and source, using depth + predicted pose + known intrinsics.\n",
    "* Min-reprojection across multiple sources.\n",
    "* Auto-mask static pixels; optionally per-pixel explainability mask.\n",
    "* Edge-aware smoothness on inverse depth.\n",
    "\n",
    "**Why this first?** It’s the classic “SfM-Learner / Monodepth2 family”—easy to fit, and you’ll learn view synthesis, warping, Jacobians, and all the VO basics.\n",
    "\n",
    "## 2) Supervised depth (optional add-on)\n",
    "\n",
    "If you want clean depth numbers without the VO loop, swap the photometric loss for **L1 + scale-invariant log loss** against GT depth on a dataset that has it (e.g., NYU-v2, KITTI depth). You can still keep PoseNet for pose-consistency regularization.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Shapes, sizes, and VRAM knobs\n",
    "\n",
    "* **Input res**: 128×416 or 192×640 (sweet spot).\n",
    "* **Batch**: start with **2** (or **1** + gradient accumulation).\n",
    "* **AMP**: `autocast` + `GradScaler` on.\n",
    "* **Pose frames**: use **(t, t±1)** or a triplet (t−1, t, t+1); predict T_{t→s}.\n",
    "* **Depth range**: predict **inverse depth** with sigmoid and scale to [d_min, d_max].\n",
    "\n",
    "---\n",
    "\n",
    "# Minimal architectures (proven + tiny)\n",
    "\n",
    "### DepthNet (ResNet18 encoder + UNet-style decoder)\n",
    "\n",
    "* Encoder: torchvision `resnet18` up to layer4.\n",
    "* Decoder: for each scale, `upsample ×2 → concat skip → conv(3×3)×2`.\n",
    "* Output: multi-scale disparity {1/8, 1/4, 1/2, 1}, supervise each scale.\n",
    "\n",
    "This is essentially **Monodepth2-style** and fits easily on 4 GB at 192×640 with batch 2.\n",
    "\n",
    "### PoseNet (very small CNN)\n",
    "\n",
    "* Input: concat two RGB frames → 6 channels (or three frames → 9 channels).\n",
    "* Conv(7×7, s=2) → Conv(5×5, s=2) → Conv(3×3, s=2) × 3 → GAP → FC(6).\n",
    "* Last layer init near zero; multiply by 0.01 to keep poses small at start.\n",
    "\n",
    "---\n",
    "\n",
    "# Training recipe (self-sup monocular VO, KITTI)\n",
    "\n",
    "1. **Preprocess**: resize to 192×640, keep fx, fy, cx, cy scaled accordingly.\n",
    "2. **Batching**: sample snippets (length 3) → (t−1, t, t+1).\n",
    "3. **Forward**:\n",
    "\n",
    "   * Depth_t = DepthNet(I_t).\n",
    "   * For each source s∈{t−1,t+1}: T_{t→s} = PoseNet(I_t, I_s).\n",
    "   * Warp I_s→t using Depth_t, T_{t→s}, K (pinhole). Compute photometric loss with SSIM+L1; take **min** over sources per pixel.\n",
    "4. **Regularize**: smoothness on Depth_t (edge-aware with image gradients).\n",
    "5. **Masking**: auto-mask when photometric error of identity warp < reprojection error (static scenes).\n",
    "6. **Optim**: AdamW, lr 1e-4 (Depth), 1e-4 (Pose); cosine decay; weight decay 1e-4.\n",
    "7. **Tricks**: random brightness/contrast jitter, random flips (careful with flips + poses).\n",
    "8. **Logging**: show sample depth maps, photometric error heatmaps, and train/val losses.\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "* **Pose**: ATE/RPE (TUM style) on TUM or EuRoC; KITTI Odometry sequence metrics (t_rel, r_rel).\n",
    "* **Depth** (if you evaluate on GT): AbsRel, SqRel, RMSE, RMSE_log, δ<1.25^n.\n",
    "* **Ablations**: with/without auto-mask; with/without min reprojection; ResNet18 vs Swin-Tiny encoder.\n",
    "\n",
    "---\n",
    "\n",
    "# If you want a Swin-based DepthNet (optional)\n",
    "\n",
    "You can swap the encoder:\n",
    "\n",
    "* Use `swin_tiny_patch4_window7_224` as a **feature pyramid** by tapping outputs after each stage (patch merges simulate downsampling).\n",
    "* Add simple lateral 1×1 convs to map Swin stage channels to decoder widths (e.g., 256→128→64→32).\n",
    "* Keep the same UNet decoder.\n",
    "  Expect **~+20–30% VRAM** vs ResNet18 at the same input size. Start with **128×416** and batch 1 if needed.\n",
    "\n",
    "---\n",
    "\n",
    "# Concrete baselines to run on 4 GB\n",
    "\n",
    "### Baseline A (fastest to success)\n",
    "\n",
    "* **DepthNet**: ResNet18 encoder + UNet decoder\n",
    "* **PoseNet**: tiny 6-layer CNN\n",
    "* **Data**: KITTI Odometry @ **192×640**\n",
    "* **Batch**: 2 (or 1 + accum 8) with **AMP**\n",
    "* **Expected**: training fits comfortably; you’ll see depth form in a few epochs\n",
    "\n",
    "### Baseline B (transformer-curious)\n",
    "\n",
    "* **DepthNet**: Swin-Tiny encoder + light decoder\n",
    "* **PoseNet**: same tiny CNN\n",
    "* **Data**: KITTI Odometry @ **128×416**\n",
    "* **Batch**: 1 (accum 16), **AMP**, maybe gradient checkpointing in Swin\n",
    "* **Expected**: slightly better edges; slower; tighter on VRAM\n",
    "\n",
    "---\n",
    "\n",
    "# Handy implementation tips\n",
    "\n",
    "* **Warper**: implement differentiable pinhole projection with `grid_sample` (mind align_corners, padding mode).\n",
    "* **Scale consistency**: monocular depth is scale-ambiguous—either use median-scaling for eval or add stereo pairs (if available) for scale.\n",
    "* **Camera intrinsics per-sample**: store K in your dataset class and scale with resizing.\n",
    "* **Stability**: start PoseNet outputs near zero; clamp inverse depth to [0.01, 10] (scene-dependent).\n",
    "* **Speed**: compute SSIM on 1/2 scale to save memory, but backprop to full-res photometric term.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bc9b8-c0c7-4fd9-b2c1-e975462bfa61",
   "metadata": {},
   "source": [
    "## City Scapes\n",
    "\n",
    "Refs: [1](https://github.com/mcordts/cityscapesScripts), [2](https://github.com/mcordts/cityscapesScripts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
