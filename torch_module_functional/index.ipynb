{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be30ac0e-483b-4cff-9e30-98d12ee0c7b9",
   "metadata": {},
   "source": [
    "\n",
    "#  torch.nn.Module vs torch.nn.functional\n",
    "\n",
    "When you're building neural networks in PyTorch, you have two main ways to define operations:\n",
    "\n",
    "| Aspect | **torch.nn.Module** | **torch.nn.functional** |\n",
    "|:--|:--|:--|\n",
    "| **What** | Defines *layers* (objects) that keep track of parameters and states. | Defines *functions* (stateless operations) you can apply directly to tensors. |\n",
    "| **Parameters** | Automatically holds and manages parameters like weights and biases. | You must **manually** pass parameters like weights. |\n",
    "| **Use Case** | When building a model class (`nn.Module`) — like building blocks. | When you want **fine-grained control** inside `forward()` without creating a separate module. |\n",
    "| **Examples** | `nn.Conv2d`, `nn.ReLU`, `nn.Linear` | `F.conv2d`, `F.relu`, `F.linear` |\n",
    "| **Good For** | Standard model definition, easier saving/loading, readable code. | Custom forward passes, experimental models. |\n",
    "\n",
    "---\n",
    "\n",
    "#  **torch.nn.Module examples**\n",
    "\n",
    "These are **objects**.  \n",
    "When you create them, they *automatically create parameters* inside, and you just call them.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.randn(1, 3, 32, 32)  # (batch, channels, height, width)\n",
    "\n",
    "# Conv2d layer\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "out = conv(x)\n",
    "\n",
    "# Activation\n",
    "relu = nn.ReLU()\n",
    "out = relu(out)\n",
    "\n",
    "# Linear (fully connected) layer\n",
    "fc = nn.Linear(16 * 32 * 32, 10)  # flatten first\n",
    "out = out.view(out.size(0), -1)  # flatten\n",
    "out = fc(out)\n",
    "\n",
    "# Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "out = softmax(out)\n",
    "\n",
    "print(out.shape)  # should be (batch_size, 10)\n",
    "```\n",
    "\n",
    "Notice:  \n",
    "- `nn.Conv2d`, `nn.Linear` automatically create weights and biases.\n",
    "- You *instantiate* them once, and use them.\n",
    "- Parameters are stored inside the objects (`conv.weight`, `conv.bias`).\n",
    "\n",
    "---\n",
    "\n",
    "#  **torch.nn.functional examples**\n",
    "\n",
    "These are **functions**.  \n",
    "You must **manually** provide inputs and parameters (sometimes).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create tensor\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Define weights manually\n",
    "weight = torch.randn(16, 3, 3, 3, requires_grad=True)\n",
    "bias = torch.randn(16, requires_grad=True)\n",
    "\n",
    "# Convolution (you must pass weights yourself!)\n",
    "out = F.conv2d(x, weight=weight, bias=bias, stride=1, padding=1)\n",
    "\n",
    "# Activation\n",
    "out = F.relu(out)\n",
    "\n",
    "# Flatten\n",
    "out = out.view(out.size(0), -1)\n",
    "\n",
    "# Define manual weights for linear layer\n",
    "linear_weight = torch.randn(10, 16 * 32 * 32, requires_grad=True)\n",
    "linear_bias = torch.randn(10, requires_grad=True)\n",
    "\n",
    "# Linear layer\n",
    "out = F.linear(out, weight=linear_weight, bias=linear_bias)\n",
    "\n",
    "# Softmax\n",
    "out = F.softmax(out, dim=1)\n",
    "\n",
    "print(out.shape)\n",
    "```\n",
    "\n",
    "✅ Notice:\n",
    "- `F.conv2d`, `F.linear` need explicit `weight` and `bias`.\n",
    "- It's **low-level** — you have **full control**.\n",
    "- Useful for building custom layers or experimenting.\n",
    "\n",
    "---\n",
    "\n",
    "#  One more simple comparison:\n",
    "\n",
    "Suppose you want a simple ReLU:\n",
    "\n",
    "```python\n",
    "# With nn.Module\n",
    "relu_layer = nn.ReLU()\n",
    "out = relu_layer(x)\n",
    "\n",
    "# With nn.functional\n",
    "out = F.relu(x)\n",
    "```\n",
    "Here for ReLU, it's almost the same — but **Conv2d**, **Linear** are very different because they have parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#  In Short:\n",
    "\n",
    "| If you need | Use |\n",
    "|:--|:--|\n",
    "| Layers with parameters | `torch.nn.Module` |\n",
    "| Simple operations without parameters | `torch.nn.functional` |\n",
    "| Fine control over forward | `torch.nn.functional` |\n",
    "| Easy readable model | `torch.nn.Module` |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
