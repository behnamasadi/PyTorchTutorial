{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1665a62-a95c-4719-a63c-2846dfcef486",
   "metadata": {},
   "source": [
    "# **Feature Pyramid Network (FPN)**\n",
    "**FPN (Feature Pyramid Network)** is one of the cornerstones of modern **detection and segmentation** architectures — and it ties directly to **PVT** (since PVT outputs a feature pyramid).\n",
    "\n",
    "\n",
    "**Two versions**:\n",
    "\n",
    "1. **Canonical ResNet-style FPN (official FPN paper)**\n",
    "2. **PVT/Swin-Transformer style FPN (for use with PVT-v2-B2)**\n",
    "\n",
    "# **Canonical FPN (ResNet Backbone)**\n",
    "\n",
    "Assume the backbone input is an image of size:\n",
    "\n",
    "**Input: $H \\times W \\times 3$**\n",
    "\n",
    "ResNet produces the following feature stages:\n",
    "\n",
    "| Stage | Source               | Spatial Resolution | Channels |\n",
    "| ----- | -------------------- | ------------------ | -------- |\n",
    "| $C_2$| After ResNet stage 2 | $H/4 \\times W/4$  | 256      |\n",
    "| $C_3$| After stage 3        | $H/8 \\times W/8$  | 512      |\n",
    "| $C_4$| After stage 4        | $H/16 \\times W/16$| 1024     |\n",
    "| $C_5$| After stage 5        | $H/32 \\times W/32$| 2048     |\n",
    "\n",
    "Now FPN converts them to:\n",
    "\n",
    "**All P-levels have the same channel count: 256**\n",
    "\n",
    "| Pyramid Level | Resolution         | Channels |\n",
    "| ------------- | ------------------ | -------- |\n",
    "| $P_5$        | $H/32 \\times W/32$| 256      |\n",
    "| $P_4$        | $H/16 \\times W/16$| 256      |\n",
    "| $P_3$        | $H/8 \\times W/8$  | 256      |\n",
    "| $P_2$        | $H/4 \\times W/4$  | 256      |\n",
    "\n",
    "---\n",
    "\n",
    "## **FPN Equations (with ResNet dimensions)**\n",
    "\n",
    "### **Step 1. Lateral 1×1 reductions**\n",
    "\n",
    "Each backbone output is projected to 256 channels:\n",
    "\n",
    "$$\n",
    "\\tilde{C}_5 = \\text{Conv}_{1\\times 1}(C_5), \\quad \\text{shape } (H/32, W/32, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_4 = \\text{Conv}_{1\\times 1}(C_4), \\quad \\text{shape } (H/16, W/16, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_3 = \\text{Conv}_{1\\times 1}(C_3), \\quad \\text{shape } (H/8, W/8, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_2 = \\text{Conv}_{1\\times 1}(C_2), \\quad \\text{shape } (H/4, W/4, 256)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2. Top–down fusion**\n",
    "\n",
    "$$\n",
    "P_5 = \\tilde{C}_5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_4 = \\tilde{C}_4 + \\text{Upsample}(P_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3 = \\tilde{C}_3 + \\text{Upsample}(P_4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2 = \\tilde{C}_2 + \\text{Upsample}(P_3)\n",
    "$$\n",
    "\n",
    "Upsample is bilinear or nearest-neighbor.\n",
    "It has **no parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3. 3×3 smoothing conv**\n",
    "\n",
    "Each $P_i$is passed through a 3×3 conv $stride 1, padding 1):\n",
    "\n",
    "$$\n",
    "P_i = \\text{Conv}_{3\\times 3}(P_i)\n",
    "$$\n",
    "\n",
    "This removes the checkerboard pattern coming from upsampling.\n",
    "\n",
    "---\n",
    "\n",
    "# **PVT / Swin-Transformer Style FPN**\n",
    "\n",
    "Backbones like **PVT-v2-B2** produce different channel counts:\n",
    "\n",
    "Using **PVT-v2-B2** as example:\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| $C_1$| $H/4$     | 64       |\n",
    "| $C_2$| $H/8$     | 128      |\n",
    "| $C_3$| $H/16$    | 320      |\n",
    "| $C_4$| $H/32$    | 512      |\n",
    "\n",
    "FPN usually uses the last 3 or 4:\n",
    "\n",
    "| C-level | Res    | Channels |\n",
    "| ------- | ------ | -------- |\n",
    "| $C_2$  | $H/4$ | 64       |\n",
    "| $C_3$  | $H/8$ | 128      |\n",
    "| $C_4$  | $H/16$| 320      |\n",
    "| $C_5$  | $H/32$| 512      |\n",
    "\n",
    "---\n",
    "\n",
    "## **Unified FPN projection**\n",
    "\n",
    "Everything is projected to **256 channels**:\n",
    "\n",
    "| Pyramid Level | Resolution | Channels |\n",
    "| ------------- | ---------- | -------- |\n",
    "| $P_5$        | $H/32$    | 256      |\n",
    "| $P_4$        | $H/16$    | 256      |\n",
    "| $P_3$        | $H/8$     | 256      |\n",
    "| $P_2$        | $H/4$     | 256      |\n",
    "\n",
    "Equations remain exactly the same:\n",
    "\n",
    "$$\n",
    "P_5 = \\text{Conv}_{1\\times1}(C_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_4 = \\text{Conv}_{1\\times1}(C_4)+ \\text{Upsample}(P_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3 = \\text{Conv}_{1\\times1}(C_3)+ \\text{Upsample}(P_4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2 = \\text{Conv}_{1\\times1}(C_2)+ \\text{Upsample}(P_3)\n",
    "$$\n",
    "\n",
    "Each $P_i$then gets a 3×3 conv.\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Clean Diagram (Explicit Shapes)**\n",
    "\n",
    "Assume input image:\n",
    "**(H = 512, W = 512)** (just as example)\n",
    "\n",
    "### **Backbone (PVT-v2-B2)**\n",
    "\n",
    "```\n",
    "C2: 128×128, 64 ch\n",
    "C3:  64× 64, 128 ch\n",
    "C4:  32× 32, 320 ch\n",
    "C5:  16× 16, 512 ch\n",
    "```\n",
    "\n",
    "### **After 1×1 lateral projection**\n",
    "\n",
    "```\n",
    "C2 → 128×128, 256 ch\n",
    "C3 →  64× 64, 256 ch\n",
    "C4 →  32× 32, 256 ch\n",
    "C5 →  16× 16, 256 ch\n",
    "```\n",
    "\n",
    "### **Top–down pathway**\n",
    "\n",
    "```\n",
    "P5 = C5                                      → 16×16,   256 ch\n",
    "P4 = C4 + up(P5)    (up: 16→32)             → 32×32,   256 ch\n",
    "P3 = C3 + up(P4)    (up: 32→64)             → 64×64,   256 ch\n",
    "P2 = C2 + up(P3)    (up: 64→128)            → 128×128, 256 ch\n",
    "```\n",
    "\n",
    "### **Final 3×3 smoothing**\n",
    "\n",
    "```\n",
    "P5: 16×16   → 256 ch\n",
    "P4: 32×32   → 256 ch\n",
    "P3: 64×64   → 256 ch\n",
    "P2: 128×128 → 256 ch\n",
    "```\n",
    "\n",
    "These four maps are then used for:\n",
    "\n",
    "* Detection heads\n",
    "* Segmentation decoders\n",
    "* Anchor-free detectors\n",
    "* Panoptic segmentation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Why it’s powerful**\n",
    "\n",
    "- ✅ Handles **objects at multiple scales** (small and large).\n",
    "- ✅ Uses **semantics from deep layers** + **resolution from shallow layers**.- \n",
    "- ✅ Simple and light, yet extremely effective.\n",
    "\n",
    "---\n",
    "\n",
    "## **Common architectures using FPN**\n",
    "\n",
    "| Architecture                 | Backbone            | FPN used for           | Output purpose                 |\n",
    "| :--------------------------- | :------------------ | :--------------------- | :----------------------------- |\n",
    "| **Faster R-CNN + FPN**       | ResNet / Swin / PVT | Object detection       | Multi-scale RoI heads          |\n",
    "| **RetinaNet**                | ResNet / PVT        | Single-stage detection | Multi-scale anchor predictions |\n",
    "| **Mask R-CNN + FPN**         | ResNet / Swin / PVT | Instance segmentation  | Mask head features             |\n",
    "| **UPerNet**                  | PVT / Swin / ViT    | Semantic segmentation  | Pyramid fusion before decoder  |\n",
    "| **Detectron2 / MMDetection** | Many                | Detection/Segmentation | Backbone + FPN combo           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "## **Backbone stages**\n",
    "\n",
    "| Level | Resolution | Channels                                |\n",
    "| ----- | ---------- | --------------------------------------- |\n",
    "| $C_2$| $H/4$     | varies (64 in PVT-v2-B2, 256 in ResNet)|\n",
    "| $C_3$| $H/8$     | varies                                  |\n",
    "| $C_4$| $H/16$    | varies                                  |\n",
    "| $C_5$| $H/32$    | varies                                  |\n",
    "\n",
    "## **FPN pyramid**\n",
    "\n",
    "| Level | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| $P_2$| $H/4$     | 256      |\n",
    "| $P_3$| $H/8$     | 256      |\n",
    "| $P_4$| $H/16$    | 256      |\n",
    "| $P_5$| $H/32$    | 256      |\n",
    "\n",
    "\n",
    "- ✅ **FPN (Feature Pyramid Network)** = multi-scale feature fusion architecture.\n",
    "- ✅ Combines high-res spatial detail (low layers) with strong semantics (deep layers).\n",
    "- ✅ Used in detection (Faster R-CNN, RetinaNet), segmentation (Mask R-CNN, UPerNet).\n",
    "- ✅ Works seamlessly with hierarchical backbones like **ResNet**, **Swin**, **PVT**.\n",
    "- ✅ In code, it’s mostly:\n",
    "\n",
    "* `1×1 conv` for lateral mapping\n",
    "* `upsample + addition`\n",
    "* `3×3 conv` for smoothing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa561da-6e07-4a26-9145-2921546ce7a0",
   "metadata": {},
   "source": [
    "# **The FPN Structure (ASCII Diagram)**\n",
    "\n",
    "```\n",
    "            +--------------------------+\n",
    "            |      Backbone (e.g. PVT) |\n",
    "            +--------------------------+\n",
    "                     │\n",
    "         ┌───────────────────────────────┐\n",
    "         │ Outputs from different stages │\n",
    "         └───────────────────────────────┘\n",
    "             C1: [B, 64, 56, 56]\n",
    "             C2: [B,128, 28, 28]\n",
    "             C3: [B,320, 14, 14]\n",
    "             C4: [B,512,  7,  7]\n",
    "\n",
    "                     ↓ (Top-down path)\n",
    "        +------------------------------------------+\n",
    "        |              FPN construction            |\n",
    "        +------------------------------------------+\n",
    "\n",
    "                             P4 ← 1×1 conv(C4)\n",
    "                              │\n",
    "                              │  (upsample by 2)\n",
    "                              ↓\n",
    "             P3 ← 1×1 conv(C3) + ↑ P4\n",
    "              │\n",
    "              │  (upsample by 2)\n",
    "              ↓\n",
    "     P2 ← 1×1 conv(C2) + ↑ P3\n",
    "      │\n",
    "      │  (upsample by 2)\n",
    "      ↓\n",
    "P1 ← 1×1 conv(C1) + ↑ P2\n",
    "\n",
    "Each Pi then → 3×3 conv smoothing\n",
    "(P1, P2, P3, P4 each: [B, 256, H_i, W_i])\n",
    "```\n",
    "\n",
    "✅ **Top-down path:**\n",
    "Upsamples deeper, low-resolution features (P4→P3→P2→P1).\n",
    "\n",
    "✅ **Lateral connections:**\n",
    "Each upsampled feature is **added** to a 1×1-convolved version of the corresponding backbone feature.\n",
    "\n",
    "✅ **3×3 conv smoothing:**\n",
    "Removes aliasing artifacts after upsampling and addition.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. The Idea in One Picture (Layer Fusion)**\n",
    "\n",
    "```\n",
    "      High-level semantics     (low resolution)\n",
    "             ↑\n",
    "             │   upsample (×2)\n",
    "      +------+------+\n",
    "      |             |\n",
    "    1×1 conv     1×1 conv\n",
    "    on C3         on C4\n",
    "      │             │\n",
    "      └────── add ──┘\n",
    "             │\n",
    "          3×3 conv\n",
    "             ↓\n",
    "            P3\n",
    "```\n",
    "\n",
    "This pattern repeats for each pyramid level — fusing information from the stage above.\n",
    "\n",
    "---\n",
    "\n",
    "# **Conceptual Analogy**\n",
    "\n",
    "Think of FPN as **a decoder inside the backbone**:\n",
    "\n",
    "* Encoder (backbone): progressively downsamples → semantic abstraction\n",
    "* FPN (top-down): progressively upsamples → detail recovery\n",
    "* Result: multi-scale, semantically rich features usable for **detection, segmentation, depth, etc.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42285f-41e1-4935-b1ea-cd1eab8b48a7",
   "metadata": {},
   "source": [
    "# **What Exactly Do we do with $P_1, P_2, P_3, P_4,$** \n",
    "\n",
    "once we have our **multi-scale pyramid outputs**\n",
    "$$P_1, P_2, P_3, P_4,$$\n",
    "**what exactly do we do with them next?**\n",
    "\n",
    "Let’s go step by step — because the answer depends on your **task** (classification, detection, segmentation, depth, etc.), but the principles are always the same.\n",
    "\n",
    "---\n",
    "\n",
    "### **What we have so far**\n",
    "\n",
    "After the **PVT → FPN pipeline**, we have:\n",
    "\n",
    "| Level | Resolution | Channels | Contains               |\n",
    "| :---: | :--------- | :------: | :--------------------- |\n",
    "|   P1  | 56×56      |    256   | fine details, edges    |\n",
    "|   P2  | 28×28      |    256   | object parts           |\n",
    "|   P3  | 14×14      |    256   | larger objects         |\n",
    "|   P4  | 7×7        |    256   | whole-object semantics |\n",
    "\n",
    "Each $ P_i $ is:\n",
    "\n",
    "* **semantically strong** (due to top-down flow),\n",
    "* **spatially meaningful** (due to lateral connections),\n",
    "* and **uniform in channels** (256).\n",
    "\n",
    "Now we choose a **head** depending on the task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317603ea-eceb-4f86-8214-cb09cc5cbe3c",
   "metadata": {},
   "source": [
    "### **Case 1 – Object Detection (e.g., RetinaNet, Faster R-CNN + FPN)**\n",
    "\n",
    "**Goal:** detect objects at different scales.\n",
    "\n",
    "Each $ P_i $ handles **objects of a specific size range**.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Pyramid | Object Size          | Used for          |\n",
    "| :------ | :------------------- | :---------------- |\n",
    "| P1      | small (16–32 px)     | small objects     |\n",
    "| P2      | medium (32–64 px)    | mid-sized objects |\n",
    "| P3      | large (64–128 px)    | large objects     |\n",
    "| P4      | very large (>128 px) | scene-level       |\n",
    "\n",
    "---\n",
    "\n",
    "### **Detection head structure**\n",
    "\n",
    "Each pyramid level is fed to the same *head* (shared weights) that predicts:\n",
    "\n",
    "1. **Class scores** (what object?)\n",
    "2. **Bounding box offsets** (where?)\n",
    "\n",
    "Each head is typically a few conv layers:\n",
    "\n",
    "```python\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels=256, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.cls = nn.Conv2d(256, num_classes, 3, padding=1)\n",
    "        self.box = nn.Conv2d(256, 4, 3, padding=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "        for x in features:  # [P1, P2, P3, P4]\n",
    "            f = self.conv(x)\n",
    "            cls_out = self.cls(f)\n",
    "            box_out = self.box(f)\n",
    "            outputs.append((cls_out, box_out))\n",
    "        return outputs\n",
    "```\n",
    "\n",
    "Each feature map produces predictions at its scale —\n",
    "the results are merged and decoded into final bounding boxes.\n",
    "\n",
    "✅ This is how **RetinaNet**, **Faster R-CNN + FPN**, and **YOLOX**-style detectors work.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a049b4b-d8b1-4af8-88d5-93283f327ebf",
   "metadata": {},
   "source": [
    "### **3. Case 2 – Semantic Segmentation (e.g., UPerNet, DeepLabV3+)**\n",
    "\n",
    "**Goal:** classify *each pixel*.\n",
    "\n",
    "All pyramid levels are **upsampled to the same resolution** (e.g., 1/4 of the input image)\n",
    "and **concatenated** (or summed) before the segmentation head.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: UPerNet-like head**\n",
    "\n",
    "```python\n",
    "upsampled = [\n",
    "    F.interpolate(P, size=P1.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    for P in [P1, P2, P3, P4]\n",
    "]\n",
    "fusion = torch.cat(upsampled, dim=1)  # [B, 256*4, 56, 56]\n",
    "\n",
    "# Final segmentation head\n",
    "seg_head = nn.Sequential(\n",
    "    nn.Conv2d(256*4, 256, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, num_classes, 1)\n",
    ")\n",
    "seg_logits = seg_head(fusion)\n",
    "print(seg_logits.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[B, num_classes, 56, 56]\n",
    "```\n",
    "\n",
    "✅ Every pixel’s prediction is now influenced by **both**\n",
    "\n",
    "* fine details (from P1)\n",
    "* and global context (from P4)\n",
    "\n",
    "This is how **UPerNet**, **DeepLabV3+**, and **MaskFormer** use the FPN pyramid.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba49a1-d04f-4ca3-a444-787c8a9714ca",
   "metadata": {},
   "source": [
    "## **PVT-v2 + FPN segmentation**\n",
    "Below is a **clean, minimal, end-to-end PVT-v2 + FPN segmentation example** using **timm**.\n",
    "\n",
    "It follows a standard structure:\n",
    "\n",
    "1. Load **PVT-v2-B2** as feature extractor\n",
    "2. Build a **top-down FPN decoder**\n",
    "3. Build a **final segmentation head**\n",
    "4. Run forward with dummy input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0c70e2-d45e-45e2-a86b-ed2805cfbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm \n",
    "from timm import create_model\n",
    "\n",
    "# fmt: on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1653a44-0835-41f2-90a9-5a00c8ba4806",
   "metadata": {},
   "source": [
    "#### **2. Load PVT-v2-B2 backbone**\n",
    "\n",
    "PVT-v2-B2 in `timm` outputs 4 feature maps (C1, C2, C3, C4):\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| C1    | 1/4        | 64       |\n",
    "| C2    | 1/8        | 128      |\n",
    "| C3    | 1/16       | 320      |\n",
    "| C4    | 1/32       | 512      |\n",
    "\n",
    "We get these with `features_only=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d447c83-fec2-4879-afae-683b93c8464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_chs': 64, 'reduction': 4, 'module': 'stages.0', 'index': 0}\n",
      "{'num_chs': 128, 'reduction': 8, 'module': 'stages.1', 'index': 1}\n",
      "{'num_chs': 320, 'reduction': 16, 'module': 'stages.2', 'index': 2}\n",
      "{'num_chs': 512, 'reduction': 32, 'module': 'stages.3', 'index': 3}\n",
      "Feature channels: [64, 128, 320, 512]\n",
      "Feature reduction: [4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "backbone = timm.create_model(\n",
    "    'pvt_v2_b2',\n",
    "    pretrained=True,\n",
    "    features_only=True\n",
    ")\n",
    "\n",
    "for i in backbone.feature_info:\n",
    "    print(i)\n",
    "\n",
    "print(f'Feature channels: {backbone.feature_info.channels()}')\n",
    "print(f'Feature reduction: {backbone.feature_info.reduction()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2e961-721d-4e35-a12b-f8ebc76809f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **3. FPN Decoder (minimal)**\n",
    "\n",
    "The FPN idea:\n",
    "\n",
    "* Convert all levels to same channel count (here 256)\n",
    "* Start from highest level C4 → produce P4\n",
    "* Upsample P4 and add to C3 → P3\n",
    "* Upsample P3 and add to C2 → P2\n",
    "* Upsample P2 and add to C1 → P1\n",
    "\n",
    "Upsampling uses **bilinear interpolation (non-learned)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44c4308d-f7b5-4a0b-98fc-00c53a2dc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256): # in_channels is : [64, 128, 320, 512]\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 lateral projections\n",
    "        self.lateral = nn.ModuleList(\n",
    "            [nn.Conv2d(c, out_channels, kernel_size=1) for c in in_channels]\n",
    "        )\n",
    "\n",
    "        # 3x3 smoothing after merging\n",
    "        self.smooth = nn.ModuleList(\n",
    "            [nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "             for _ in range(len(in_channels))]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features = [C1, C2, C3, C4]\n",
    "        C1, C2, C3, C4 = features\n",
    "\n",
    "        P4 = self.lateral[3](C4)\n",
    "        P3 = self.lateral[2](C3) + F.interpolate(P4, size=C3.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        P2 = self.lateral[1](C2) + F.interpolate(P3, size=C2.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        P1 = self.lateral[0](C1) + F.interpolate(P2, size=C1.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        P1 = self.smooth[0](P1)\n",
    "        P2 = self.smooth[1](P2)\n",
    "        P3 = self.smooth[2](P3)\n",
    "        P4 = self.smooth[3](P4)\n",
    "\n",
    "        return [P1, P2, P3, P4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed7766-23f8-4fd6-995f-38369ad64cc1",
   "metadata": {},
   "source": [
    "#### Why a `nn.ModuleList` and not a plain Python `list` and `nn.ModuleList` for `self.lateral`\n",
    "\n",
    "\n",
    "\n",
    "**Short Answer**\n",
    "\n",
    "You **must** use `nn.ModuleList` (not a plain `list`) **when storing layers/modules** inside an `nn.Module` (your network), **if you want PyTorch to track their parameters and move them to GPU/CPU properly**.\n",
    "\n",
    "---\n",
    "\n",
    "Why **not** use a plain Python list?\n",
    "\n",
    "When you write this:\n",
    "\n",
    "```python\n",
    "self.lateral = [nn.Conv2d(c, out_channels, 1) for c in in_channels]\n",
    "```\n",
    "\n",
    "You’re storing the layers in a plain list. But PyTorch **won’t register** them as part of your model.\n",
    "\n",
    "--- \n",
    "\n",
    "As a result:\n",
    "\n",
    "* `model.parameters()` **won’t include them**\n",
    "* `.cuda()` / `.to(device)` **won’t move them**\n",
    "* They **won’t show up** in `model.named_parameters()` or `model.state_dict()`\n",
    "* **They won’t be trained!**\n",
    "* Saving/loading the model will silently skip them\n",
    "\n",
    "This is one of the most common beginner mistakes in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why use `nn.ModuleList`?\n",
    "\n",
    "```python\n",
    "self.lateral = nn.ModuleList([\n",
    "    nn.Conv2d(c, out_channels, 1) for c in in_channels\n",
    "])\n",
    "```\n",
    "\n",
    "This tells PyTorch:\n",
    "**“These are submodules. Track them. Register their parameters.”**\n",
    "\n",
    "Now:\n",
    "\n",
    "* Parameters **will be included in `.parameters()`**\n",
    "* You can move them to GPU with `.cuda()` or `.to('cuda')`\n",
    "* They will be saved/loaded with the model checkpoint\n",
    "* You can iterate over them in `forward()` as usual\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ When to use `nn.ModuleList` vs `nn.Sequential`\n",
    "\n",
    "| Use `nn.Sequential` when...                         | Use `nn.ModuleList` when...                                                      |\n",
    "| --------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| Modules are applied **in order, one after another** | You need **more flexible logic**, e.g., skip connections, top-down fusion, loops |\n",
    "| Example: classic feedforward or MLP layers          | Example: FPN, U-Net skip connections, layer-wise operations                      |\n",
    "\n",
    "In FPN:\n",
    "\n",
    "```python\n",
    "for i in range(4):\n",
    "    P[i] = self.lateral[i](C[i]) + upsample(P[i+1])\n",
    "```\n",
    "\n",
    "So we need indexed access — this can’t be done with `Sequential`.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Common mistake clarified\n",
    "\n",
    "You wrote:\n",
    "\n",
    "> I have only seen that when we create our network model, we inherit from nn.ModuleList\n",
    "\n",
    "That’s a **misunderstanding**:\n",
    "\n",
    "You **don’t** inherit from `nn.ModuleList` in typical models.\n",
    "\n",
    "Instead, you define your model as:\n",
    "\n",
    "```python\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([...])\n",
    "```\n",
    "\n",
    "So `nn.ModuleList` is **used inside a module**, not inherited directly — unless you're doing something very special like building an entire network as a list (rare and not recommended for general models).\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Visual demo\n",
    "\n",
    "Let’s verify it:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class BadNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(10, 10) for _ in range(3)]\n",
    "\n",
    "model = BadNet()\n",
    "print(list(model.parameters()))  # ❌ Empty list!\n",
    "```\n",
    "\n",
    "Now with `ModuleList`:\n",
    "\n",
    "```python\n",
    "class GoodNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(3)])\n",
    "\n",
    "model = GoodNet()\n",
    "print(list(model.parameters()))  # ✅ Contains all parameters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Summary\n",
    "\n",
    "Use `nn.ModuleList` whenever:\n",
    "\n",
    "* You are storing a list of `nn.Module` layers\n",
    "* You need PyTorch to register and track those layers\n",
    "* You want to iterate or use layers flexibly in `forward()`\n",
    "\n",
    "❌ **Never** store layers in plain lists/tuples if you want them trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821779e0-e3d1-4852-97ab-8d049d1dc776",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4. Final Segmentation Head**\n",
    "\n",
    "A simple head:\n",
    "\n",
    "* Concatenate multi-scale FPN features\n",
    "* Upsample to input size\n",
    "* 3×3 Conv → output logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245d4957-e6ad-4ea5-97d3-7a9ff6189a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, fpn_channels=256, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(fpn_channels * 4, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, P):\n",
    "        # P = [P1, P2, P3, P4]\n",
    "        P1, P2, P3, P4 = P\n",
    "\n",
    "        size = P1.shape[-2:]\n",
    "\n",
    "        P2 = F.interpolate(P2, size=size, mode='bilinear', align_corners=False)\n",
    "        P3 = F.interpolate(P3, size=size, mode='bilinear', align_corners=False)\n",
    "        P4 = F.interpolate(P4, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat([P1, P2, P3, P4], dim=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e153f-3f89-4371-9e7f-b3ae8023f80f",
   "metadata": {},
   "source": [
    "#### **5. Full PVT-FPN Segmentation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a5ac49-0dbf-440e-8831-aab68889a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_FPN_Segmentation(nn.Module):\n",
    "    def __init__(self, backbone_name='pvt_v2_b2', num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            features_only=True\n",
    "        )\n",
    "\n",
    "        in_channels = self.backbone.feature_info.channels()\n",
    "\n",
    "        self.fpn = FPN(in_channels, out_channels=256)\n",
    "        self.head = SegmentationHead(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)          # C1,C2,C3,C4\n",
    "        P = self.fpn(features)               # P1,P2,P3,P4\n",
    "        logits = self.head(P)               # final prediction\n",
    "        logits = F.interpolate(logits, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5aa66-7806-43c4-829f-faf97e4b753e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **6. Test the whole pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc6b073f-4f53-4a19-9065-04b3d5595b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = PVT_FPN_Segmentation(num_classes=1)\n",
    "\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    y = model(x)\n",
    "\n",
    "    print(\"Output:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34323034-b1b9-4ea4-bdff-a49397a0a5d1",
   "metadata": {},
   "source": [
    "Expected:\n",
    "\n",
    "```\n",
    "Output: torch.Size([1, 1, 224, 224])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Summary of what happens**\n",
    "\n",
    "**PVT-v2 backbone:**\n",
    "\n",
    "* Extract features at 4 scales\n",
    "\n",
    "**FPN decoder:**\n",
    "\n",
    "* Upsample from deep → shallow\n",
    "* Add skip connections\n",
    "* Produce 4 aligned maps P1–P4\n",
    "\n",
    "**Head:**\n",
    "\n",
    "* Bring all P maps to same size\n",
    "* Concatenate\n",
    "* Predict segmentation mask\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also provide:\n",
    "\n",
    "* A **U-Net style decoder** for PVT\n",
    "* Training loop (Dice + BCE)\n",
    "* Visualization code\n",
    "* Export to ONNX\n",
    "* Variant with **Mask2Former-style** PVT + FPN + Transformer decoder\n",
    "\n",
    "Just tell me what you want next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25422d1f-b1d4-4f20-9089-39133d9d711e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a2cf53-2377-461b-b784-29fdf4b40948",
   "metadata": {},
   "source": [
    "### **4. Case 3 – Depth Estimation / Optical Flow / Reconstruction**\n",
    "\n",
    "For dense regression tasks (depth, disparity, etc.),\n",
    "you can similarly **upsample and fuse** P1–P4, then predict a continuous map.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "depth_map = torch.sigmoid(seg_head(fusion)) * max_depth\n",
    "```\n",
    "\n",
    "✅ Benefit: FPN gives **multi-scale awareness** → depth edges are sharper and large planes smoother.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c89639-600b-4064-9c27-d4f7e0dca61e",
   "metadata": {},
   "source": [
    "### **5. Case 4 – Feature Fusion / Global Pooling**\n",
    "\n",
    "For classification or global tasks, sometimes we don’t need all scales.\n",
    "You can do a **global average pooling** on the coarsest level:\n",
    "\n",
    "$\\text{cls\\_vector} = \\text{GAP}(P_4)$\n",
    "\n",
    "and send it to a linear classifier.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = F.adaptive_avg_pool2d(P4, 1).flatten(1)\n",
    "out = nn.Linear(256, num_classes)(x)\n",
    "```\n",
    "\n",
    "✅ This is how **FPN backbones** can still perform classification.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b110036-8a4c-42fe-9118-6d7f03173ba6",
   "metadata": {},
   "source": [
    "### **Summary of How Each P_i is Used**\n",
    "\n",
    "| Task               | What we do with $P_1, P_2, P_3, P_4$                               | Output             |\n",
    "| :----------------- | :----------------------------------------------------------------- | :----------------- |\n",
    "| **Detection**      | Apply detection head to each Pᵢ separately (multi-scale)           | Class + bbox maps  |\n",
    "| **Segmentation**   | Upsample all Pᵢ to same size, concatenate, predict per-pixel class | Segmentation mask  |\n",
    "| **Depth / Flow**   | Similar to segmentation, but regression output                     | Depth / motion map |\n",
    "| **Classification** | Use highest-level (P4), global avg pool                            | Image class        |\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Intuition**\n",
    "\n",
    "* **Backbone (PVT)** builds *a hierarchical pyramid of features* (edges → parts → objects).\n",
    "* **FPN** refines it into *a multi-scale feature space* with uniform channels.\n",
    "* **Heads** (detector / segmenter / depth regressor) consume those pyramids differently depending on the task.\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "Image\n",
    "  ↓\n",
    "PVT Backbone → [C1..C4]\n",
    "  ↓\n",
    "FPN → [P1..P4]\n",
    "  ↓\n",
    "Task-specific head → output (boxes / masks / depth / class)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **8. Bonus: Example of integration**\n",
    "\n",
    "If you use **timm** and **torchvision.ops.FeaturePyramidNetwork**, you can combine them easily:\n",
    "\n",
    "```python\n",
    "import timm\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "# 1. Backbone\n",
    "backbone = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "channels = backbone.feature_info.channels()\n",
    "\n",
    "# 2. FPN\n",
    "fpn = FeaturePyramidNetwork(in_channels_list=channels, out_channels=256)\n",
    "\n",
    "# 3. Forward\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = backbone(x)\n",
    "pyramid = fpn({f\"c{i}\": f for i, f in enumerate(features)})\n",
    "\n",
    "for k, v in pyramid.items():\n",
    "    print(k, v.shape)\n",
    "```\n",
    "\n",
    "✅ You get a dictionary like:\n",
    "\n",
    "```\n",
    "c0 torch.Size([1, 256, 56, 56])\n",
    "c1 torch.Size([1, 256, 28, 28])\n",
    "c2 torch.Size([1, 256, 14, 14])\n",
    "c3 torch.Size([1, 256, 7, 7])\n",
    "```\n",
    "\n",
    "Perfect for plugging into a detector or segmenter.\n",
    "\n",
    "---\n",
    "\n",
    "# **9. In summary**\n",
    "\n",
    "✅ FPN produces **P1–P4**, a multi-scale, uniform set of features.\n",
    "✅ Each PVT output (C_i) contributes to one pyramid level.\n",
    "✅ What happens next depends on your task:\n",
    "\n",
    "| Task               | How you use FPN outputs                               |\n",
    "| :----------------- | :---------------------------------------------------- |\n",
    "| **Detection**      | One prediction head per level (boxes + classes)       |\n",
    "| **Segmentation**   | Upsample + fuse all levels for dense pixel prediction |\n",
    "| **Depth / Flow**   | Same as segmentation but with regression              |\n",
    "| **Classification** | Pool highest-level P4 feature                         |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **minimal end-to-end example** (in code) of using PVT + FPN + RetinaNet-style detection head on dummy input, just to see how each pyramid level produces predictions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
