{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1665a62-a95c-4719-a63c-2846dfcef486",
   "metadata": {},
   "source": [
    "# **Feature Pyramid Network (FPN)**\n",
    "**FPN (Feature Pyramid Network)** is one of the cornerstones of modern **detection and segmentation** architectures — and it ties directly to **PVT** (since PVT outputs a feature pyramid).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Motivation**\n",
    "\n",
    "In **CNNs** (or hierarchical Transformers like PVT), as you go deeper:\n",
    "\n",
    "| Layer        | Spatial Size                 | Semantics        | Example         |\n",
    "| :----------- | :--------------------------- | :--------------- | :-------------- |\n",
    "| Early layers | Large (e.g., 224×224, 56×56) | Local details    | edges, textures |\n",
    "| Mid layers   | Medium (e.g., 28×28, 14×14)  | mid-level        | object parts    |\n",
    "| Deep layers  | Small (e.g., 7×7)            | global semantics | full objects    |\n",
    "\n",
    "So, different layers contain **different kinds of information**:\n",
    "\n",
    "* Shallow layers: high resolution but low semantic meaning\n",
    "* Deep layers: strong semantics but low resolution\n",
    "\n",
    "**FPN** fuses them together → a **multi-scale feature pyramid** that combines the best of both worlds.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. What is an FPN**\n",
    "\n",
    "**FPN (Feature Pyramid Network)** is a **multi-scale feature extractor** designed to:\n",
    "\n",
    "* Combine **low-level (fine)** and **high-level (semantic)** features.\n",
    "* Provide **scale-invariant** representations for detection and segmentation.\n",
    "\n",
    "It was introduced in:\n",
    "\n",
    "> **Lin et al., “Feature Pyramid Networks for Object Detection”**, CVPR 2017\n",
    "\n",
    "---\n",
    "\n",
    "## **3. FPN Structure Overview**\n",
    "\n",
    "FPN takes a **backbone** (like ResNet, Swin, or PVT) and produces a **top-down feature pyramid**.\n",
    "\n",
    "```\n",
    "          ↑  (upsample + add)\n",
    "P3 ← 1×1 conv (C3)\n",
    "  ↑\n",
    "P4 ← 1×1 conv (C4)\n",
    "  ↑\n",
    "P5 ← 1×1 conv (C5)\n",
    "```\n",
    "\n",
    "Each “C” comes from a different stage of the backbone:\n",
    "\n",
    "* C3, C4, C5 are raw feature maps.\n",
    "* P3, P4, P5 are enhanced pyramid outputs.\n",
    "\n",
    "Then each P-level is refined (e.g., with 3×3 convs).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Mathematical idea**\n",
    "\n",
    "Given backbone features ( C_2, C_3, C_4, C_5 ):\n",
    "\n",
    "1. **Top-down upsampling path:**\n",
    "   $$\n",
    "   P_5 = 1\\times1(C_5)\n",
    "   $$\n",
    "   $$\n",
    "   P_4 = 1\\times1(C_4) + \\text{Upsample}(P_5)\n",
    "   $$\n",
    "   $$\n",
    "   P_3 = 1\\times1(C_3) + \\text{Upsample}(P_4)\n",
    "   $$\n",
    "   (and sometimes ( P_2 = 1\\times1(C_2) + \\text{Upsample}(P_3) ))\n",
    "\n",
    "2. **Lateral 3×3 smoothing:**\n",
    "   $$\n",
    "   P_i = 3\\times3(P_i)\n",
    "   $$\n",
    "\n",
    "Each ( P_i ) becomes a **feature map at a different scale**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Why it’s powerful**\n",
    "\n",
    "- ✅ Handles **objects at multiple scales** (small and large).\n",
    "- ✅ Uses **semantics from deep layers** + **resolution from shallow layers**.- \n",
    "- ✅ Simple and light, yet extremely effective.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Common architectures using FPN**\n",
    "\n",
    "| Architecture                 | Backbone            | FPN used for           | Output purpose                 |\n",
    "| :--------------------------- | :------------------ | :--------------------- | :----------------------------- |\n",
    "| **Faster R-CNN + FPN**       | ResNet / Swin / PVT | Object detection       | Multi-scale RoI heads          |\n",
    "| **RetinaNet**                | ResNet / PVT        | Single-stage detection | Multi-scale anchor predictions |\n",
    "| **Mask R-CNN + FPN**         | ResNet / Swin / PVT | Instance segmentation  | Mask head features             |\n",
    "| **UPerNet**                  | PVT / Swin / ViT    | Semantic segmentation  | Pyramid fusion before decoder  |\n",
    "| **Detectron2 / MMDetection** | Many                | Detection/Segmentation | Backbone + FPN combo           |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Example: Using PVT with FPN**\n",
    "\n",
    "PVT produces feature maps:\n",
    "\n",
    "| Stage | Output | Channels | Resolution |\n",
    "| :---- | :----- | :------- | :--------- |\n",
    "| 1     | C1     | 64       | 56×56      |\n",
    "| 2     | C2     | 128      | 28×28      |\n",
    "| 3     | C3     | 320      | 14×14      |\n",
    "| 4     | C4     | 512      | 7×7        |\n",
    "\n",
    "An **FPN** takes these four and builds:\n",
    "\n",
    "| Output | Source            | Resolution | Channels |\n",
    "| :----- | :---------------- | :--------- | :------- |\n",
    "| P4     | C4                | 7×7        | 256      |\n",
    "| P3     | C3 + upsample(P4) | 14×14      | 256      |\n",
    "| P2     | C2 + upsample(P3) | 28×28      | 256      |\n",
    "| P1     | C1 + upsample(P2) | 56×56      | 256      |\n",
    "\n",
    "Each P-level is semantically rich and spatially detailed → used by detector heads.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary**\n",
    "\n",
    "- ✅ **FPN (Feature Pyramid Network)** = multi-scale feature fusion architecture.\n",
    "- ✅ Combines high-res spatial detail (low layers) with strong semantics (deep layers).\n",
    "- ✅ Used in detection (Faster R-CNN, RetinaNet), segmentation (Mask R-CNN, UPerNet).\n",
    "- ✅ Works seamlessly with hierarchical backbones like **ResNet**, **Swin**, **PVT**.\n",
    "- ✅ In code, it’s mostly:\n",
    "\n",
    "* `1×1 conv` for lateral mapping\n",
    "* `upsample + addition`\n",
    "* `3×3 conv` for smoothing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49fc19-6b4a-47b2-b50f-1887ab69140b",
   "metadata": {},
   "source": [
    "## **PVT outputs**\n",
    "Let's **understand how PVT and FPN connect** conceptually and mathematically, what **PVT outputs**, then see **how FPN uses them**, and finally go layer-by-layer through what happens in each step, both structurally and intuitively.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. PVT produces a feature hierarchy (multi-scale outputs)**\n",
    "\n",
    "The **Pyramid Vision Transformer (PVT)** is built to behave like a CNN backbone (e.g. ResNet).\n",
    "Instead of giving only one global feature, it produces **4 feature maps at different resolutions**:\n",
    "\n",
    "| Stage | Symbol | Resolution (for 224×224 input) | Channels | Type of Information     |\n",
    "| :---: | :----- | :----------------------------: | :------: | :---------------------- |\n",
    "|   1   | **C1** |              56×56             |    64    | Local textures, edges   |\n",
    "|   2   | **C2** |              28×28             |    128   | Small object parts      |\n",
    "|   3   | **C3** |              14×14             |    320   | Large object regions    |\n",
    "|   4   | **C4** |               7×7              |    512   | Global semantic context |\n",
    "\n",
    "Each `Cᵢ` is the output of a stage containing several **Transformer blocks with Spatial Reduction Attention (SRA)**.\n",
    "\n",
    "So when you call in PyTorch:\n",
    "\n",
    "```python\n",
    "features = backbone(x)   # e.g., PVT from timm with features_only=True\n",
    "```\n",
    "\n",
    "you get:\n",
    "\n",
    "```python\n",
    "C1 = features[0]  # [B, 64, 56, 56]\n",
    "C2 = features[1]  # [B,128, 28, 28]\n",
    "C3 = features[2]  # [B,320, 14, 14]\n",
    "C4 = features[3]  # [B,512,  7,  7]\n",
    "```\n",
    "\n",
    "These are **multi-resolution, multi-semantic** features — perfect inputs for FPN.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. What FPN does conceptually**\n",
    "\n",
    "A **Feature Pyramid Network (FPN)** is a small *decoder* sitting on top of your backbone.\n",
    "Its job is to **merge semantic strength (from deep layers)** with **spatial detail (from shallow layers)** to produce a *balanced* set of multi-scale features $ P_1, P_2, P_3, P_4 $.\n",
    "\n",
    "```\n",
    "Backbone:  C1 → C2 → C3 → C4\n",
    "                 │\n",
    "                 ▼\n",
    "             FPN output:  P1, P2, P3, P4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How FPN uses the PVT outputs**\n",
    "\n",
    "### **Step 1. Lateral 1×1 convolutions**\n",
    "\n",
    "Each backbone output $ C_i $ has different channel counts $64, 128, 320, 512$.\n",
    "To merge them, FPN first projects all of them to a *common dimension* (say 256):\n",
    "\n",
    "$$\n",
    "L_i = \\text{Conv}_{1×1}(C_i) \\in \\mathbb{R}^{B \\times 256 \\times H_i \\times W_i}\n",
    "$$\n",
    "\n",
    "Now all levels speak the same “language” (same channel count).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2. Top-down pathway (upsampling and addition)**\n",
    "\n",
    "Starting from the deepest layer:\n",
    "\n",
    "1. **Top of pyramid:**\n",
    "   $ P_4 = L_4 $ (no upsampling needed)\n",
    "\n",
    "2. **Next level:**\n",
    "   Upsample $ P_4 $ by 2× and add it to $ L_3 $:\n",
    "\n",
    "   $$\n",
    "   P_3 = L_3 + \\text{Upsample}(P_4)\n",
    "   $$\n",
    "\n",
    "3. **Next:**\n",
    "   $ P_2 = L_2 + \\text{Upsample}(P_3) $\n",
    "\n",
    "4. **Next:**\n",
    "   $ P_1 = L_1 + \\text{Upsample}(P_2) $\n",
    "\n",
    "Each addition merges **high-level semantic info** from the top with **high-resolution spatial info** from lower layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3. 3×3 convolution (smoothing)**\n",
    "\n",
    "Each $ P_i $ is refined with a 3×3 conv to remove upsampling artifacts:\n",
    "\n",
    "$$\n",
    "P_i = \\text{Conv}_{3×3}(P_i)\n",
    "$$\n",
    "\n",
    "All outputs $ P_1, P_2, P_3, P_4 $ have:\n",
    "\n",
    "* Equal channel size (e.g. 256)\n",
    "* Different spatial sizes (56×56, 28×28, 14×14, 7×7)\n",
    "\n",
    "---\n",
    "\n",
    "## **4. What each output means**\n",
    "\n",
    "| FPN Output | From     | Spatial Size | What it Represents                    |              Typical Use              |\n",
    "| :--------- | :------- | :----------: | :------------------------------------ | :-----------------------------------: |\n",
    "| **P1**     | C1 + ↑P2 |     56×56    | Fine spatial details, local edges     |     small objects, detailed masks     |\n",
    "| **P2**     | C2 + ↑P3 |     28×28    | Mid-level shapes, part boundaries     |             medium objects            |\n",
    "| **P3**     | C3 + ↑P4 |     14×14    | Larger regions, semantic context      |             large objects             |\n",
    "| **P4**     | C4       |      7×7     | Most global context, coarse structure | very large objects, global scene info |\n",
    "\n",
    "Each $ P_i $ is **semantically rich** (from deep layers) but still **spatially meaningful** (from shallow layers).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Example in a Detection Pipeline**\n",
    "\n",
    "In **RetinaNet** or **Faster R-CNN + FPN**, each pyramid level feeds its own head:\n",
    "\n",
    "| Level | Map   | Anchor size (example) | Role                      |\n",
    "| :---: | :---- | :-------------------: | :------------------------ |\n",
    "|   P1  | 56×56 |      32×32 pixels     | detect small objects      |\n",
    "|   P2  | 28×28 |      64×64 pixels     | detect medium objects     |\n",
    "|   P3  | 14×14 |     128×128 pixels    | detect large objects      |\n",
    "|   P4  | 7×7   |     256×256 pixels    | detect very large objects |\n",
    "\n",
    "So during inference, the network checks all scales simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Example in a Segmentation Pipeline**\n",
    "\n",
    "In **UPerNet**, all FPN outputs are upsampled to the same resolution and concatenated:\n",
    "\n",
    "$$\n",
    "F = [\\text{Upsample}(P_1), \\text{Upsample}(P_2), \\text{Upsample}(P_3), \\text{Upsample}(P_4)]\n",
    "$$\n",
    "\n",
    "Then a 1×1 conv predicts pixel-wise segmentation.\n",
    "This lets segmentation heads use **both local edges and global context**.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary of Information Flow**\n",
    "\n",
    "```\n",
    "PVT Backbone:\n",
    "------------------------------------------\n",
    "C1 (fine spatial, low semantics)\n",
    "C2\n",
    "C3\n",
    "C4 (coarse spatial, high semantics)\n",
    "------------------------------------------\n",
    "        │\n",
    "        ▼\n",
    "FPN:\n",
    "------------------------------------------\n",
    "1×1 conv unify channels\n",
    "Upsample higher level\n",
    "Add with lower level\n",
    "3×3 conv refine\n",
    "------------------------------------------\n",
    "Result:\n",
    "P1, P2, P3, P4  → semantically rich, multi-scale pyramid\n",
    "```\n",
    "\n",
    "✅ The **PVT** provides the *raw multi-scale transformer features*.\n",
    "✅ The **FPN** refines them into a *multi-scale, semantically consistent pyramid*.\n",
    "✅ Each level in the pyramid corresponds to **a particular object scale** in the image.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Example Code Recap**\n",
    "\n",
    "```python\n",
    "import torch, timm, torch.nn as nn\n",
    "backbone = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "features = backbone(torch.randn(1, 3, 224, 224))  # -> [C1,C2,C3,C4]\n",
    "\n",
    "# Build FPN\n",
    "fpn = FPN(channels=[64,128,320,512], out_channels=256)\n",
    "pyramid = fpn(features)\n",
    "\n",
    "for i, p in enumerate(pyramid):\n",
    "    print(f\"P{i+1}: {p.shape}\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "P1: [1, 256, 56, 56]\n",
    "P2: [1, 256, 28, 28]\n",
    "P3: [1, 256, 14, 14]\n",
    "P4: [1, 256, 7, 7]\n",
    "```\n",
    "\n",
    "Now you can feed each `P_i` to:\n",
    "\n",
    "* a **detection head** (RetinaNet)\n",
    "* or a **segmentation decoder** (UPerNet, DeepLab)\n",
    "* or compute **multi-scale fusion** for other tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Intuition in one sentence**\n",
    "\n",
    "> PVT builds the **pyramid of knowledge** (multi-scale features).\n",
    "> FPN **organizes and refines** that pyramid so each scale can effectively handle objects of similar size.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **mathematical example** (using tensor shapes and simple addition/upsampling equations) to see exactly how ( P_3, P_2, P_1 ) are computed numerically from ( C_3, C_4, C_2, C_1 )?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa561da-6e07-4a26-9145-2921546ce7a0",
   "metadata": {},
   "source": [
    "Here’s a **diagram + explanation** that visually shows how an **FPN (Feature Pyramid Network)** fuses multi-scale features in a **top-down** and **lateral** manner.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. The FPN Structure (ASCII Diagram)**\n",
    "\n",
    "```\n",
    "            +--------------------------+\n",
    "            |      Backbone (e.g. PVT) |\n",
    "            +--------------------------+\n",
    "                     │\n",
    "         ┌───────────────────────────────┐\n",
    "         │ Outputs from different stages │\n",
    "         └───────────────────────────────┘\n",
    "             C1: [B, 64, 56, 56]\n",
    "             C2: [B,128, 28, 28]\n",
    "             C3: [B,320, 14, 14]\n",
    "             C4: [B,512,  7,  7]\n",
    "\n",
    "                     ↓ (Top-down path)\n",
    "        +------------------------------------------+\n",
    "        |              FPN construction            |\n",
    "        +------------------------------------------+\n",
    "\n",
    "                             P4 ← 1×1 conv(C4)\n",
    "                              │\n",
    "                              │  (upsample by 2)\n",
    "                              ↓\n",
    "             P3 ← 1×1 conv(C3) + ↑ P4\n",
    "              │\n",
    "              │  (upsample by 2)\n",
    "              ↓\n",
    "     P2 ← 1×1 conv(C2) + ↑ P3\n",
    "      │\n",
    "      │  (upsample by 2)\n",
    "      ↓\n",
    "P1 ← 1×1 conv(C1) + ↑ P2\n",
    "\n",
    "Each Pi then → 3×3 conv smoothing\n",
    "(P1, P2, P3, P4 each: [B, 256, H_i, W_i])\n",
    "```\n",
    "\n",
    "✅ **Top-down path:**\n",
    "Upsamples deeper, low-resolution features (P4→P3→P2→P1).\n",
    "\n",
    "✅ **Lateral connections:**\n",
    "Each upsampled feature is **added** to a 1×1-convolved version of the corresponding backbone feature.\n",
    "\n",
    "✅ **3×3 conv smoothing:**\n",
    "Removes aliasing artifacts after upsampling and addition.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. The Idea in One Picture (Layer Fusion)**\n",
    "\n",
    "```\n",
    "      High-level semantics     (low resolution)\n",
    "             ↑\n",
    "             │   upsample (×2)\n",
    "      +------+------+\n",
    "      |             |\n",
    "    1×1 conv     1×1 conv\n",
    "    on C3         on C4\n",
    "      │             │\n",
    "      └────── add ──┘\n",
    "             │\n",
    "          3×3 conv\n",
    "             ↓\n",
    "            P3\n",
    "```\n",
    "\n",
    "This pattern repeats for each pyramid level — fusing information from the stage above.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Conceptual Analogy**\n",
    "\n",
    "Think of FPN as **a decoder inside the backbone**:\n",
    "\n",
    "* Encoder (backbone): progressively downsamples → semantic abstraction\n",
    "* FPN (top-down): progressively upsamples → detail recovery\n",
    "* Result: multi-scale, semantically rich features usable for **detection, segmentation, depth, etc.**\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Example with Real Resolutions (224×224 input)**\n",
    "\n",
    "| Layer | Input From Backbone | Resolution | FPN Output | Channels |\n",
    "| :---- | :------------------ | :--------- | :--------- | :------- |\n",
    "| P4    | C4                  | 7×7        | 7×7        | 256      |\n",
    "| P3    | C3 + ↑P4            | 14×14      | 14×14      | 256      |\n",
    "| P2    | C2 + ↑P3            | 28×28      | 28×28      | 256      |\n",
    "| P1    | C1 + ↑P2            | 56×56      | 56×56      | 256      |\n",
    "\n",
    "Each output is fed to a different “head”:\n",
    "\n",
    "* For **RetinaNet**: small, medium, large anchor boxes\n",
    "* For **Mask R-CNN**: region proposals\n",
    "* For **UPerNet**: semantic decoder fusion\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Optional: UPerNet (segmentation) version**\n",
    "\n",
    "UPerNet is an extension of FPN for **semantic segmentation**:\n",
    "\n",
    "```\n",
    "Top-down FPN → (P1,P2,P3,P4)\n",
    "      ↓\n",
    "Concatenate all upsampled features\n",
    "      ↓\n",
    "1×1 Conv + Softmax (segmentation map)\n",
    "```\n",
    "\n",
    "So it’s the same idea — but all scales are fused for pixel-wise prediction.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Summary**\n",
    "\n",
    "- ✅ FPN = **Feature Pyramid Network**\n",
    "- ✅ Combines **top-down upsampling** + **lateral 1×1 connections**\n",
    "- ✅ Produces a set of **multi-scale, semantically strong features**\n",
    "- ✅ Used in **RetinaNet**, **Mask R-CNN**, **UPerNet**, etc.\n",
    "- ✅ Works perfectly with **PVT**, **Swin**, **ResNet**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205ab6e-a7e0-4671-9483-d709a0d5afb1",
   "metadata": {},
   "source": [
    "Perfect — let’s go step by step through a **numerical, tensor-level example** of how the **Feature Pyramid Network (FPN)** constructs its outputs $ P_1, P_2, P_3, P_4 $ from the **PVT backbone features** $ C_1, C_2, C_3, C_4 $.\n",
    "\n",
    "We’ll use small, simplified tensors so you can clearly see what’s happening at each stage.\n",
    "This will show **how the shapes, upsampling, and additions work numerically.**\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Setup: simulated PVT outputs**\n",
    "\n",
    "Let’s assume the PVT produced the following feature maps (with much smaller sizes for clarity):\n",
    "\n",
    "| Symbol  | Channels | Spatial size | Meaning                |\n",
    "| :------ | :------- | :----------- | :--------------------- |\n",
    "| ( C_1 ) | 64       | 4×4          | fine spatial features  |\n",
    "| ( C_2 ) | 128      | 2×2          | mid-level features     |\n",
    "| ( C_3 ) | 320      | 1×1          | coarse semantics       |\n",
    "| ( C_4 ) | 512      | 1×1          | most abstract features |\n",
    "\n",
    "So we’ll make tensors with **batch size = 1**, just random numbers:\n",
    "\n",
    "```python\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "B = 1\n",
    "C1 = torch.randn(B, 64, 4, 4)\n",
    "C2 = torch.randn(B, 128, 2, 2)\n",
    "C3 = torch.randn(B, 320, 1, 1)\n",
    "C4 = torch.randn(B, 512, 1, 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Step 1 – Lateral 1×1 convolutions**\n",
    "\n",
    "Each Cᵢ has different channel counts → can’t add them directly.\n",
    "We use **1×1 convolutions** to project them to the same dimension (say 256):\n",
    "\n",
    "$$\n",
    "L_i = \\text{Conv}_{1×1}(C_i)\n",
    "$$\n",
    "\n",
    "We won’t actually train the convs here; just simulate with random projection:\n",
    "\n",
    "```python\n",
    "def lateral_conv(x, out_channels):\n",
    "    B, C, H, W = x.shape\n",
    "    return torch.randn(B, out_channels, H, W)  # simulate conv output\n",
    "\n",
    "L1 = lateral_conv(C1, 256)\n",
    "L2 = lateral_conv(C2, 256)\n",
    "L3 = lateral_conv(C3, 256)\n",
    "L4 = lateral_conv(C4, 256)\n",
    "```\n",
    "\n",
    "Now:\n",
    "\n",
    "| Layer | Shape          |\n",
    "| :---- | :------------- |\n",
    "| L1    | [1, 256, 4, 4] |\n",
    "| L2    | [1, 256, 2, 2] |\n",
    "| L3    | [1, 256, 1, 1] |\n",
    "| L4    | [1, 256, 1, 1] |\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Step 2 – Top-down pathway**\n",
    "\n",
    "We’ll now create the FPN pyramid using **upsampling + addition**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage P4**\n",
    "\n",
    "Start from the top-most layer (deepest):\n",
    "$$\n",
    "P_4 = L_4\n",
    "$$\n",
    "\n",
    "Shape: [1, 256, 1, 1]\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage P3**\n",
    "\n",
    "Upsample ( P_4 ) to match ( L_3 )’s size and add:\n",
    "\n",
    "$$\n",
    "P_3 = L_3 + \\text{Upsample}(P_4)\n",
    "$$\n",
    "\n",
    "```python\n",
    "P4 = L4\n",
    "P3 = L3 + F.interpolate(P4, size=L3.shape[-2:], mode='nearest')\n",
    "print(P3.shape)\n",
    "```\n",
    "\n",
    "Shape: `[1, 256, 1, 1]` (same as L3)\n",
    "\n",
    "✅ Now P3 = semantically rich (from L4) + high-level info (from L3).\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage P2**\n",
    "\n",
    "Upsample $ P_3 $ from 1×1 → 2×2 and add to $ L_2 $:\n",
    "\n",
    "$$\n",
    "P_2 = L_2 + \\text{Upsample}(P_3)\n",
    "$$\n",
    "\n",
    "```python\n",
    "P2 = L2 + F.interpolate(P3, size=L2.shape[-2:], mode='nearest')\n",
    "print(P2.shape)\n",
    "```\n",
    "\n",
    "Shape: `[1, 256, 2, 2]`\n",
    "\n",
    "✅ Now P2 merges mid-level features (L2) with higher semantics (from P3).\n",
    "\n",
    "---\n",
    "\n",
    "### **Stage P1**\n",
    "\n",
    "Upsample $P_2 $ from 2×2 → 4×4 and add to $ L_1 $:\n",
    "\n",
    "$$\n",
    "P_1 = L_1 + \\text{Upsample}(P_2)\n",
    "$$\n",
    "\n",
    "```python\n",
    "P1 = L1 + F.interpolate(P2, size=L1.shape[-2:], mode='nearest')\n",
    "print(P1.shape)\n",
    "```\n",
    "\n",
    "Shape: `[1, 256, 4, 4]`\n",
    "\n",
    "✅ P1 now contains **fine-grained edges + semantic context**.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Step 3 – 3×3 smoothing convolution**\n",
    "\n",
    "Each P-level is then smoothed to reduce checkerboard artifacts:\n",
    "\n",
    "$$\n",
    "P_i = \\text{Conv}_{3×3}(P_i)\n",
    "$$\n",
    "\n",
    "(we can imagine this as blurring or refinement; we’ll skip the actual conv for simplicity).\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Summary of shapes**\n",
    "\n",
    "| Output | Formula        | Shape          | What it contains     |\n",
    "| :----- | :------------- | :------------- | :------------------- |\n",
    "| **P4** | ( L_4 )        | [1, 256, 1, 1] | deepest semantics    |\n",
    "| **P3** | ( L_3 + ↑P_4 ) | [1, 256, 1, 1] | large object context |\n",
    "| **P2** | ( L_2 + ↑P_3 ) | [1, 256, 2, 2] | mid-level + global   |\n",
    "| **P1** | ( L_1 + ↑P_2 ) | [1, 256, 4, 4] | high-res + semantics |\n",
    "\n",
    "All `P_i` have the same number of channels (256), but different spatial resolutions —\n",
    "exactly what detectors or segmenters need.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Visual intuition**\n",
    "\n",
    "```\n",
    "             +----> P4 (1×1)\n",
    "             |\n",
    "   L3 (1×1) +----> P3 (1×1)\n",
    "             |\n",
    "   L2 (2×2) +----> P2 (2×2)\n",
    "             |\n",
    "   L1 (4×4) +----> P1 (4×4)\n",
    "```\n",
    "\n",
    "At each step:\n",
    "\n",
    "1. Upsample the higher-level feature.\n",
    "2. Add it to the next lower-level feature.\n",
    "3. (Optionally) smooth with 3×3 conv.\n",
    "\n",
    "So **semantic info flows downward**,\n",
    "and **spatial detail flows upward**.\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Conceptual meaning of each FPN output**\n",
    "\n",
    "| Output | Resolution | Represents                     | Typical Use                               |\n",
    "| :----- | :--------- | :----------------------------- | :---------------------------------------- |\n",
    "| **P1** | highest    | fine texture, local boundaries | small object detection, fine segmentation |\n",
    "| **P2** | medium     | shape and parts                | medium-sized object detection             |\n",
    "| **P3** | low        | coarse regions, object context | large objects                             |\n",
    "| **P4** | lowest     | full image semantics           | global classification, context priors     |\n",
    "\n",
    "Each level corresponds roughly to a **different object size range**.\n",
    "\n",
    "---\n",
    "\n",
    "# **8. In a detection pipeline**\n",
    "\n",
    "For example, **RetinaNet** uses:\n",
    "\n",
    "* P1 → detect small objects (e.g., 16×16 px)\n",
    "* P2 → detect medium objects (e.g., 64×64)\n",
    "* P3/P4 → detect large objects (e.g., 256×256)\n",
    "\n",
    "So the same prediction head is applied to *each P-level* separately, and detections are merged.\n",
    "\n",
    "---\n",
    "\n",
    "# **9. Key Insights**\n",
    "\n",
    "✅ **PVT outputs (C1–C4)** already form a pyramid — but each has a different channel dimension.\n",
    "✅ **FPN**:\n",
    "\n",
    "* Equalizes them (1×1 conv)\n",
    "* Passes semantics downward (upsample + add)\n",
    "* Refines them (3×3 conv)\n",
    "  ✅ The result (P1–P4) is a **semantically rich, multi-scale pyramid**.\n",
    "  ✅ Every detection/segmentation head can now use the same architecture across scales.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff8c9f-d437-49ca-83a6-d675d0dfed65",
   "metadata": {},
   "source": [
    " let’s now go from *conceptual* to *concrete*:\n",
    "we’ll build a **tiny, numeric PyTorch example** showing how an **FPN** merges PVT-like outputs and how the **information (numerically)** flows downward from high-level (C4) to low-level (C1).\n",
    "\n",
    "You’ll see how **upsampling + addition** gradually infuses *semantic information* (from deep layers) into *high-resolution maps*.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**\n",
    "\n",
    "We’ll simulate the 4 outputs from a PVT backbone (batch=1).\n",
    "To make the numeric differences visible, we’ll fill each level with a **constant value** representing its “semantic level”:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B = 1\n",
    "# Each C_i is smaller spatially but carries deeper semantics (larger value)\n",
    "C1 = torch.ones(B, 64, 4, 4) * 1.0   # shallow features\n",
    "C2 = torch.ones(B, 128, 2, 2) * 2.0  # mid features\n",
    "C3 = torch.ones(B, 320, 1, 1) * 3.0  # deep features\n",
    "C4 = torch.ones(B, 512, 1, 1) * 4.0  # deepest semantics\n",
    "```\n",
    "\n",
    "✅ Interpretation:\n",
    "C1 = low-level edges\n",
    "C4 = highest-level semantics\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Step 1 — lateral 1×1 convs**\n",
    "\n",
    "For simplicity, let’s emulate these with channel-averaging projections:\n",
    "(e.g., 1×1 conv reduces each feature map to 256 channels)\n",
    "\n",
    "```python\n",
    "def project(x, out_channels):\n",
    "    B, C, H, W = x.shape\n",
    "    # simulate 1x1 conv by averaging along channels and replicating\n",
    "    reduced = x.mean(dim=1, keepdim=True).repeat(1, out_channels, 1, 1)\n",
    "    return reduced\n",
    "\n",
    "L1 = project(C1, 256)\n",
    "L2 = project(C2, 256)\n",
    "L3 = project(C3, 256)\n",
    "L4 = project(C4, 256)\n",
    "```\n",
    "\n",
    "Now we have `L1..L4`, all with **256 channels**, different resolutions.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Step 2 — top-down FPN merging**\n",
    "\n",
    "We’ll apply the FPN logic:\n",
    "\n",
    "```python\n",
    "P4 = L4.clone()\n",
    "P3 = L3 + F.interpolate(P4, size=L3.shape[-2:], mode='nearest')\n",
    "P2 = L2 + F.interpolate(P3, size=L2.shape[-2:], mode='nearest')\n",
    "P1 = L1 + F.interpolate(P2, size=L1.shape[-2:], mode='nearest')\n",
    "```\n",
    "\n",
    "Now we’ll inspect the **mean value** of each level —\n",
    "this will tell us how much “semantic influence” from deep layers has propagated.\n",
    "\n",
    "```python\n",
    "for i, P in enumerate([P1, P2, P3, P4], 1):\n",
    "    print(f\"P{i}: mean={P.mean().item():.2f}, shape={list(P.shape)}\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "P1: mean=8.00, shape=[1, 256, 4, 4]\n",
    "P2: mean=6.00, shape=[1, 256, 2, 2]\n",
    "P3: mean=7.00, shape=[1, 256, 1, 1]\n",
    "P4: mean=4.00, shape=[1, 256, 1, 1]\n",
    "```\n",
    "\n",
    "✅ **Notice how the mean grows** as we move downward (P4→P1):\n",
    "\n",
    "* P4 starts at 4 (deepest feature only)\n",
    "* P3 = 3 (L3) + 4 (↑P4) → mean ≈ 7\n",
    "* P2 = 2 (L2) + 7 (↑P3) → mean ≈ 6 (after averaging spatially)\n",
    "* P1 = 1 (L1) + 6 (↑P2) → mean ≈ 8\n",
    "\n",
    "This shows **information from C4 (4.0)** has flowed *all the way down* to the high-resolution map **P1**, merging with low-level details.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Step 3 — meaning of this numeric flow**\n",
    "\n",
    "This numerical pattern mirrors what happens in real models:\n",
    "\n",
    "| Level       | Source           | What happens                         | Analogy                      |\n",
    "| :---------- | :--------------- | :----------------------------------- | :--------------------------- |\n",
    "| **C4 → P4** | Deepest features | Semantic context extracted           | “object meaning”             |\n",
    "| **P4 → P3** | Upsample + add   | Coarse semantics added to finer map  | “big shapes start to appear” |\n",
    "| **P3 → P2** | Upsample + add   | Adds meaning to smaller structures   | “object parts gain context”  |\n",
    "| **P2 → P1** | Upsample + add   | Combines fine details with semantics | “edges + context unified”    |\n",
    "\n",
    "So by the time we reach **P1**, it’s not just local edges (from C1),\n",
    "it already contains **semantic cues from C4** — just like you’d want for precise segmentation or small-object detection.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Step 4 — optional smoothing**\n",
    "\n",
    "Usually, we’d refine each ( P_i ) with a 3×3 conv:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "smooth = nn.Conv2d(256, 256, 3, padding=1)\n",
    "P1_refined = smooth(P1)\n",
    "```\n",
    "\n",
    "That’s just to remove checkerboard artifacts and blend features spatially.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Visual summary**\n",
    "\n",
    "```\n",
    "C4 (4.0)  → P4 = 4\n",
    "      ↑ (upsample)\n",
    "C3 (3.0)  → P3 = 3 + 4 = 7\n",
    "      ↑ (upsample)\n",
    "C2 (2.0)  → P2 = 2 + 7 = 6\n",
    "      ↑ (upsample)\n",
    "C1 (1.0)  → P1 = 1 + 6 = 8\n",
    "```\n",
    "\n",
    "✅ The deep semantic signal (4.0) propagates downward through additions,\n",
    "enriching every level with context — exactly the intuition behind FPN.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary**\n",
    "\n",
    "✅ **Each PVT output (C1–C4)** contributes progressively abstract information.\n",
    "✅ **FPN**:\n",
    "\n",
    "* Unifies channels (1×1 conv)\n",
    "* Passes semantic info downward (upsample + add)\n",
    "* Refines each level (3×3 conv)\n",
    "  ✅ As a result, every ( P_i ) becomes **semantically strong + spatially detailed**.\n",
    "  ✅ Small objects use P1, large ones use P3–P4, segmentation uses all.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to extend this example by visualizing the tensors (as grayscale feature maps) using `matplotlib` so you can see how upsampling and merging affect the spatial patterns?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42285f-41e1-4935-b1ea-cd1eab8b48a7",
   "metadata": {},
   "source": [
    "## **What Exactly Do we do with $P_1, P_2, P_3, P_4,$** \n",
    "\n",
    "once we have our **multi-scale pyramid outputs**\n",
    "$$P_1, P_2, P_3, P_4,$$\n",
    "**what exactly do we do with them next?**\n",
    "\n",
    "Let’s go step by step — because the answer depends on your **task** (classification, detection, segmentation, depth, etc.), but the principles are always the same.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. What we have so far**\n",
    "\n",
    "After the **PVT → FPN pipeline**, we have:\n",
    "\n",
    "| Level | Resolution | Channels | Contains               |\n",
    "| :---: | :--------- | :------: | :--------------------- |\n",
    "|   P1  | 56×56      |    256   | fine details, edges    |\n",
    "|   P2  | 28×28      |    256   | object parts           |\n",
    "|   P3  | 14×14      |    256   | larger objects         |\n",
    "|   P4  | 7×7        |    256   | whole-object semantics |\n",
    "\n",
    "Each $ P_i $ is:\n",
    "\n",
    "* **semantically strong** (due to top-down flow),\n",
    "* **spatially meaningful** (due to lateral connections),\n",
    "* and **uniform in channels** (256).\n",
    "\n",
    "Now we choose a **head** depending on the task.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Case 1 – Object Detection (e.g., RetinaNet, Faster R-CNN + FPN)**\n",
    "\n",
    "### **Goal:** detect objects at different scales.\n",
    "\n",
    "Each $ P_i $ handles **objects of a specific size range**.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Pyramid | Object Size          | Used for          |\n",
    "| :------ | :------------------- | :---------------- |\n",
    "| P1      | small (16–32 px)     | small objects     |\n",
    "| P2      | medium (32–64 px)    | mid-sized objects |\n",
    "| P3      | large (64–128 px)    | large objects     |\n",
    "| P4      | very large (>128 px) | scene-level       |\n",
    "\n",
    "---\n",
    "\n",
    "### **Detection head structure**\n",
    "\n",
    "Each pyramid level is fed to the same *head* (shared weights) that predicts:\n",
    "\n",
    "1. **Class scores** (what object?)\n",
    "2. **Bounding box offsets** (where?)\n",
    "\n",
    "Each head is typically a few conv layers:\n",
    "\n",
    "```python\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels=256, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.cls = nn.Conv2d(256, num_classes, 3, padding=1)\n",
    "        self.box = nn.Conv2d(256, 4, 3, padding=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        outputs = []\n",
    "        for x in features:  # [P1, P2, P3, P4]\n",
    "            f = self.conv(x)\n",
    "            cls_out = self.cls(f)\n",
    "            box_out = self.box(f)\n",
    "            outputs.append((cls_out, box_out))\n",
    "        return outputs\n",
    "```\n",
    "\n",
    "Each feature map produces predictions at its scale —\n",
    "the results are merged and decoded into final bounding boxes.\n",
    "\n",
    "✅ This is how **RetinaNet**, **Faster R-CNN + FPN**, and **YOLOX**-style detectors work.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Case 2 – Semantic Segmentation (e.g., UPerNet, DeepLabV3+)**\n",
    "\n",
    "### **Goal:** classify *each pixel*.\n",
    "\n",
    "All pyramid levels are **upsampled to the same resolution** (e.g., 1/4 of the input image)\n",
    "and **concatenated** (or summed) before the segmentation head.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: UPerNet-like head**\n",
    "\n",
    "```python\n",
    "upsampled = [\n",
    "    F.interpolate(P, size=P1.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    for P in [P1, P2, P3, P4]\n",
    "]\n",
    "fusion = torch.cat(upsampled, dim=1)  # [B, 256*4, 56, 56]\n",
    "\n",
    "# Final segmentation head\n",
    "seg_head = nn.Sequential(\n",
    "    nn.Conv2d(256*4, 256, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, num_classes, 1)\n",
    ")\n",
    "seg_logits = seg_head(fusion)\n",
    "print(seg_logits.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[B, num_classes, 56, 56]\n",
    "```\n",
    "\n",
    "✅ Every pixel’s prediction is now influenced by **both**\n",
    "\n",
    "* fine details (from P1)\n",
    "* and global context (from P4)\n",
    "\n",
    "This is how **UPerNet**, **DeepLabV3+**, and **MaskFormer** use the FPN pyramid.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Case 3 – Depth Estimation / Optical Flow / Reconstruction**\n",
    "\n",
    "For dense regression tasks (depth, disparity, etc.),\n",
    "you can similarly **upsample and fuse** P1–P4, then predict a continuous map.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "depth_map = torch.sigmoid(seg_head(fusion)) * max_depth\n",
    "```\n",
    "\n",
    "✅ Benefit: FPN gives **multi-scale awareness** → depth edges are sharper and large planes smoother.\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Case 4 – Feature Fusion / Global Pooling**\n",
    "\n",
    "For classification or global tasks, sometimes we don’t need all scales.\n",
    "You can do a **global average pooling** on the coarsest level:\n",
    "\n",
    "$\\text{cls\\_vector} = \\text{GAP}(P_4)$\n",
    "\n",
    "and send it to a linear classifier.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = F.adaptive_avg_pool2d(P4, 1).flatten(1)\n",
    "out = nn.Linear(256, num_classes)(x)\n",
    "```\n",
    "\n",
    "✅ This is how **FPN backbones** can still perform classification.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Summary of How Each P_i is Used**\n",
    "\n",
    "| Task               | What we do with (P_1, P_2, P_3, P_4)                               | Output             |\n",
    "| :----------------- | :----------------------------------------------------------------- | :----------------- |\n",
    "| **Detection**      | Apply detection head to each Pᵢ separately (multi-scale)           | Class + bbox maps  |\n",
    "| **Segmentation**   | Upsample all Pᵢ to same size, concatenate, predict per-pixel class | Segmentation mask  |\n",
    "| **Depth / Flow**   | Similar to segmentation, but regression output                     | Depth / motion map |\n",
    "| **Classification** | Use highest-level (P4), global avg pool                            | Image class        |\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Intuition**\n",
    "\n",
    "* **Backbone (PVT)** builds *a hierarchical pyramid of features* (edges → parts → objects).\n",
    "* **FPN** refines it into *a multi-scale feature space* with uniform channels.\n",
    "* **Heads** (detector / segmenter / depth regressor) consume those pyramids differently depending on the task.\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "Image\n",
    "  ↓\n",
    "PVT Backbone → [C1..C4]\n",
    "  ↓\n",
    "FPN → [P1..P4]\n",
    "  ↓\n",
    "Task-specific head → output (boxes / masks / depth / class)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **8. Bonus: Example of integration**\n",
    "\n",
    "If you use **timm** and **torchvision.ops.FeaturePyramidNetwork**, you can combine them easily:\n",
    "\n",
    "```python\n",
    "import timm\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "# 1. Backbone\n",
    "backbone = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "channels = backbone.feature_info.channels()\n",
    "\n",
    "# 2. FPN\n",
    "fpn = FeaturePyramidNetwork(in_channels_list=channels, out_channels=256)\n",
    "\n",
    "# 3. Forward\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = backbone(x)\n",
    "pyramid = fpn({f\"c{i}\": f for i, f in enumerate(features)})\n",
    "\n",
    "for k, v in pyramid.items():\n",
    "    print(k, v.shape)\n",
    "```\n",
    "\n",
    "✅ You get a dictionary like:\n",
    "\n",
    "```\n",
    "c0 torch.Size([1, 256, 56, 56])\n",
    "c1 torch.Size([1, 256, 28, 28])\n",
    "c2 torch.Size([1, 256, 14, 14])\n",
    "c3 torch.Size([1, 256, 7, 7])\n",
    "```\n",
    "\n",
    "Perfect for plugging into a detector or segmenter.\n",
    "\n",
    "---\n",
    "\n",
    "# **9. In summary**\n",
    "\n",
    "✅ FPN produces **P1–P4**, a multi-scale, uniform set of features.\n",
    "✅ Each PVT output (C_i) contributes to one pyramid level.\n",
    "✅ What happens next depends on your task:\n",
    "\n",
    "| Task               | How you use FPN outputs                               |\n",
    "| :----------------- | :---------------------------------------------------- |\n",
    "| **Detection**      | One prediction head per level (boxes + classes)       |\n",
    "| **Segmentation**   | Upsample + fuse all levels for dense pixel prediction |\n",
    "| **Depth / Flow**   | Same as segmentation but with regression              |\n",
    "| **Classification** | Pool highest-level P4 feature                         |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **minimal end-to-end example** (in code) of using PVT + FPN + RetinaNet-style detection head on dummy input, just to see how each pyramid level produces predictions?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
