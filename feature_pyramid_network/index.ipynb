{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1665a62-a95c-4719-a63c-2846dfcef486",
   "metadata": {},
   "source": [
    "# **Feature Pyramid Network (FPN)**\n",
    "**FPN (Feature Pyramid Network)** is one of the cornerstones of modern **detection and segmentation** architectures — and it ties directly to **PVT** (since PVT outputs a feature pyramid).\n",
    "\n",
    "\n",
    "**Two versions**:\n",
    "\n",
    "1. **Canonical ResNet-style FPN (official FPN paper)**\n",
    "2. **PVT/Swin-Transformer style FPN (for use with PVT-v2-B2)**\n",
    "\n",
    "# **Canonical FPN (ResNet Backbone)**\n",
    "\n",
    "Assume the backbone input is an image of size:\n",
    "\n",
    "**Input: $H \\times W \\times 3$**\n",
    "\n",
    "ResNet produces the following feature stages:\n",
    "\n",
    "| Stage | Source               | Spatial Resolution | Channels |\n",
    "| ----- | -------------------- | ------------------ | -------- |\n",
    "| $C_2$| After ResNet stage 2 | $H/4 \\times W/4$  | 256      |\n",
    "| $C_3$| After stage 3        | $H/8 \\times W/8$  | 512      |\n",
    "| $C_4$| After stage 4        | $H/16 \\times W/16$| 1024     |\n",
    "| $C_5$| After stage 5        | $H/32 \\times W/32$| 2048     |\n",
    "\n",
    "Now FPN converts them to:\n",
    "\n",
    "**All P-levels have the same channel count: 256**\n",
    "\n",
    "| Pyramid Level | Resolution         | Channels |\n",
    "| ------------- | ------------------ | -------- |\n",
    "| $P_5$        | $H/32 \\times W/32$| 256      |\n",
    "| $P_4$        | $H/16 \\times W/16$| 256      |\n",
    "| $P_3$        | $H/8 \\times W/8$  | 256      |\n",
    "| $P_2$        | $H/4 \\times W/4$  | 256      |\n",
    "\n",
    "---\n",
    "\n",
    "## **FPN Equations (with ResNet dimensions)**\n",
    "\n",
    "### **Step 1. Lateral 1×1 reductions**\n",
    "\n",
    "Each backbone output is projected to 256 channels:\n",
    "\n",
    "$$\n",
    "\\tilde{C}_5 = \\text{Conv}_{1\\times 1}(C_5), \\quad \\text{shape } (H/32, W/32, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_4 = \\text{Conv}_{1\\times 1}(C_4), \\quad \\text{shape } (H/16, W/16, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_3 = \\text{Conv}_{1\\times 1}(C_3), \\quad \\text{shape } (H/8, W/8, 256)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_2 = \\text{Conv}_{1\\times 1}(C_2), \\quad \\text{shape } (H/4, W/4, 256)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2. Top–down fusion**\n",
    "\n",
    "$$\n",
    "P_5 = \\tilde{C}_5\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_4 = \\tilde{C}_4 + \\text{Upsample}(P_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3 = \\tilde{C}_3 + \\text{Upsample}(P_4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2 = \\tilde{C}_2 + \\text{Upsample}(P_3)\n",
    "$$\n",
    "\n",
    "Upsample is bilinear or nearest-neighbor.\n",
    "It has **no parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3. 3×3 smoothing conv**\n",
    "\n",
    "Each $P_i$is passed through a 3×3 conv $stride 1, padding 1):\n",
    "\n",
    "$$\n",
    "P_i = \\text{Conv}_{3\\times 3}(P_i)\n",
    "$$\n",
    "\n",
    "This removes the checkerboard pattern coming from upsampling.\n",
    "\n",
    "---\n",
    "\n",
    "# **PVT / Swin-Transformer Style FPN**\n",
    "\n",
    "Backbones like **PVT-v2-B2** produce different channel counts:\n",
    "\n",
    "Using **PVT-v2-B2** as example:\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| $C_1$| $H/4$     | 64       |\n",
    "| $C_2$| $H/8$     | 128      |\n",
    "| $C_3$| $H/16$    | 320      |\n",
    "| $C_4$| $H/32$    | 512      |\n",
    "\n",
    "FPN usually uses the last 3 or 4:\n",
    "\n",
    "| C-level | Res    | Channels |\n",
    "| ------- | ------ | -------- |\n",
    "| $C_2$  | $H/4$ | 64       |\n",
    "| $C_3$  | $H/8$ | 128      |\n",
    "| $C_4$  | $H/16$| 320      |\n",
    "| $C_5$  | $H/32$| 512      |\n",
    "\n",
    "---\n",
    "\n",
    "## **Unified FPN projection**\n",
    "\n",
    "Everything is projected to **256 channels**:\n",
    "\n",
    "| Pyramid Level | Resolution | Channels |\n",
    "| ------------- | ---------- | -------- |\n",
    "| $P_5$        | $H/32$    | 256      |\n",
    "| $P_4$        | $H/16$    | 256      |\n",
    "| $P_3$        | $H/8$     | 256      |\n",
    "| $P_2$        | $H/4$     | 256      |\n",
    "\n",
    "Equations remain exactly the same:\n",
    "\n",
    "$$\n",
    "P_5 = \\text{Conv}_{1\\times1}(C_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_4 = \\text{Conv}_{1\\times1}(C_4)+ \\text{Upsample}(P_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3 = \\text{Conv}_{1\\times1}(C_3)+ \\text{Upsample}(P_4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2 = \\text{Conv}_{1\\times1}(C_2)+ \\text{Upsample}(P_3)\n",
    "$$\n",
    "\n",
    "Each $P_i$then gets a 3×3 conv.\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Clean Diagram (Explicit Shapes)**\n",
    "\n",
    "Assume input image:\n",
    "**(H = 512, W = 512)** (just as example)\n",
    "\n",
    "### **Backbone (PVT-v2-B2)**\n",
    "\n",
    "```\n",
    "C2: 128×128, 64 ch\n",
    "C3:  64× 64, 128 ch\n",
    "C4:  32× 32, 320 ch\n",
    "C5:  16× 16, 512 ch\n",
    "```\n",
    "\n",
    "### **After 1×1 lateral projection**\n",
    "\n",
    "```\n",
    "C2 → 128×128, 256 ch\n",
    "C3 →  64× 64, 256 ch\n",
    "C4 →  32× 32, 256 ch\n",
    "C5 →  16× 16, 256 ch\n",
    "```\n",
    "\n",
    "### **Top–down pathway**\n",
    "\n",
    "```\n",
    "P5 = C5                                      → 16×16,   256 ch\n",
    "P4 = C4 + up(P5)    (up: 16→32)             → 32×32,   256 ch\n",
    "P3 = C3 + up(P4)    (up: 32→64)             → 64×64,   256 ch\n",
    "P2 = C2 + up(P3)    (up: 64→128)            → 128×128, 256 ch\n",
    "```\n",
    "\n",
    "### **Final 3×3 smoothing**\n",
    "\n",
    "```\n",
    "P5: 16×16   → 256 ch\n",
    "P4: 32×32   → 256 ch\n",
    "P3: 64×64   → 256 ch\n",
    "P2: 128×128 → 256 ch\n",
    "```\n",
    "\n",
    "These four maps are then used for:\n",
    "\n",
    "* Detection heads\n",
    "* Segmentation decoders\n",
    "* Anchor-free detectors\n",
    "* Panoptic segmentation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Why it’s powerful**\n",
    "\n",
    "- ✅ Handles **objects at multiple scales** (small and large).\n",
    "- ✅ Uses **semantics from deep layers** + **resolution from shallow layers**.- \n",
    "- ✅ Simple and light, yet extremely effective.\n",
    "\n",
    "---\n",
    "\n",
    "## **Common architectures using FPN**\n",
    "\n",
    "| Architecture                 | Backbone            | FPN used for           | Output purpose                 |\n",
    "| :--------------------------- | :------------------ | :--------------------- | :----------------------------- |\n",
    "| **Faster R-CNN + FPN**       | ResNet / Swin / PVT | Object detection       | Multi-scale RoI heads          |\n",
    "| **RetinaNet**                | ResNet / PVT        | Single-stage detection | Multi-scale anchor predictions |\n",
    "| **Mask R-CNN + FPN**         | ResNet / Swin / PVT | Instance segmentation  | Mask head features             |\n",
    "| **UPerNet**                  | PVT / Swin / ViT    | Semantic segmentation  | Pyramid fusion before decoder  |\n",
    "| **Detectron2 / MMDetection** | Many                | Detection/Segmentation | Backbone + FPN combo           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "## **Backbone stages**\n",
    "\n",
    "| Level | Resolution | Channels                                |\n",
    "| ----- | ---------- | --------------------------------------- |\n",
    "| $C_2$| $H/4$     | varies (64 in PVT-v2-B2, 256 in ResNet)|\n",
    "| $C_3$| $H/8$     | varies                                  |\n",
    "| $C_4$| $H/16$    | varies                                  |\n",
    "| $C_5$| $H/32$    | varies                                  |\n",
    "\n",
    "## **FPN pyramid**\n",
    "\n",
    "| Level | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| $P_2$| $H/4$     | 256      |\n",
    "| $P_3$| $H/8$     | 256      |\n",
    "| $P_4$| $H/16$    | 256      |\n",
    "| $P_5$| $H/32$    | 256      |\n",
    "\n",
    "\n",
    "- ✅ **FPN (Feature Pyramid Network)** = multi-scale feature fusion architecture.\n",
    "- ✅ Combines high-res spatial detail (low layers) with strong semantics (deep layers).\n",
    "- ✅ Used in detection (Faster R-CNN, RetinaNet), segmentation (Mask R-CNN, UPerNet).\n",
    "- ✅ Works seamlessly with hierarchical backbones like **ResNet**, **Swin**, **PVT**.\n",
    "- ✅ In code, it’s mostly:\n",
    "\n",
    "* `1×1 conv` for lateral mapping\n",
    "* `upsample + addition`\n",
    "* `3×3 conv` for smoothing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa561da-6e07-4a26-9145-2921546ce7a0",
   "metadata": {},
   "source": [
    "# **The FPN Structure (ASCII Diagram)**\n",
    "\n",
    "```\n",
    "            +--------------------------+\n",
    "            |      Backbone (e.g. PVT) |\n",
    "            +--------------------------+\n",
    "                     │\n",
    "         ┌───────────────────────────────┐\n",
    "         │ Outputs from different stages │\n",
    "         └───────────────────────────────┘\n",
    "             C1: [B, 64, 56, 56]\n",
    "             C2: [B,128, 28, 28]\n",
    "             C3: [B,320, 14, 14]\n",
    "             C4: [B,512,  7,  7]\n",
    "\n",
    "                     ↓ (Top-down path)\n",
    "        +------------------------------------------+\n",
    "        |              FPN construction            |\n",
    "        +------------------------------------------+\n",
    "\n",
    "                             P4 ← 1×1 conv(C4)\n",
    "                              │\n",
    "                              │  (upsample by 2)\n",
    "                              ↓\n",
    "             P3 ← 1×1 conv(C3) + ↑ P4\n",
    "              │\n",
    "              │  (upsample by 2)\n",
    "              ↓\n",
    "     P2 ← 1×1 conv(C2) + ↑ P3\n",
    "      │\n",
    "      │  (upsample by 2)\n",
    "      ↓\n",
    "P1 ← 1×1 conv(C1) + ↑ P2\n",
    "\n",
    "Each Pi then → 3×3 conv smoothing\n",
    "(P1, P2, P3, P4 each: [B, 256, H_i, W_i])\n",
    "```\n",
    "\n",
    "✅ **Top-down path:**\n",
    "Upsamples deeper, low-resolution features (P4→P3→P2→P1).\n",
    "\n",
    "✅ **Lateral connections:**\n",
    "Each upsampled feature is **added** to a 1×1-convolved version of the corresponding backbone feature.\n",
    "\n",
    "✅ **3×3 conv smoothing:**\n",
    "Removes aliasing artifacts after upsampling and addition.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42285f-41e1-4935-b1ea-cd1eab8b48a7",
   "metadata": {},
   "source": [
    "# **What Exactly Do we do with $P_1, P_2, P_3, P_4,$** \n",
    "\n",
    "once we have our **multi-scale pyramid outputs**\n",
    "$$P_1, P_2, P_3, P_4,$$\n",
    "**what exactly do we do with them next?**\n",
    "\n",
    "Let’s go step by step — because the answer depends on your **task** (classification, detection, segmentation, depth, etc.), but the principles are always the same.\n",
    "\n",
    "---\n",
    "\n",
    "### **What we have so far**\n",
    "\n",
    "After the **PVT → FPN pipeline**, we have:\n",
    "\n",
    "| Level | Resolution | Channels | Contains               |\n",
    "| :---: | :--------- | :------: | :--------------------- |\n",
    "|   P1  | 56×56      |    256   | fine details, edges    |\n",
    "|   P2  | 28×28      |    256   | object parts           |\n",
    "|   P3  | 14×14      |    256   | larger objects         |\n",
    "|   P4  | 7×7        |    256   | whole-object semantics |\n",
    "\n",
    "Each $ P_i $ is:\n",
    "\n",
    "* **semantically strong** (due to top-down flow),\n",
    "* **spatially meaningful** (due to lateral connections),\n",
    "* and **uniform in channels** (256).\n",
    "\n",
    "Now we choose a **head** depending on the task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba49a1-d04f-4ca3-a444-787c8a9714ca",
   "metadata": {},
   "source": [
    "## **PVT-v2 + FPN segmentation**\n",
    "Below is a **clean, minimal, end-to-end PVT-v2 + FPN segmentation example** using **timm**.\n",
    "\n",
    "It follows a standard structure:\n",
    "\n",
    "1. Load **PVT-v2-B2** as feature extractor\n",
    "2. Build a **top-down FPN decoder**\n",
    "3. Build a **final segmentation head**\n",
    "4. Run forward with dummy input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0c70e2-d45e-45e2-a86b-ed2805cfbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm \n",
    "from timm import create_model\n",
    "\n",
    "# fmt: on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1653a44-0835-41f2-90a9-5a00c8ba4806",
   "metadata": {},
   "source": [
    "#### **2. Load PVT-v2-B2 backbone**\n",
    "\n",
    "PVT-v2-B2 in `timm` outputs 4 feature maps (C1, C2, C3, C4):\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| C1    | 1/4        | 64       |\n",
    "| C2    | 1/8        | 128      |\n",
    "| C3    | 1/16       | 320      |\n",
    "| C4    | 1/32       | 512      |\n",
    "\n",
    "We get these with `features_only=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d447c83-fec2-4879-afae-683b93c8464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_chs': 64, 'reduction': 4, 'module': 'stages.0', 'index': 0}\n",
      "{'num_chs': 128, 'reduction': 8, 'module': 'stages.1', 'index': 1}\n",
      "{'num_chs': 320, 'reduction': 16, 'module': 'stages.2', 'index': 2}\n",
      "{'num_chs': 512, 'reduction': 32, 'module': 'stages.3', 'index': 3}\n",
      "Feature channels: [64, 128, 320, 512]\n",
      "Feature reduction: [4, 8, 16, 32]\n",
      "torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 320, 14, 14])\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([56, 56])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([14, 14])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "backbone = timm.create_model(\n",
    "    'pvt_v2_b2',\n",
    "    pretrained=True,\n",
    "    features_only=True\n",
    ")\n",
    "\n",
    "for i in backbone.feature_info:\n",
    "    print(i)\n",
    "\n",
    "print(f'Feature channels: {backbone.feature_info.channels()}')\n",
    "print(f'Feature reduction: {backbone.feature_info.reduction()}')\n",
    "\n",
    "\n",
    "H, W = 224, 224\n",
    "x = torch.randn(1, 3, H, W)\n",
    "features = backbone(x)\n",
    "for c in features:\n",
    "    print(c.shape)\n",
    "\n",
    "C1, C2, C3, C4 = features\n",
    "\n",
    "print(C1.shape[-2:])\n",
    "print(C2.shape[-2:])\n",
    "print(C3.shape[-2:])\n",
    "print(C4.shape[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2e961-721d-4e35-a12b-f8ebc76809f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **3. FPN Decoder (minimal)**\n",
    "\n",
    "The FPN idea:\n",
    "\n",
    "* Convert all levels to same channel count (here 256)\n",
    "* Start from highest level C4 → produce P4\n",
    "* Upsample P4 and add to C3 → P3\n",
    "* Upsample P3 and add to C2 → P2\n",
    "* Upsample P2 and add to C1 → P1\n",
    "\n",
    "Upsampling uses **bilinear interpolation (non-learned)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c4308d-f7b5-4a0b-98fc-00c53a2dc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, channels, out_channels=256):\n",
    "        super().__init__()\n",
    "        # 1x1 lateral layers\n",
    "        self.lateral = nn.ModuleList(\n",
    "            [nn.Conv2d(c, out_channels, 1) for c in channels])\n",
    "        # 3x3 smooth layers\n",
    "        self.smooth = nn.ModuleList(\n",
    "            [nn.Conv2d(out_channels, out_channels, 3, padding=1) for _ in channels])\n",
    "\n",
    "    def forward(self, features):\n",
    "        C1, C2, C3, C4 = features\n",
    "\n",
    "        # C1, C2, C3, C4 are\n",
    "        # torch.Size([1, 64, 56, 56])\n",
    "        # torch.Size([1, 128, 28, 28])\n",
    "        # torch.Size([1, 320, 14, 14])\n",
    "        # torch.Size([1, 512, 7, 7])\n",
    "        # so\n",
    "\n",
    "        # C1.shape[-2:] is torch.Size([56, 56])\n",
    "        # C2.shape[-2:] is torch.Size([28, 28])\n",
    "        # C3.shape[-2:] is torch.Size([14, 14])\n",
    "        # C4.shape[-2:] is torch.Size([7, 7])\n",
    "\n",
    "        P4 = self.lateral[3](C4)\n",
    "\n",
    "        P3 = self.lateral[2](C3) + nn.functional.interpolate(P4,\n",
    "                                                             size=C3.shape[-2:], mode='nearest')\n",
    "        P2 = self.lateral[1](C2) + nn.functional.interpolate(P3,\n",
    "                                                             size=C2.shape[-2:], mode='nearest')\n",
    "        P1 = self.lateral[0](C1) + nn.functional.interpolate(P2,\n",
    "                                                             size=C1.shape[-2:], mode='nearest')\n",
    "\n",
    "        P1 = self.smooth[0](P1)\n",
    "        P2 = self.smooth[1](P2)\n",
    "        P3 = self.smooth[2](P3)\n",
    "        P4 = self.smooth[3](P4)\n",
    "\n",
    "        for P in [P4, P3, P2, P1]:\n",
    "            print(\"P:\", P.shape)\n",
    "            print(\"--------\")\n",
    "        return [P1, P2, P3, P4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed7766-23f8-4fd6-995f-38369ad64cc1",
   "metadata": {},
   "source": [
    "#### Why a `nn.ModuleList` and not a plain Python `list` and `nn.ModuleList` for `self.lateral`\n",
    "\n",
    "\n",
    "\n",
    "**Short Answer**\n",
    "\n",
    "You **must** use `nn.ModuleList` (not a plain `list`) **when storing layers/modules** inside an `nn.Module` (your network), **if you want PyTorch to track their parameters and move them to GPU/CPU properly**.\n",
    "\n",
    "---\n",
    "\n",
    "Why **not** use a plain Python list?\n",
    "\n",
    "When you write this:\n",
    "\n",
    "```python\n",
    "self.lateral = [nn.Conv2d(c, out_channels, 1) for c in in_channels]\n",
    "```\n",
    "\n",
    "You’re storing the layers in a plain list. But PyTorch **won’t register** them as part of your model.\n",
    "\n",
    "--- \n",
    "\n",
    "As a result:\n",
    "\n",
    "* `model.parameters()` **won’t include them**\n",
    "* `.cuda()` / `.to(device)` **won’t move them**\n",
    "* They **won’t show up** in `model.named_parameters()` or `model.state_dict()`\n",
    "* **They won’t be trained!**\n",
    "* Saving/loading the model will silently skip them\n",
    "\n",
    "This is one of the most common beginner mistakes in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why use `nn.ModuleList`?\n",
    "\n",
    "```python\n",
    "self.lateral = nn.ModuleList([\n",
    "    nn.Conv2d(c, out_channels, 1) for c in in_channels\n",
    "])\n",
    "```\n",
    "\n",
    "This tells PyTorch:\n",
    "**“These are submodules. Track them. Register their parameters.”**\n",
    "\n",
    "Now:\n",
    "\n",
    "* Parameters **will be included in `.parameters()`**\n",
    "* You can move them to GPU with `.cuda()` or `.to('cuda')`\n",
    "* They will be saved/loaded with the model checkpoint\n",
    "* You can iterate over them in `forward()` as usual\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ When to use `nn.ModuleList` vs `nn.Sequential`\n",
    "\n",
    "| Use `nn.Sequential` when...                         | Use `nn.ModuleList` when...                                                      |\n",
    "| --------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| Modules are applied **in order, one after another** | You need **more flexible logic**, e.g., skip connections, top-down fusion, loops |\n",
    "| Example: classic feedforward or MLP layers          | Example: FPN, U-Net skip connections, layer-wise operations                      |\n",
    "\n",
    "In FPN:\n",
    "\n",
    "```python\n",
    "for i in range(4):\n",
    "    P[i] = self.lateral[i](C[i]) + upsample(P[i+1])\n",
    "```\n",
    "\n",
    "So we need indexed access — this can’t be done with `Sequential`.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Common mistake clarified\n",
    "\n",
    "You wrote:\n",
    "\n",
    "> I have only seen that when we create our network model, we inherit from nn.ModuleList\n",
    "\n",
    "That’s a **misunderstanding**:\n",
    "\n",
    "You **don’t** inherit from `nn.ModuleList` in typical models.\n",
    "\n",
    "Instead, you define your model as:\n",
    "\n",
    "```python\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([...])\n",
    "```\n",
    "\n",
    "So `nn.ModuleList` is **used inside a module**, not inherited directly — unless you're doing something very special like building an entire network as a list (rare and not recommended for general models).\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Visual demo\n",
    "\n",
    "Let’s verify it:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class BadNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(10, 10) for _ in range(3)]\n",
    "\n",
    "model = BadNet()\n",
    "print(list(model.parameters()))  # ❌ Empty list!\n",
    "```\n",
    "\n",
    "Now with `ModuleList`:\n",
    "\n",
    "```python\n",
    "class GoodNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(3)])\n",
    "\n",
    "model = GoodNet()\n",
    "print(list(model.parameters()))  # ✅ Contains all parameters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Summary\n",
    "\n",
    "Use `nn.ModuleList` whenever:\n",
    "\n",
    "* You are storing a list of `nn.Module` layers\n",
    "* You need PyTorch to register and track those layers\n",
    "* You want to iterate or use layers flexibly in `forward()`\n",
    "\n",
    "❌ **Never** store layers in plain lists/tuples if you want them trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821779e0-e3d1-4852-97ab-8d049d1dc776",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4. Final Segmentation Head**\n",
    "\n",
    "A simple head:\n",
    "\n",
    "* Concatenate multi-scale FPN features\n",
    "* Upsample to input size\n",
    "* 3×3 Conv → output logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245d4957-e6ad-4ea5-97d3-7a9ff6189a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, fpn_channels=256, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(fpn_channels * 4, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, P):\n",
    "        # P = [P1, P2, P3, P4]\n",
    "        P1, P2, P3, P4 = P\n",
    "\n",
    "        size = P1.shape[-2:]\n",
    "\n",
    "        P2 = F.interpolate(P2, size=size, mode='bilinear', align_corners=False)\n",
    "        P3 = F.interpolate(P3, size=size, mode='bilinear', align_corners=False)\n",
    "        P4 = F.interpolate(P4, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat([P1, P2, P3, P4], dim=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e153f-3f89-4371-9e7f-b3ae8023f80f",
   "metadata": {},
   "source": [
    "#### **5. Full PVT-FPN Segmentation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a5ac49-0dbf-440e-8831-aab68889a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVT_FPN_Segmentation(nn.Module):\n",
    "    def __init__(self, backbone_name='pvt_v2_b2', num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            features_only=True\n",
    "        )\n",
    "\n",
    "        in_channels = self.backbone.feature_info.channels()\n",
    "\n",
    "        self.fpn = FPN(in_channels, out_channels=256)\n",
    "        self.head = SegmentationHead(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)          # C1,C2,C3,C4\n",
    "        P = self.fpn(features)               # P1,P2,P3,P4\n",
    "        logits = self.head(P)               # final prediction\n",
    "        logits = F.interpolate(logits, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
