{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d9f6fd-ceb8-48e3-8eaf-30f66dfc18d2",
   "metadata": {},
   "source": [
    "Here's a **comprehensive checklist** of techniques to maximize **GPU utilization and training throughput in PyTorch**, categorized by priority and **estimated impact**.\n",
    "\n",
    "---\n",
    "\n",
    "#  **Maximizing GPU Utilization in PyTorch**\n",
    "\n",
    "|  # | Technique                                  | Description                                               | Expected Impact                           | Action                                         |\n",
    "| -: | ------------------------------------------ | --------------------------------------------------------- | ----------------------------------------- | ---------------------------------------------- |\n",
    "|  1 | `pin_memory=True` in DataLoader            | Enables faster CPU→GPU transfers via page-locked memory   | **Moderate** (\\~10–20%)                   | `DataLoader(..., pin_memory=True)`             |\n",
    "|  2 | `num_workers>0` in DataLoader              | Enables multi-threaded CPU data loading                   | **High** (\\~20–50%) if CPU bottlenecked   | `DataLoader(..., num_workers=4 or 8)`          |\n",
    "|  3 | Larger `batch_size`                        | Reduces number of GPU kernel launches per epoch           | **High**, depends on GPU memory           | Try `256`, `512`, `1024` if possible           |\n",
    "|  4 | `torch.backends.cudnn.benchmark=True`      | Optimizes cuDNN kernel selection when input size is fixed | **Moderate** (5–15%)                      | Add to script startup                          |\n",
    "|  5 | Move `loss` and criterion to GPU           | Prevents CPU→GPU transfer during training                 | **Small** (\\~1–3%)                        | `criterion = nn.CrossEntropyLoss().to(device)` |\n",
    "|  6 | Use `torch.no_grad()` in eval loop         | Avoids unnecessary graph computation                      | **Small**, but essential for memory       |                                  |\n",
    "|  7 | Profile with `torch.profiler` or `nvprof`  | Finds performance bottlenecks per op                      | **High** insight, indirect gain           | Use when fine-tuning                           |\n",
    "|  8 | Prefetching with `prefetch_factor`         | Loads batches in background while GPU trains              | **Moderate** (\\~10–20%)                   | `DataLoader(..., prefetch_factor=2)`           |\n",
    "|  9 | Use `torch.compile(model)` (PyTorch 2.x)   | Traces and compiles model for faster inference/training   | **High** (10–40%) in PyTorch 2.x+         | Requires PyTorch 2.0+                          |\n",
    "| 10 | Use AMP (mixed-precision) training         | Reduces memory and speeds up math ops on Tensor Cores     | **Very High** (2× speed on RTX/A100 GPUs) | Use `torch.cuda.amp` or `Lightning`            |\n",
    "| 11 | Use `torch.jit.script` / `torch.jit.trace` | Optimizes graph and inlines ops                           | **Moderate** (\\~5–15%)                    | For static models                              |\n",
    "| 12 | Use `nvprof`, `nvtop`, `nvidia-smi`        | Monitor live GPU usage & bottlenecks                      | **Essential diagnostics**                 | CLI tools                                      |\n",
    "| 13 | Reduce CPU bottlenecks                     | e.g., avoid slow disk reads, keep data in RAM             | **High** in I/O-heavy workloads           | Load data in RAM, SSD preferred                |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7887a2a-9433-4e65-bef6-9c8825ee4115",
   "metadata": {},
   "source": [
    "# **3. batch_size**\n",
    "\n",
    "In general, **larger `batch_size`** has significant impact on **training performance, accuracy, generalization, and hardware utilization**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.1. Training Speed / Performance**\n",
    "\n",
    "* **✅ Pros:**\n",
    "\n",
    "  * Better GPU utilization due to parallelism.\n",
    "  * Fewer parameter updates per epoch → less overhead from optimizer and backpropagation calls.\n",
    "  * More stable and accurate gradient estimation per batch.\n",
    "* **❌ Cons:**\n",
    "\n",
    "  * Consumes more GPU memory → might not fit in memory, leading to OOM (Out of Memory).\n",
    "  * Diminishing returns beyond a certain size.\n",
    "\n",
    "> **Rule of thumb:** Increase `batch_size` until you hit GPU memory limits.\n",
    "\n",
    "---\n",
    "\n",
    "####  **3.2. Model Accuracy / Generalization**\n",
    "\n",
    "* **Small `batch_size` (e.g., 32 or 64):**\n",
    "\n",
    "  * Noisy gradients → acts as a regularizer → better generalization.\n",
    "* **❌ Large `batch_size` (e.g., 1024+):**\n",
    "\n",
    "  * Smooth gradients → can lead to faster convergence but **poorer generalization**.\n",
    "  * Might converge to **sharp minima** → worse performance on test data.\n",
    "\n",
    "> **Empirical studies** (e.g., Keskar et al., 2016) showed that very large batch sizes can hurt generalization.\n",
    "\n",
    "---\n",
    "\n",
    "####  **3.3. Loss Surface & Convergence**\n",
    "\n",
    "* **Large batches** tend to:\n",
    "\n",
    "  * Follow **flatter paths** during training (less stochastic noise).\n",
    "  * Require **more careful learning rate scheduling** (e.g., linear warmup + decay).\n",
    "\n",
    "> When increasing `batch_size`, consider **increasing learning rate proportionally** (see **linear scaling rule**).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  **3.4. Practical Advice**\n",
    "\n",
    "* **Start small (32–128)** for better generalization.\n",
    "* If using **batch norm**, large batch size can help stabilize the estimates.\n",
    "* For very large `batch_size`, use:\n",
    "\n",
    "  * Learning rate scaling: `new_lr = base_lr * (new_batch / base_batch)`\n",
    "  * **Gradient accumulation** if GPU can't fit a large batch.\n",
    "  * Mixed-precision training (AMP) to reduce memory footprint.\n",
    "\n",
    "---\n",
    "\n",
    "**Code Snippet for Gradient Accumulation**\n",
    "\n",
    "```python\n",
    "accum_iter = 4  # simulate batch_size * 4\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y) / accum_iter\n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accum_iter == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107676ca-aa2a-4ef9-b250-674388c83c24",
   "metadata": {},
   "source": [
    "# **4. torch.backends.cudnn.benchmark**\n",
    "\n",
    "When you enable:\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmark = True\n",
    "```\n",
    "\n",
    "you tell cuDNN to:**Profile multiple convolution algorithms** at runtime.\n",
    "\n",
    "For every convolution layer (e.g., `Conv2d`), cuDNN has several possible algorithms (`GEMM`, `FFT`, `Winograd`, etc.) to execute it. Each has different performance depending on:\n",
    "\n",
    "* Input size\n",
    "* Kernel size\n",
    "* Stride\n",
    "* Padding\n",
    "* GPU architecture\n",
    "\n",
    "cuDNN **benchmarks all available algorithms** for your given layer configuration (on the first forward pass), **times them**, and then **caches the fastest one** for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "**How it speeds up training**\n",
    "\n",
    "If input sizes are **constant**, then cuDNN can:\n",
    "\n",
    "* **Choose the best kernel** once\n",
    "* **Avoid slower default algorithms**\n",
    "* **Reuse the fast kernel efficiently** across all batches\n",
    "\n",
    "This can lead to **significant performance gains** (10%–50% in some CNN-heavy models).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.1. Why is it **not** enabled by default?**\n",
    "\n",
    "\n",
    "\n",
    "**Benchmarking costs time**\n",
    "    * On first forward pass with new input size, **all candidate algorithms are tested**.\n",
    "    * This can cause a **noticeable delay** if input shapes keep changing (e.g., variable image sizes).\n",
    "    * Also, benchmarking might allocate **more memory** during evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2.When it's harmful:**\n",
    "\n",
    "| Scenario                                                    | Problem                                                     |\n",
    "| ----------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Input sizes vary (e.g. data augmentation, NLP with padding) | cuDNN must **re-benchmark every time**, which adds overhead |\n",
    "| You need **deterministic behavior**                         | Some fast algorithms are **non-deterministic**              |\n",
    "| You're running on constrained GPU memory                    | Benchmarking might cause **out-of-memory errors**           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Optional Best Practice for Training Script**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True       # Enable fastest algorithm selection\n",
    "torch.backends.cudnn.deterministic = False  # (optional) For speed over reproducibility\n",
    "```\n",
    "\n",
    "And if you care about **reproducibility**, flip them:\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6066a-df53-4564-8c0f-f76632c9de50",
   "metadata": {},
   "source": [
    "# **5.torch.backends.cudnn.deterministic = True**\n",
    "\n",
    "It forces **cuDNN to use only deterministic algorithms**, meaning that **running your model multiple times with the same input will produce the exact same output** — **bit for bit** — every time. In practice, **deep learning on GPUs involves non-determinism** due to **parallel computation, low-level optimizations, and randomness**. There are several sources of non-determinism in training and inference:\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.1.cuDNN Algorithm Choice**\n",
    "\n",
    "cuDNN (and other GPU libraries) offer **multiple ways to compute convolutions**, pooling, etc. Some are non-deterministic:\n",
    "\n",
    "* They use **atomic operations** (whose order is not guaranteed)\n",
    "* The order of operations in **parallel threads** may vary → rounding errors differ → result differs slightly\n",
    "\n",
    "\n",
    "A **cuDNN kernel** refers to a **highly optimized GPU function** provided by NVIDIA’s cuDNN (CUDA Deep Neural Network) library, which accelerates **deep learning operations** on NVIDIA GPUs, such as:\n",
    "\n",
    "* Convolutions (forward & backward)\n",
    "* Pooling\n",
    "* Activation functions (ReLU, tanh, etc.)\n",
    "* Normalization (batch norm, LRN)\n",
    "* RNNs, LSTMs, GRUs\n",
    "* Tensor transformations\n",
    "\n",
    "---\n",
    "\n",
    "For example:\n",
    "\n",
    "* A 2D convolution used in a CNN might be executed using a cuDNN kernel that selects the best algorithm (e.g., Winograd, FFT, direct GEMM) based on input shapes and GPU architecture.\n",
    "* cuDNN will dynamically choose and launch the most efficient kernel for that specific workload.\n",
    "\n",
    "---\n",
    "\n",
    "**Example in Practice (PyTorch or TensorFlow):**\n",
    "\n",
    "When you run a convolution layer in PyTorch like:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "```\n",
    "\n",
    "under the hood, PyTorch (if CUDA is available) will use cuDNN to choose and run a convolution kernel optimized for your hardware.\n",
    "\n",
    "You can often see messages like:\n",
    "\n",
    "```\n",
    "Using cuDNN backend: conv2d_forward_algo_1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2.Random Number Generators (RNGs)**\n",
    "\n",
    "* Weight initialization\n",
    "* Data augmentation (flips, rotations)\n",
    "* Dropout\n",
    "* Shuffling of training data\n",
    "\n",
    "Unless you seed *all* RNGs (Python, NumPy, PyTorch), you'll get different results each time.\n",
    "\n",
    "#### **5.3.Multi-threading / CUDA kernel scheduling**\n",
    "\n",
    "* CUDA kernel execution order can vary slightly depending on GPU load or thread scheduling.\n",
    "* Even slight differences can **accumulate** during training (especially with float32).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4789b0e4-d65a-4fa9-a47b-a67f5ffcccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # don't search for fastest algo\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32, device='cuda')\n",
    "conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "\n",
    "# This will now always produce the same output\n",
    "y1 = conv(x)\n",
    "y2 = conv(x)\n",
    "\n",
    "print(torch.allclose(y1, y2)) # Will be True if weights are fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a34be-9f7b-4f58-90aa-ad30a9c29fdf",
   "metadata": {},
   "source": [
    "# **8. Prefetching with `prefetch_factor`**\n",
    "\n",
    "\n",
    "In PyTorch, **prefetching** is a technique that allows the `DataLoader` to **prepare data batches ahead of time**, so your model doesn’t have to wait for data. This is especially helpful when data loading (e.g., image decoding, transforms) is a bottleneck.\n",
    "\n",
    "**How `prefetch_factor` works:**\n",
    "\n",
    "* `prefetch_factor` is a parameter of `torch.utils.data.DataLoader`.\n",
    "* It determines how **many batches per worker** are preloaded **in advance**.\n",
    "* Only used when `num_workers > 0`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "If you set:\n",
    "\n",
    "```python\n",
    "DataLoader(..., num_workers=4, prefetch_factor=2)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* Each of the 4 workers will prefetch **2 batches**, so **8 batches total** are being prepared while the model trains on current data.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use it:**\n",
    "\n",
    "* Use `prefetch_factor > 2` **if your GPU is under-utilized** and **data loading is slow**.\n",
    "* Tune it along with `num_workers` to find the optimal setup.\n",
    "* Avoid setting it too high — it increases memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Example\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,          # Use multiple workers\n",
    "    prefetch_factor=4,      # Default is 2\n",
    "    pin_memory=True         # Speeds up transfer to GPU\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for images, labels in loader:\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Tips for performance:\n",
    "\n",
    "| Parameter                 | Description                                               |\n",
    "| ------------------------- | --------------------------------------------------------- |\n",
    "| `num_workers`             | More workers = more parallel data loading                 |\n",
    "| `prefetch_factor`         | Higher = more preloaded batches (good for I/O-heavy data) |\n",
    "| `pin_memory=True`         | Use when transferring to CUDA                             |\n",
    "| `persistent_workers=True` | Keeps workers alive across epochs (PyTorch 1.7+)          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "To see the effect, use a timing tool like:\n",
    "\n",
    "```python\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in loader:\n",
    "    pass\n",
    "print(\"Time:\", time.time() - start)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766c4b-d1f2-431a-ab2b-82da40a35d46",
   "metadata": {},
   "source": [
    "# **9. torch.compile(model)**\n",
    "\n",
    "`torch.compile(model)` is used to optimize and speed up the execution of your model by compiling it into a more efficient backend representation using **TorchDynamo**, **AOTAutograd**, and **Inductor** (by default). ---\n",
    "\n",
    "####  **9.1.Where to use `torch.compile(model)`**\n",
    "\n",
    "You typically apply it **after creating your model but before training or inference**:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18()\n",
    "model = model.to('cuda')  # or 'cpu'\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "# Then use compiled_model for training or inference\n",
    "```\n",
    "\n",
    "> Use `torch.compile()` once, ideally after model instantiation and before the training loop.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.2.What happens when you do `torch.compile(model)`?** \n",
    "\n",
    "PyTorch 2.0 introduces a compiler stack that includes:\n",
    "\n",
    "1. **TorchDynamo**: Captures Python bytecode of your model, intercepts tensor operations.\n",
    "2. **AOTAutograd**: Ahead-of-Time Autograd tracing for forward and backward passes.\n",
    "3. **Inductor**: Converts the traced graph into highly efficient C++/CUDA kernels.\n",
    "\n",
    "This process removes Python overhead and fuses operations, leading to much faster execution, especially for large models on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "####  **9.3.Where does the compiled code go?**\n",
    "\n",
    "* **In-memory**: By default, the compiled code is not written to disk — it’s **kept in memory** for runtime execution.\n",
    "* **Caching**: Some components (e.g., `torch._dynamo`) might cache intermediate results in RAM.\n",
    "* **Debugging**: You can inspect generated code with environment variables:\n",
    "\n",
    "  ```bash\n",
    "  TORCH_LOGS=\"dynamo\" python your_script.py\n",
    "  ```\n",
    "\n",
    "If you want to **export and save** compiled models, look into `torch.export`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.4.Things to be careful about** \n",
    "\n",
    "* Use it with **training or eval mode** set correctly before compiling (`model.eval()` or `model.train()`).\n",
    "* Some models or operations might not be fully supported (especially dynamic control flow).\n",
    "* You can toggle back to eager mode by calling:\n",
    "\n",
    "  ```python\n",
    "  model = compiled_model._orig_mod\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.5.When NOT to use it?** \n",
    "\n",
    "* Very small models with negligible Python overhead.\n",
    "* Highly dynamic models with control flows that resist optimization.\n",
    "* If you already use other tracing tools like TorchScript and want full control over the tracing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdfb20-e118-42ff-bc5a-0f63a01bb8ba",
   "metadata": {},
   "source": [
    "# **10. Use AMP (mixed-precision) training**\n",
    "\n",
    "In PyTorch, the **default data type** for tensors is:\n",
    "\n",
    "**`torch.float32` (i.e., `float`)**\n",
    "When you create a tensor like this:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94777b32-5778-44e0-86f7-8a8d5f95c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ffa8a-ad76-40ed-8311-8b5bab66186c",
   "metadata": {},
   "source": [
    "**AMP** stands for **Automatic Mixed Precision**. It's a technique that allows your model to use both **float32 (FP32)** and **float16 (FP16)** during training to **speed up computation and reduce memory usage** — **without significantly affecting model accuracy**.\n",
    "\n",
    "\n",
    "Normally, training uses 32-bit floating-point (float32) numbers. Mixed precision uses:\n",
    "\n",
    "* **float16 (half precision, dynamic range: ~1e-5 to 6e+4)** for most operations including `Conv`, `Linear`, `ReLU`, `matmul`, `activations` (faster, less memory)\n",
    "* **float32 (single precision, dynamic range: ~1e-38 to 1e+38)** for critical operations including `Loss`, `normalization`, `softmax`, `batchnorm` (to maintain accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "####  **10.1. Training loop using `autocast` and `GradScaler`**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast('cuda'):  # enables mixed precision\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    scaler.scale(loss).backward()          # scale the loss\n",
    "    scaler.step(optimizer)                 # unscale + step\n",
    "    scaler.update()                        # update scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**What Each Component Does**\n",
    "\n",
    "| Component      | Role                                                   |\n",
    "| -------------- | ------------------------------------------------------ |\n",
    "| `autocast()`   | Runs ops in FP16 when safe, otherwise in FP32          |\n",
    "| `GradScaler()` | Prevents underflow when backpropagating FP16 gradients |\n",
    "| `.scale(loss)` | Scales the loss for FP16 safe backward                 |\n",
    "| `.step()`      | Unscales gradients before optimizer step               |\n",
    "| `.update()`    | Adjusts scaling factor dynamically                     |\n",
    "\n",
    "---\n",
    "\n",
    "#### **10.2 Benefits/ Cons of Using AMP**\n",
    "\n",
    "Use AMP **whenever you're training on a GPU that supports it**, especially:\n",
    "\n",
    "* On **NVIDIA Volta, Turing, or Ampere** (e.g., RTX 30xx, A100)\n",
    "* With **large models** or **high-resolution inputs**\n",
    "* For **faster training + lower memory footprint**\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Benefits**\n",
    "\n",
    "*  **Faster training** on GPUs that support Tensor Cores (e.g., NVIDIA RTX/Volta/Ampere)\n",
    "*  **Reduced memory usage**, allowing larger batch sizes or models\n",
    "\n",
    "\n",
    "**❌ Cons**\n",
    "\n",
    "* Slight chance of numerical instability in rare cases\n",
    "* Not all ops are safe in FP16 — PyTorch handles most of this automatically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433ada0f-db52-4794-82c7-85ae4695a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "FP32 time: 4.290900707244873\n",
      "AMP time: 3.6157121658325195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "import time\n",
    "\n",
    "# Set device (automatically checks for CUDA)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 1024)\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scaler = GradScaler(device.type)  # 'cuda' or 'cpu'\n",
    "\n",
    "x = torch.randn(512, 1024, device=device)\n",
    "y = torch.randn(512, 1024, device=device)\n",
    "\n",
    "epochs=1000\n",
    "\n",
    "# Normal FP32\n",
    "start = time.time()\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"FP32 time:\", time.time() - start)\n",
    "\n",
    "# Mixed Precision (AMP) - Only works on CUDA\n",
    "if device.type == 'cuda':\n",
    "    start = time.time()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device.type):  # 'cuda' only\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    print(\"AMP time:\", time.time() - start)\n",
    "else:\n",
    "    print(\"AMP not supported on CPU, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852e705-a55b-44e0-a4be-3adac44e271e",
   "metadata": {},
   "source": [
    "# **11.`torch.jit.script` and `torch.jit.trace`**\n",
    "\n",
    "In PyTorch, `torch.jit.script` and `torch.jit.trace` are part of **TorchScript**, a way to convert PyTorch models into a **serializable and optimizable intermediate representation**. This can improve inference speed, allow deployment in C++ environments, and enable graph-level optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "**Two Ways to Create TorchScript Models**\n",
    "\n",
    "| Method             | Use When...                                                              |\n",
    "| ------------------ | ------------------------------------------------------------------------ |\n",
    "| `torch.jit.trace`  | The model is **static**, i.e., no control flow depending on input values |\n",
    "| `torch.jit.script` | The model has **conditionals, loops, or input-dependent control flow**   |\n",
    "\n",
    "---\n",
    "\n",
    "####  **11.1. `torch.jit.trace` Example (Static Model)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MyModel()\n",
    "example_input = torch.randn(1, 10)\n",
    "\n",
    "# Traced model\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"traced_model.pt\")  # Save\n",
    "```\n",
    "\n",
    "> ⚠️ Use this **only** if the model’s computation graph does **not depend on input data** (e.g., no `if`, `for`).\n",
    "\n",
    "---\n",
    "\n",
    "####  **11.2. `torch.jit.script` Example (Dynamic Control Flow)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:  # Dynamic condition\n",
    "            return x * 2\n",
    "        else:\n",
    "            return x - 2\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Scripted model\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"scripted_model.pt\")  # Save\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Benefits of `torch.jit.script` and `torch.jit.trace`**\n",
    "\n",
    "1. **Speed**: TorchScript compiles models to an optimized graph, often faster for inference (especially on CPU).\n",
    "2. **Deployment**: Models can be loaded in C++ via LibTorch.\n",
    "3. **Serialization**: You can save and load complete models easily (`.pt` format).\n",
    "4. **Cross-platform**: Useful for mobile (PyTorch Mobile).\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Benchmark Example (Speed Comparison)\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "x = torch.randn(1000, 10)\n",
    "\n",
    "# Eager model\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    model(x)\n",
    "print(\"Eager time:\", time.time() - start)\n",
    "\n",
    "# TorchScript model\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    traced_model(x)\n",
    "print(\"TorchScript time:\", time.time() - start)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
