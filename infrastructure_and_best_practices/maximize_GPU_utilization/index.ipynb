{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d9f6fd-ceb8-48e3-8eaf-30f66dfc18d2",
   "metadata": {},
   "source": [
    "Here's a **comprehensive checklist** of techniques to maximize **GPU utilization and training throughput in PyTorch**, categorized by priority and **estimated impact**.\n",
    "\n",
    "---\n",
    "\n",
    "#  **Maximizing GPU Utilization in PyTorch**\n",
    "\n",
    "|  # | Technique                                  | Description                                               | Expected Impact                           | Action                                         |\n",
    "| -: | ------------------------------------------ | --------------------------------------------------------- | ----------------------------------------- | ---------------------------------------------- |\n",
    "|  1 | `pin_memory=True` in DataLoader            | Enables faster CPU→GPU transfers via page-locked memory   | **Moderate** (\\~10–20%)                   | `DataLoader(..., pin_memory=True)`             |\n",
    "|  2 | `num_workers>0` in DataLoader              | Enables multi-threaded CPU data loading                   | **High** (\\~20–50%) if CPU bottlenecked   | `DataLoader(..., num_workers=4 or 8)`          |\n",
    "|  3 | Larger `batch_size`                        | Reduces number of GPU kernel launches per epoch           | **High**, depends on GPU memory           | Try `256`, `512`, `1024` if possible           |\n",
    "|  4 | `torch.backends.cudnn.benchmark=True`      | Optimizes cuDNN kernel selection when input size is fixed | **Moderate** (5–15%)                      | Add to script startup                          |\n",
    "|  5 | Move `loss` and criterion to GPU           | Prevents CPU→GPU transfer during training                 | **Small** (\\~1–3%)                        | `criterion = nn.CrossEntropyLoss().to(device)` |\n",
    "|  6 | Use `torch.no_grad()` in eval loop         | Avoids unnecessary graph computation                      | **Small**, but essential for memory       |                                  |\n",
    "|  7 | Profile with `torch.profiler` or `nvprof`  | Finds performance bottlenecks per op                      | **High** insight, indirect gain           | Use when fine-tuning                           |\n",
    "|  8 | Prefetching with `prefetch_factor`         | Loads batches in background while GPU trains              | **Moderate** (\\~10–20%)                   | `DataLoader(..., prefetch_factor=2)`           |\n",
    "|  9 | Use `torch.compile(model)` (PyTorch 2.x)   | Traces and compiles model for faster inference/training   | **High** (10–40%) in PyTorch 2.x+         | Requires PyTorch 2.0+                          |\n",
    "| 10 | Use AMP (mixed-precision) training         | Reduces memory and speeds up math ops on Tensor Cores     | **Very High** (2× speed on RTX/A100 GPUs) | Use `torch.cuda.amp` or `Lightning`            |\n",
    "| 11 | Use `torch.jit.script` / `torch.jit.trace` | Optimizes graph and inlines ops                           | **Moderate** (\\~5–15%)                    | For static models                              |\n",
    "| 12 | Use `nvprof`, `nvtop`, `nvidia-smi`        | Monitor live GPU usage & bottlenecks                      | **Essential diagnostics**                 | CLI tools                                      |\n",
    "| 13 | Reduce CPU bottlenecks                     | e.g., avoid slow disk reads, keep data in RAM             | **High** in I/O-heavy workloads           | Load data in RAM, SSD preferred                |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7887a2a-9433-4e65-bef6-9c8825ee4115",
   "metadata": {},
   "source": [
    "# **3. batch_size**\n",
    "\n",
    "In general, **larger `batch_size`** has significant impact on **training performance, accuracy, generalization, and hardware utilization**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.1. Training Speed / Performance**\n",
    "\n",
    "* **✅ Pros:**\n",
    "\n",
    "  * Better GPU utilization due to parallelism.\n",
    "  * Fewer parameter updates per epoch → less overhead from optimizer and backpropagation calls.\n",
    "  * More stable and accurate gradient estimation per batch.\n",
    "* **❌ Cons:**\n",
    "\n",
    "  * Consumes more GPU memory → might not fit in memory, leading to OOM (Out of Memory).\n",
    "  * Diminishing returns beyond a certain size.\n",
    "\n",
    "> **Rule of thumb:** Increase `batch_size` until you hit GPU memory limits.\n",
    "\n",
    "---\n",
    "\n",
    "####  **3.2. Model Accuracy / Generalization**\n",
    "\n",
    "* **Small `batch_size` (e.g., 32 or 64):**\n",
    "\n",
    "  * Noisy gradients → acts as a regularizer → better generalization.\n",
    "* **❌ Large `batch_size` (e.g., 1024+):**\n",
    "\n",
    "  * Smooth gradients → can lead to faster convergence but **poorer generalization**.\n",
    "  * Might converge to **sharp minima** → worse performance on test data.\n",
    "\n",
    "> **Empirical studies** (e.g., Keskar et al., 2016) showed that very large batch sizes can hurt generalization.\n",
    "\n",
    "---\n",
    "\n",
    "####  **3.3. Loss Surface & Convergence**\n",
    "\n",
    "* **Large batches** tend to:\n",
    "\n",
    "  * Follow **flatter paths** during training (less stochastic noise).\n",
    "  * Require **more careful learning rate scheduling** (e.g., linear warmup + decay).\n",
    "\n",
    "> When increasing `batch_size`, consider **increasing learning rate proportionally** (see **linear scaling rule**).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  **3.4. Practical Advice**\n",
    "\n",
    "* **Start small (32–128)** for better generalization.\n",
    "* If using **batch norm**, large batch size can help stabilize the estimates.\n",
    "* For very large `batch_size`, use:\n",
    "\n",
    "  * Learning rate scaling: `new_lr = base_lr * (new_batch / base_batch)`\n",
    "  * **Gradient accumulation** if GPU can't fit a large batch.\n",
    "  * Mixed-precision training (AMP) to reduce memory footprint.\n",
    "\n",
    "---\n",
    "\n",
    "**Code Snippet for Gradient Accumulation**\n",
    "\n",
    "```python\n",
    "accum_iter = 4  # simulate batch_size * 4\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y) / accum_iter\n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accum_iter == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107676ca-aa2a-4ef9-b250-674388c83c24",
   "metadata": {},
   "source": [
    "# **4. torch.backends.cudnn.benchmark**\n",
    "\n",
    "When you enable:\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmark = True\n",
    "```\n",
    "\n",
    "you tell cuDNN to:**Profile multiple convolution algorithms** at runtime.\n",
    "\n",
    "For every convolution layer (e.g., `Conv2d`), cuDNN has several possible algorithms (`GEMM`, `FFT`, `Winograd`, etc.) to execute it. Each has different performance depending on:\n",
    "\n",
    "* Input size\n",
    "* Kernel size\n",
    "* Stride\n",
    "* Padding\n",
    "* GPU architecture\n",
    "\n",
    "cuDNN **benchmarks all available algorithms** for your given layer configuration (on the first forward pass), **times them**, and then **caches the fastest one** for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "**How it speeds up training**\n",
    "\n",
    "If input sizes are **constant**, then cuDNN can:\n",
    "\n",
    "* **Choose the best kernel** once\n",
    "* **Avoid slower default algorithms**\n",
    "* **Reuse the fast kernel efficiently** across all batches\n",
    "\n",
    "This can lead to **significant performance gains** (10%–50% in some CNN-heavy models).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.1. Why is it **not** enabled by default?**\n",
    "\n",
    "\n",
    "\n",
    "**Benchmarking costs time**\n",
    "    * On first forward pass with new input size, **all candidate algorithms are tested**.\n",
    "    * This can cause a **noticeable delay** if input shapes keep changing (e.g., variable image sizes).\n",
    "    * Also, benchmarking might allocate **more memory** during evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2.When it's harmful:**\n",
    "\n",
    "| Scenario                                                    | Problem                                                     |\n",
    "| ----------------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| Input sizes vary (e.g. data augmentation, NLP with padding) | cuDNN must **re-benchmark every time**, which adds overhead |\n",
    "| You need **deterministic behavior**                         | Some fast algorithms are **non-deterministic**              |\n",
    "| You're running on constrained GPU memory                    | Benchmarking might cause **out-of-memory errors**           |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Optional Best Practice for Training Script**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True       # Enable fastest algorithm selection\n",
    "torch.backends.cudnn.deterministic = False  # (optional) For speed over reproducibility\n",
    "```\n",
    "\n",
    "And if you care about **reproducibility**, flip them:\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6066a-df53-4564-8c0f-f76632c9de50",
   "metadata": {},
   "source": [
    "# **5.torch.backends.cudnn.deterministic = True**\n",
    "\n",
    "It forces **cuDNN to use only deterministic algorithms**, meaning that **running your model multiple times with the same input will produce the exact same output** — **bit for bit** — every time. In practice, **deep learning on GPUs involves non-determinism** due to **parallel computation, low-level optimizations, and randomness**. There are several sources of non-determinism in training and inference:\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.1.cuDNN Algorithm Choice**\n",
    "\n",
    "cuDNN (and other GPU libraries) offer **multiple ways to compute convolutions**, pooling, etc. Some are non-deterministic:\n",
    "\n",
    "* They use **atomic operations** (whose order is not guaranteed)\n",
    "* The order of operations in **parallel threads** may vary → rounding errors differ → result differs slightly\n",
    "\n",
    "\n",
    "A **cuDNN kernel** refers to a **highly optimized GPU function** provided by NVIDIA’s cuDNN (CUDA Deep Neural Network) library, which accelerates **deep learning operations** on NVIDIA GPUs, such as:\n",
    "\n",
    "* Convolutions (forward & backward)\n",
    "* Pooling\n",
    "* Activation functions (ReLU, tanh, etc.)\n",
    "* Normalization (batch norm, LRN)\n",
    "* RNNs, LSTMs, GRUs\n",
    "* Tensor transformations\n",
    "\n",
    "---\n",
    "\n",
    "For example:\n",
    "\n",
    "* A 2D convolution used in a CNN might be executed using a cuDNN kernel that selects the best algorithm (e.g., Winograd, FFT, direct GEMM) based on input shapes and GPU architecture.\n",
    "* cuDNN will dynamically choose and launch the most efficient kernel for that specific workload.\n",
    "\n",
    "---\n",
    "\n",
    "**Example in Practice (PyTorch or TensorFlow):**\n",
    "\n",
    "When you run a convolution layer in PyTorch like:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "```\n",
    "\n",
    "under the hood, PyTorch (if CUDA is available) will use cuDNN to choose and run a convolution kernel optimized for your hardware.\n",
    "\n",
    "You can often see messages like:\n",
    "\n",
    "```\n",
    "Using cuDNN backend: conv2d_forward_algo_1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2.Random Number Generators (RNGs)**\n",
    "\n",
    "* Weight initialization\n",
    "* Data augmentation (flips, rotations)\n",
    "* Dropout\n",
    "* Shuffling of training data\n",
    "\n",
    "Unless you seed *all* RNGs (Python, NumPy, PyTorch), you'll get different results each time.\n",
    "\n",
    "#### **5.3.Multi-threading / CUDA kernel scheduling**\n",
    "\n",
    "* CUDA kernel execution order can vary slightly depending on GPU load or thread scheduling.\n",
    "* Even slight differences can **accumulate** during training (especially with float32).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4789b0e4-d65a-4fa9-a47b-a67f5ffcccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # don't search for fastest algo\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32, device='cuda')\n",
    "conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "\n",
    "# This will now always produce the same output\n",
    "y1 = conv(x)\n",
    "y2 = conv(x)\n",
    "\n",
    "print(torch.allclose(y1, y2)) # Will be True if weights are fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a34be-9f7b-4f58-90aa-ad30a9c29fdf",
   "metadata": {},
   "source": [
    "# **8. Prefetching with `prefetch_factor`**\n",
    "\n",
    "\n",
    "In PyTorch, **prefetching** is a technique that allows the `DataLoader` to **prepare data batches ahead of time**, so your model doesn’t have to wait for data. This is especially helpful when data loading (e.g., image decoding, transforms) is a bottleneck.\n",
    "\n",
    "**How `prefetch_factor` works:**\n",
    "\n",
    "* `prefetch_factor` is a parameter of `torch.utils.data.DataLoader`.\n",
    "* It determines how **many batches per worker** are preloaded **in advance**.\n",
    "* Only used when `num_workers > 0`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "If you set:\n",
    "\n",
    "```python\n",
    "DataLoader(..., num_workers=4, prefetch_factor=2)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* Each of the 4 workers will prefetch **2 batches**, so **8 batches total** are being prepared while the model trains on current data.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use it:**\n",
    "\n",
    "* Use `prefetch_factor > 2` **if your GPU is under-utilized** and **data loading is slow**.\n",
    "* Tune it along with `num_workers` to find the optimal setup.\n",
    "* Avoid setting it too high — it increases memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Example\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,          # Use multiple workers\n",
    "    prefetch_factor=4,      # Default is 2\n",
    "    pin_memory=True         # Speeds up transfer to GPU\n",
    ")\n",
    "\n",
    "# training loop\n",
    "for images, labels in loader:\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Tips for performance:\n",
    "\n",
    "| Parameter                 | Description                                               |\n",
    "| ------------------------- | --------------------------------------------------------- |\n",
    "| `num_workers`             | More workers = more parallel data loading                 |\n",
    "| `prefetch_factor`         | Higher = more preloaded batches (good for I/O-heavy data) |\n",
    "| `pin_memory=True`         | Use when transferring to CUDA                             |\n",
    "| `persistent_workers=True` | Keeps workers alive across epochs (PyTorch 1.7+)          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "To see the effect, use a timing tool like:\n",
    "\n",
    "```python\n",
    "import time\n",
    "start = time.time()\n",
    "for batch in loader:\n",
    "    pass\n",
    "print(\"Time:\", time.time() - start)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766c4b-d1f2-431a-ab2b-82da40a35d46",
   "metadata": {},
   "source": [
    "# **9. torch.compile(model)**\n",
    "\n",
    "`torch.compile(model)` is used to optimize and speed up the execution of your model by compiling it into a more efficient backend representation using **TorchDynamo**, **AOTAutograd**, and **Inductor** (by default). ---\n",
    "\n",
    "####  **9.1.Where to use `torch.compile(model)`**\n",
    "\n",
    "You typically apply it **after creating your model but before training or inference**:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18()\n",
    "model = model.to('cuda')  # or 'cpu'\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "# Then use compiled_model for training or inference\n",
    "```\n",
    "\n",
    "> Use `torch.compile()` once, ideally after model instantiation and before the training loop.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.2.What happens when you do `torch.compile(model)`?** \n",
    "\n",
    "PyTorch 2.0 introduces a compiler stack that includes:\n",
    "\n",
    "1. **TorchDynamo**: Captures Python bytecode of your model, intercepts tensor operations.\n",
    "2. **AOTAutograd**: Ahead-of-Time Autograd tracing for forward and backward passes.\n",
    "3. **Inductor**: Converts the traced graph into highly efficient C++/CUDA kernels.\n",
    "\n",
    "This process removes Python overhead and fuses operations, leading to much faster execution, especially for large models on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "####  **9.3.Where does the compiled code go?**\n",
    "\n",
    "* **In-memory**: By default, the compiled code is not written to disk — it’s **kept in memory** for runtime execution.\n",
    "* **Caching**: Some components (e.g., `torch._dynamo`) might cache intermediate results in RAM.\n",
    "* **Debugging**: You can inspect generated code with environment variables:\n",
    "\n",
    "  ```bash\n",
    "  TORCH_LOGS=\"dynamo\" python your_script.py\n",
    "  ```\n",
    "\n",
    "If you want to **export and save** compiled models, look into `torch.export`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.4.Things to be careful about** \n",
    "\n",
    "* Use it with **training or eval mode** set correctly before compiling (`model.eval()` or `model.train()`).\n",
    "* Some models or operations might not be fully supported (especially dynamic control flow).\n",
    "* You can toggle back to eager mode by calling:\n",
    "\n",
    "  ```python\n",
    "  model = compiled_model._orig_mod\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.5.When NOT to use it?** \n",
    "\n",
    "* Very small models with negligible Python overhead.\n",
    "* Highly dynamic models with control flows that resist optimization.\n",
    "* If you already use other tracing tools like TorchScript and want full control over the tracing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdfb20-e118-42ff-bc5a-0f63a01bb8ba",
   "metadata": {},
   "source": [
    "# **10. Use AMP (mixed-precision) training**\n",
    "\n",
    "In PyTorch, the **default data type** for tensors is:\n",
    "\n",
    "**`torch.float32` (i.e., `float`)**\n",
    "When you create a tensor like this:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94777b32-5778-44e0-86f7-8a8d5f95c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ffa8a-ad76-40ed-8311-8b5bab66186c",
   "metadata": {},
   "source": [
    "**AMP** stands for **Automatic Mixed Precision**. It's a technique that allows your model to use both **float32 (FP32)** and **float16 (FP16)** during training to **speed up computation and reduce memory usage** — **without significantly affecting model accuracy**.\n",
    "\n",
    "\n",
    "Normally, training uses 32-bit floating-point (float32) numbers. Mixed precision uses:\n",
    "\n",
    "* **float16 (half precision, dynamic range: ~1e-5 to 6e+4)** for most operations including `Conv`, `Linear`, `ReLU`, `matmul`, `activations` (faster, less memory)\n",
    "* **float32 (single precision, dynamic range: ~1e-38 to 1e+38)** for critical operations including `Loss`, `normalization`, `softmax`, `batchnorm` (to maintain accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "####  **10.1. Training loop using `autocast` and `GradScaler`**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast('cuda'):  # enables mixed precision\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    scaler.scale(loss).backward()          # scale the loss\n",
    "    scaler.step(optimizer)                 # unscale + step\n",
    "    scaler.update()                        # update scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**What Each Component Does**\n",
    "\n",
    "| Component      | Role                                                   |\n",
    "| -------------- | ------------------------------------------------------ |\n",
    "| `autocast()`   | Runs ops in FP16 when safe, otherwise in FP32          |\n",
    "| `GradScaler()` | Prevents underflow when backpropagating FP16 gradients |\n",
    "| `.scale(loss)` | Scales the loss for FP16 safe backward                 |\n",
    "| `.step()`      | Unscales gradients before optimizer step               |\n",
    "| `.update()`    | Adjusts scaling factor dynamically                     |\n",
    "\n",
    "---\n",
    "\n",
    "#### **10.2 Benefits/ Cons of Using AMP**\n",
    "\n",
    "Use AMP **whenever you're training on a GPU that supports it**, especially:\n",
    "\n",
    "* On **NVIDIA Volta, Turing, or Ampere** (e.g., RTX 30xx, A100)\n",
    "* With **large models** or **high-resolution inputs**\n",
    "* For **faster training + lower memory footprint**\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Benefits**\n",
    "\n",
    "*  **Faster training** on GPUs that support Tensor Cores (e.g., NVIDIA RTX/Volta/Ampere)\n",
    "*  **Reduced memory usage**, allowing larger batch sizes or models\n",
    "\n",
    "\n",
    "**❌ Cons**\n",
    "\n",
    "* Slight chance of numerical instability in rare cases\n",
    "* Not all ops are safe in FP16 — PyTorch handles most of this automatically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433ada0f-db52-4794-82c7-85ae4695a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "FP32 time: 4.290900707244873\n",
      "AMP time: 3.6157121658325195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "import time\n",
    "\n",
    "# Set device (automatically checks for CUDA)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 1024)\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scaler = GradScaler(device.type)  # 'cuda' or 'cpu'\n",
    "\n",
    "x = torch.randn(512, 1024, device=device)\n",
    "y = torch.randn(512, 1024, device=device)\n",
    "\n",
    "epochs=1000\n",
    "\n",
    "# Normal FP32\n",
    "start = time.time()\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"FP32 time:\", time.time() - start)\n",
    "\n",
    "# Mixed Precision (AMP) - Only works on CUDA\n",
    "if device.type == 'cuda':\n",
    "    start = time.time()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device.type):  # 'cuda' only\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    print(\"AMP time:\", time.time() - start)\n",
    "else:\n",
    "    print(\"AMP not supported on CPU, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33a64e",
   "metadata": {},
   "source": [
    "# **12. GPU Monitoring with GPUtil**\n",
    "\n",
    "**GPUtil** is a Python library that provides an easy interface to **monitor GPU usage in real-time** during training. This is essential for:\n",
    "\n",
    "- **Identifying bottlenecks** (is your GPU actually being used?)\n",
    "- **Optimizing batch sizes** (maximize GPU memory utilization)\n",
    "- **Monitoring temperature** and preventing overheating\n",
    "- **Tracking training efficiency** over time\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.1. Installation and Basic Usage**\n",
    "\n",
    "```bash\n",
    "pip install GPUtil psutil\n",
    "```\n",
    "\n",
    "**Basic GPU monitoring:**\n",
    "\n",
    "```python\n",
    "import GPUtil\n",
    "import psutil\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Get current GPU utilization\"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    \n",
    "    if gpus:\n",
    "        gpu = gpus[0]  # First GPU\n",
    "        return {\n",
    "            'name': gpu.name,\n",
    "            'utilization': gpu.load * 100,      # GPU utilization %\n",
    "            'memory_used': gpu.memoryUsed,      # Used memory in MB\n",
    "            'memory_total': gpu.memoryTotal,    # Total memory in MB\n",
    "            'memory_percent': gpu.memoryUtil * 100,  # Memory usage %\n",
    "            'temperature': gpu.temperature       # Temperature in °C\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "gpu_info = get_gpu_info()\n",
    "if gpu_info:\n",
    "    print(f\"GPU: {gpu_info['name']}\")\n",
    "    print(f\"Utilization: {gpu_info['utilization']:.1f}%\")\n",
    "    print(f\"Memory: {gpu_info['memory_used']}/{gpu_info['memory_total']}MB ({gpu_info['memory_percent']:.1f}%)\")\n",
    "    print(f\"Temperature: {gpu_info['temperature']}°C\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.2. Integration with Training Loop**\n",
    "\n",
    "Here's how to integrate GPU monitoring into your training:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import GPUtil\n",
    "import wandb  # For logging\n",
    "\n",
    "def train_with_monitoring(model, dataloader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Monitor GPU every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                gpu_info = get_gpu_info()\n",
    "                if gpu_info:\n",
    "                    print(f\"Batch {batch_idx} | \"\n",
    "                          f\"Loss: {loss.item():.4f} | \"\n",
    "                          f\"GPU: {gpu_info['utilization']:.1f}% | \"\n",
    "                          f\"VRAM: {gpu_info['memory_percent']:.1f}%\")\n",
    "                    \n",
    "                    # Log to wandb\n",
    "                    wandb.log({\n",
    "                        'batch_loss': loss.item(),\n",
    "                        'gpu_utilization': gpu_info['utilization'],\n",
    "                        'gpu_memory_percent': gpu_info['memory_percent'],\n",
    "                        'gpu_temperature': gpu_info['temperature']\n",
    "                    })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed | Avg Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72419c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical GPUtil Demo - Monitor your GPU right now!\n",
    "import GPUtil\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def print_system_info():\n",
    "    \"\"\"Print comprehensive system information\"\"\"\n",
    "    print(\"🖥️  SYSTEM HARDWARE INFORMATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CPU Information\n",
    "    cpu_count = psutil.cpu_count(logical=True)\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"💻 CPU: {cpu_count} cores @ {cpu_percent:.1f}% usage\")\n",
    "    \n",
    "    # Memory Information\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_gb = memory.total / (1024**3)\n",
    "    print(f\"🧠 RAM: {memory_gb:.1f}GB total, {memory.percent:.1f}% used\")\n",
    "    \n",
    "    # GPU Information\n",
    "    print(f\"\\n🚀 GPU INFORMATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✅ CUDA Available: {torch.version.cuda}\")\n",
    "        print(f\"🎯 PyTorch CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "        \n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            \n",
    "            if gpus:\n",
    "                for i, gpu in enumerate(gpus):\n",
    "                    print(f\"\\nGPU {i}: {gpu.name}\")\n",
    "                    print(f\"  📊 Utilization: {gpu.load * 100:.1f}%\")\n",
    "                    print(f\"  🧠 Memory: {gpu.memoryUsed:.0f}/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil * 100:.1f}%)\")\n",
    "                    print(f\"  💾 Free Memory: {gpu.memoryTotal - gpu.memoryUsed:.0f}MB\")\n",
    "                    print(f\"  🌡️  Temperature: {gpu.temperature}°C\")\n",
    "                    print(f\"  🆔 UUID: {gpu.uuid}\")\n",
    "                    \n",
    "                    # Memory recommendations\n",
    "                    free_memory = gpu.memoryTotal - gpu.memoryUsed\n",
    "                    if free_memory > 3000:\n",
    "                        print(f\"  💡 Recommendation: You can use large batch sizes (128+)\")\n",
    "                    elif free_memory > 1500:\n",
    "                        print(f\"  💡 Recommendation: Use medium batch sizes (64-128)\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️  Warning: Limited memory, use small batch sizes (16-32)\")\n",
    "            else:\n",
    "                print(\"❌ No GPUs detected by GPUtil\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error accessing GPU info: {e}\")\n",
    "    else:\n",
    "        print(\"❌ CUDA not available - using CPU only\")\n",
    "\n",
    "# Run the system check\n",
    "print_system_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376194d",
   "metadata": {},
   "source": [
    "#### **12.3. Real-time GPU Monitoring During Training**\n",
    "\n",
    "Here's a practical example that monitors GPU usage while training a model:\n",
    "\n",
    "```python\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "def monitor_training_with_gpu(model, dataloader, optimizer, criterion, epochs=3):\n",
    "    \"\"\"Train model while monitoring GPU utilization\"\"\"\n",
    "    \n",
    "    # Storage for monitoring data\n",
    "    gpu_utilization = deque(maxlen=100)\n",
    "    gpu_memory = deque(maxlen=100)\n",
    "    gpu_temp = deque(maxlen=100)\n",
    "    timestamps = deque(maxlen=100)\n",
    "    \n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"🔥 Training with GPU Monitoring\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Epoch | Batch | Loss   | GPU%  | VRAM% | Temp°C | Time\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            # Training step\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Monitor GPU every batch\n",
    "            gpu_info = get_gpu_info()\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            if gpu_info:\n",
    "                gpu_utilization.append(gpu_info['utilization'])\n",
    "                gpu_memory.append(gpu_info['memory_percent'])\n",
    "                gpu_temp.append(gpu_info['temperature'])\n",
    "                timestamps.append(current_time)\n",
    "                \n",
    "                # Print every 5 batches\n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f\"{epoch+1:5d} | {batch_idx:5d} | {loss.item():.3f} | \"\n",
    "                          f\"{gpu_info['utilization']:4.1f} | {gpu_info['memory_percent']:5.1f} | \"\n",
    "                          f\"{gpu_info['temperature']:6.1f} | {current_time:6.1f}s\")\n",
    "    \n",
    "    return list(gpu_utilization), list(gpu_memory), list(gpu_temp), list(timestamps)\n",
    "\n",
    "# Example usage (uncomment to run with your data)\n",
    "# gpu_util, gpu_mem, gpu_temp, times = monitor_training_with_gpu(model, train_loader, optimizer, criterion)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.4. GPU Memory Optimization Helper**\n",
    "\n",
    "```python\n",
    "def optimize_batch_size(model, sample_input, max_memory_percent=90):\n",
    "    \"\"\"\n",
    "    Automatically find the optimal batch size for your GPU\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 32  # Default for CPU\n",
    "    \n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    # Start with small batch size\n",
    "    batch_size = 16\n",
    "    optimal_batch_size = 16\n",
    "    \n",
    "    print(\"🔍 Finding optimal batch size...\")\n",
    "    print(\"Batch Size | GPU Memory % | Status\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    while batch_size <= 512:  # Max reasonable batch size\n",
    "        try:\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Create batch\n",
    "            if len(sample_input.shape) == 4:  # Image data (B, C, H, W)\n",
    "                batch = sample_input[:1].repeat(batch_size, 1, 1, 1).cuda()\n",
    "            else:  # Other data\n",
    "                batch = sample_input[:1].repeat(batch_size, 1).cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = model(batch)\n",
    "            \n",
    "            # Check GPU memory\n",
    "            gpu_info = get_gpu_info()\n",
    "            if gpu_info:\n",
    "                memory_percent = gpu_info['memory_percent']\n",
    "                print(f\"{batch_size:10d} | {memory_percent:11.1f}% | \", end=\"\")\n",
    "                \n",
    "                if memory_percent < max_memory_percent:\n",
    "                    optimal_batch_size = batch_size\n",
    "                    print(\"✅ Good\")\n",
    "                    batch_size *= 2  # Try larger\n",
    "                else:\n",
    "                    print(\"❌ Too high\")\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"{batch_size:10d} | {'OOM':>11s} | ❌ Out of Memory\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n💡 Recommended batch size: {optimal_batch_size}\")\n",
    "    return optimal_batch_size\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a330f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the system information check\n",
    "print_system_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42494c5",
   "metadata": {},
   "source": [
    "# **13. Complete GPU Optimization Pipeline**\n",
    "\n",
    "Based on the Brain Cancer MRI project implementation, here's a **complete optimization pipeline** that maximizes GPU utilization:\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.1. Model-Specific Batch Size Optimization**\n",
    "\n",
    "Different models have different memory requirements. Here's how to configure optimal batch sizes:\n",
    "\n",
    "```python\n",
    "# Model-specific configurations for RTX 3050 (4GB VRAM)\n",
    "MODEL_CONFIGS = {\n",
    "    'resnet18': {'batch_size': 128, 'lr': 0.001},      # Lightweight CNN\n",
    "    'resnet50': {'batch_size': 64, 'lr': 0.001},       # Deeper CNN  \n",
    "    'efficientnet_b0': {'batch_size': 128, 'lr': 0.001}, # Efficient CNN\n",
    "    'swin_t': {'batch_size': 32, 'lr': 0.0001},        # Transformer\n",
    "    'vit_b_16': {'batch_size': 16, 'lr': 0.0001},      # Large transformer\n",
    "}\n",
    "\n",
    "def get_optimal_config(model_name, gpu_memory_gb):\n",
    "    \"\"\"Get optimal configuration based on model and GPU memory\"\"\"\n",
    "    base_config = MODEL_CONFIGS.get(model_name, {'batch_size': 32, 'lr': 0.001})\n",
    "    \n",
    "    # Scale batch size based on available GPU memory\n",
    "    memory_scale = gpu_memory_gb / 4.0  # Baseline: 4GB\n",
    "    optimal_batch = int(base_config['batch_size'] * memory_scale)\n",
    "    \n",
    "    return {\n",
    "        'batch_size': optimal_batch,\n",
    "        'lr': base_config['lr']\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.2. Complete Training Setup with All Optimizations**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import GPUtil\n",
    "\n",
    "def setup_optimized_training(model, train_dataset, val_dataset, config):\n",
    "    \"\"\"Setup training with maximum GPU utilization\"\"\"\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Performance optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes\n",
    "        \n",
    "        # Memory format optimization\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    # Model compilation (PyTorch 2.0+)\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"✅ Model compiled for faster training\")\n",
    "    except:\n",
    "        print(\"⚠️  Model compilation not available\")\n",
    "    \n",
    "    # Optimized data loading\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': config['batch_size'],\n",
    "        'num_workers': 8,  # Use all CPU cores\n",
    "        'pin_memory': True,\n",
    "        'prefetch_factor': 4,\n",
    "        'persistent_workers': True,\n",
    "        'drop_last': True  # Ensures consistent batch sizes\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **dataloader_kwargs)\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, train_loader, val_loader, optimizer, criterion, scaler, device\n",
    "\n",
    "# Usage example\n",
    "# model, train_loader, val_loader, optimizer, criterion, scaler, device = setup_optimized_training(\n",
    "#     model, train_dataset, val_dataset, config\n",
    "# )\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b271100",
   "metadata": {},
   "source": [
    "#### **13.3. Advanced GPU Utilization Techniques**\n",
    "\n",
    "Here are additional techniques for maximizing GPU performance:\n",
    "\n",
    "---\n",
    "\n",
    "**🔥 Gradient Accumulation for Larger Effective Batch Sizes**\n",
    "\n",
    "When GPU memory limits your batch size, use gradient accumulation:\n",
    "\n",
    "```python\n",
    "def train_with_gradient_accumulation(model, dataloader, optimizer, criterion, \n",
    "                                   accumulation_steps=4, use_amp=True):\n",
    "    \"\"\"Training with gradient accumulation to simulate larger batch sizes\"\"\"\n",
    "    \n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # Normalize loss by accumulation steps\n",
    "        with autocast() if use_amp else nullcontext():\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) / accumulation_steps\n",
    "        \n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            if use_amp:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Monitor GPU\n",
    "            gpu_info = get_gpu_info()\n",
    "            if gpu_info:\n",
    "                print(f\"Step {batch_idx//accumulation_steps} | \"\n",
    "                      f\"Loss: {loss.item()*accumulation_steps:.4f} | \"\n",
    "                      f\"GPU: {gpu_info['utilization']:.1f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**⚡ Dynamic Batch Size Scaling**\n",
    "\n",
    "Automatically adjust batch size based on available GPU memory:\n",
    "\n",
    "```python\n",
    "def dynamic_batch_scaling(model, dataset, target_memory_percent=85):\n",
    "    \"\"\"Dynamically scale batch size based on available GPU memory\"\"\"\n",
    "    \n",
    "    gpu_info = get_gpu_info()\n",
    "    if not gpu_info:\n",
    "        return 32\n",
    "    \n",
    "    available_memory = gpu_info['memory_total'] - gpu_info['memory_used']\n",
    "    target_memory = gpu_info['memory_total'] * (target_memory_percent / 100)\n",
    "    \n",
    "    # Estimate memory per sample (rough approximation)\n",
    "    sample_memory_mb = 10  # Adjust based on your data\n",
    "    max_batch_size = int(target_memory / sample_memory_mb)\n",
    "    \n",
    "    # Find optimal batch size through binary search\n",
    "    optimal_batch = optimize_batch_size(model, dataset[0][0].unsqueeze(0), target_memory_percent)\n",
    "    \n",
    "    return min(optimal_batch, max_batch_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**🌡️ Thermal Throttling Prevention**\n",
    "\n",
    "Monitor and prevent GPU overheating:\n",
    "\n",
    "```python\n",
    "def check_thermal_throttling():\n",
    "    \"\"\"Check if GPU is thermal throttling\"\"\"\n",
    "    gpu_info = get_gpu_info()\n",
    "    \n",
    "    if gpu_info:\n",
    "        temp = gpu_info['temperature']\n",
    "        \n",
    "        if temp > 83:  # RTX 3050 throttling threshold\n",
    "            print(f\"🔥 WARNING: GPU temperature high ({temp}°C)\")\n",
    "            print(\"💡 Consider: reducing batch size, improving cooling, or taking breaks\")\n",
    "            return True\n",
    "        elif temp > 75:\n",
    "            print(f\"⚠️  GPU temperature elevated ({temp}°C)\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"✅ GPU temperature normal ({temp}°C)\")\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Usage in training loop\n",
    "if check_thermal_throttling():\n",
    "    time.sleep(30)  # Cool down period\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad057e55",
   "metadata": {},
   "source": [
    "# **14. Comprehensive GPU Monitoring Dashboard**\n",
    "\n",
    "Create a real-time monitoring dashboard for your training:\n",
    "\n",
    "---\n",
    "\n",
    "#### **14.1. Real-time GPU Monitoring Class**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class GPUMonitor:\n",
    "    \"\"\"Real-time GPU monitoring during training\"\"\"\n",
    "    \n",
    "    def __init__(self, update_interval=2):\n",
    "        self.update_interval = update_interval\n",
    "        self.monitoring = False\n",
    "        self.data = {\n",
    "            'timestamps': [],\n",
    "            'gpu_utilization': [],\n",
    "            'gpu_memory': [],\n",
    "            'gpu_temperature': [],\n",
    "            'cpu_percent': [],\n",
    "            'ram_percent': []\n",
    "        }\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start background monitoring thread\"\"\"\n",
    "        self.monitoring = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.monitor_thread.daemon = True\n",
    "        self.monitor_thread.start()\n",
    "        print(\"🔍 GPU monitoring started...\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop monitoring and show final plot\"\"\"\n",
    "        self.monitoring = False\n",
    "        if hasattr(self, 'monitor_thread'):\n",
    "            self.monitor_thread.join()\n",
    "        print(\"⏹️  GPU monitoring stopped\")\n",
    "        self.plot_results()\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Background monitoring loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.monitoring:\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            # Get GPU info\n",
    "            gpu_info = get_gpu_info()\n",
    "            \n",
    "            # Get CPU/RAM info\n",
    "            cpu_percent = psutil.cpu_percent(interval=None)\n",
    "            ram_percent = psutil.virtual_memory().percent\n",
    "            \n",
    "            # Store data\n",
    "            self.data['timestamps'].append(current_time)\n",
    "            self.data['cpu_percent'].append(cpu_percent)\n",
    "            self.data['ram_percent'].append(ram_percent)\n",
    "            \n",
    "            if gpu_info:\n",
    "                self.data['gpu_utilization'].append(gpu_info['utilization'])\n",
    "                self.data['gpu_memory'].append(gpu_info['memory_percent'])\n",
    "                self.data['gpu_temperature'].append(gpu_info['temperature'])\n",
    "            else:\n",
    "                self.data['gpu_utilization'].append(0)\n",
    "                self.data['gpu_memory'].append(0)\n",
    "                self.data['gpu_temperature'].append(0)\n",
    "            \n",
    "            time.sleep(self.update_interval)\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot monitoring results\"\"\"\n",
    "        if not self.data['timestamps']:\n",
    "            print(\"No data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        times = self.data['timestamps']\n",
    "        \n",
    "        # GPU Utilization\n",
    "        ax1.plot(times, self.data['gpu_utilization'], 'g-', linewidth=2)\n",
    "        ax1.set_title('🚀 GPU Utilization (%)')\n",
    "        ax1.set_ylabel('Utilization %')\n",
    "        ax1.grid(True)\n",
    "        ax1.set_ylim(0, 100)\n",
    "        \n",
    "        # GPU Memory\n",
    "        ax2.plot(times, self.data['gpu_memory'], 'b-', linewidth=2)\n",
    "        ax2.set_title('🧠 GPU Memory Usage (%)')\n",
    "        ax2.set_ylabel('Memory %')\n",
    "        ax2.grid(True)\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        # GPU Temperature\n",
    "        ax3.plot(times, self.data['gpu_temperature'], 'r-', linewidth=2)\n",
    "        ax3.set_title('🌡️ GPU Temperature (°C)')\n",
    "        ax3.set_ylabel('Temperature °C')\n",
    "        ax3.set_xlabel('Time (seconds)')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # CPU and RAM\n",
    "        ax4.plot(times, self.data['cpu_percent'], 'orange', label='CPU %', linewidth=2)\n",
    "        ax4.plot(times, self.data['ram_percent'], 'purple', label='RAM %', linewidth=2)\n",
    "        ax4.set_title('💻 System Resources')\n",
    "        ax4.set_ylabel('Usage %')\n",
    "        ax4.set_xlabel('Time (seconds)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "        ax4.set_ylim(0, 100)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        if self.data['gpu_utilization']:\n",
    "            avg_gpu = sum(self.data['gpu_utilization']) / len(self.data['gpu_utilization'])\n",
    "            max_gpu = max(self.data['gpu_utilization'])\n",
    "            avg_temp = sum(self.data['gpu_temperature']) / len(self.data['gpu_temperature'])\n",
    "            max_temp = max(self.data['gpu_temperature'])\n",
    "            \n",
    "            print(f\"\\n📊 MONITORING SUMMARY:\")\n",
    "            print(f\"🚀 Average GPU Utilization: {avg_gpu:.1f}%\")\n",
    "            print(f\"🔥 Peak GPU Utilization: {max_gpu:.1f}%\")\n",
    "            print(f\"🌡️  Average Temperature: {avg_temp:.1f}°C\")\n",
    "            print(f\"🔥 Peak Temperature: {max_temp:.1f}°C\")\n",
    "\n",
    "# Usage example:\n",
    "# monitor = GPUMonitor()\n",
    "# monitor.start_monitoring()\n",
    "# \n",
    "# # Your training code here\n",
    "# train_model()\n",
    "# \n",
    "# monitor.stop_monitoring()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9604c1",
   "metadata": {},
   "source": [
    "# **15. Updated Optimization Checklist with GPUtil**\n",
    "\n",
    "Here's an **enhanced checklist** with GPUtil integration and the latest optimization techniques:\n",
    "\n",
    "---\n",
    "\n",
    "| # | Technique | Expected Impact | Implementation | GPUtil Usage |\n",
    "|---|-----------|----------------|----------------|--------------|\n",
    "| **1** | **Model-specific batch sizes** | **Very High** (2-4x throughput) | Use optimized batch sizes per model | Monitor memory usage |\n",
    "| **2** | **Mixed Precision (AMP)** | **Very High** (2x speed, 50% memory) | `autocast()` + `GradScaler()` | Track memory savings |\n",
    "| **3** | **PyTorch 2.0 Compilation** | **High** (20-30% speedup) | `torch.compile(model)` | Monitor utilization increase |\n",
    "| **4** | **Optimized DataLoader** | **High** (20-50% if I/O bound) | 8 workers + prefetching | Track CPU usage |\n",
    "| **5** | **GPU Memory Optimization** | **High** | `channels_last` + `pin_memory` | Monitor memory efficiency |\n",
    "| **6** | **CUDNN Benchmarking** | **Moderate** (5-15%) | `cudnn.benchmark=True` | Verify consistent utilization |\n",
    "| **7** | **Gradient Accumulation** | **High** (larger effective batches) | Accumulate over multiple steps | Monitor during accumulation |\n",
    "| **8** | **Thermal Management** | **Critical** (prevents throttling) | Monitor temperature | Real-time temp tracking |\n",
    "| **9** | **Hardware Monitoring** | **Essential** (bottleneck identification) | GPUtil + psutil | Continuous monitoring |\n",
    "| **10** | **Dynamic Scaling** | **Moderate** (adaptive optimization) | Auto-adjust based on resources | Memory-based scaling |\n",
    "\n",
    "---\n",
    "\n",
    "#### **15.1. Quick GPU Health Check**\n",
    "\n",
    "Before starting any training, run this health check:\n",
    "\n",
    "```python\n",
    "def gpu_health_check():\n",
    "    \"\"\"Comprehensive GPU health and optimization check\"\"\"\n",
    "    print(\"🏥 GPU HEALTH CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"❌ CUDA not available\")\n",
    "        return False\n",
    "    \n",
    "    # GPU information\n",
    "    gpu_info = get_gpu_info()\n",
    "    if not gpu_info:\n",
    "        print(\"❌ No GPU detected by GPUtil\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✅ GPU: {gpu_info['name']}\")\n",
    "    \n",
    "    # Temperature check\n",
    "    temp = gpu_info['temperature']\n",
    "    if temp > 80:\n",
    "        print(f\"🔥 WARNING: High temperature ({temp}°C)\")\n",
    "    else:\n",
    "        print(f\"✅ Temperature: {temp}°C\")\n",
    "    \n",
    "    # Memory check\n",
    "    memory_percent = gpu_info['memory_percent']\n",
    "    free_memory = gpu_info['memory_total'] - gpu_info['memory_used']\n",
    "    \n",
    "    print(f\"💾 Memory: {gpu_info['memory_used']:.0f}/{gpu_info['memory_total']:.0f}MB ({memory_percent:.1f}%)\")\n",
    "    print(f\"💾 Free Memory: {free_memory:.0f}MB\")\n",
    "    \n",
    "    # Batch size recommendations\n",
    "    if free_memory > 3000:\n",
    "        recommended_batch = \"128+\"\n",
    "        print(\"💡 Recommendation: Large batch sizes (128+)\")\n",
    "    elif free_memory > 1500:\n",
    "        recommended_batch = \"64-128\"\n",
    "        print(\"💡 Recommendation: Medium batch sizes (64-128)\")\n",
    "    else:\n",
    "        recommended_batch = \"16-32\"\n",
    "        print(\"⚠️  Recommendation: Small batch sizes (16-32)\")\n",
    "    \n",
    "    # Utilization check\n",
    "    utilization = gpu_info['utilization']\n",
    "    if utilization < 10:\n",
    "        print(\"⚠️  Low GPU utilization - consider increasing batch size or checking bottlenecks\")\n",
    "    elif utilization > 90:\n",
    "        print(\"✅ Excellent GPU utilization!\")\n",
    "    else:\n",
    "        print(f\"📊 GPU utilization: {utilization:.1f}%\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the health check\n",
    "gpu_health_check()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **15.2. Training Optimization Workflow**\n",
    "\n",
    "Here's the complete workflow for maximizing GPU utilization:\n",
    "\n",
    "```python\n",
    "def optimize_training_pipeline(model_name, dataset):\n",
    "    \"\"\"Complete optimization pipeline\"\"\"\n",
    "    \n",
    "    print(\"🔧 TRAINING OPTIMIZATION PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Check GPU health\n",
    "    if not gpu_health_check():\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Get optimal configuration\n",
    "    gpu_info = get_gpu_info()\n",
    "    gpu_memory_gb = gpu_info['memory_total'] / 1024\n",
    "    config = get_optimal_config(model_name, gpu_memory_gb)\n",
    "    \n",
    "    print(f\"\\n⚙️  Optimal Configuration:\")\n",
    "    print(f\"   📦 Batch Size: {config['batch_size']}\")\n",
    "    print(f\"   📈 Learning Rate: {config['lr']}\")\n",
    "    \n",
    "    # Step 3: Setup monitoring\n",
    "    monitor = GPUMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Step 4: Setup optimized training\n",
    "    # (Your training setup code here)\n",
    "    \n",
    "    # Step 5: Training with monitoring\n",
    "    print(\"\\n🚀 Starting optimized training...\")\n",
    "    \n",
    "    # Your training loop here\n",
    "    # train_model_with_optimizations()\n",
    "    \n",
    "    # Step 6: Stop monitoring and analyze\n",
    "    monitor.stop_monitoring()\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Example usage:\n",
    "# config = optimize_training_pipeline('resnet18', your_dataset)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852e705-a55b-44e0-a4be-3adac44e271e",
   "metadata": {},
   "source": [
    "# **11.`torch.jit.script` and `torch.jit.trace`**\n",
    "\n",
    "In PyTorch, `torch.jit.script` and `torch.jit.trace` are part of **TorchScript**, a way to convert PyTorch models into a **serializable and optimizable intermediate representation**. This can improve inference speed, allow deployment in C++ environments, and enable graph-level optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "**Two Ways to Create TorchScript Models**\n",
    "\n",
    "| Method             | Use When...                                                              |\n",
    "| ------------------ | ------------------------------------------------------------------------ |\n",
    "| `torch.jit.trace`  | The model is **static**, i.e., no control flow depending on input values |\n",
    "| `torch.jit.script` | The model has **conditionals, loops, or input-dependent control flow**   |\n",
    "\n",
    "---\n",
    "\n",
    "####  **11.1. `torch.jit.trace` Example (Static Model)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MyModel()\n",
    "example_input = torch.randn(1, 10)\n",
    "\n",
    "# Traced model\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"traced_model.pt\")  # Save\n",
    "```\n",
    "\n",
    "> ⚠️ Use this **only** if the model’s computation graph does **not depend on input data** (e.g., no `if`, `for`).\n",
    "\n",
    "---\n",
    "\n",
    "####  **11.2. `torch.jit.script` Example (Dynamic Control Flow)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:  # Dynamic condition\n",
    "            return x * 2\n",
    "        else:\n",
    "            return x - 2\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Scripted model\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"scripted_model.pt\")  # Save\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Benefits of `torch.jit.script` and `torch.jit.trace`**\n",
    "\n",
    "1. **Speed**: TorchScript compiles models to an optimized graph, often faster for inference (especially on CPU).\n",
    "2. **Deployment**: Models can be loaded in C++ via LibTorch.\n",
    "3. **Serialization**: You can save and load complete models easily (`.pt` format).\n",
    "4. **Cross-platform**: Useful for mobile (PyTorch Mobile).\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Benchmark Example (Speed Comparison)\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "x = torch.randn(1000, 10)\n",
    "\n",
    "# Eager model\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    model(x)\n",
    "print(\"Eager time:\", time.time() - start)\n",
    "\n",
    "# TorchScript model\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    traced_model(x)\n",
    "print(\"TorchScript time:\", time.time() - start)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
