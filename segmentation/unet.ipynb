{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262c1578-41d1-4bd9-88f1-aac61f2019bb",
   "metadata": {},
   "source": [
    "## **Standard U-Net Channel Flow (base=64):**\n",
    "\n",
    "### **Encoder Flow:**\n",
    "\n",
    "| Level | Input | Conv1 Output | Conv2 Output | Pool Output | Spatial |\n",
    "|-------|-------|--------------|--------------|-------------|---------|\n",
    "| **Input** | 3 | - | - | - | 256×256 |\n",
    "| **E1** | 3 | → **64** | → **64** | → **64** | 256×256 → 128×128 |\n",
    "| **E2** | 64 | → **128** | → **128** | → **128** | 128×128 → 64×64 |\n",
    "| **E3** | 128 | → **256** | → **256** | → **256** | 64×64 → 32×32 |\n",
    "| **E4** | 256 | → **512** | → **512** | → **512** | 32×32 → 16×16 |\n",
    "| **Bottleneck** | 512 | → **1024** | → **1024** | - | 16×16 |\n",
    "\n",
    "### **Decoder Flow:**\n",
    "\n",
    "| Level | Input | Upsample | Concat Skip | Conv1 Output | Conv2 Output | Spatial |\n",
    "|-------|-------|----------|-------------|--------------|--------------|---------|\n",
    "| **D4** | 1024 | → 512 | +512 = 1024 | → **512** | → **512** | 32×32 |\n",
    "| **D3** | 512 | → 256 | +256 = 512 | → **256** | → **256** | 64×64 |\n",
    "| **D2** | 256 | → 128 | +128 = 256 | → **128** | → **128** | 128×128 |\n",
    "| **D1** | 128 | → 64 | +64 = 128 | → **64** | → **64** | 256×256 |\n",
    "| **Final** | 64 | - | - | → **num_classes** | - | 256×256 |\n",
    "\n",
    "## **Complete Architecture Visualization:**\n",
    "\n",
    "```\n",
    "Input: 256×256×3\n",
    "  ↓\n",
    "[E1] 256×256×64 ──┐\n",
    "  ↓ pool          │ Skip Connection\n",
    "128×128×64        │\n",
    "  ↓               │\n",
    "[E2] 128×128×128 ─┼──┐\n",
    "  ↓ pool          │  │ Skip Connection  \n",
    "64×64×128         │  │\n",
    "  ↓               │  │\n",
    "[E3] 64×64×256  ──┼──┼──┐\n",
    "  ↓ pool          │  │  │ Skip Connection\n",
    "32×32×256         │  │  │\n",
    "  ↓               │  │  │\n",
    "[E4] 32×32×512  ──┼──┼──┼──┐\n",
    "  ↓ pool          │  │  │  │ Skip Connection\n",
    "16×16×512         │  │  │  │\n",
    "  ↓               │  │  │  │\n",
    "[Bottleneck] 16×16×1024\n",
    "  ↓\n",
    "[D4] 32×32×512  ←──┼──┼──┼── (Upsample + Concat)\n",
    "  ↓                │  │  │\n",
    "[D3] 64×64×256  ←──┼──┼── (Upsample + Concat)\n",
    "  ↓                │  │\n",
    "[D2] 128×128×128 ←─┼── (Upsample + Concat)\n",
    "  ↓                │\n",
    "[D1] 256×256×64  ←── (Upsample + Concat)\n",
    "  ↓\n",
    "[Final] 256×256×num_classes\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffe3ec-a6e6-4cb1-a4c2-61f573680d72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Bilinear upsampling\n",
    "\n",
    "A **non-learnable interpolation** — it uses bilinear interpolation to enlarge the feature map.\n",
    "\n",
    "```python\n",
    "x_up = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "Each new pixel value is a *weighted average* of the four nearest pixels in the original feature map.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Very cheap in memory and compute.\n",
    "* Smooth, artifact-free (no checkerboard patterns).\n",
    "* Works great when skip connections provide enough spatial detail.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Fixed interpolation → no learnable parameters.\n",
    "* Can slightly blur sharp boundaries if used alone (without skip fusion).\n",
    "\n",
    "**Best for:**\n",
    "\n",
    "* U-Nets with strong skip connections (like DepthNet).\n",
    "* Lightweight models (4 GB GPU-friendly).\n",
    "* Tasks like depth, segmentation, or flow where smooth outputs are desired.\n",
    "\n",
    "---\n",
    "\n",
    "### Deconvolution (Transposed Convolution)\n",
    "\n",
    "\n",
    "A **learnable upsampling layer**, implemented as `ConvTranspose2d`.\n",
    "\n",
    "```python\n",
    "self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "It “spreads out” the input feature map and learns weights that interpolate between pixels.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Learnable → can adapt to texture and structure.\n",
    "* Can produce sharper details if trained well.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* More parameters and memory usage.\n",
    "* Can create **checkerboard artifacts** if kernel/stride aren’t handled carefully.\n",
    "\n",
    "**Best for:**\n",
    "\n",
    "* When you want the model to *learn* the upsampling itself (e.g., GANs, super-resolution).\n",
    "* When skip connections are weak or missing.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4be47-82a5-4b4e-b0dc-dabb98a345a7",
   "metadata": {},
   "source": [
    "### What skip connections actually do\n",
    "\n",
    "In a U-Net or encoder–decoder, each encoder block **reduces spatial resolution** but **increases semantic abstraction**:\n",
    "\n",
    "```\n",
    "Input → [H,W,3]\n",
    "↓\n",
    "Encoder stage 1 → [H/2,W/2,64]\n",
    "↓\n",
    "Encoder stage 4 → [H/32,W/32,512]\n",
    "```\n",
    "\n",
    "To reconstruct a dense output (like depth), the decoder upsamples these low-resolution features.\n",
    "But low-res features alone lose local details (edges, boundaries).\n",
    "**Skip connections** fix that by directly connecting encoder feature maps to decoder stages at matching scales:\n",
    "\n",
    "```\n",
    "Encoder x4 --------------→ Decoder d4\n",
    "Encoder x3 --------------→ Decoder d3\n",
    "Encoder x2 --------------→ Decoder d2\n",
    "Encoder x1 --------------→ Decoder d1\n",
    "```\n",
    "\n",
    "So, during upsampling, the decoder fuses **semantic information (from deep layers)** and **spatial detail (from shallow layers)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### What makes a “strong” vs. “weak” skip connection?\n",
    "\n",
    "Let’s define it conceptually and then give examples.\n",
    "\n",
    "| Type            | Meaning                                                                                                                     | Effect                                                                                             |\n",
    "| --------------- | --------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **Strong skip** | The skip features contain rich **spatial detail** that the decoder can directly use to reconstruct sharp boundaries.        | Decoder doesn’t need to “learn” how to upsample spatial structure → Bilinear upsampling is enough. |\n",
    "| **Weak skip**   | The skip features are **semantically abstract** (low-frequency, little texture) or have **mismatched resolution/channels**. | Decoder must learn to recover detail on its own → learnable upsampling (e.g., deconv) helps.       |\n",
    "\n",
    "---\n",
    "\n",
    "#### Strong skip connection examples\n",
    "\n",
    "* **U-Net / ResNet encoders with early feature taps**\n",
    "\n",
    "  * E.g., connecting `conv1`, `layer1`, `layer2`, `layer3`.\n",
    "  * These are high-resolution and carry fine gradients and textures.\n",
    "  * Decoder can simply **interpolate (bilinear)** and merge; no need to learn spatial mapping.\n",
    "  * → Produces **smooth**, **artifact-free**, and **memory-light** results.\n",
    "\n",
    " Your `ResNetUNet` uses **strong skip connections** → each decoder level gets spatially aligned features directly from the encoder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Weak skip connection examples\n",
    "\n",
    "* Architectures like **DeepLabV3+**, **SegNet**, or **Transformer encoders (Swin, ViT)**:\n",
    "\n",
    "  * Only output low-res tokens or patch embeddings.\n",
    "  * Skip features may be coarse (e.g., stride 16 or 32 only).\n",
    "  * Spatial alignment is implicit, not explicit (need reshaping or upsampling).\n",
    "  * Decoder has to **learn** spatial reconstruction → deconv layers, up-convs, or learned attention.\n",
    "\n",
    "Example:\n",
    "In **Vision Transformers**, tokens don’t carry precise pixel positions, so you need learnable upsampling or fusion modules → **weak skip connections**.\n",
    "\n",
    "---\n",
    "\n",
    "### How skip connection strength affects your design choices\n",
    "\n",
    "| Aspect        | Strong skips (U-Net, ResNet) | Weak skips (Transformer, deep CNN) |\n",
    "| ------------- | ---------------------------- | ---------------------------------- |\n",
    "| Upsampling    | Bilinear interpolate fine    | Needs learnable deconv             |\n",
    "| Memory        | Lower                        | Higher                             |\n",
    "| Artifacts     | Smooth                       | Risk of checkerboard / aliasing    |\n",
    "| Training      | Stable                       | Sometimes unstable                 |\n",
    "| Resulting map | Clean, soft edges            | Potentially sharper but noisier    |\n",
    "\n",
    "---\n",
    "\n",
    "### Visual intuition\n",
    "\n",
    "Imagine the decoder at scale 1/4 is trying to reconstruct building edges:\n",
    "\n",
    "* If you **pass `layer1`** (stride 4) features directly → it already knows where the edges are (strong skip).\n",
    "* If you **only pass `layer4`** (stride 32) features → edges are lost, decoder must guess (weak skip).\n",
    "\n",
    "That’s why **bilinear interpolation** works fine when your skip connections deliver detailed information at every stage — the decoder doesn’t have to *invent* geometry.\n",
    "\n",
    "---\n",
    "\n",
    "###  In your DepthNet (ResNet18 encoder)\n",
    "\n",
    "| Encoder layer      | Skip strength | Why                                  |\n",
    "| ------------------ | ------------- | ------------------------------------ |\n",
    "| `conv1` / `layer1` |  Strong     | High spatial resolution (stride 2–4) |\n",
    "| `layer2`           |  Strong     | Moderate spatial detail              |\n",
    "| `layer3`           |  Medium     | Semantic + spatial                   |\n",
    "| `layer4`           |  Weakest    | Very coarse (stride 32)              |\n",
    "\n",
    "Your decoder’s **multi-level skip fusion** restores details progressively →\n",
    "so bilinear upsampling works beautifully.\n",
    "\n",
    "---\n",
    "\n",
    "###  TL;DR summary\n",
    "\n",
    "| Skip connection quality          | Decoder upsampling choice                              |\n",
    "| -------------------------------- | ------------------------------------------------------ |\n",
    "| **Strong (U-Net, ResNet18)**     |  Bilinear upsample (simple, smooth)                   |\n",
    "| **Weak (Transformer, deep CNN)** |  Deconvolution / learned upsampling                  |\n",
    "| **No skip (autoencoder)**        |  Must learn upsampling (deconv, pixel shuffle, etc.) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cad6a-f372-4015-8001-4bf1cd508bef",
   "metadata": {},
   "source": [
    "## U-Net Encoder-decoder for medical image segmentation\n",
    "\n",
    "## nnU-Net Self-adapting framework for medical images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
