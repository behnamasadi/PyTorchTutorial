{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff242515-2f5d-4f56-861c-73d82dba428e",
   "metadata": {},
   "source": [
    "### **Semantic Segmentation vs. Instance Segmentation**\n",
    "\n",
    "\n",
    "![](images/classification_semantic_segmentation_object_detection_instance_segmentation.png)\n",
    "\n",
    "\n",
    "### **Key Differences**  \n",
    "| **Aspect**                | **Semantic Segmentation**                     | **Instance Segmentation**                     |\n",
    "|---------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Granularity**           | Class-level (groups all objects of same class) | Object-level (distinguishes individual objects) |\n",
    "| **Output**                | Single class label per pixel                  | Class label + instance ID per pixel           |\n",
    "| **Object Differentiation** | No differentiation within same class          | Differentiates individual instances           |\n",
    "| **Complexity**            | Simpler, focuses only on class prediction      | More complex, combines detection and segmentation |\n",
    "| **Example Models**        | U-Net, DeepLab, FCN                           | Mask R-CNN, YOLACT, SOLO                     |\n",
    "\n",
    "\n",
    "Both approaches are critical in computer vision, with semantic segmentation being sufficient for class-based tasks and instance segmentation required for applications needing individual object tracking or counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51935a4-0d0a-43f7-98ad-dad76e2acf8d",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "\n",
    "Dice loss is a **very common loss function for segmentation tasks** in deep learning, especially when dealing with **imbalanced datasets** where the foreground (object of interest) occupies only a small part of the image. The **Dice coefficient** (also called **Sørensen–Dice index**) is a **similarity measure** between two sets.\n",
    "If you have two sets $A$ (ground truth) and $B$ (prediction), the Dice coefficient is:\n",
    "\n",
    "$$\n",
    "\\text{Dice}(A, B) = \\frac{2|A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "\n",
    "* $|A|$ = number of elements (pixels) in set $A$ (ground truth foreground pixels)\n",
    "* $|B|$ = number of elements (pixels) in set $B$ (predicted foreground pixels)\n",
    "* $|A \\cap B|$ = overlap between $A$ and $B$\n",
    "\n",
    "So **Dice = 1** means perfect overlap (prediction = ground truth)\n",
    "and **Dice = 0** means no overlap at all.\n",
    "\n",
    "---\n",
    "\n",
    "**Why it’s Useful**?\n",
    "\n",
    "In segmentation, we want our predicted mask to match the ground truth mask as much as possible.\n",
    "Dice coefficient directly measures **overlap**, so it is more robust to class imbalance than just pixel-wise accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## From Dice Coefficient to Dice Loss\n",
    "\n",
    "Since we minimize loss functions during training, we use:\n",
    "\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "$$\n",
    "\n",
    "This makes the loss **small when overlap is high** and **large when overlap is poor**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0e746-1cd0-437b-b3dd-29b8767f6942",
   "metadata": {},
   "source": [
    "\n",
    "## **Numerical Example**\n",
    "\n",
    "Let’s say you have a very small image with 6 pixels:\n",
    "\n",
    "Ground truth: `1 0 0 1 0 0`\n",
    "Prediction:   `1 0 1 1 0 0`\n",
    "\n",
    "* Intersection (where both are 1): `1 0 0 1 0 0` → **2 pixels**\n",
    "* Ground truth positives: **2**\n",
    "* Prediction positives: **3**\n",
    "\n",
    "Dice coefficient:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 \\times 2}{2 + 3} = \\frac{4}{5} = 0.8\n",
    "$$\n",
    "\n",
    "So Dice loss = $1 - 0.8 = 0.2$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c759cf9-74c7-4e4f-9ec1-774a15b2bc3f",
   "metadata": {},
   "source": [
    "## Mathematical Formulation for Deep Learning Soft Dice\n",
    "\n",
    "For pixels:\n",
    "\n",
    "$$\n",
    "|A \\cap B| = \\sum_i g_i p_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "|A| = \\sum_i g_i, \\quad |B| = \\sum_i p_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Make It “Soft”**\n",
    "\n",
    "In deep learning, the network predicts **probabilities** $p_i \\in [0,1]$ (sigmoid output for binary segmentation).\n",
    "So we simply **do not round** them — we keep them continuous.\n",
    "\n",
    "Thus, the **soft Dice coefficient** becomes:\n",
    "\n",
    "$$\n",
    "\\text{SoftDice}(p, g) = \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where $g_i$ is still binary (0 or 1), but $p_i$ is a probability.\n",
    "$\\epsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "**Soft Dice Loss**:\n",
    "\n",
    "Since we minimize losses, we define:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Soft Dice Loss} = 1 - \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i + \\epsilon}}\n",
    "$$\n",
    "\n",
    "* When prediction $p = g$ (perfect match), numerator = denominator → loss = 0.\n",
    "* When prediction is bad (no overlap), numerator ≈ 0 → loss ≈ 1.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Dice Loss\n",
    "\n",
    "**Best for segmentation with class imbalance**, like:\n",
    "\n",
    "* Medical image segmentation (tumor occupies tiny fraction of image)\n",
    "* Road/lane detection\n",
    "* Object segmentation with sparse objects\n",
    "\n",
    "Sometimes people combine **Dice loss + Cross Entropy loss** to benefit from both:\n",
    "\n",
    "* Cross-entropy gives good per-pixel supervision.\n",
    "* Dice focuses on overall overlap (global structure).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ac416-a6e5-4fb5-95ff-031844b9155c",
   "metadata": {},
   "source": [
    "## **Numerical Example**\n",
    "\n",
    "We have **4 pixels** in an image.\n",
    "\n",
    "| Pixel | Ground Truth $g_i$ | Prediction $p_i$ |\n",
    "| ----- | ------------------ | ---------------- |\n",
    "| 1     | 1                  | 0.9              |\n",
    "| 2     | 1                  | 0.6              |\n",
    "| 3     | 0                  | 0.4              |\n",
    "| 4     | 0                  | 0.1              |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Step 1 — Compute the Numerator:**\n",
    "\n",
    "$$\n",
    "\\text{Numerator} = 2 \\sum (p_i g_i)\n",
    "$$\n",
    "\n",
    "Only pixels where $g_i=1$ contribute:\n",
    "\n",
    "$$\n",
    "p_1 g_1 = 0.9 \\times 1 = 0.9,\\quad\n",
    "p_2 g_2 = 0.6 \\times 1 = 0.6\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum p_i g_i = 0.9 + 0.6 = 1.5\n",
    "$$\n",
    "\n",
    "So numerator:\n",
    "\n",
    "$$\n",
    "2 \\times 1.5 = 3.0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2 — Compute the Denominator:**\n",
    "\n",
    "$$\n",
    "\\text{Denominator} = \\sum p_i + \\sum g_i\n",
    "$$\n",
    "\n",
    "Predictions sum:\n",
    "\n",
    "$$\n",
    "0.9 + 0.6 + 0.4 + 0.1 = 2.0\n",
    "$$\n",
    "\n",
    "Ground truth sum:\n",
    "\n",
    "$$\n",
    "1 + 1 + 0 + 0 = 2.0\n",
    "$$\n",
    "\n",
    "So denominator:\n",
    "\n",
    "$$\n",
    "2.0 + 2.0 = 4.0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3 — Compute Soft Dice Coefficient:** \n",
    "\n",
    "$$\n",
    "\\text{SoftDice} = \\frac{3.0}{4.0} = 0.75\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4 — Convert to Loss**:\n",
    "\n",
    "$$\n",
    "\\text{Soft Dice Loss} = 1 - 0.75 = 0.25\n",
    "$$\n",
    "\n",
    "So the **loss = 0.25** (lower is better).\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition from the Example**\n",
    "\n",
    "* We predicted pretty well for pixels 1 & 2 (close to 1), so we got a high overlap.\n",
    "* But we also predicted some false positives (0.4, 0.1 where $g=0$), which reduced our score.\n",
    "* Dice loss of **0.25** means we have a decent overlap, but not perfect — the model can still improve.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456db7a-70fe-4a81-832c-4b78aa9c46e5",
   "metadata": {},
   "source": [
    "### Soft Dice Multi Class Semantic Segmentation\n",
    "An example of **3-class semantic segmentation** (classes A,B,C) on a **2×2 image** (4 pixels). We’ll (1) turn **logits → probabilities** with **softmax**, then (2) compute **soft Dice per class**, and (3) average for the final loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) From logits to probabilities (softmax)\n",
    "\n",
    "Suppose the network outputs these **logits** per pixel (order = [A,B,C]):\n",
    "\n",
    "* Pixel 1: `[1, 0, 0]`\n",
    "* Pixel 2: `[0, 1, 0]`\n",
    "* Pixel 3: `[0, 0, 1]`\n",
    "* Pixel 4: `[0.5, 0.5, 0]`\n",
    "\n",
    "Softmax at a pixel:\n",
    "\n",
    "$$\n",
    "p_c=\\frac{e^{z_c}}{\\sum_{k} e^{z_k}}\n",
    "$$\n",
    "\n",
    "Using $e\\approx2.71828$ and $e^{0.5}\\approx1.64872$:\n",
    "\n",
    "* **Pixel 1**: exp = `[2.71828, 1, 1]`, sum = `4.71828` → probs ≈ `[0.5761, 0.2119, 0.2119]`\n",
    "* **Pixel 2**: exp = `[1, 2.71828, 1]`, sum = `4.71828` → probs ≈ `[0.2119, 0.5761, 0.2119]`\n",
    "* **Pixel 3**: exp = `[1, 1, 2.71828]`, sum = `4.71828` → probs ≈ `[0.2119, 0.2119, 0.5761]`\n",
    "* **Pixel 4**: exp = `[1.64872, 1.64872, 1]`, sum = `4.29744` → probs ≈ `[0.3838, 0.3838, 0.2327]`\n",
    "\n",
    "(rounded to 4 decimals)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Ground truth (one-hot masks)\n",
    "\n",
    "Let the **GT class indices** for the 4 pixels be:\n",
    "`[A, B, B, C]` → counts: $|A|=1, |B|=2, |C|=1$.\n",
    "\n",
    "Convert to one-hot (per class, it’s 1 at pixels of that class, 0 elsewhere).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### 3) Soft Dice (per class → macro average)\n",
    "\n",
    "We have 4 pixels ($i=1,2,3,4$), 3 classes ($A,B,C$).\n",
    "Predicted probabilities $p_{i,c}$ are from the softmax in step 1.\n",
    "Ground-truth one-hot $y_{i,c}$ is from step 2.\n",
    "\n",
    "\n",
    "Formula (per class $c$):\n",
    "\n",
    "$$\n",
    "\\text{Dice}_c = \\frac{2\\sum_i p_{i,c}y_{i,c} + \\epsilon}{\\sum_i p_{i,c} + \\sum_i y_{i,c} + \\epsilon}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Sums you need (with variables + numbers)\n",
    "\n",
    "* For **Class A**:\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,A} = p_{1,A} + p_{2,A} + p_{3,A} + p_{4,A}\n",
    "= 0.5761 + 0.2119 + 0.2119 + 0.3838 = 1.3837\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i y_{i,A} = y_{1,A}+y_{2,A}+y_{3,A}+y_{4,A}\n",
    "= 1+0+0+0 = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,A}y_{i,A} = p_{1,A}\\cdot y_{1,A} = 0.5761 \\quad(\\text{since only pixel 1 is class A})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "* For **Class B**:\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,B} = p_{1,B} + p_{2,B} + p_{3,B} + p_{4,B}\n",
    "= 0.2119 + 0.5761 + 0.2119 + 0.3838 = 1.3837\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i y_{i,B} = y_{1,B}+y_{2,B}+y_{3,B}+y_{4,B}\n",
    "= 0+1+1+0 = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,B}y_{i,B} = p_{2,B}\\cdot y_{2,B} + p_{3,B}\\cdot y_{3,B}\n",
    "= 0.5761 + 0.2119 = 0.7880\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "* For **Class C**:\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,C} = p_{1,C} + p_{2,C} + p_{3,C} + p_{4,C}\n",
    "= 0.2119 + 0.2119 + 0.5761 + 0.2327 = 1.2326\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i y_{i,C} = y_{1,C}+y_{2,C}+y_{3,C}+y_{4,C}\n",
    "= 0+0+0+1 = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_i p_{i,C}y_{i,C} = p_{4,C}\\cdot y_{4,C} = 0.2327\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Per-class Dice with variables\n",
    "\n",
    "* **Class A**:\n",
    "\n",
    "$$\n",
    "\\text{Dice}_A = \\frac{2\\sum_i p_{i,A}y_{i,A}}{\\sum_i p_{i,A} + \\sum_i y_{i,A}}\n",
    "= \\frac{2(0.5761)}{1.3837 + 1}\n",
    "= \\frac{1.1522}{2.3837} \\approx 0.4834\n",
    "$$\n",
    "\n",
    "* **Class B**:\n",
    "\n",
    "$$\n",
    "\\text{Dice}_B = \\frac{2\\sum_i p_{i,B}y_{i,B}}{\\sum_i p_{i,B} + \\sum_i y_{i,B}}\n",
    "= \\frac{2(0.7880)}{1.3837 + 2}\n",
    "= \\frac{1.5760}{3.3837} \\approx 0.4657\n",
    "$$\n",
    "\n",
    "* **Class C**:\n",
    "\n",
    "$$\n",
    "\\text{Dice}_C = \\frac{2\\sum_i p_{i,C}y_{i,C}}{\\sum_i p_{i,C} + \\sum_i y_{i,C}}\n",
    "= \\frac{2(0.2327)}{1.2326 + 1}\n",
    "= \\frac{0.4654}{2.2326} \\approx 0.2085\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Macro Dice and Soft Dice loss\n",
    "\n",
    "$$\n",
    "\\text{Macro Dice} = \\frac{\\text{Dice}_A + \\text{Dice}_B + \\text{Dice}_C}{3}\n",
    "= \\frac{0.4834 + 0.4657 + 0.2085}{3}\n",
    "\\approx 0.3859\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Soft Dice Loss} = 1 - \\text{Macro Dice} \\approx 0.6141\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* We **never argmax** for soft Dice; we use **probabilities** from **softmax(logits)** directly.\n",
    "* If you have an **ignore index** (e.g., 255), **exclude** those pixels from *all* sums for every class.\n",
    "* Averaging choices:\n",
    "\n",
    "  * **Macro** (mean over classes) as shown above.\n",
    "  * **Weighted** by class size to reduce small-class noise.\n",
    "* Add a small $\\epsilon$ (e.g., $1\\text{e-}6$) to numerator & denominator for stability.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
