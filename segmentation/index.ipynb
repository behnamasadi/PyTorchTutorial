{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff242515-2f5d-4f56-861c-73d82dba428e",
   "metadata": {},
   "source": [
    "### **Semantic Segmentation vs. Instance Segmentation**\n",
    "\n",
    "\n",
    "![](images/classification_semantic_segmentation_object_detection_instance_segmentation.png)\n",
    "\n",
    "\n",
    "### **Key Differences**  \n",
    "| **Aspect**                | **Semantic Segmentation**                     | **Instance Segmentation**                     |\n",
    "|---------------------------|-----------------------------------------------|-----------------------------------------------|\n",
    "| **Granularity**           | Class-level (groups all objects of same class) | Object-level (distinguishes individual objects) |\n",
    "| **Output**                | Single class label per pixel                  | Class label + instance ID per pixel           |\n",
    "| **Object Differentiation** | No differentiation within same class          | Differentiates individual instances           |\n",
    "| **Complexity**            | Simpler, focuses only on class prediction      | More complex, combines detection and segmentation |\n",
    "| **Example Models**        | U-Net, DeepLab, FCN                           | Mask R-CNN, YOLACT, SOLO                     |\n",
    "\n",
    "\n",
    "Both approaches are critical in computer vision, with semantic segmentation being sufficient for class-based tasks and instance segmentation required for applications needing individual object tracking or counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51935a4-0d0a-43f7-98ad-dad76e2acf8d",
   "metadata": {},
   "source": [
    "## Dice Loss\n",
    "\n",
    "Dice loss is a **very common loss function for segmentation tasks** in deep learning, especially when dealing with **imbalanced datasets** where the foreground (object of interest) occupies only a small part of the image. The **Dice coefficient** (also called **Sørensen–Dice index**) is a **similarity measure** between two sets.\n",
    "If you have two sets $A$ (ground truth) and $B$ (prediction), the Dice coefficient is:\n",
    "\n",
    "$$\n",
    "\\text{Dice}(A, B) = \\frac{2|A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "\n",
    "* $|A|$ = number of elements (pixels) in set $A$ (ground truth foreground pixels)\n",
    "* $|B|$ = number of elements (pixels) in set $B$ (predicted foreground pixels)\n",
    "* $|A \\cap B|$ = overlap between $A$ and $B$\n",
    "\n",
    "So **Dice = 1** means perfect overlap (prediction = ground truth)\n",
    "and **Dice = 0** means no overlap at all.\n",
    "\n",
    "---\n",
    "\n",
    "**Why it’s Useful**?\n",
    "\n",
    "In segmentation, we want our predicted mask to match the ground truth mask as much as possible.\n",
    "Dice coefficient directly measures **overlap**, so it is more robust to class imbalance than just pixel-wise accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## From Dice Coefficient to Dice Loss\n",
    "\n",
    "Since we minimize loss functions during training, we use:\n",
    "\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "$$\n",
    "\n",
    "This makes the loss **small when overlap is high** and **large when overlap is poor**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0e746-1cd0-437b-b3dd-29b8767f6942",
   "metadata": {},
   "source": [
    "\n",
    "## **Numerical Example**\n",
    "\n",
    "Let’s say you have a very small image with 6 pixels:\n",
    "\n",
    "Ground truth: `1 0 0 1 0 0`\n",
    "Prediction:   `1 0 1 1 0 0`\n",
    "\n",
    "* Intersection (where both are 1): `1 0 0 1 0 0` → **2 pixels**\n",
    "* Ground truth positives: **2**\n",
    "* Prediction positives: **3**\n",
    "\n",
    "Dice coefficient:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 \\times 2}{2 + 3} = \\frac{4}{5} = 0.8\n",
    "$$\n",
    "\n",
    "So Dice loss = $1 - 0.8 = 0.2$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c759cf9-74c7-4e4f-9ec1-774a15b2bc3f",
   "metadata": {},
   "source": [
    "## Mathematical Formulation for Deep Learning Soft Dice\n",
    "\n",
    "For pixels:\n",
    "\n",
    "$$\n",
    "|A \\cap B| = \\sum_i g_i p_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "|A| = \\sum_i g_i, \\quad |B| = \\sum_i p_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Make It “Soft”**\n",
    "\n",
    "In deep learning, the network predicts **probabilities** $p_i \\in [0,1]$ (sigmoid output for binary segmentation).\n",
    "So we simply **do not round** them — we keep them continuous.\n",
    "\n",
    "Thus, the **soft Dice coefficient** becomes:\n",
    "\n",
    "$$\n",
    "\\text{SoftDice}(p, g) = \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where $g_i$ is still binary (0 or 1), but $p_i$ is a probability.\n",
    "$\\epsilon$ is a small constant to avoid division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "**Soft Dice Loss**:\n",
    "\n",
    "Since we minimize losses, we define:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Soft Dice Loss} = 1 - \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i + \\epsilon}}\n",
    "$$\n",
    "\n",
    "* When prediction $p = g$ (perfect match), numerator = denominator → loss = 0.\n",
    "* When prediction is bad (no overlap), numerator ≈ 0 → loss ≈ 1.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Dice Loss\n",
    "\n",
    "**Best for segmentation with class imbalance**, like:\n",
    "\n",
    "* Medical image segmentation (tumor occupies tiny fraction of image)\n",
    "* Road/lane detection\n",
    "* Object segmentation with sparse objects\n",
    "\n",
    "Sometimes people combine **Dice loss + Cross Entropy loss** to benefit from both:\n",
    "\n",
    "* Cross-entropy gives good per-pixel supervision.\n",
    "* Dice focuses on overall overlap (global structure).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ac416-a6e5-4fb5-95ff-031844b9155c",
   "metadata": {},
   "source": [
    "## **Numerical Example**\n",
    "\n",
    "We have **4 pixels** in an image.\n",
    "\n",
    "| Pixel | Ground Truth $g_i$ | Prediction $p_i$ |\n",
    "| ----- | ------------------ | ---------------- |\n",
    "| 1     | 1                  | 0.9              |\n",
    "| 2     | 1                  | 0.6              |\n",
    "| 3     | 0                  | 0.4              |\n",
    "| 4     | 0                  | 0.1              |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Step 1 — Compute the Numerator:**\n",
    "\n",
    "$$\n",
    "\\text{Numerator} = 2 \\sum (p_i g_i)\n",
    "$$\n",
    "\n",
    "Only pixels where $g_i=1$ contribute:\n",
    "\n",
    "$$\n",
    "p_1 g_1 = 0.9 \\times 1 = 0.9,\\quad\n",
    "p_2 g_2 = 0.6 \\times 1 = 0.6\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum p_i g_i = 0.9 + 0.6 = 1.5\n",
    "$$\n",
    "\n",
    "So numerator:\n",
    "\n",
    "$$\n",
    "2 \\times 1.5 = 3.0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2 — Compute the Denominator:**\n",
    "\n",
    "$$\n",
    "\\text{Denominator} = \\sum p_i + \\sum g_i\n",
    "$$\n",
    "\n",
    "Predictions sum:\n",
    "\n",
    "$$\n",
    "0.9 + 0.6 + 0.4 + 0.1 = 2.0\n",
    "$$\n",
    "\n",
    "Ground truth sum:\n",
    "\n",
    "$$\n",
    "1 + 1 + 0 + 0 = 2.0\n",
    "$$\n",
    "\n",
    "So denominator:\n",
    "\n",
    "$$\n",
    "2.0 + 2.0 = 4.0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3 — Compute Soft Dice Coefficient:** \n",
    "\n",
    "$$\n",
    "\\text{SoftDice} = \\frac{3.0}{4.0} = 0.75\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4 — Convert to Loss**:\n",
    "\n",
    "$$\n",
    "\\text{Soft Dice Loss} = 1 - 0.75 = 0.25\n",
    "$$\n",
    "\n",
    "So the **loss = 0.25** (lower is better).\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition from the Example**\n",
    "\n",
    "* We predicted pretty well for pixels 1 & 2 (close to 1), so we got a high overlap.\n",
    "* But we also predicted some false positives (0.4, 0.1 where $g=0$), which reduced our score.\n",
    "* Dice loss of **0.25** means we have a decent overlap, but not perfect — the model can still improve.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cf0a9-57a7-4ce6-99d6-352a489e5a86",
   "metadata": {},
   "source": [
    "## Ground Truth  Values\n",
    "\n",
    "###  **Binary Segmentation**\n",
    "\n",
    "* **Ground truth (GT)** is usually a **binary mask**:\n",
    "\n",
    "  * `1` = foreground (object of interest)\n",
    "  * `0` = background\n",
    "* So yes, GT pixels are **either 0 or 1**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "GT:  [1, 1, 0, 0]\n",
    "```\n",
    "\n",
    "means pixels 1 and 2 belong to the object.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Multi-Class Segmentation**\n",
    "\n",
    "* GT is usually stored as **class indices**:\n",
    "\n",
    "  * `0` = background\n",
    "  * `1` = class A\n",
    "  * `2` = class B\n",
    "  * etc.\n",
    "* Before computing Dice loss, we often **convert it to one-hot encoding** (per class 0/1 mask).\n",
    "\n",
    "Example:\n",
    "GT: `[0, 2, 1]` → One-hot (for 3 classes):\n",
    "\n",
    "```\n",
    "Class 0: [1, 0, 0]\n",
    "Class 1: [0, 0, 1]\n",
    "Class 2: [0, 1, 0]\n",
    "```\n",
    "\n",
    "Then Dice loss is computed **per class** and averaged.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Soft Ground Truth**\n",
    "\n",
    "Sometimes GT can also have **values between 0 and 1** (soft labels):\n",
    "\n",
    "* Example: if GT comes from averaging multiple annotators' segmentations.\n",
    "* Then you can still use soft Dice — it works fine.\n",
    "\n",
    "---\n",
    "\n",
    "So **in most binary segmentation problems**, yes — GT is just 0 or 1.\n",
    "\n",
    "---\n",
    "\n",
    "###  Perfect Prediction Example (Your Request)\n",
    "\n",
    "Let’s use the same setup as before:\n",
    "\n",
    "| Pixel | GT $g_i$ | Prediction $p_i$ |\n",
    "| ----- | -------- | ---------------- |\n",
    "| 1     | 1        | 1.0              |\n",
    "| 2     | 1        | 1.0              |\n",
    "| 3     | 0        | 0.0              |\n",
    "| 4     | 0        | 0.0              |\n",
    "\n",
    "**Step 1 — Numerator**\n",
    "\n",
    "$$\n",
    "\\sum p_i g_i = 1.0 + 1.0 = 2.0 \\quad \\Rightarrow \\quad 2 \\times 2.0 = 4.0\n",
    "$$\n",
    "\n",
    "**Step 2 — Denominator**\n",
    "\n",
    "$$\n",
    "\\sum p_i = 2.0,\\quad \\sum g_i = 2.0 \\quad \\Rightarrow \\quad 2.0+2.0=4.0\n",
    "$$\n",
    "\n",
    "**Step 3 — Soft Dice**\n",
    "\n",
    "$$\n",
    "\\frac{4.0}{4.0} = 1.0\n",
    "$$\n",
    "\n",
    "**Step 4 — Loss**\n",
    "\n",
    "$$\n",
    "\\text{Loss} = 1 - 1.0 = 0.0 \\quad ✅ \\text{(perfect prediction)}\n",
    "$$\n",
    "\n",
    "So **perfect overlap ⇒ Dice loss = 0** (which is what we want).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da7769-ac4f-44fa-a27a-d65dcb112449",
   "metadata": {},
   "source": [
    "### **Overview of Model Architectures**\n",
    "\n",
    "1. **FCN (Fully Convolutional Network)**  \n",
    "   - **Concept**: Introduced in 2015, FCN was one of the first architectures for semantic segmentation, replacing fully connected layers in traditional CNNs with convolutional layers to produce pixel-wise predictions. It uses a backbone (e.g., VGG or ResNet) for feature extraction, followed by upsampling to recover spatial resolution.  \n",
    "   - **Architecture**:  \n",
    "     - **Encoder**: A pre-trained CNN (e.g., VGG16, ResNet) extracts features at multiple scales, producing feature maps of decreasing resolution.  \n",
    "     - **Upsampling**: Uses transposed convolutions or bilinear upsampling to restore the feature map to the input image size.  \n",
    "     - **Skip Connections**: Combines feature maps from earlier layers with upsampled outputs to recover spatial details lost during downsampling.  \n",
    "     - **Output**: A pixel-wise classification map with class probabilities.  \n",
    "   - **Pros**: Simple concept, leverages pre-trained backbones, good baseline for segmentation.  \n",
    "   - **Cons**: Can struggle with fine details due to coarse upsampling, less sophisticated than newer models.  \n",
    "   - **Use Case**: General-purpose semantic segmentation, foundational for later models.\n",
    "\n",
    "2. **U-Net**  \n",
    "   - **Concept**: Introduced in 2015 for medical imaging, U-Net is designed for precise segmentation with limited data. Its symmetric encoder-decoder structure resembles a \"U\" shape, hence the name. It’s highly intuitive and effective for small datasets.  \n",
    "   - **Architecture**:  \n",
    "     - **Encoder (Contracting Path)**: A series of convolutional and max-pooling layers that downsample the input image to extract features at multiple scales.  \n",
    "     - **Decoder (Expanding Path)**: Symmetric upsampling layers (via transposed convolutions or interpolation) to recover spatial resolution.  \n",
    "     - **Skip Connections**: Concatenates feature maps from the encoder to the decoder at each level, preserving fine-grained spatial details.  \n",
    "     - **Output**: A dense pixel-wise classification map.  \n",
    "   - **Pros**:  \n",
    "     - Simple and symmetric design, easy to understand.  \n",
    "     - Highly effective for small datasets (common in medical imaging).  \n",
    "     - Skip connections help retain fine details, making it great for precise boundaries.  \n",
    "   - **Cons**: Less flexible for very deep architectures or complex tasks compared to DeepLab. May require modifications for large-scale datasets.  \n",
    "   - **Use Case**: Medical imaging (e.g., cell segmentation, organ segmentation), small-scale datasets, or when precise boundaries are critical.\n",
    "\n",
    "3. **DeepLab**  \n",
    "   - **Concept**: Developed by Google, DeepLab (v1–v3+) is a family of models that use atrous (dilated) convolutions and advanced techniques to capture multi-scale context and improve segmentation accuracy. It’s more complex but highly effective for large-scale datasets.  \n",
    "   - **Architecture (DeepLabv3+ as an example)**:  \n",
    "     - **Encoder**: Uses a backbone (e.g., ResNet, Xception) with atrous convolutions to maintain higher-resolution feature maps and capture multi-scale context.  \n",
    "     - **Atrous Spatial Pyramid Pooling (ASPP)**: Applies atrous convolutions at different rates to capture features at multiple scales.  \n",
    "     - **Decoder**: Upsamples the feature maps and refines boundaries using low-level features from the backbone.  \n",
    "     - **Output**: A refined pixel-wise segmentation map.  \n",
    "   - **Pros**:  \n",
    "     - Excels at capturing multi-scale context, ideal for complex scenes (e.g., autonomous driving).  \n",
    "     - State-of-the-art performance on large datasets like Cityscapes or PASCAL VOC.  \n",
    "   - **Cons**:  \n",
    "     - More complex to understand and implement due to atrous convolutions and ASPP.  \n",
    "     - Computationally intensive, requiring more resources.  \n",
    "   - **Use Case**: Large-scale, complex datasets like urban scene segmentation or natural images.\n",
    "\n",
    "### **Comparison and Recommendation**\n",
    "\n",
    "| **Model** | **Ease of Understanding** | **Ease of Use** | **Performance** | **Best For** |\n",
    "|-----------|---------------------------|-----------------|-----------------|--------------|\n",
    "| **FCN**   | Moderate (simple but dated) | Moderate (requires tuning) | Good but basic | General-purpose, baseline tasks |\n",
    "| **U-Net** | High (intuitive U-shape)   | High (simple to implement) | Excellent for small datasets | Medical imaging, precise boundaries |\n",
    "| **DeepLab**| Low (complex components)   | Moderate (pre-trained models available) | State-of-the-art for complex tasks | Large-scale, multi-scale datasets |\n",
    "\n",
    "**U-Net is the Best Starting Point**:  \n",
    "- **Intuitive Design**: The symmetric encoder-decoder structure with skip connections is easy to visualize and understand, making it ideal for beginners.  \n",
    "- **Ease of Implementation**: U-Net is straightforward to implement in frameworks like PyTorch or TensorFlow. Many tutorials and pre-trained models are available, especially for medical imaging.  \n",
    "- **Effective for Small Datasets**: Unlike DeepLab, which shines with large datasets, U-Net performs well even with limited labeled data, which is common for beginners or specific domains.  \n",
    "- **Wide Adoption**: U-Net is widely used in both research and industry (e.g., medical imaging, satellite imagery), so learning it provides a strong foundation.  \n",
    "- **Flexibility**: While originally designed for medical imaging, U-Net can be adapted for other tasks with minor modifications (e.g., changing the backbone or loss function).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
