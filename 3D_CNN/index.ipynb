{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0addb0a-c56b-4dc0-bdbe-dacd373f1cc4",
   "metadata": {},
   "source": [
    "# What is a 3D CNN for video?\n",
    "\n",
    "A 3D CNN extends 2D convs into time. Instead of convolving over $(H, W)$, kernels slide over $(T, H, W)$ to learn **spatiotemporal** features.\n",
    "\n",
    "If an input clip is $x \\in \\mathbb{R}^{B \\times C \\times T \\times H \\times W}$ and a kernel is $w \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times K_t \\times K_h \\times K_w}$, the 3D convolution at output channel $o$ and position $(t,i,j)$ is:\n",
    "$$\n",
    "y_{o,t,i,j} ;=; \\sum_{c=1}^{C_{\\text{in}}};\\sum_{\\tau=0}^{K_t-1};\\sum_{m=0}^{K_h-1};\\sum_{n=0}^{K_w-1}\n",
    "w_{o,c,\\tau,m,n};x_{c,,t + \\tau s_t - p_t,, i + m s_h - p_h,, j + n s_w - p_w}.\n",
    "$$\n",
    "Parameters: $C_{\\text{out}}\\cdot C_{\\text{in}}\\cdot K_t K_h K_w$.\n",
    "\n",
    "# Common model families (mental map)\n",
    "\n",
    "* **C3D**: Plain 3D convs everywhere (e.g., $3{\\times}3{\\times}3$). Simple, heavy.\n",
    "* **I3D**: Inflate 2D kernels to 3D (copy/average weights along time); leverage 2D ImageNet pretraining.\n",
    "* **(2+1)D / R(2+1)D**: Factor $3{\\times}3{\\times}3$ into $1{\\times}3{\\times}3$ (spatial) then $3{\\times}1{\\times}1$ (temporal). Fewer params + extra nonlinearity.\n",
    "  $$\n",
    "  \\text{3D}(K_t,K_h,K_w) \\approx \\text{2D}(K_h,K_w) ;\\to; \\text{1D}_t(K_t).\n",
    "  $$\n",
    "* **P3D**: Parallel or cascaded spatial/temporal branches (various topologies).\n",
    "* **3D ResNets / R3D**: Residual networks with 3D blocks.\n",
    "* **SlowFast**: Two pathways—Slow (low fps, high channels) + Fast (high fps, low channels). Fuse lateral connections.\n",
    "* **X3D**: Efficient compound scaling of width, depth, resolution, and time for mobile/edge.\n",
    "* **TSM/Temporal-Shift (2D-heavy)**: Keep 2D convs; shift channels temporally to mix frames (near 2D cost).\n",
    "\n",
    "# Key design knobs\n",
    "\n",
    "* **Temporal sampling**: choose clip length $T$ and stride $s_t$. Example: sample 8–32 frames at strides 2–8.\n",
    "* **Temporal receptive field**: increase with $K_t$, temporal stride, or temporal dilation $\\delta_t$.\n",
    "* **Factorization**: (2+1)D often gives better accuracy/efficiency than full $3{\\times}3{\\times}3$.\n",
    "* **Normalization**: 3D BN/SyncBN; LayerNorm is common when mixing with attention.\n",
    "* **Pooling/heads**:\n",
    "\n",
    "  * **Video classification**: global average pool over $(T,H,W)$ then MLP.\n",
    "    $$\n",
    "    \\hat{y}=\\mathrm{softmax}\\left(W,\\mathrm{GAP}_{t,h,w}(F)+b\\right).\n",
    "    $$\n",
    "  * **Temporal localization**: keep $T$; pool only spatially.\n",
    "  * **Spatiotemporal detection/segmentation**: 3D FPN/decoder; keep spatial map; optionally reduce $T$ later.\n",
    "\n",
    "# Data pipeline (typical)\n",
    "\n",
    "1. **Decode**: uniformly sample $T$ frames from each video or multi-clip sampling at test time.\n",
    "2. **Augment**: random crop, horizontal flip, color jitter, RandAugment (frame-wise or consistent across time).\n",
    "3. **Normalize**: per-channel mean/std.\n",
    "4. **Label granularity**: video-level vs frame-level vs tube-level depends on task.\n",
    "\n",
    "# Training recipes (solid defaults)\n",
    "\n",
    "* Optimizer: AdamW or SGD+momentum.\n",
    "* LR: cosine decay with warmup; batch-size scaled LR.\n",
    "* Regularization: label smoothing, dropout/stochastic depth, mixup/cutmix (applied consistently over time).\n",
    "* Pretraining: 2D ImageNet (for I3D) or Kinetics-400/600/700 checkpoints (if available).\n",
    "* Inference: clip sampling (e.g., 10-crop × 3 clips) for a small gain if latency permits.\n",
    "\n",
    "# Efficient configs (4–6 GB VRAM)\n",
    "\n",
    "* **R(2+1)D-18**: $T{=}8$ or $16$, $224^2$, batch 4–8 (accumulation if needed).\n",
    "* **X3D-S/XS**: $T{=}13$–$16$, $160$–$224$.\n",
    "* **TSM-ResNet-18 (2D)**: $T{=}8$–$16$ with temporal shift—very memory-friendly.\n",
    "\n",
    "# Minimal PyTorch: basic 3D block and tiny classifier\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Basic3DBlock(nn.Module):\n",
    "    def __init__(self, cin, cout, kt=3, kh=3, kw=3, stride=(1,2,2), t_stride=1):\n",
    "        super().__init__()\n",
    "        # stride is (t,h,w); you can pass stride=(t_stride,2,2) for downsampling\n",
    "        self.conv1 = nn.Conv3d(cin, cout, kernel_size=(kt,kh,kw),\n",
    "                               stride=stride, padding=(kt//2, kh//2, kw//2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(cout)\n",
    "        self.conv2 = nn.Conv3d(cout, cout, kernel_size=(kt,kh,kw),\n",
    "                               stride=1, padding=(kt//2, kh//2, kw//2), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(cout)\n",
    "        self.down = None\n",
    "        if stride != 1 or cin != cout:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(cout),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        out = F.relu(out + identity, inplace=True)\n",
    "        return out\n",
    "\n",
    "class Tiny3DClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv3d(3, 32, kernel_size=(3,7,7), stride=(1,2,2), padding=(1,3,3), bias=False)\n",
    "        self.bn = nn.BatchNorm3d(32)\n",
    "        self.layer1 = Basic3DBlock(32, 64, stride=(1,2,2))\n",
    "        self.layer2 = Basic3DBlock(64, 128, stride=(2,2,2))  # temporal downsample\n",
    "        self.layer3 = Basic3DBlock(128, 256, stride=(1,2,2))\n",
    "        self.head = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [B,3,T,H,W]\n",
    "        x = F.relu(self.bn(self.stem(x)), inplace=True)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)     # [B,C,T',H',W']\n",
    "        x = x.mean(dim=(2,3,4))  # GAP over (T,H,W)\n",
    "        return self.head(x)\n",
    "```\n",
    "\n",
    "# Minimal PyTorch: (2+1)D factorized block\n",
    "\n",
    "```python\n",
    "class R2Plus1DBlock(nn.Module):\n",
    "    def __init__(self, cin, cout, kt=3, kh=3, kw=3, stride=(1,1,1)):\n",
    "        super().__init__()\n",
    "        # Spatial 2D first\n",
    "        self.spatial = nn.Conv3d(cin, cout, kernel_size=(1,kh,kw),\n",
    "                                 stride=(1, stride[1], stride[2]),\n",
    "                                 padding=(0, kh//2, kw//2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(cout)\n",
    "        # Temporal 1D next\n",
    "        self.temporal = nn.Conv3d(cout, cout, kernel_size=(kt,1,1),\n",
    "                                  stride=(stride[0],1,1),\n",
    "                                  padding=(kt//2,0,0), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(cout)\n",
    "        self.down = None\n",
    "        if stride != (1,1,1) or cin != cout:\n",
    "            self.down = nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(cout),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = F.relu(self.bn1(self.spatial(x)), inplace=True)\n",
    "        x = self.bn2(self.temporal(x))\n",
    "        if self.down is not None:\n",
    "            identity = self.down(identity)\n",
    "        return F.relu(x + identity, inplace=True)\n",
    "```\n",
    "\n",
    "# When to prefer which\n",
    "\n",
    "* Need **speed/low memory**: TSM or X3D-XS.\n",
    "* Need **strong accuracy** on actions with motion cues: R(2+1)D / SlowFast / X3D-M.\n",
    "* Need **long-range** context (≫32 frames): combine 3D CNN backbone with temporal pooling, dilations, or a lightweight temporal transformer head.\n",
    "\n",
    "# Practical tips\n",
    "\n",
    "* **Clip length vs stride**: for fast actions use shorter $T$ and smaller stride; for long actions sample sparse but longer clips.\n",
    "* **Temporal augmentation**: random frame-rate jitter (vary stride) improves robustness.\n",
    "* **Class imbalance**: use weighted loss or focal loss for detection/localization.\n",
    "* **Evaluation protocol**: report top-1/top-5 for classification; mAP for detection; mIoU for segmentation.\n",
    "\n",
    "# Quick sanity check (toy)\n",
    "\n",
    "* Input: $B{=}2,;C{=}3,;T{\\approx}8,;H{=}112,;W{=}112$.\n",
    "* Start with $T{=}8$ clips during training; at test time average predictions across 3 temporal crops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecee22-067a-4b45-8be3-f0738b01728e",
   "metadata": {},
   "source": [
    "## **Output of a 3D CNN**\n",
    "The **output of a 3D CNN** depends on *what task* you’re solving and *how you handle the time dimension*.\n",
    "Let’s break it down clearly using the notation:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times C_{in} \\times T \\times H \\times W}\n",
    "$$\n",
    "where:\n",
    "\n",
    "* $B$: batch size\n",
    "* $C_{in}$: input channels (e.g. RGB → 3)\n",
    "* $T$: number of frames\n",
    "* $H, W$: spatial resolution\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Video Classification (most common)**\n",
    "\n",
    "Goal: predict **one label per video clip**, e.g. “playing guitar.”\n",
    "\n",
    "### Output shape\n",
    "\n",
    "$$\n",
    "y \\in \\mathbb{R}^{B \\times \\text{num_classes}}\n",
    "$$\n",
    "\n",
    "### What happens internally\n",
    "\n",
    "1. After the 3D convolutions, you get feature maps\n",
    "   $F \\in \\mathbb{R}^{B \\times C_{feat} \\times T' \\times H' \\times W'}$\n",
    "2. You **average pool over (T', H', W')** →\n",
    "   $$\n",
    "   f = \\mathrm{GAP}*{T',H',W'}(F) \\in \\mathbb{R}^{B \\times C*{feat}}\n",
    "   $$\n",
    "3. Then apply a **fully connected (linear) layer**:\n",
    "   $$\n",
    "   \\hat{y} = \\mathrm{softmax}(W f + b)\n",
    "   $$\n",
    "   which gives per-class probabilities.\n",
    "\n",
    "### Example (from the code above)\n",
    "\n",
    "```python\n",
    "x = torch.randn(2, 3, 8, 112, 112)   # batch=2, RGB, 8 frames\n",
    "model = Tiny3DClassifier(num_classes=10)\n",
    "out = model(x)\n",
    "print(out.shape)  # torch.Size([2, 10])\n",
    "```\n",
    "\n",
    "✅ So here the output is **one 10-class prediction per video clip**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Frame-level Prediction (per-frame label)**\n",
    "\n",
    "For tasks like **temporal action segmentation** or **frame-wise emotion detection**,\n",
    "you keep the temporal dimension and average only spatially.\n",
    "\n",
    "### Output shape\n",
    "\n",
    "$$\n",
    "y \\in \\mathbb{R}^{B \\times \\text{num_classes} \\times T'}\n",
    "$$\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "x = model.extract_features(video)        # [B, C, T', H', W']\n",
    "x = x.mean(dim=(3,4))                    # average over H,W → [B, C, T']\n",
    "out = head(x)                            # linear projection → [B, num_classes, T']\n",
    "```\n",
    "\n",
    "✅ One label per frame (or short temporal window).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Spatiotemporal Detection or Segmentation**\n",
    "\n",
    "Here we want a **map** in time and space — e.g., localizing an action *and where it happens in the frame*.\n",
    "\n",
    "### Output shape\n",
    "\n",
    "$$\n",
    "y \\in \\mathbb{R}^{B \\times \\text{num_classes} \\times T' \\times H' \\times W'}\n",
    "$$\n",
    "Each voxel has a class probability.\n",
    "\n",
    "This is used in **action localization**, **video object segmentation**, or **3D medical imaging** (like CT/MRI).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Feature Extractor / Backbone Output**\n",
    "\n",
    "If you remove the classifier head and just want **spatiotemporal features**,\n",
    "the output is the **final 3D tensor** before pooling:\n",
    "$$\n",
    "F \\in \\mathbb{R}^{B \\times C_{feat} \\times T' \\times H' \\times W'}\n",
    "$$\n",
    "You can feed this into:\n",
    "\n",
    "* a temporal transformer,\n",
    "* an RNN,\n",
    "* or another detection/segmentation head.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "features = model.layer3(video)  # torch.Size([B, 256, 2, 7, 7])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick summary table\n",
    "\n",
    "| Task                               | Output shape                 | Meaning                       |\n",
    "| ---------------------------------- | ---------------------------- | ----------------------------- |\n",
    "| **Video classification**           | [B, num_classes]             | One label per clip            |\n",
    "| **Frame-level classification**     | [B, num_classes, T’]         | Label per frame               |\n",
    "| **Video segmentation / detection** | [B, num_classes, T’, H’, W’] | Label per pixel per frame     |\n",
    "| **Feature extraction**             | [B, C_feat, T’, H’, W’]      | Spatiotemporal feature tensor |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
