{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88038b87-4d12-40f7-acee-eb3a506d0b9d",
   "metadata": {},
   "source": [
    "**Double Descent** is a phenomenon in deep learning and modern machine learning where the **test error** (or generalization error) does **not** behave as expected according to classical bias–variance theory.\n",
    "\n",
    "Traditionally, we expect this curve:\n",
    "\n",
    "* As model complexity increases:\n",
    "\n",
    "  * **Bias decreases** (model fits data better)\n",
    "  * **Variance increases** (model overfits)\n",
    "* The **test error** follows a **U-shaped curve** — decreasing at first, reaching a minimum (the “sweet spot”), and then increasing again as the model starts to overfit.\n",
    "\n",
    "However, with deep networks (and other overparameterized models), the actual curve often looks like this:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Classical View vs Double Descent\n",
    "\n",
    "| Stage                         | Description                                                                      | Behavior                          |\n",
    "| :---------------------------- | :------------------------------------------------------------------------------- | :-------------------------------- |\n",
    "| **Underparameterized regime** | Model too simple to fit the data                                                 | High bias, high error             |\n",
    "| **Interpolation threshold**   | Model capacity just enough to fit (or “interpolate”) the training data perfectly | Variance peaks, test error spikes |\n",
    "| **Overparameterized regime**  | Model capacity much higher than number of training points                        | Test error *decreases again*      |\n",
    "\n",
    "So the **test error first decreases, then increases, then decreases again** → forming a **“double descent”** curve.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Does This Happen?\n",
    "\n",
    "At the **interpolation threshold**, the model just barely fits the data — it memorizes the noise or small idiosyncrasies of the dataset.\n",
    "However, as capacity grows *further*, the model has enough degrees of freedom to **find simpler (smoother)** solutions among the many that fit the data perfectly.\n",
    "\n",
    "This happens because:\n",
    "\n",
    "1. **Overparameterized models have many zero-training-loss solutions.**\n",
    "   Gradient descent tends to converge to those with smaller norms or smoother behavior (implicit regularization).\n",
    "\n",
    "2. **Neural networks generalize due to inductive bias**, not because they are small.\n",
    "   Even with millions of parameters, optimization plus architecture bias (e.g., convolution, normalization, residuals) favor “simpler” functions.\n",
    "\n",
    "3. **Random features and linear models** also show double descent — not only deep nets — suggesting this is a more general property of high-dimensional models.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Visualization of the Test Error Curve\n",
    "\n",
    "```\n",
    "Test Error\n",
    "   |\n",
    "   |      /\\\n",
    "   |     /  \\        <-- Classical U-shape\n",
    "   |    /    \\\n",
    "   |   /      \\____\n",
    "   |  /             \\____\n",
    "   |_/____________________ Model complexity\n",
    "       ↑        ↑\n",
    "       |        |\n",
    "   Underfit   Interpolation\n",
    "               Threshold\n",
    "```\n",
    "\n",
    "At very high capacity, the test error drops again — the **second descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Intuition (Simplified)\n",
    "\n",
    "For a dataset with **n** training samples and a linear model with **p** parameters:\n",
    "\n",
    "* When **p < n**, the system is **underdetermined**, and the least-squares solution minimizes bias but can’t fit all samples.\n",
    "* When **p = n**, the system is just determined — the solution exactly fits training data.\n",
    "* When **p > n**, there are infinitely many zero-error solutions; gradient descent tends to pick the **minimum-norm** one.\n",
    "\n",
    "The norm-minimizing solution has better generalization, explaining the **second descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Implications in Deep Learning\n",
    "\n",
    "* Large models (e.g., ResNets, Transformers) are **heavily overparameterized**, yet generalize better than smaller ones.\n",
    "* Increasing model size beyond what’s needed to fit training data can **reduce** test error.\n",
    "* This motivates **scaling laws** and **large model training**: more parameters + more data often improves performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Key Takeaways\n",
    "\n",
    "✅ Classical bias–variance trade-off **does not hold** beyond the interpolation threshold.\n",
    "✅ **Overparameterization can improve generalization** if optimized properly.\n",
    "✅ The **implicit regularization** of gradient-based optimization and architecture structure is critical.\n",
    "✅ The **Double Descent curve** describes this full behavior — first descent (classical regime), peak (interpolation), second descent (overparameterized regime).\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7da71-ba8e-4343-bd50-7377a5d09d43",
   "metadata": {},
   "source": [
    "Let’s create a **numerical experiment** that clearly shows the **double descent phenomenon** using **linear regression**.\n",
    "\n",
    "We’ll generate synthetic data and vary the **model complexity** (number of features), observing how **training** and **test error** evolve.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Concept**\n",
    "\n",
    "We’ll create:\n",
    "\n",
    "* A true function:\n",
    "  $$ y = X_{true} \\cdot w_{true} + \\text{noise} $$\n",
    "* A model with **p features**, where we vary **p** from small (underparameterized) to large (overparameterized).\n",
    "* When **p ≈ n** (number of samples), the model will interpolate (error spike).\n",
    "* For **p > n**, the model will overparameterize, and the **test error decreases again**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Code Example**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_samples = 100         # number of data points\n",
    "max_features = 300      # vary model complexity\n",
    "noise_std = 0.1\n",
    "\n",
    "# True function: y = X_true @ w_true + noise\n",
    "X_true = np.random.randn(n_samples, 50)\n",
    "w_true = np.random.randn(50)\n",
    "y = X_true @ w_true + noise_std * np.random.randn(n_samples)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_true, y, test_size=0.3, random_state=42)\n",
    "\n",
    "train_errors, test_errors = [], []\n",
    "feature_range = range(1, max_features + 1)\n",
    "\n",
    "for p in feature_range:\n",
    "    # Generate random features (model complexity = p)\n",
    "    Xp_train = np.random.randn(len(X_train), p)\n",
    "    Xp_test = np.random.randn(len(X_test), p)\n",
    "    \n",
    "    # Fit least squares (pseudo-inverse)\n",
    "    w_hat = np.linalg.pinv(Xp_train) @ y_train\n",
    "    \n",
    "    # Compute errors\n",
    "    train_pred = Xp_train @ w_hat\n",
    "    test_pred = Xp_test @ w_hat\n",
    "    \n",
    "    train_mse = np.mean((train_pred - y_train)**2)\n",
    "    test_mse = np.mean((test_pred - y_test)**2)\n",
    "    \n",
    "    train_errors.append(train_mse)\n",
    "    test_errors.append(test_mse)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(feature_range, test_errors, label='Test Error', linewidth=2)\n",
    "plt.plot(feature_range, train_errors, label='Train Error', linestyle='--')\n",
    "plt.axvline(n_samples, color='gray', linestyle=':', label='Interpolation Threshold (p = n)')\n",
    "plt.xlabel('Model Complexity (Number of Parameters)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Double Descent Phenomenon in Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Explanation**\n",
    "\n",
    "* When **p < n**, model cannot fit data → **underfitting** (high bias).\n",
    "* When **p ≈ n**, model fits training data perfectly → **variance spike** (test error high).\n",
    "* When **p > n**, there are many perfect fits; pseudo-inverse picks **minimum-norm** solution → smoother fit, **test error drops again**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Typical Output**\n",
    "\n",
    "You’ll see a plot like this:\n",
    "\n",
    "```\n",
    "Test Error\n",
    "   |\n",
    "   |       /\\           <- first peak (interpolation)\n",
    "   |      /  \\\n",
    "   |     /    \\____\n",
    "   |    /          \\\n",
    "   |___/            \\____\n",
    "         p=n        -----> model complexity\n",
    "```\n",
    "\n",
    "✅ **Training error**: decreases monotonically.\n",
    "✅ **Test error**: decreases → spikes → decreases again (**double descent**).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **neural network version** (with hidden layers) that also exhibits double descent using PyTorch? It helps visualize the same concept in a nonlinear setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbf180-cd19-49ac-8bd6-65b4286f96fb",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=z64a7USuGX0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
