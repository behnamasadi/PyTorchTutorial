{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd1406c-4497-476d-81e9-a3ec5119ee89",
   "metadata": {},
   "source": [
    "## **Contrastive Learning**\n",
    "#### **1. Motivation**\n",
    "\n",
    "In supervised learning, we learn from **labeled pairs** of input and output, e.g.\n",
    "image → class label.\n",
    "\n",
    "But **contrastive learning** is a **self-supervised** technique — it doesn’t need explicit labels.\n",
    "Instead, it learns by **comparing pairs of samples** and figuring out which ones should be **similar** (positive pairs) and which ones should be **different** (negative pairs).\n",
    "\n",
    "The goal:\n",
    "Learn a representation (embedding) where similar inputs are close together, and dissimilar inputs are far apart in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Contrastive Learning Intuition**\n",
    "\n",
    "We take an image, apply **two random augmentations**, and feed both into an encoder (e.g., ResNet).\n",
    "These two augmented views form a **positive pair** because they come from the same image.\n",
    "Augmentations of *other* images are **negative pairs**.\n",
    "\n",
    "So, during training:\n",
    "\n",
    "* Pull positive pairs **together** in embedding space.\n",
    "* Push negative pairs **apart**.\n",
    "\n",
    "This produces an embedding function\n",
    "$$\n",
    "f(x): \\mathbb{R}^{H\\times W \\times 3} \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "that maps similar samples close together.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Contrastive Loss**\n",
    "\n",
    "The core of contrastive learning is the **contrastive loss function**.\n",
    "There are two popular formulations:\n",
    "\n",
    "---\n",
    "\n",
    "## **(a) Classic Contrastive Loss (Siamese Networks)**\n",
    "\n",
    "Used in **Siamese networks** (e.g., face verification).\n",
    "\n",
    "For two samples $x_i$ and $x_j$ with label\n",
    "(y_{ij}=1) if they are similar (positive pair), else (y_{ij}=0):\n",
    "\n",
    "$$\n",
    "L_{ij} = y_{ij} , D_{ij}^2 + (1 - y_{ij}) , \\max(0, m - D_{ij})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ D_{ij} = | f(x_i) - f(x_j) |_2 $ is the distance in feature space\n",
    "* $ m $ is a margin — ensures dissimilar samples are at least distance (m) apart\n",
    "\n",
    "✅ If the pair is positive → minimize their distance\n",
    "✅ If the pair is negative → make sure they’re far apart (beyond margin)\n",
    "\n",
    "---\n",
    "\n",
    "## **(b) InfoNCE Loss (used in SimCLR, MoCo, CLIP, etc.)**\n",
    "\n",
    "Modern contrastive learning methods use a **softmax-based loss**:\n",
    "\n",
    "Given an anchor $ i $, a positive sample $ j $, and $N-1$ negative samples:\n",
    "\n",
    "$$\n",
    "L_i = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $z_i = f(x_i)$ and (z_j = f(x_j)$ are normalized embeddings\n",
    "* $\\text{sim}(a,b) = a^\\top b$ (cosine similarity)\n",
    "* $\\tau$ is a temperature hyperparameter controlling sharpness\n",
    "\n",
    "This is similar to a cross-entropy loss where we classify the correct positive among all negatives.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. How the Learning Works**\n",
    "\n",
    "1. **Data Augmentation**\n",
    "   Create two random augmentations $x_i$ and $x_j$ from the same image.\n",
    "\n",
    "2. **Encoder Network**\n",
    "   Pass both through an encoder $f(\\cdot)$, e.g. ResNet, to get embeddings $z_i$, $z_j$.\n",
    "\n",
    "3. **Normalization**\n",
    "   Normalize each embedding to unit length.\n",
    "\n",
    "4. **Loss Computation**\n",
    "   Apply contrastive loss to pull together embeddings of same images and push apart others.\n",
    "\n",
    "---\n",
    "\n",
    "# **5. After Training**\n",
    "\n",
    "The encoder $f(\\cdot)$ learns a **semantic representation**:\n",
    "\n",
    "* Similar images → nearby in embedding space\n",
    "* Dissimilar images → far apart\n",
    "\n",
    "Then you can:\n",
    "\n",
    "* Freeze $f(\\cdot)$ and train a small linear classifier (Linear Evaluation Protocol)\n",
    "* Fine-tune on downstream tasks (classification, detection, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Example (Toy Numerical Example)**\n",
    "\n",
    "Suppose we have 3 embeddings (unit normalized):\n",
    "\n",
    "|        Sample       | Embedding (z) |\n",
    "| :-----------------: | :------------ |\n",
    "|          A₁         | [1, 0]        |\n",
    "|   A₂ (same image)   | [0.9, 0.1]    |\n",
    "| B (different image) | [0, 1]        |\n",
    "\n",
    "Compute cosine similarities:\n",
    "\n",
    "$$\n",
    "\\text{sim}(A₁, A₂) = 0.9 \\quad \\text{(positive pair)}\n",
    "$$\n",
    "$$\n",
    "\\text{sim}(A₁, B) = 0.0 \\quad \\text{(negative pair)}\n",
    "$$\n",
    "\n",
    "Then for anchor A₁:\n",
    "\n",
    "$$\n",
    "L_{A₁} = -\\log \\frac{\\exp(0.9/\\tau)}{\\exp(0.9/\\tau) + \\exp(0.0/\\tau)}\n",
    "$$\n",
    "\n",
    "If $\\tau = 0.1$,\n",
    "\n",
    "$$\n",
    "L_{A₁} \\approx -\\log \\frac{e^{9}}{e^{9}+e^{0}} = -\\log \\frac{8103}{8104} \\approx 0.00012\n",
    "$$\n",
    "\n",
    "Very small loss ⇒ the model did a good job (positive much higher than negative).\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Summary**\n",
    "\n",
    "| Concept        | Description                                                             |\n",
    "| :------------- | :---------------------------------------------------------------------- |\n",
    "| Goal           | Learn embeddings where similar samples are close, dissimilar are far    |\n",
    "| Label type     | Self-supervised (no manual labels)                                      |\n",
    "| Positive pairs | Different augmentations of same image                                   |\n",
    "| Negative pairs | Other images in batch                                                   |\n",
    "| Typical loss   | InfoNCE                                                                 |\n",
    "| Examples       | SimCLR, MoCo, CLIP, BYOL (no negatives), DINO (teacher-student variant) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b780e77-0e02-4c56-ac54-e248c0cf71b7",
   "metadata": {},
   "source": [
    " famous **contrastive learning frameworks** or architectures that use **contrastive loss** (or related self-supervised objectives).\n",
    "\n",
    "Here’s a structured list, from classical to modern, showing the evolution of **contrastive learning** in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Classical Contrastive Networks (Metric Learning)**\n",
    "\n",
    "| Method              | Year | Key Idea                                                                                          | Typical Architecture        |\n",
    "| :------------------ | :--: | :------------------------------------------------------------------------------------------------ | :-------------------------- |\n",
    "| **Siamese Network** | 1993 | Two identical networks (shared weights) compare two inputs; use **contrastive loss** with margin. | Twin CNNs or MLPs           |\n",
    "| **Triplet Network** | 2015 | Uses triplets: (anchor, positive, negative) with **triplet loss** to learn embedding distances.   | CNN backbone (e.g. FaceNet) |\n",
    "| **FaceNet**         | 2015 | Learns facial embeddings via triplet loss for face recognition.                                   | Inception-style CNN         |\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Modern Contrastive Learning (Self-Supervised Representation Learning)**\n",
    "\n",
    "| Method                                            |         Year         | Framework                                   | Key Idea                                                                                        |\n",
    "| :------------------------------------------------ | :------------------: | :------------------------------------------ | :---------------------------------------------------------------------------------------------- |\n",
    "| **SimCLR**                                        |     2020 (Google)    | Contrastive, simple & direct                | Two augmented views → encoder (ResNet) → projection MLP → InfoNCE loss. Needs large batch size. |\n",
    "| **MoCo (Momentum Contrast)**                      | 2020 (Facebook/Meta) | Contrastive + memory queue                  | Keeps a momentum encoder and a dictionary of past embeddings (avoids large batch).              |\n",
    "| **BYOL (Bootstrap Your Own Latent)**              |    2020 (DeepMind)   | Self-distillation, no negatives             | Uses an online & target network; learns from similarity of embeddings, no negative pairs.       |\n",
    "| **SwAV (Swapping Assignments)**                   |    2020 (Facebook)   | Cluster-based contrastive                   | Learns by predicting cluster assignments of augmentations (no explicit negatives).              |\n",
    "| **SimSiam**                                       |    2021 (Facebook)   | Simple, no negatives, no momentum           | Like BYOL but simpler; prevents collapse via stop-gradient.                                     |\n",
    "| **Barlow Twins**                                  |    2021 (Facebook)   | Redundancy reduction                        | Encourages embeddings of two views to be similar (diagonal of cross-correlation = 1).           |\n",
    "| **DINO**                                          |    2021 (Facebook)   | Self-distillation with teacher–student ViTs | Contrastive-like, uses cross-view prediction between teacher and student ViTs.                  |\n",
    "| **CLIP (Contrastive Language–Image Pretraining)** |     2021 (OpenAI)    | Cross-modal contrastive                     | Aligns image and text embeddings using contrastive loss across modalities.                      |\n",
    "| **ALIGN**                                         |     2021 (Google)    | CLIP-like, large scale                      | Same idea as CLIP but trained on massive noisy web data.                                        |\n",
    "| **SLIP**                                          |         2022         | CLIP + SimCLR                               | Combines supervised and self-supervised contrastive objectives.                                 |\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Specialized or Extended Contrastive Frameworks**\n",
    "\n",
    "| Method                                  | Year | Domain              | Description                                                     |\n",
    "| :-------------------------------------- | :--: | :------------------ | :-------------------------------------------------------------- |\n",
    "| **CPC (Contrastive Predictive Coding)** | 2018 | Audio, vision, NLP  | Predicts future latent representations using contrastive loss.  |\n",
    "| **CMC (Contrastive Multiview Coding)**  | 2019 | Multi-view          | Learns shared representations from multiple views/modalities.   |\n",
    "| **TCL (Temporal Contrastive Learning)** | 2020 | Video / time-series | Uses temporal augmentations to learn temporal consistency.      |\n",
    "| **PixPro**                              | 2021 | Dense prediction    | Contrastive learning for pixel-level features.                  |\n",
    "| **DetCon**                              | 2021 | Object-level        | Instance-level contrastive learning for segmentation/detection. |\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Key Variations by Type**\n",
    "\n",
    "| Type                                           | Examples            | Description                                           |\n",
    "| :--------------------------------------------- | :------------------ | :---------------------------------------------------- |\n",
    "| **Instance-level contrastive**                 | SimCLR, MoCo        | Treats each image as its own class                    |\n",
    "| **Cluster-based contrastive**                  | SwAV, DeepCluster   | Learns by grouping similar embeddings                 |\n",
    "| **Cross-modal contrastive**                    | CLIP, ALIGN         | Aligns different modalities (image–text, audio–video) |\n",
    "| **Teacher–student contrastive (distillation)** | BYOL, DINO, SimSiam | Learns without explicit negatives                     |\n",
    "| **Predictive contrastive**                     | CPC, PixPro         | Predicts representations across space or time         |\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Summary Table of Core Ideas**\n",
    "\n",
    "| Method            |  Negatives? | Architecture                 | Notes               |\n",
    "| :---------------- | :---------: | :--------------------------- | :------------------ |\n",
    "| Siamese / Triplet |      ✅      | Twin CNNs                    | Metric learning     |\n",
    "| SimCLR            |      ✅      | Shared ResNet + MLP          | Needs large batches |\n",
    "| MoCo              |      ✅      | Two encoders (momentum)      | Queue of negatives  |\n",
    "| BYOL              |      ❌      | Online / target              | Self-distillation   |\n",
    "| SimSiam           |      ❌      | Online / stop-grad           | Simpler BYOL        |\n",
    "| SwAV              | ⚙️ implicit | Shared encoder + prototypes  | Cluster-based       |\n",
    "| CLIP              |      ✅      | Image encoder + text encoder | Cross-modal         |\n",
    "| DINO              | ⚙️ implicit | Student / teacher ViTs       | No labels           |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to organize them visually — e.g., in a **chronological timeline diagram** showing how each evolved from the previous one (Siamese → SimCLR → BYOL → DINO → CLIP)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d959eb-daf8-497f-ac63-f56988a89686",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **1. The setup**\n",
    "\n",
    "You have a batch of images:\n",
    "$$\n",
    "{A, B, C, D, \\dots}\n",
    "$$\n",
    "\n",
    "For each image, you create **two random augmentations**, for example:\n",
    "\n",
    "| Original | Augmentation 1 | Augmentation 2 |\n",
    "| :------- | :------------- | :------------- |\n",
    "| A        | A₁             | A₂             |\n",
    "| B        | B₁             | B₂             |\n",
    "| C        | C₁             | C₂             |\n",
    "\n",
    "So now the batch size (after augmentation) is **2 × N**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Pass all of them through the encoder**\n",
    "\n",
    "You pass **all** (2N) augmented images through the same encoder $f(\\cdot)$ (e.g. ResNet).\n",
    "You get embeddings:\n",
    "\n",
    "$$\n",
    "{z_{A₁}, z_{A₂}, z_{B₁}, z_{B₂}, \\dots, z_{N₁}, z_{N₂}}\n",
    "$$\n",
    "\n",
    "These are typically L2-normalized.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Define positives and negatives**\n",
    "\n",
    "* Each pair ((A₁, A₂)) is a **positive pair** — they come from the same original image.\n",
    "* Everything else (e.g. (A₁) vs (B₁), (A₁) vs (B₂), etc.) are **negative pairs**.\n",
    "\n",
    "So for each anchor (say (A₁)):\n",
    "\n",
    "* **Positive:** (A₂)\n",
    "* **Negatives:** all other (2N - 2) embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Compute all similarities**\n",
    "\n",
    "You compute **cosine similarities** between every pair of embeddings in the batch:\n",
    "\n",
    "$$\n",
    "\\text{sim}(z_i, z_j) = \\frac{z_i^\\top z_j}{|z_i||z_j|}\n",
    "$$\n",
    "\n",
    "That gives you a $2N \\times 2N$ similarity matrix.\n",
    "\n",
    "Example for a batch with A and B (N=2):\n",
    "\n",
    "| Anchor | A₁    | A₂    | B₁    | B₂    |\n",
    "| :----- | :---- | :---- | :---- | :---- |\n",
    "| A₁     | –     | **+** | –     | –     |\n",
    "| A₂     | **+** | –     | –     | –     |\n",
    "| B₁     | –     | –     | –     | **+** |\n",
    "| B₂     | –     | –     | **+** | –     |\n",
    "\n",
    "(“+” means positive pair)\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Apply the InfoNCE loss**\n",
    "\n",
    "For anchor $i$ (say A₁), the loss is:\n",
    "\n",
    "$$\n",
    "L_i = -\\log \\frac{\\exp(\\text{sim}(z_i, z_{pos})/\\tau)}{\\sum_{k \\neq i} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "So it tries to make:\n",
    "\n",
    "* $\\text{sim}(z_i, z_{pos})$ large (positive pair close)\n",
    "* $\\text{sim}(z_i, z_{neg})$ small (negative pairs far apart)\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Overall training objective**\n",
    "\n",
    "Average over all anchors:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum_{i=1}^{2N} L_i\n",
    "$$\n",
    "\n",
    "The network learns an encoder $f(\\cdot)$ that produces embeddings where:\n",
    "\n",
    "* Two augmentations of the same image → high cosine similarity\n",
    "* Augmentations from different images → low cosine similarity\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary**\n",
    "\n",
    "| Step | Description                                                        |\n",
    "| :--- | :----------------------------------------------------------------- |\n",
    "| 1    | Start with batch of N images                                       |\n",
    "| 2    | Create two augmented views → total 2N samples                      |\n",
    "| 3    | Encode all 2N images → embeddings                                  |\n",
    "| 4    | Compute cosine similarity matrix                                   |\n",
    "| 5    | For each sample, find its positive and treat the rest as negatives |\n",
    "| 6    | Compute InfoNCE loss                                               |\n",
    "| 7    | Backpropagate, update encoder weights                              |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f14cd-a19f-4900-96de-93dfd1cd5f53",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **1. What we learn**\n",
    "\n",
    "In contrastive learning, we are **not** learning a classifier (like in supervised learning).\n",
    "We are learning a **representation function** (an encoder) that maps raw data (e.g., images) to a **semantic embedding space**.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "f_\\theta: \\mathbb{R}^{H \\times W \\times 3} \\rightarrow \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "where $D = 2048$ for ResNet-50 (after global average pooling).\n",
    "\n",
    "The idea is to learn parameters $ \\theta $ so that:\n",
    "\n",
    "* If two images are semantically similar → their embeddings $ f_\\theta(x_1) $ and $ f_\\theta(x_2) $ have **high cosine similarity**.\n",
    "* If two images are different → their embeddings have **low cosine similarity**.\n",
    "\n",
    "So, the encoder learns to **cluster semantically similar inputs together** in feature space — **without any labels**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why we remove the classification head**\n",
    "\n",
    "The classification head (usually a `Linear(2048, num_classes)`) is task-specific.\n",
    "But in self-supervised contrastive learning, there are **no class labels**.\n",
    "So we remove the head and only keep the encoder backbone (like ResNet-50 up to global pooling).\n",
    "\n",
    "Sometimes we add a **projection head**, typically a small MLP (used in SimCLR):\n",
    "\n",
    "$$\n",
    "z = g(f_\\theta(x))\n",
    "$$\n",
    "\n",
    "where $ g(\\cdot) $ is a small 2-layer MLP that projects the representation into a latent space where the contrastive loss is applied.\n",
    "\n",
    "At inference time, we **discard** $g(\\cdot)$ and only use $f_\\theta(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. What happens inside**\n",
    "\n",
    "By training with contrastive loss, the encoder (f_\\theta) gradually learns:\n",
    "\n",
    "* To ignore **augmentations** (e.g., color jitter, crop, rotation)\n",
    "* To capture **semantic meaning** (what’s actually in the image)\n",
    "\n",
    "So the embedding becomes *invariant* to data augmentations and *discriminative* between different images.\n",
    "\n",
    "In effect, the model builds **features similar to those learned with labels**, but purely from self-supervision.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Example: ResNet-50 Encoder**\n",
    "\n",
    "Let’s say we use ResNet-50 pretrained with SimCLR.\n",
    "\n",
    "* Input: (x \\in \\mathbb{R}^{224\\times224\\times3})\n",
    "* Encoder (ResNet-50, no head): outputs a 2048-dimensional vector\n",
    "* Projection head: maps 2048 → 128 (for contrastive loss)\n",
    "* Loss: InfoNCE applied on those 128-D vectors\n",
    "\n",
    "After training, we discard the projection head and use the **2048-D feature vectors** as rich, semantically meaningful representations.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. What do these 2048-D embeddings represent?**\n",
    "\n",
    "Each dimension in the 2048-D embedding does not correspond to an explicit human concept.\n",
    "But together, they represent the *position* of an image in a semantic space where:\n",
    "\n",
    "* Images of **cats** cluster near each other\n",
    "* Images of **cars** form another cluster\n",
    "* Similar poses, lighting, or styles are close as well\n",
    "\n",
    "This is called a **representation space** or **feature space**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Using these learned features**\n",
    "\n",
    "Once trained, you can use (f_\\theta) for downstream tasks:\n",
    "\n",
    "| Downstream Task                     | How to Use (f_\\theta)                                       |\n",
    "| :---------------------------------- | :---------------------------------------------------------- |\n",
    "| **Image Classification**            | Freeze (f_\\theta), train a linear classifier on top         |\n",
    "| **Object Detection / Segmentation** | Fine-tune (f_\\theta) as the backbone                        |\n",
    "| **Clustering / Retrieval**          | Use embeddings directly (e.g., cosine similarity search)    |\n",
    "| **Visualization**                   | Reduce 2048-D to 2-D with t-SNE or UMAP to inspect clusters |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key takeaway**\n",
    "\n",
    "| Concept                    | Meaning                                                                |\n",
    "| :------------------------- | :--------------------------------------------------------------------- |\n",
    "| Training objective         | Learn embeddings where similar inputs are close and dissimilar are far |\n",
    "| What’s learned             | Feature extractor (f_\\theta) (e.g., ResNet without head)               |\n",
    "| What’s used after training | Encoder output (e.g., 2048-D embedding)                                |\n",
    "| Why useful                 | Works as a pretrained backbone for many downstream tasks               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744ee9a-9065-4587-9c92-837a153bf416",
   "metadata": {},
   "source": [
    " we update the **ResNet (encoder) parameters** (and optionally the small projection head), just like in supervised training.\n",
    "\n",
    "Let’s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What parameters exist**\n",
    "\n",
    "When doing contrastive learning (e.g., SimCLR), we typically have two learnable modules:\n",
    "\n",
    "1. **Encoder** ( f_\\theta(\\cdot) ):\n",
    "\n",
    "   * A backbone CNN (like ResNet-50)\n",
    "   * Parameters ( \\theta ) are all convolution weights, batch norm, etc.\n",
    "   * Output: a 2048-dim feature vector\n",
    "\n",
    "2. **Projection Head** ( g_\\phi(\\cdot) ):\n",
    "\n",
    "   * Usually a small MLP (e.g. 2048→512→128)\n",
    "   * Parameters ( \\phi ) are learnable weights of that MLP\n",
    "   * Output: a low-dim embedding ( z ) used in the loss\n",
    "\n",
    "So the full forward path for one image is:\n",
    "\n",
    "$$\n",
    "z = g_\\phi(f_\\theta(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **2. What we optimize**\n",
    "\n",
    "We compute the **contrastive loss (InfoNCE)** over all positive and negative pairs:\n",
    "\n",
    "$$\n",
    "L = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "$$\n",
    "\n",
    "Then we perform **backpropagation**:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta,\\phi} L\n",
    "$$\n",
    "\n",
    "So the gradients flow **through the whole network** — both the projection head (g_\\phi) and the encoder (f_\\theta).\n",
    "\n",
    "That means **all ResNet layers (convolutions, BatchNorm, etc.) are updated.**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. What happens at inference time**\n",
    "\n",
    "After training:\n",
    "\n",
    "* The **projection head (g_\\phi)** is discarded.\n",
    "* We keep only the **encoder (f_\\theta)**.\n",
    "\n",
    "Why?\n",
    "Because (f_\\theta) now produces embeddings that are **semantically meaningful** — we don’t need the contrastive projection head anymore.\n",
    "\n",
    "So in downstream use (classification, detection, etc.), we use:\n",
    "\n",
    "$$\n",
    "h = f_\\theta(x)\n",
    "$$\n",
    "\n",
    "where (h \\in \\mathbb{R}^{2048}).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Optional frozen variants**\n",
    "\n",
    "Sometimes people:\n",
    "\n",
    "* **Freeze** the encoder (f_\\theta) and train only (g_\\phi) (for ablation)\n",
    "* Or, in methods like MoCo, **maintain a momentum encoder** (a moving average of parameters, not directly updated by gradient descent)\n",
    "\n",
    "But in standard **SimCLR-style contrastive learning**, **both (f_\\theta) and (g_\\phi) are optimized jointly**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. In summary**\n",
    "\n",
    "| Component                       | Learnable? | Updated during training? | Used after training? |\n",
    "| :------------------------------ | :--------: | :----------------------: | :------------------: |\n",
    "| Encoder (ResNet, (f_\\theta))    |      ✅     |             ✅            |           ✅          |\n",
    "| Projection Head (MLP, (g_\\phi)) |      ✅     |             ✅            |     ❌ (discarded)    |\n",
    "| Classification Head             |      ❌     |             ❌            |     ❌ (not used)     |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Gradient flow (schematic)**\n",
    "\n",
    "```\n",
    "x1 ──► fθ ──► gφ ──► z1\n",
    "x2 ──► fθ ──► gφ ──► z2\n",
    "             │\n",
    "             ▼\n",
    "         Contrastive Loss (InfoNCE)\n",
    "             │\n",
    "             ▼\n",
    "        Backprop → updates θ, φ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "So yes — **the ResNet parameters are updated** to learn a general-purpose visual representation that encodes semantic similarity, even though we never gave it labels.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f1255-cf9a-4b99-8497-6643f62b055b",
   "metadata": {},
   "source": [
    "Let’s go through a **minimal, fully working PyTorch example** that shows how **InfoNCE contrastive loss** works step by step — with actual tensor values you can inspect.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**\n",
    "\n",
    "We’ll simulate a small batch of “images” (just random vectors here for simplicity),\n",
    "compute two augmented versions of each, and apply the InfoNCE loss.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# === Parameters ===\n",
    "batch_size = 4       # number of original images\n",
    "feature_dim = 128     # embedding size (like output of projection head)\n",
    "temperature = 0.1\n",
    "\n",
    "# === Simulate embeddings for 2 augmentations of each image ===\n",
    "# Normally, you'd pass images through encoder f(x) + projection head g(x)\n",
    "z1 = F.normalize(torch.randn(batch_size, feature_dim), dim=1)\n",
    "z2 = F.normalize(torch.randn(batch_size, feature_dim), dim=1)\n",
    "\n",
    "# Concatenate them to form 2N embeddings (each original image has 2 views)\n",
    "z = torch.cat([z1, z2], dim=0)   # shape [2N, D]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Compute cosine similarity matrix**\n",
    "\n",
    "We compute cosine similarity for all pairs (2N × 2N matrix):\n",
    "\n",
    "```python\n",
    "# Cosine similarity between every pair\n",
    "sim_matrix = torch.matmul(z, z.T) / temperature\n",
    "\n",
    "# To avoid comparing a sample with itself\n",
    "mask = torch.eye(2 * batch_size, dtype=torch.bool)\n",
    "sim_matrix.masked_fill_(mask, -9e15)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Define positive pairs**\n",
    "\n",
    "In SimCLR:\n",
    "\n",
    "* For each sample `i`, its positive pair is `i + N` (the augmented view)\n",
    "* So, for example, z[0] ↔ z[4], z[1] ↔ z[5], etc.\n",
    "\n",
    "```python\n",
    "# Positive pairs: index offset by N\n",
    "pos_indices = torch.arange(batch_size, 2 * batch_size)\n",
    "pos_sim_1 = torch.diag(sim_matrix, batch_size)\n",
    "pos_sim_2 = torch.diag(sim_matrix, -batch_size)\n",
    "\n",
    "# Combine the positive similarities for all anchors\n",
    "positives = torch.cat([pos_sim_1, pos_sim_2], dim=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Compute InfoNCE loss**\n",
    "\n",
    "We treat the positive sample as the “correct class” among all negatives.\n",
    "\n",
    "```python\n",
    "# Numerator: exp(similarity of positive pair / tau)\n",
    "exp_pos = torch.exp(positives)\n",
    "\n",
    "# Denominator: sum of exp(similarity to all others)\n",
    "exp_all = torch.exp(sim_matrix).sum(dim=1)\n",
    "\n",
    "# InfoNCE loss\n",
    "loss = -torch.log(exp_pos / exp_all)\n",
    "loss = loss.mean()\n",
    "\n",
    "print(f\"Contrastive (InfoNCE) Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Output example**\n",
    "\n",
    "You’ll get something like:\n",
    "\n",
    "```\n",
    "Contrastive (InfoNCE) Loss: 5.3162\n",
    "```\n",
    "\n",
    "The exact value depends on random initialization, but lower loss means\n",
    "the model’s embeddings bring positives closer and push negatives apart.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Recap**\n",
    "\n",
    "Here’s what we did conceptually:\n",
    "\n",
    "| Step | Operation                                  | Purpose                                       |\n",
    "| :--- | :----------------------------------------- | :-------------------------------------------- |\n",
    "| 1    | Created two augmented batches (`z1`, `z2`) | Positive pairs                                |\n",
    "| 2    | Normalized embeddings                      | For cosine similarity                         |\n",
    "| 3    | Built similarity matrix                    | All pairwise similarities                     |\n",
    "| 4    | Applied InfoNCE loss                       | Pull positives together, push negatives apart |\n",
    "| 5    | Averaged loss                              | Backprop to update encoder & projection head  |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to extend this example to include a **real ResNet backbone** and **projection head** (so you can see how gradients flow through the network)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
