{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a7c67e-69eb-4903-a632-a40c3ab3f78e",
   "metadata": {},
   "source": [
    "##  **1. Custom Dataset Class**\n",
    "\n",
    "```\n",
    "torch/\n",
    "    __init__.py\n",
    "    utils/\n",
    "        __init__.py\n",
    "        data/\n",
    "            __init__.py  # This is a crucial file that makes 'data' a Python module\n",
    "            dataset.py   # This file defines the base Dataset class and random_split\n",
    "            dataloader.py  # This file defines the DataLoader class\n",
    "            sampler.py\n",
    "```\n",
    "\n",
    "\n",
    "So, you can use: \n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "```\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "import torch.utils.data.dataset as dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "dataset.random_split()\n",
    "dataloader.DataLoader()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "You define your dataset by subclassing `torch.utils.data.Dataset` and overriding `__len__()` and `__getitem__()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27788f3d-9559-4514-a68b-31189c2d868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "\n",
    "\n",
    "class MyCustomDataSet(Dataset):\n",
    "    def __init__(self, data, lables):\n",
    "        self.data = data\n",
    "        self.lables = lables\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.lables[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69386d8-18c5-46c5-93fd-84d5874c2f5f",
   "metadata": {},
   "source": [
    "##  **2. `random_split`** \n",
    "\n",
    "`torch.utils.data.random_split(dataset, lengths, generator=None)`: splits a dataset into non-overlapping new datasets of given lengths.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **2.1. Only Index-Based – No Data Copy**\n",
    "\n",
    "* `random_split` **does not copy** the underlying data.\n",
    "* It **wraps the original dataset** and uses internally shuffled indices to simulate subsets.\n",
    "* Memory usage is minimal because it's just a view via `Subset`.\n",
    "\n",
    "Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cb5fae-6f40-4606-8068-965be038b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "sample_size = 100\n",
    "data = torch.randn(sample_size, 2)\n",
    "lables = torch.randint(0, 2, (sample_size,))\n",
    "\n",
    "dataset = MyCustomDataSet(data=data, lables=lables)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "val_size = int(0.15*len(dataset))\n",
    "test_size = len(dataset)-train_size-val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5438d20-c333-4950-a9e6-6e2a130b2cfa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  **2.2. Reproducibility with Generator**\n",
    "\n",
    "To ensure reproducibility (same split every run), pass a seeded `torch.Generator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b78021-8272-4cfd-bb86-b911b8772b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6a7c2-2379-41ca-a4d2-917ef9820347",
   "metadata": {},
   "source": [
    "If you don't pass a generator, a random seed is used from the system, and results will vary across runs.\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.3. How It Works Internally**\n",
    "\n",
    "* Internally, it:\n",
    "\n",
    "  * Shuffles indices using the generator (if given),\n",
    "  * Splits them into the specified sizes,\n",
    "  * Creates `Subset(dataset, indices)` for each split.\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.4.Common Pitfalls**\n",
    "\n",
    "* **Don't modify the original dataset in-place** after splitting. The splits reference it.\n",
    "* Be careful with imbalanced class distributions — `random_split` does **not** preserve class ratios.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e670f8f-8c37-4e75-8394-cd890ed16c9c",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3c8717-cafc-407e-ae97-eef037874a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2\n",
      "0 3\n",
      "7 2\n",
      "10 3\n",
      "6 3\n",
      "15 0\n",
      "2 2\n",
      "13 2\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data = torch.randint(low=0, high=20, size=(10,))\n",
    "labels = torch.randint(low=0, high=4, size=(10,))\n",
    "\n",
    "dataset = MRIDataset(data=data, labels=labels)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [0.7, 0.15, 0.15], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "\n",
    "for data, label in train_dataset:\n",
    "    print(data.item(), label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367aa8d3-b870-43d6-9b8d-7860c90cbe56",
   "metadata": {},
   "source": [
    "## **3.`Subset`**\n",
    "\n",
    "`Subset` creates a **view** of a dataset using a list of indices. It’s a wrapper that lets you work with just a portion of a dataset **without copying** the data.\n",
    "\n",
    "```python\n",
    "torch.utils.data.Subset(dataset, indices)\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a21177-54ed-4289-9a54-e2170f7f01ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2), tensor(2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "subset = Subset(dataset, indices)\n",
    "\n",
    "print(dataset[0])\n",
    "\n",
    "subset.dataset   # Original dataset\n",
    "subset.indices   # List of indices used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2ce91-b603-4091-aaf0-7f660264118b",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1. **No Data Copy**\n",
    "\n",
    "* Like `random_split`, `Subset` does **not duplicate data** — it just stores references (indices).\n",
    "* It’s memory-efficient and fast.\n",
    "\n",
    "#### 3.2. **How It Works**\n",
    "\n",
    "* Internally, `Subset` defines `__getitem__` like this:\n",
    "\n",
    "  ```python\n",
    "  def __getitem__(self, idx):\n",
    "      return self.dataset[self.indices[idx]]\n",
    "  ```\n",
    "* So each item access fetches from the original dataset using the provided index mapping.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.3 Stratified Splits with scikit-learn**\n",
    "\n",
    "\n",
    "\n",
    "You can use `StratifiedShuffleSplit` to split based on labels and then wrap them in `Subset`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "targets = dataset.targets  # Or dataset.labels depending on the dataset\n",
    "\n",
    "for train_idx, val_idx in sss.split(X=targets, y=targets):\n",
    "    train_ds = Subset(dataset, train_idx)\n",
    "    val_ds = Subset(dataset, val_idx)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3f302-ac2b-49d7-ae96-5319dd5d76f0",
   "metadata": {},
   "source": [
    "##  **4. ImageFolder**\n",
    "\n",
    "\n",
    "In PyTorch, `torchvision.datasets.ImageFolder` is a utility class for loading image datasets arranged in a specific directory structure. It automatically assigns labels based on subdirectory names, making it ideal for classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Directory Structure**\n",
    "\n",
    "`ImageFolder` expects the dataset directory to be structured like this:\n",
    "\n",
    "```\n",
    "root/\n",
    "    class1/\n",
    "        img1.png\n",
    "        img2.png\n",
    "        ...\n",
    "    class2/\n",
    "        img3.png\n",
    "        img4.png\n",
    "        ...\n",
    "```\n",
    "\n",
    "* Each **subfolder** under `root` is treated as a class.\n",
    "* All images inside a class folder are treated as samples of that class.\n",
    "\n",
    "---\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define optional transforms (resizing, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root='path/to/root', transform=transform)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Labels and Classes**\n",
    "\n",
    "* `dataset.classes`: list of class names (e.g., `['cat', 'dog']`)\n",
    "* `dataset.class_to_idx`: dict mapping class names to label indices (e.g., `{'cat': 0, 'dog': 1}`)\n",
    "* Each sample is a tuple: `(image_tensor, label)`\n",
    "\n",
    "You can access an image and its label like this:\n",
    "\n",
    "```python\n",
    "img, label = dataset[0]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268bef9-3fd7-4f4c-992b-10a821e185b9",
   "metadata": {},
   "source": [
    "##  **5. DataLoader**\n",
    "\n",
    "\n",
    "```\n",
    "torch/\n",
    "    __init__.py\n",
    "    utils/\n",
    "        __init__.py\n",
    "        data/\n",
    "            __init__.py  # This is a crucial file that makes 'data' a Python module\n",
    "            dataset.py   # This file defines the base Dataset class and random_split\n",
    "            dataloader.py  # This file defines the DataLoader class\n",
    "            sampler.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81982fa5-c3c2-4b8f-8fea-cc40485c7ba7",
   "metadata": {},
   "source": [
    "So, you can use: \n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "```\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "import torch.utils.data.dataset as dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "dataset.random_split()\n",
    "dataloader.DataLoader()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59840f-7604-4373-9924-3ee62202d471",
   "metadata": {},
   "source": [
    "\n",
    "The `DataLoader` handles batching, shuffling, and loading the data in parallel with multiple workers.\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MyDataset(data=torch.randn(100, 3, 32, 32), labels=torch.randint(0, 10, (100,)))\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  **Using the DataLoader in Training**\n",
    "\n",
    "```python\n",
    "for batch in loader:\n",
    "    inputs, targets = batch\n",
    "    # your training loop here\n",
    "```\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3887aff4-110c-4042-b4e4-05780e6094eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 16,  4, 15])\n",
      "tensor([3, 2, 3, 0])\n",
      "==================================================\n",
      "tensor([13,  6, 14,  0])\n",
      "tensor([2, 3, 2, 3])\n",
      "==================================================\n",
      "tensor([2, 7])\n",
      "tensor([2, 2])\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 4\n",
    "loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                    shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "for batch in loader:\n",
    "    targets, input = batch\n",
    "    print(targets)\n",
    "    print(input)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02a5c5-704e-43db-8696-93c8a0471c7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Alternative: Use Built-in Datasets\n",
    "\n",
    "PyTorch provides several datasets in `torchvision.datasets` (for images) and `torchtext`, `torchaudio`, etc.\n",
    "\n",
    "Example with CIFAR-10:\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c11eb8-92c4-4fc6-bade-ac2439175405",
   "metadata": {},
   "source": [
    "## Standard Practice For Loading Dataset\n",
    "\n",
    "The **common and correct** approach in PyTorch is:\n",
    "\n",
    "\n",
    "#### **1. Keep ALL dataset tensors on CPU**\n",
    "\n",
    "You do **not** move the raw dataset to GPU.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = torch.arange(0, 10, 0.5)   # CPU\n",
    "labels = func(x)               # CPU\n",
    "```\n",
    "\n",
    "#### **2. Use DataLoader with:**\n",
    "\n",
    "* `pin_memory=True`\n",
    "* `num_workers > 0` (for speed)\n",
    "\n",
    "```python\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### **3. Move batches to GPU inside the training loop**\n",
    "\n",
    "This is where `.to(device)` is used:\n",
    "\n",
    "```python\n",
    "for xb, yb in loader:\n",
    "    xb = xb.to(device, non_blocking=True)\n",
    "    yb = yb.to(device, non_blocking=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why this is the correct approach?\n",
    "\n",
    "#### **Reason 1 — GPU memory is limited**\n",
    "\n",
    "Datasets can be gigabytes.\n",
    "GPU memory is 4–24 GB.\n",
    "\n",
    "#### **Reason 2 — pin_memory=True speeds up CPU → GPU transfer**\n",
    "\n",
    "Pinned memory works only for CPU tensors.\n",
    "It allows **DMA** fast transfer to GPU.\n",
    "\n",
    "#### **Reason 3 — DataLoader performs prefetching & asynchronous transfer**\n",
    "\n",
    "When you use:\n",
    "\n",
    "```python\n",
    "pin_memory=True\n",
    "non_blocking=True\n",
    "```\n",
    "\n",
    "PyTorch automatically overlaps:\n",
    "\n",
    "* next batch loading\n",
    "* with GPU training of current batch\n",
    "\n",
    "Which speeds training significantly.\n",
    "\n",
    "#### **Reason 4 — You avoid the error**\n",
    "\n",
    "GPU tensors cannot be pinned.\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ WRONG approach (your original)\n",
    "\n",
    "```python\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "```\n",
    "\n",
    "Dataset stays on GPU, so:\n",
    "\n",
    "* DataLoader cannot use pinned memory\n",
    "* GPU memory gets filled\n",
    "* CPU → GPU pipeline is broken\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
