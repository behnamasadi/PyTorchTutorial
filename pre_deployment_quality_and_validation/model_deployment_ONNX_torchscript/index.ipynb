{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91cfbc9b-c082-4601-b5e1-87caf93b05ba",
   "metadata": {},
   "source": [
    "- **If you want the simplest, cloud-native path (CPU/GPU, canary, autoscale):**\n",
    "  - **KServe (Istio) + ONNX Runtime** for online inference. Easy CRDs, autoscaling by QPS/latency, built-in canary, model stored on S3/GCS.\n",
    "- **If you need ultra-high throughput on NVIDIA GPUs or multi-model serving:**\n",
    "  - **NVIDIA Triton Inference Server** (serves PyTorch, TorchScript, ONNX, TensorRT) + Horizontal Pod Autoscaler/KEDA.\n",
    "\n",
    "Both run beautifully on k8s. TorchServe and Seldon Core are good too, but KServe and Triton cover most needs with less glue.\n",
    "\n",
    "---\n",
    "\n",
    "# Reference architectures\n",
    "\n",
    "## A) KServe + ONNX Runtime (recommended default)\n",
    "\n",
    "**Why:** Minimal boilerplate, great for HTTP/REST, autoscaling, canary, and observability.\n",
    "\n",
    "**Flow**\n",
    "\n",
    "1. Export model → `model.onnx` (or TorchScript `.pt`).\n",
    "2. Push to object storage (S3/GCS/MinIO).\n",
    "3. Deploy KServe `InferenceService` CRD that pulls the model.\n",
    "4. Ingress via Istio → autoscale with HPA/KPA → Prometheus metrics → Grafana.\n",
    "\n",
    "**ONNX example (single model):**\n",
    "\n",
    "```yaml\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: classifier-v1\n",
    "spec:\n",
    "  predictor:\n",
    "    onnx:\n",
    "      storageUri: \"s3://models/classifier/v1/\"\n",
    "      resources:\n",
    "        requests: { cpu: \"1\", memory: \"2Gi\" }\n",
    "        limits:   { cpu: \"2\", memory: \"4Gi\" }\n",
    "      runtimeVersion: \"1.17.3\"  # ONNX Runtime CPU image tag used by KServe\n",
    "```\n",
    "\n",
    "**GPU?** Switch to TensorRT or Triton runtime (below), or use KServe’s `triton` predictor with `nodeSelector`/`nvidia.com/gpu` limits.\n",
    "\n",
    "**Canary traffic split (v1=80%, v2=20%):**\n",
    "\n",
    "```yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: classifier\n",
    "spec:\n",
    "  predictor:\n",
    "    canaryTrafficPercent: 20\n",
    "    onnx:\n",
    "      storageUri: \"s3://models/classifier/v2/\"\n",
    "  predictorFormer:\n",
    "    onnx:\n",
    "      storageUri: \"s3://models/classifier/v1/\"\n",
    "```\n",
    "\n",
    "> Use KServe autoscaling by concurrency/QPS; add request/limit resources; wire Prometheus for p50/p95 latency dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## B) NVIDIA Triton on k8s (best for GPUs, ensembles, batching)\n",
    "\n",
    "**Why:** Highest throughput on NVIDIA GPUs, dynamic batching, multi-model/multi-framework, model ensembles, TensorRT acceleration.\n",
    "\n",
    "**Flow**\n",
    "\n",
    "1. Export → `model.onnx` (or `.pt` / TensorRT engine).\n",
    "2. Create Triton **model repository** (S3/PVC).\n",
    "\n",
    "   ```\n",
    "   repo/\n",
    "     model_x/1/model.onnx\n",
    "     model_x/config.pbtxt\n",
    "   ```\n",
    "3. Run Triton Deployment with GPU limits; expose via Ingress or KServe’s Triton runtime.\n",
    "\n",
    "**Minimal `config.pbtxt` (ONNX):**\n",
    "\n",
    "```\n",
    "name: \"model_x\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "dynamic_batching { preferred_batch_size: [4,8,16] max_queue_delay_microseconds: 2000 }\n",
    "input { name: \"input\" data_type: TYPE_FP32 dims: [3,224,224] }\n",
    "output { name: \"logits\" data_type: TYPE_FP32 dims: [10] }\n",
    "```\n",
    "\n",
    "**k8s Deployment (bare Triton):**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata: { name: triton-gpu }\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector: { matchLabels: { app: triton } }\n",
    "  template:\n",
    "    metadata: { labels: { app: triton } }\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: triton\n",
    "        image: nvcr.io/nvidia/tritonserver:24.05-py3\n",
    "        args: [\"tritonserver\",\"--model-repository=/models\",\"--strict-model-config=false\"]\n",
    "        ports: [{containerPort:8000},{containerPort:8001},{containerPort:8002}]\n",
    "        resources:\n",
    "          limits: { nvidia.com/gpu: 1, cpu: \"4\", memory: \"16Gi\" }\n",
    "          requests: { nvidia.com/gpu: 1, cpu: \"2\", memory: \"8Gi\" }\n",
    "        volumeMounts: [{ name: model-repo, mountPath: /models }]\n",
    "      volumes:\n",
    "      - name: model-repo\n",
    "        persistentVolumeClaim: { claimName: triton-models-pvc }\n",
    "      nodeSelector: { \"nvidia.com/gpu.present\": \"true\" }\n",
    "```\n",
    "\n",
    "Pair with **KEDA** (scale on GPU utilization or queue depth) or HPA (scale by QPS/p95).\n",
    "\n",
    "---\n",
    "\n",
    "# Export formats vs runtimes (quick guide)\n",
    "\n",
    "* **CPU only:** ONNX Runtime (fast), TorchScript (fine).\n",
    "* **NVIDIA GPU:** ONNX → TensorRT via Triton (fastest), or TorchScript on Triton/PyTorch backend (good).\n",
    "* **Heterogeneous cluster:** ONNX + KServe: one spec, many backends.\n",
    "* **Custom Python preprocessing:** KServe’s Python server (wrap pre/post), or Triton Python backend.\n",
    "\n",
    "---\n",
    "\n",
    "# CI/CD & model registry\n",
    "\n",
    "* Store `model.onnx`/`.pt` in S3/MinIO with semantic versions (`/classifier/v1/`, `/v2/`).\n",
    "* GitHub Actions:\n",
    "\n",
    "  1. export model,\n",
    "  2. push to S3 (`s3://models/app/model/vX/`),\n",
    "  3. `kubectl apply` updated `InferenceService` or bump Helm values,\n",
    "  4. run smoke test job hitting `/v1/models/.../infer`.\n",
    "* Keep a **shadow (champion–challenger)** job comparing outputs (tolerances) before promoting traffic.\n",
    "\n",
    "---\n",
    "\n",
    "# Observability & reliability\n",
    "\n",
    "* **Metrics:** Prometheus scrape (QPS, p50/p95/p99, GPU util, batch size). Grafana dashboards.\n",
    "* **Tracing:** OpenTelemetry on FastAPI edge (if you add a thin gateway) + Istio tracing.\n",
    "* **Logging:** JSON logs; centralize via Fluent Bit → Loki/ELK.\n",
    "* **Scaling:** HPA by concurrency/latency; KEDA by queue length (if using a message broker for async).\n",
    "* **Resilience:** liveness/readiness probes, resource requests/limits, PodDisruptionBudget, topology spread.\n",
    "\n",
    "---\n",
    "\n",
    "# Security & cost notes\n",
    "\n",
    "* Private models in S3: use IAM roles/IRSA or k8s Secrets; avoid embedding keys.\n",
    "* NetworkPolicies to restrict egress/ingress; mTLS with Istio.\n",
    "* Right-size: start CPU (ORT) if latency SLO allows; move hot models to GPU/TensorRT; enable dynamic batching.\n",
    "\n",
    "---\n",
    "\n",
    "# What I’d do for your cluster (opinionated plan)\n",
    "\n",
    "1. **Export** to **ONNX** (opset 17), keep TorchScript as a fallback.\n",
    "2. Put models in **MinIO** (dev) → S3 (prod).\n",
    "3. Use **KServe**:\n",
    "\n",
    "   * CPU services: `onnx` predictor (ORT).\n",
    "   * GPU services: `triton` predictor with dynamic batching; convert to TensorRT for hottest paths.\n",
    "4. Canary with KServe traffic split; autoscale via concurrency.\n",
    "5. Prometheus/Grafana + alerting on p95 latency & error rate.\n",
    "6. CI/CD: GitHub Actions builds/export/tests → updates `InferenceService` with versioned `storageUri`.\n",
    "\n",
    "If you share your target latency/QPS and whether you have GPUs, I’ll drop in ready-to-apply YAML (KServe CRDs) for your exact model shape and a one-click Helm setup (dev/prod values).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
