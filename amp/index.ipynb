{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f47000-d25d-47fe-a686-7c4644142ae5",
   "metadata": {},
   "source": [
    "# Automatic Mixed Precision amp\n",
    "\n",
    "\n",
    "## 1. What is `torch.amp.autocast`\n",
    "\n",
    "`torch.amp.autocast` automatically **casts operations to mixed precision** (float16 / bfloat16 and float32) to improve performance and reduce GPU memory usage, while maintaining numerical stability.\n",
    "\n",
    "It selectively runs operations in lower precision (e.g., `float16`) **when it’s safe**, and in full precision (e.g., `float32`) **when it’s necessary**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Basic Syntax\n",
    "\n",
    "In modern PyTorch (≥1.10, ≥2.0), it’s used like this:\n",
    "\n",
    "```python\n",
    "from torch import autocast\n",
    "\n",
    "with autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "```\n",
    "\n",
    "or (explicit import path):\n",
    "\n",
    "```python\n",
    "from torch.amp import autocast\n",
    "\n",
    "with autocast('cuda'):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. When to Use\n",
    "\n",
    "**During forward pass only**, i.e., inside `model(input)` and loss computation.\n",
    "The backward pass should use `torch.cuda.amp.GradScaler`.\n",
    "\n",
    "Typical usage is inside your training loop.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Complete Example (Training Loop)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "model = nn.Linear(512, 10).cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = GradScaler()  # scales loss to avoid underflow in float16\n",
    "\n",
    "for epoch in range(10):\n",
    "    for input, target in dataloader:\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision forward + loss computation\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # Scaled backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Step optimizer and update scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Explanation of Each Step\n",
    "\n",
    "| Step  | Code                            | Description                                     |\n",
    "| ----- | ------------------------------- | ----------------------------------------------- |\n",
    "| **1** | `with autocast(...)`            | Forward pass in mixed precision.                |\n",
    "| **2** | `loss = criterion(...)`         | Loss is computed in mixed precision.            |\n",
    "| **3** | `scaler.scale(loss).backward()` | Scales loss to prevent `inf`/`NaN` gradients.   |\n",
    "| **4** | `scaler.step(optimizer)`        | Unscales and applies gradients.                 |\n",
    "| **5** | `scaler.update()`               | Adjusts scaling dynamically for next iteration. |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. How `GradScaler` Works in Code\n",
    "\n",
    "```python\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "with autocast('cuda'):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "# Step 1: Scale the loss\n",
    "scaler.scale(loss).backward()  # multiplies loss by a large scale factor\n",
    "\n",
    "# Step 2: Unscale before optimizer step\n",
    "scaler.step(optimizer)         # divides grads back by the same scale\n",
    "\n",
    "# Step 3: Adjust the scale factor for next iteration\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 7. Notes and Best Practices\n",
    "\n",
    "1. **Do not** use autocast around the backward pass.\n",
    "   Only use it around the forward pass.\n",
    "\n",
    "2. **Always use GradScaler** when training in mixed precision on GPUs with FP16.\n",
    "   (Not needed for bfloat16 on newer GPUs like A100, H100.)\n",
    "\n",
    "3. If your model uses operations not compatible with FP16 (e.g., some custom CUDA ops), wrap them in:\n",
    "\n",
    "   ```python\n",
    "   with autocast(enabled=False):\n",
    "       x = custom_op(x)\n",
    "   ```\n",
    "\n",
    "4. You can disable autocast dynamically:\n",
    "\n",
    "   ```python\n",
    "   with autocast(enabled=False):\n",
    "       output = model(input)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. For Inference\n",
    "\n",
    "No `GradScaler` needed:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad(), autocast('cuda'):\n",
    "    output = model(input)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Autocast on CPU (optional)\n",
    "\n",
    "You can also use autocast for CPU with `dtype=torch.bfloat16`:\n",
    "\n",
    "```python\n",
    "with autocast(device_type='cpu', dtype=torch.bfloat16):\n",
    "    output = model(input)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
