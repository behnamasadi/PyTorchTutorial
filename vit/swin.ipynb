{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3768141f-8f0c-4521-ac46-6c52785add0a",
   "metadata": {},
   "source": [
    "# **Swin Transformer – A Step-by-Step Tutorial**\n",
    "\n",
    "The **Swin Transformer** (Shifted Window Transformer, Liu et al., 2021) extends the **Vision Transformer (ViT)** to handle **high-resolution** and **dense prediction tasks** (e.g., detection, segmentation) efficiently — without losing its transformer flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "**ViT** treats an image as a sequence of patches and applies **global self-attention**.\n",
    "While this works well for classification, it faces key limitations:\n",
    "\n",
    "* **Quadratic complexity** in the number of patches.\n",
    "* **No local inductive bias** (poor handling of fine details).\n",
    "* **Fixed spatial resolution**, unsuitable for dense predictions.\n",
    "\n",
    "The **Swin Transformer** solves these problems by:\n",
    "\n",
    "1. Applying **local attention** inside non-overlapping windows (reducing complexity).\n",
    "2. **Shifting windows** between layers to connect across regions.\n",
    "3. Introducing **patch merging** to build a **hierarchical (multi-scale)** representation — similar to CNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Architecture Overview\n",
    "\n",
    "Swin Transformer follows a **hierarchical pyramid design**, much like ResNet:\n",
    "\n",
    "| Stage   | Input Resolution | Patch Operation | Output Channels | Description      |\n",
    "| ------- | ---------------- | --------------- | --------------- | ---------------- |\n",
    "| Stage 1 | 4×4 patches      | Patch Embedding | 96              | Linear embedding |\n",
    "| Stage 2 | 1/2 spatial size | Patch Merging   | 192             | Downsampling     |\n",
    "| Stage 3 | 1/4 spatial size | Patch Merging   | 384             | Downsampling     |\n",
    "| Stage 4 | 1/8 spatial size | Patch Merging   | 768             | Downsampling     |\n",
    "\n",
    "Each stage contains several **Swin Transformer Blocks**, each block consisting of:\n",
    "\n",
    "1. **W-MSA** (Window-based Multi-Head Self-Attention)\n",
    "2. **SW-MSA** (Shifted-Window Multi-Head Self-Attention)\n",
    "3. **Feed-forward MLP**\n",
    "4. **LayerNorm + Residual connections**\n",
    "\n",
    "\n",
    "<img src=\"images/two_successive_swin_transformer_blocks.png\" height=\"30%\" width=\"30%\" />\n",
    "\n",
    "\n",
    "<img src=\"images/swin_architecture.png\" height=\"60%\" width=\"60%\" />\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad71b8e-7349-49df-b63d-c1a76a007c0b",
   "metadata": {},
   "source": [
    "- Swin-T: $C = 96$, $ \\text{layer numbers} =\\{2, 2, 6, 2\\}$\n",
    "- Swin-S: $C = 96$, $ \\text{layer numbers} =\\{2, 2, 18, 2\\}$\n",
    "- Swin-B: $C = 128$, $ \\text{layer numbers}= \\{2, 2, 18, 2\\}$\n",
    "- Swin-L: $C = 192$, $ \\text{layer numbers} =\\{2, 2, 18, 2\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfae8b0-2101-42e2-9672-e6beac50b992",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 3. Step-by-Step Pipeline\n",
    "\n",
    "### Step 1: Patch Partition and Embedding\n",
    "\n",
    "Split the input image into non-overlapping 4×4 patches.\n",
    "\n",
    "If the image is of size\n",
    "$$ H \\times W \\times 3 $$\n",
    "then after partition:\n",
    "$$ \\frac{H}{4} \\times \\frac{W}{4} \\times 48 $$\n",
    "since each patch has $ 4 \\times 4 \\times 3 = 48 $ elements.\n",
    "\n",
    "A **linear projection** maps these 48-d vectors into an embedding dimension (e.g., 96):\n",
    "$$ X \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times 96} $$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Window-based Multi-Head Self-Attention (W-MSA)\n",
    "\n",
    "Each feature map is divided into **non-overlapping windows** (e.g., 7×7 patches).\n",
    "Attention is computed **independently** within each window.\n",
    "\n",
    "This reduces computational cost from global $ O((HW)^2) $ to\n",
    "$$ O(M^2HW) $$\n",
    "where $ M $ is the window size (e.g., 7).\n",
    "\n",
    "In this setup,\n",
    "\n",
    "> **All queries within a window share the same key set.**\n",
    "\n",
    "That is, every patch inside a window can only attend to other patches inside **that same window**, not across windows.\n",
    "\n",
    "Formally, for window $ w $:\n",
    "$$\n",
    "Q^{(w)} = X^{(w)}W^Q,\\quad\n",
    "K^{(w)} = X^{(w)}W^K,\\quad\n",
    "V^{(w)} = X^{(w)}W^V\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{Attention}^{(w)} = \\text{Softmax}!\\left(\\frac{Q^{(w)}{K^{(w)}}^T}{\\sqrt{d}}\\right)V^{(w)}\n",
    "$$\n",
    "\n",
    "This local attention structure introduces **spatial locality** and scales efficiently with image size.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Shifted-Window Multi-Head Self-Attention (SW-MSA)\n",
    "\n",
    "In the **next block**, windows are **shifted by half the window size** (e.g., 3 pixels if ( M=7 )).\n",
    "This shift allows patches that were previously in separate windows to now fall in the same window.\n",
    "\n",
    "Thus, alternating between **W-MSA** and **SW-MSA** layers enables:\n",
    "\n",
    "* Local attention within windows.\n",
    "* Cross-window communication.\n",
    "* Gradual expansion of the receptive field — achieving a global view over multiple layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Patch Merging – Building a Hierarchy\n",
    "\n",
    "To reduce spatial resolution and increase semantic richness, Swin introduces **Patch Merging**, analogous to CNN downsampling.\n",
    "\n",
    "Given:\n",
    "$$ X \\in \\mathbb{R}^{H \\times W \\times C} $$\n",
    "\n",
    "1. **Group 2×2 neighboring patches:**\n",
    "   Each group of four patches is concatenated:\n",
    "   $$\n",
    "   [x_{00}, x_{01}, x_{10}, x_{11}] \\in \\mathbb{R}^{4C}\n",
    "   $$\n",
    "\n",
    "2. **Linear Projection:**\n",
    "   Reduce dimensionality from $ 4C $ → $ 2C $:\n",
    "   $$\n",
    "   X' = \\text{Linear}(\\text{Concat}_{2\\times2}(X)) \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times 2C}\n",
    "   $$\n",
    "\n",
    "Thus:\n",
    "\n",
    "* Spatial size halves.\n",
    "* Channel dimension doubles.\n",
    "\n",
    "After several patch-merging steps, the model forms a **feature pyramid** where deeper stages capture more abstract semantics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inside a Swin Transformer Block\n",
    "\n",
    "Given an input $ X \\in \\mathbb{R}^{H \\times W \\times C} $:\n",
    "\n",
    "1. **LayerNorm:**\n",
    "   $$\n",
    "   \\hat{X} = \\text{LN}(X)\n",
    "   $$\n",
    "2. **(Shifted) Window Attention:**\n",
    "   $$\n",
    "   X' = X + \\text{WindowAttention}(\\hat{X})\n",
    "   $$\n",
    "3. **Feed-forward (MLP) with residual:**\n",
    "   $$\n",
    "   X'' = X' + \\text{MLP}(\\text{LN}(X'))\n",
    "   $$\n",
    "4. **MLP structure:**\n",
    "   $$\n",
    "   \\text{MLP}(x) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(x)))\n",
    "   $$\n",
    "   where hidden dimension = $ 4C $.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Multi-Head Self-Attention Inside a Window\n",
    "\n",
    "Within each window:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}!\\left(\\frac{QK^T}{\\sqrt{d}} + B\\right)V\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $ B $ — learnable **relative position bias** for spatial awareness.\n",
    "* $ Q, K, V $ — derived from the same window.\n",
    "* Computation is efficient since windows are small.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Hierarchical Output Example\n",
    "\n",
    "For a 224×224 input image, Swin Transformer produces multi-scale features:\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| 1     | 56×56      | 96       |\n",
    "| 2     | 28×28      | 192      |\n",
    "| 3     | 14×14      | 384      |\n",
    "| 4     | 7×7        | 768      |\n",
    "\n",
    "These outputs form a **feature pyramid** — ideal for:\n",
    "\n",
    "* **Classification:** via global average pooling.\n",
    "* **Detection/Segmentation:** as inputs to FPNs (e.g., Mask R-CNN).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Intuitive Summary\n",
    "\n",
    "* **ViT**: global attention, single-scale, quadratic complexity.\n",
    "* **Swin**: local attention, hierarchical, linear complexity.\n",
    "* **Shifted windows**: enable cross-region communication.\n",
    "* **Patch merging**: provides multi-scale features like CNNs.\n",
    "\n",
    " **In one sentence:**\n",
    "\n",
    "> Swin Transformer limits self-attention to local windows (shared key sets), shifts them between layers for global context, and builds a hierarchical multi-scale representation through patch merging — combining the strengths of CNNs and Transformers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b78497-e393-4a6f-8c68-2b3feff35714",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
