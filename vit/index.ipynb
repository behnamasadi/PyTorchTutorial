{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1988c73c-a78c-459a-b8d2-2919d3ff1c64",
   "metadata": {},
   "source": [
    "# **PatchEmbedding**\n",
    "\n",
    "Difference between **vision transformers** and **language models**: how images get turned into sequences of tokens.\n",
    "\n",
    "\n",
    "```python\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)  # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, embed_dim]\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c03cf1-7b1f-4b6e-a3b2-30b1061d1ae5",
   "metadata": {},
   "source": [
    "**Goal of PatchEmbedding:**  Turn a **2D image** of shape `[B, 3, 224, 224]` into a **sequence of patch tokens**:\n",
    "\n",
    "```\n",
    "[B, N, D]  ‚Üê like `[batch, sequence_length, embedding_dim]`\n",
    "```\n",
    "\n",
    "This is just like in NLP, where a sentence becomes a sequence of token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Image input**:\n",
    "\n",
    "* Shape: `[B, 3, 224, 224]`\n",
    "* That is: batch of RGB images\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Patchifying via `Conv2d`**\n",
    "\n",
    "Here‚Äôs the trick: instead of manually slicing the image into patches, we use a `Conv2d` to do **both patch extraction and linear projection** in one step.\n",
    "\n",
    "```python\n",
    "self.proj = nn.Conv2d(\n",
    "    in_channels=3,         # RGB channels\n",
    "    out_channels=768,      # embedding dim (D)\n",
    "    kernel_size=16,        # patch size (P)\n",
    "    stride=16              # non-overlapping patches\n",
    ")\n",
    "```\n",
    "\n",
    "What this does:\n",
    "\n",
    "* The kernel slides across the image in 16√ó16 steps.\n",
    "* For each 16√ó16√ó3 patch, it applies a **learned linear projection** into a 768-dimensional vector.\n",
    "* So you get:\n",
    "\n",
    "  ```\n",
    "  Output shape: [B, 768, 14, 14]\n",
    "  ```\n",
    "\n",
    "Why 14x14?\n",
    "\n",
    "* Because:\n",
    "\n",
    "  ```\n",
    "  224 (image size) / 16 (patch size) = 14 patches along each dimension\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Flatten and reshape**:\n",
    "\n",
    "```python\n",
    "x = x.flatten(2)       # [B, 768, 14*14] ‚Üí [B, 768, 196]\n",
    "x = x.transpose(1, 2)  # [B, 196, 768]\n",
    "```\n",
    "\n",
    "Now you have:\n",
    "\n",
    "* 196 tokens (patches),\n",
    "* each of size 768 (embedding dimension),\n",
    "* just like a sentence of 196 words, each mapped to a 768-dim word embedding.\n",
    "\n",
    "---\n",
    "\n",
    "###  How is this different from text models like GPT?\n",
    "\n",
    "| Aspect          | NLP (GPT, LLMs)                              | ViT (PatchEmbedding)                      |\n",
    "| --------------- | -------------------------------------------- | ----------------------------------------- |\n",
    "| Tokens          | Discrete: words, subwords (e.g. 12288 vocab) | Continuous: patches of pixels             |\n",
    "| Token embedding | Look-up: `nn.Embedding(vocab_size, D)`       | Projection via `Conv2d` per patch         |\n",
    "| Token meaning   | Symbolic (cat, run, the...)                  | Visual (16√ó16 image patches)              |\n",
    "| Sequence length | Varies, e.g. 512 tokens                      | Fixed: depends on image size / patch size |\n",
    "| Embedding init  | Pretrained embeddings, or random             | Learnable `Conv2d` weights                |\n",
    "\n",
    "---\n",
    "\n",
    "###  Why `Conv2d` for projection?\n",
    "\n",
    "Because:\n",
    "\n",
    "* It mimics the behavior of **flattening + linear projection** of each patch.\n",
    "* But it‚Äôs faster and GPU-friendly.\n",
    "* Equivalent to slicing out each 16√ó16 patch, flattening it into `[768]`, and applying a `Linear(3*16*16, 768)`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary of Dimensions\n",
    "\n",
    "| Stage               | Shape                                    |\n",
    "| ------------------- | ---------------------------------------- |\n",
    "| Input Image         | `[B, 3, 224, 224]`                       |\n",
    "| Conv2d Output       | `[B, 768, 14, 14]`                       |\n",
    "| Flatten + Transpose | `[B, 196, 768]`                          |\n",
    "| Output Tokens       | 196 patch tokens per image, each `[768]` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdda16-f173-4792-928f-fb9b651e39a8",
   "metadata": {},
   "source": [
    "## Setup (ViT-style):\n",
    "\n",
    "```python\n",
    "class PatchEmbedding(nn.Module): \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Goal of PatchEmbedding:\n",
    "\n",
    "Turn a **2D image** of shape `[B, 3, 224, 224]` into a **sequence of patch tokens**:\n",
    "\n",
    "```\n",
    "[B, N, D]  ‚Üê like `[batch, sequence_length, embedding_dim]`\n",
    "```\n",
    "\n",
    "This is just like in NLP, where a sentence becomes a sequence of token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "##  Step-by-step Explanation:\n",
    "\n",
    "### 1. **Image input**:\n",
    "\n",
    "* Shape: `[B, 3, 224, 224]`\n",
    "* That is: batch of RGB images\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Patchifying via `Conv2d`**\n",
    "\n",
    "Here‚Äôs the trick: instead of manually slicing the image into patches, we use a `Conv2d` to do **both patch extraction and linear projection** in one step.\n",
    "\n",
    "```python\n",
    "self.proj = nn.Conv2d(\n",
    "    in_channels=3,         # RGB channels\n",
    "    out_channels=768,      # embedding dim (D)\n",
    "    kernel_size=16,        # patch size (P)\n",
    "    stride=16              # non-overlapping patches\n",
    ")\n",
    "```\n",
    "\n",
    "#### What this does:\n",
    "\n",
    "* The kernel slides across the image in 16√ó16 steps.\n",
    "* For each 16√ó16√ó3 patch, it applies a **learned linear projection** into a 768-dimensional vector.\n",
    "* So you get:\n",
    "\n",
    "  ```\n",
    "  Output shape: [B, 768, 14, 14]\n",
    "  ```\n",
    "\n",
    "Why 14x14?\n",
    "\n",
    "* Because:\n",
    "\n",
    "  ```\n",
    "  224 (image size) / 16 (patch size) = 14 patches along each dimension\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Flatten and reshape**:\n",
    "\n",
    "```python\n",
    "x = x.flatten(2)       # [B, 768, 14*14] ‚Üí [B, 768, 196]\n",
    "x = x.transpose(1, 2)  # [B, 196, 768]\n",
    "```\n",
    "\n",
    "Now you have:\n",
    "\n",
    "* 196 tokens (patches),\n",
    "* each of size 768 (embedding dimension),\n",
    "* just like a sentence of 196 words, each mapped to a 768-dim word embedding.\n",
    "\n",
    "---\n",
    "\n",
    "##  How is this different from text models like GPT?\n",
    "\n",
    "| Aspect          | NLP (GPT, LLMs)                              | ViT (PatchEmbedding)                      |\n",
    "| --------------- | -------------------------------------------- | ----------------------------------------- |\n",
    "| Tokens          | Discrete: words, subwords (e.g. 12288 vocab) | Continuous: patches of pixels             |\n",
    "| Token embedding | Look-up: `nn.Embedding(vocab_size, D)`       | Projection via `Conv2d` per patch         |\n",
    "| Token meaning   | Symbolic (cat, run, the...)                  | Visual (16√ó16 image patches)              |\n",
    "| Sequence length | Varies, e.g. 512 tokens                      | Fixed: depends on image size / patch size |\n",
    "| Embedding init  | Pretrained embeddings, or random             | Learnable `Conv2d` weights                |\n",
    "\n",
    "---\n",
    "\n",
    "##  Why `Conv2d` for projection?\n",
    "\n",
    "Because:\n",
    "\n",
    "* It mimics the behavior of **flattening + linear projection** of each patch.\n",
    "* But it‚Äôs faster and GPU-friendly.\n",
    "* Equivalent to slicing out each 16√ó16 patch, flattening it into `[768]`, and applying a `Linear(3*16*16, 768)`.\n",
    "\n",
    "```python\n",
    "# Conceptually similar to:\n",
    "patch = image[:, :, i*16:(i+1)*16, j*16:(j+1)*16].reshape(B, -1)  # [B, 768]\n",
    "patch_token = linear(patch)  # [B, 768]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary of Dimensions\n",
    "\n",
    "| Stage               | Shape                                    |\n",
    "| ------------------- | ---------------------------------------- |\n",
    "| Input Image         | `[B, 3, 224, 224]`                       |\n",
    "| Conv2d Output       | `[B, 768, 14, 14]`                       |\n",
    "| Flatten + Transpose | `[B, 196, 768]`                          |\n",
    "| Output Tokens       | 196 patch tokens per image, each `[768]` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b59628-2848-4bbe-97fe-15bf30af56ad",
   "metadata": {},
   "source": [
    "## Class Overview: `MiniViT`\n",
    "Its job is to take an image and output a **classification prediction** using a Vision Transformer.\n",
    "\n",
    "```python\n",
    "class MiniViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_classes=10, depth=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(\n",
    "            1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # [B, N, D]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "        x = x + self.pos_embed[:, :x.size(1)]  # positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # [B, N+1, D]\n",
    "        cls_out = x[:, 0]  # CLS token output\n",
    "        return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abb66e-578c-4c55-9a17-0d6ba62a2d17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### patch_embed\n",
    "\n",
    "```python\n",
    "self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Converts the image into a sequence of **patch tokens**.\n",
    "* Output shape: `[B, N, D]`, where:\n",
    "\n",
    "  * `B` = batch size\n",
    "  * `N = (img_size // patch_size)^2` = number of patches (e.g., 14√ó14 = 196)\n",
    "  * `D = embed_dim` (e.g., 768)\n",
    "\n",
    "---\n",
    "\n",
    "#### cls_token\n",
    "\n",
    "```python\n",
    "self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "```\n",
    "\n",
    "\n",
    "* Defines a **learnable `[CLS]` token** ‚Äî shared across all images.\n",
    "* At each forward pass, it's **expanded** to batch size `[B, 1, D]`.\n",
    "\n",
    "---\n",
    "#### pos_embed\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Positional embedding for each token, including `[CLS]`.\n",
    "* Shape:\n",
    "\n",
    "  * `[1, 197, 768]` if `N=196`\n",
    "* Added to the input sequence to inject spatial info.\n",
    "\n",
    "\n",
    "\n",
    "`pos_embed` is a tensor of shape:\n",
    "\n",
    "```python\n",
    "[1, N+1, embed_dim]\n",
    "```\n",
    "\n",
    "* `N` = number of patches per image\n",
    "* `+1` = to account for the `[CLS]` token\n",
    "* `embed_dim` = same dimension as the patch embeddings\n",
    "\n",
    "Each vector in `pos_embed` is a **learnable position encoding vector**, and its job is to **inject spatial order** into the transformer‚Äôs input.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we need it?**\n",
    "\n",
    "Transformers are **permutation invariant** ‚Äî they don‚Äôt care about the order of tokens unless you tell them.\n",
    "\n",
    "In vision:\n",
    "\n",
    "* Images have a clear spatial structure (top-left to bottom-right), but patch embeddings **lose this spatial information** once flattened.\n",
    "* `pos_embed` gives **each patch a unique positional tag** so the model knows \"where\" each patch came from.\n",
    "\n",
    "So even though each patch is a vector of size `embed_dim`, the **position vector** is added to it:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "This is like giving each token a GPS tag so the transformer knows where it belongs in the image.\n",
    "\n",
    "---\n",
    "\n",
    "**Does `pos_embed` allow adding or subtracting patches?**\n",
    "\n",
    "Not exactly in a literal sense ‚Äî the positional embeddings:\n",
    "\n",
    "* **Don‚Äôt allow adding or subtracting patches spatially**, like in a convolution or image coordinate system.\n",
    "* But they do allow the model to **learn relationships between positions**, i.e., between patches at different locations.\n",
    "\n",
    "So in a way, yes: by giving each patch a unique positional identity, the model can **infer spatial relationships** (e.g., proximity, layout) during training.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* `pos_embed` ‚â† spatial transformer or convolution\n",
    "* `pos_embed` = learnable tag for each token position\n",
    "* Enables the transformer to **make sense of spatial structure** across patches\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### transformer\n",
    "```python\n",
    "self.transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
    "    num_layers=depth\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Standard transformer encoder stack:\n",
    "\n",
    "  * Multi-head self-attention\n",
    "  * Feedforward layers\n",
    "  * LayerNorm and residual connections\n",
    "* Processes all tokens (patches + CLS)\n",
    "* Input: `[B, N+1, D]` ‚Üí Output: `[B, N+1, D]`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### mlp_head\n",
    "\n",
    "```python\n",
    "self.mlp_head = nn.Sequential(\n",
    "    nn.LayerNorm(embed_dim),\n",
    "    nn.Linear(embed_dim, num_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Post-transformer classification head\n",
    "* Takes only `[CLS]` token output: `[B, D]`\n",
    "* Maps to class scores: `[B, num_classes]`\n",
    "\n",
    "---\n",
    "\n",
    "###  Full Forward Pass\n",
    "\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    B = x.size(0)\n",
    "```\n",
    "\n",
    " Gets batch size.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    " Image ‚Üí patch tokens\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6797509-129a-4c6a-96ad-b5b69c35cf99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**`cls_token` is shared for all images**\n",
    "\n",
    "* The learnable `cls_token` is of shape `[1, 1, embed_dim]`.\n",
    "* It is initialized once, and **learned during training**.\n",
    "* It is **shared across the entire dataset** ‚Äî meaning:\n",
    "\n",
    "  * Same `cls_token` is used for **every image**, in **every batch**, throughout training.\n",
    "  * It is copied (not re-learned) across the batch like this:\n",
    "\n",
    "    ```python\n",
    "    cls_tokens = self.cls_token.expand(B, -1, -1)  # shape [B, 1, embed_dim]\n",
    "    ```\n",
    "* After the transformer processes the sequence (which now includes the `[CLS]` token), we extract the output corresponding to the `[CLS]` position:\n",
    "\n",
    "  ```python\n",
    "  cls_out = x[:, 0, :]  # [B, embed_dim]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    " **Then we pass that through an MLP to get class scores**\n",
    "\n",
    "That's 100% correct:\n",
    "\n",
    "* If you have `embed_dim = 768` and `num_classes = 10`, then the final classification head is:\n",
    "\n",
    "  ```python\n",
    "  nn.Linear(embed_dim, num_classes)  # 768 ‚Üí 10\n",
    "  ```\n",
    "\n",
    "* This produces output logits of shape `[B, 10]`, one row per image, each row containing scores for the 10 classes.\n",
    "\n",
    "Example in your code:\n",
    "\n",
    "```python\n",
    "self.mlp_head = nn.Sequential(\n",
    "    nn.LayerNorm(embed_dim),\n",
    "    nn.Linear(embed_dim, num_classes)  # shape: [B, embed_dim] ‚Üí [B, 10]\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a6b03-cbff-4f9f-b469-8c7fc0fdf350",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "in this line:\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "```\n",
    "\n",
    "and you're wondering:\n",
    "\n",
    "> \"Wait ‚Äî if we‚Äôre expanding `cls_token` across the batch, and now each image has its own copy, won‚Äôt they be updated separately during backprop? So how is this the same shared token?\"\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**`expand()` in PyTorch does *not* create separate memory copies.**\n",
    "\n",
    "* It‚Äôs **not cloning** the tensor.\n",
    "* It creates a **view** on the **same underlying data** ‚Äî like a broadcasted reference.\n",
    "* So even though `cls_tokens` appears to be `[B, 1, D]`, it‚Äôs still backed by **one shared parameter** (`self.cls_token`).\n",
    "\n",
    "Therefore, **during backpropagation, the gradient updates only a single shared `cls_token` parameter.**\n",
    "\n",
    "---\n",
    "\n",
    "Contrast: If you had used `.repeat()` instead of `.expand()`:\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.repeat(B, 1, 1)  # BAD if you want sharing!\n",
    "```\n",
    "\n",
    "That **would** create `B` separate memory copies ‚Äî now you‚Äôd have `B` independent tokens, and gradients would not be shared. But this is **not** what we want.\n",
    "\n",
    "---\n",
    "\n",
    "###  What's really happening at runtime?\n",
    "\n",
    "Let‚Äôs walk through the actual logic again:\n",
    "\n",
    "1. `self.cls_token`: a learnable parameter, shape `[1, 1, D]`.\n",
    "2. `cls_tokens = self.cls_token.expand(B, -1, -1)`:\n",
    "\n",
    "   * No new data.\n",
    "   * Just a **view** that makes the tensor behave like `[B, 1, D]`.\n",
    "   * **Still one parameter** being learned.\n",
    "3. After processing through the transformer, we extract `x[:, 0, :]`:\n",
    "\n",
    "   * This is the **output** of the `[CLS]` token for each image.\n",
    "   * Each is now different, because it‚Äôs been transformed differently based on the image.\n",
    "4. Then we apply the classifier:\n",
    "\n",
    "   ```python\n",
    "   return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b54bf1-dd2e-4d97-ac79-1ddd421737a6",
   "metadata": {},
   "source": [
    "### **What is happening in forward pass of a Vision Transformer (ViT) model**\n",
    "\n",
    "\n",
    "```python\n",
    "B = x.size(0) \n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "x = x + self.pos_embed[:, :x.size(1)]  # [B, N+1, D]\n",
    "x = self.transformer(x)  # [B, N+1, D]\n",
    "cls_out = x[:, 0]  # [B, D]\n",
    "return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### `B = x.size(0)`\n",
    "\n",
    "* **What:** Gets the batch size.\n",
    "* **Why:** Used to expand the `[CLS]` token across the batch later.\n",
    "* ‚úÖ Example: If `x.shape = [32, 3, 224, 224]`, then `B = 32`.\n",
    "\n",
    "---\n",
    "\n",
    "### `x = self.patch_embed(x)  # [B, N, D]`\n",
    "\n",
    "* **What:** Converts input images into patch embeddings.\n",
    "* **How:**\n",
    "\n",
    "  * Uses a Conv2D with `kernel_size = stride = patch_size` to split and embed patches.\n",
    "  * Output shape becomes `[B, N, D]`, where:\n",
    "\n",
    "    * `N = (img_size / patch_size)^2` (number of patches per image),\n",
    "    * `D = embed_dim` (embedding dimension).\n",
    "* ‚úÖ Example: If 224√ó224 image and 16√ó16 patches, then `N = 196`.\n",
    "\n",
    "---\n",
    "\n",
    "### `cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]`\n",
    "\n",
    "* **What:** Creates a batch of `[CLS]` tokens by expanding the one learnable token.\n",
    "* **Why:** Every image in the batch needs its own `[CLS]` token (same vector, shared).\n",
    "* ‚ö†Ô∏è Important: This does **not** clone the token ‚Äî it's one learnable parameter reused via a **view**.\n",
    "\n",
    "---\n",
    "\n",
    "### `x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]`\n",
    "\n",
    "* **What:** Prepends `[CLS]` token to the sequence of patch embeddings.\n",
    "* **Why:** The `[CLS]` token will aggregate global information from all patches during transformer attention.\n",
    "* ‚úÖ Now the transformer input sequence has length `N+1`.\n",
    "\n",
    "---\n",
    "\n",
    "### `x = x + self.pos_embed[:, :x.size(1)]`\n",
    "\n",
    "* **What:** Adds positional encodings to the tokens (patches + `[CLS]`).\n",
    "* **Why:** Transformers are order-agnostic; this injects positional structure so the model knows where each patch came from.\n",
    "* ‚úÖ `self.pos_embed` has shape `[1, N+1, D]` ‚Äî one learnable vector per position.\n",
    "* Only a slice `:x.size(1)` is used in case some tokens were masked or removed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, D))  # [1, N+1, embed_dim]\n",
    "```\n",
    "\n",
    "* `1` ‚Üí This is the **batch dimension** (a placeholder for broadcasting).\n",
    "* `N+1` ‚Üí The number of **tokens**:\n",
    "\n",
    "  * `N` = number of image patches.\n",
    "  * `+1` = extra token for the `[CLS]` token.\n",
    "* `D` ‚Üí The embedding dimension.\n",
    "\n",
    "This means: for each **position in the token sequence** (patch 0, patch 1, ..., patch N, and CLS), there‚Äôs a learnable vector of size `D`.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ **What does `pos_embed[:, :x.size(1)]` mean?**\n",
    "\n",
    "Suppose `x.shape = [B, N+1, D]` ‚Äî after concatenating `[CLS]` and the patch embeddings.\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "self.pos_embed[:, :x.size(1)]  # shape: [1, N+1, D]\n",
    "```\n",
    "\n",
    "This:\n",
    "\n",
    "* **Selects the first `x.size(1)` positional vectors**,\n",
    "* Where `x.size(1)` is the current number of tokens (which is usually `N+1`),\n",
    "* And preserves batch dimension (`:` in dim 0).\n",
    "\n",
    "We do this in case the number of tokens can vary ‚Äî e.g. in some models with dynamic input size or patch masking.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Why is `pos_embed` shared across the batch?**\n",
    "\n",
    "Because:\n",
    "\n",
    "* `self.pos_embed` has shape `[1, N+1, D]`,\n",
    "* And we **broadcast** it across the batch dimension during addition:\n",
    "\n",
    "  ```python\n",
    "  x = x + self.pos_embed[:, :x.size(1)]  # [B, N+1, D]\n",
    "  ```\n",
    "\n",
    "This adds the **same positional vector** to each image‚Äôs token sequence.\n",
    "\n",
    "‚úÖ This is what we want:\n",
    "\n",
    "> The same position in every image (e.g., patch 0 or `[CLS]`) should use the **same positional embedding**.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ **Is `pos_embed` shared across the entire training set?**\n",
    "\n",
    "‚úÖ **Yes** ‚Äî absolutely.\n",
    "\n",
    "* `pos_embed` is a **single learnable parameter of the model**, just like `cls_token`.\n",
    "* It's learned from all training images, and it is **shared** across the entire training set.\n",
    "* Every image in every batch adds these **same positional embeddings** to its tokens ‚Äî the only difference is the **content** of the tokens.\n",
    "\n",
    "So over training:\n",
    "\n",
    "* The model learns **how position affects meaning** (e.g., \"patch 0\" is usually top-left).\n",
    "* This is what enables the transformer to retain spatial structure.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary\n",
    "\n",
    "| Concept                    | Meaning                                             |\n",
    "| -------------------------- | --------------------------------------------------- |\n",
    "| `pos_embed[:, :x.size(1)]` | Selects first N+1 position vectors for the sequence |\n",
    "| Shape of `pos_embed`       | `[1, N+1, D]`, ready to broadcast to `[B, N+1, D]`  |\n",
    "| Shared across batch?       | ‚úÖ Yes                                               |\n",
    "| Shared across dataset?     | ‚úÖ Yes ‚Äî it‚Äôs a single learnable model parameter     |\n",
    "\n",
    "---\n",
    "\n",
    "### `x = self.transformer(x)  # [B, N+1, D]`\n",
    "\n",
    "* **What:** Feeds the token sequence (with positions) into a Transformer encoder.\n",
    "* **Why:** The transformer processes all tokens with multi-head self-attention, allowing tokens to interact and share context.\n",
    "\n",
    "---\n",
    "\n",
    "### `cls_out = x[:, 0]  # [B, D]`\n",
    "\n",
    "* **What:** Extracts the output of the `[CLS]` token for each image.\n",
    "* **Why:** This token is expected to **summarize the entire image** after going through the transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "### `return self.mlp_head(cls_out)  # [B, num_classes]`\n",
    "\n",
    "* **What:** Passes the `[CLS]` token through a final MLP head (e.g., `LayerNorm ‚Üí Linear`).\n",
    "* **Why:** Outputs classification scores per image.\n",
    "* ‚úÖ Output shape: `[B, num_classes]`, ready for softmax or cross-entropy loss.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary (in words)\n",
    "\n",
    "1. Split the image into patches and embed each patch.\n",
    "2. Prepend a learnable `[CLS]` token to each sequence.\n",
    "3. Add positional encoding so the transformer knows the order.\n",
    "4. Run through a Transformer to model relationships between patches.\n",
    "5. Use the transformed `[CLS]` token to classify the whole image.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d7e87-6acf-4f2a-a38d-f208adee2335",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### üìå **Code line in question:**\n",
    " that line is crucial for giving **spatial awareness** to the Vision Transformer. Let‚Äôs unpack it precisely:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "### üéØ Goal:\n",
    "\n",
    "To **inject positional information** into the patch + `[CLS]` token sequence, so the Transformer knows where each token comes from spatially.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is `x` at this point?\n",
    "\n",
    "Right before this line:\n",
    "\n",
    "```python\n",
    "x = torch.cat((cls_tokens, patch_embeddings), dim=1)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* `x.shape = [B, N+1, D]`\n",
    "\n",
    "  * `B` = batch size\n",
    "  * `N` = number of patches per image\n",
    "  * `+1` = for the `[CLS]` token\n",
    "  * `D` = embedding dimension\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is `self.pos_embed`?\n",
    "\n",
    "Defined in the constructor:\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, D))\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* Shape = `[1, N+1, D]`\n",
    "* It holds **learnable positional encodings**, one for each token position (including the `[CLS]` token).\n",
    "* These are **shared across the batch**, just like `cls_token`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What does `self.pos_embed[:, :x.size(1)]` mean?\n",
    "\n",
    "It selects a **slice** of the positional embeddings that matches the actual input sequence length.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "* To handle dynamic token lengths, e.g. if you change image size or use fewer patches (in some variants).\n",
    "\n",
    "More concretely:\n",
    "\n",
    "* `x.size(1)` = `N+1`\n",
    "* So `self.pos_embed[:, :x.size(1)]` ‚Üí shape `[1, N+1, D]`\n",
    "\n",
    "This is **broadcasted** over the batch when added to `x`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What does the `+` operation do?\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "This performs **element-wise addition** between:\n",
    "\n",
    "* Each token's embedding (per image)\n",
    "* Its corresponding positional encoding (shared across the batch)\n",
    "\n",
    "So:\n",
    "\n",
    "* The embedding for Patch 0 gets the positional vector for position 0\n",
    "* Patch 1 gets the vector for position 1\n",
    "* `[CLS]` gets the vector for position 0 (since it‚Äôs at the start)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why is this necessary?\n",
    "\n",
    "Transformers don‚Äôt have **any built-in notion of order or position** ‚Äî unlike CNNs or RNNs.\n",
    "\n",
    "* Without this line, the model sees a bag of tokens and doesn‚Äôt know that one patch is \"top left\" and another is \"bottom right.\"\n",
    "* The positional encoding gives it the **sequence index**, so attention can model **spatial relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary\n",
    "\n",
    "| Component                       | Meaning                                                 |\n",
    "| ------------------------------- | ------------------------------------------------------- |\n",
    "| `self.pos_embed`                | Learnable positional vectors `[1, N+1, D]`              |\n",
    "| `self.pos_embed[:, :x.size(1)]` | Slices the first `N+1` positions to match input         |\n",
    "| `x + ...`                       | Adds position to token embeddings for spatial awareness |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc97c6-81a5-42cc-8dcc-8840639629e3",
   "metadata": {},
   "source": [
    "### Why  nn.TransformerEncoder and not nn.Transformer\n",
    "\n",
    "\n",
    "The difference between `nn.Transformer` vs `nn.TransformerEncoder` in PyTorch, and why `nn.TransformerEncoder` is used in the Vision Transformer (ViT) code. Let's break this down clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß 1. **What is `nn.Transformer` in PyTorch?**\n",
    "\n",
    "`nn.Transformer` is a **complete Transformer** model that includes both:\n",
    "\n",
    "* an **encoder**, and\n",
    "* a **decoder**.\n",
    "\n",
    "This is the original full architecture used in **sequence-to-sequence** tasks like:\n",
    "\n",
    "* Machine Translation (e.g., English ‚Üí French),\n",
    "* Text summarization,\n",
    "* Image captioning.\n",
    "\n",
    "```python\n",
    "transformer = nn.Transformer(\n",
    "    d_model=768,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "The full transformer processes:\n",
    "\n",
    "* an **input sequence** (to the encoder),\n",
    "* and a **target sequence** (to the decoder).\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 2. **What is `nn.TransformerEncoder`?**\n",
    "\n",
    "`nn.TransformerEncoder` includes **only the encoder part** of a Transformer.\n",
    "\n",
    "It consists of a stack of `nn.TransformerEncoderLayer`s.\n",
    "\n",
    "```python\n",
    "transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=768, nhead=8),\n",
    "    num_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "It's designed for tasks where:\n",
    "\n",
    "* You just need to **encode** the input sequence,\n",
    "* There's **no target sequence** to decode.\n",
    "\n",
    "‚úÖ **This is exactly what you need in ViT**, where:\n",
    "\n",
    "* The model receives a sequence of patch embeddings + `[CLS]` token,\n",
    "* And **outputs a representation of the image** (via the `[CLS]` token),\n",
    "* There's no need to generate a new sequence (no decoder!).\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ So why does ViT use `nn.TransformerEncoder`?\n",
    "\n",
    "Because:\n",
    "\n",
    "* We‚Äôre doing **classification**, not sequence generation.\n",
    "* We just need to **encode** the input image patches and summarize with `[CLS]`.\n",
    "* No decoder is needed.\n",
    "\n",
    "If we used `nn.Transformer`, we‚Äôd be adding unnecessary complexity (and unused parameters).\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Module                  | Includes          | Used for                                | ViT uses? |\n",
    "| ----------------------- | ----------------- | --------------------------------------- | --------- |\n",
    "| `nn.Transformer`        | Encoder + Decoder | Sequence-to-sequence (e.g. translation) | ‚ùå No      |\n",
    "| `nn.TransformerEncoder` | Encoder only      | Sequence classification, ViT            | ‚úÖ Yes     |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6debb-d4ca-4fbd-9955-6822bc53cab7",
   "metadata": {},
   "source": [
    "### Why do we set pos_embed  zeros and not random values?\n",
    "\n",
    "Excellent question ‚Äî and it's one that often comes up when people look at Vision Transformer (ViT) code:\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "### üîπ Why initialize `pos_embed` with **zeros**, not random values?\n",
    "\n",
    "The short answer is: **because it's safe, standard, and effective** ‚Äî and it allows the model to **learn positional embeddings from scratch** during training.\n",
    "\n",
    "Let‚Äôs dive deeper:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **It‚Äôs a Learnable Parameter**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(...)\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "* It will be **learned during training** via backpropagation.\n",
    "* The values will not stay at zero ‚Äî they‚Äôll be **updated with gradients** from the loss.\n",
    "* So initialization is just a **starting point**, not the final state.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 2. **Why zeros specifically?**\n",
    "\n",
    "* **Zero is neutral**: It doesn‚Äôt bias any patch toward any direction at the start.\n",
    "* In early training steps, it allows the model to first rely more on the content of the patch embeddings (`x`), and slowly learn how position contributes.\n",
    "* If we started with random values (e.g. `torch.randn`), the model might initially be confused by meaningless noise patterns in position embeddings.\n",
    "\n",
    "> So initializing with zeros is **simple, stable**, and lets the model **\"bootstrap\" the learning of positional importance gradually**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ 3. **Alternative: Sinusoidal or Random Init**\n",
    "\n",
    "Some models **do** use non-zero initializations:\n",
    "\n",
    "* **Sinusoidal (fixed)**: Used in original Transformer paper. No learning, just injects structure.\n",
    "* **Random normal**: Sometimes used with `nn.init.trunc_normal_()` in improved ViT variants (e.g. DeiT).\n",
    "\n",
    "Example from DeiT (by Facebook):\n",
    "\n",
    "```python\n",
    "nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "```\n",
    "\n",
    "This helps with training stability in larger models or when pretraining is involved.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "| Init Method     | Learnable? | Why Used                               |\n",
    "| --------------- | ---------- | -------------------------------------- |\n",
    "| `zeros`         | ‚úÖ Yes      | Simple, stable, unbiased start         |\n",
    "| `torch.randn`   | ‚úÖ Yes      | More variance ‚Äî may help in some cases |\n",
    "| `trunc_normal_` | ‚úÖ Yes      | Common in pretrained ViTs like DeiT    |\n",
    "| **Sinusoidal**  | ‚ùå No       | Used in NLP, fixed, no learning        |\n",
    "\n",
    "> üí° For many small to medium models trained from scratch, starting with zeros for `pos_embed` works just fine and avoids unnecessary noise early on.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779dc65-6831-4379-abed-2ab509d458d9",
   "metadata": {},
   "source": [
    " Adds `[CLS]` token as the first token of each image sequence.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]  # [B, N+1, D]\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Adds positional encoding to each token.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "x = self.transformer(x)  # [B, N+1, D]\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Transformer processes all tokens using attention.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "cls_out = x[:, 0]  # [B, D]\n",
    "return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```\n",
    "\n",
    "‚úîÔ∏è Extracts the `[CLS]` token‚Äôs output (first token),\n",
    "‚úîÔ∏è Classifies via MLP\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary of Dimensions\n",
    "\n",
    "| Step                    | Shape              | Description                          |\n",
    "| ----------------------- | ------------------ | ------------------------------------ |\n",
    "| Input image             | `[B, 3, 224, 224]` | RGB input                            |\n",
    "| PatchEmbedding          | `[B, 196, 768]`    | One token per 16√ó16 patch            |\n",
    "| Add `[CLS]` token       | `[B, 197, 768]`    | CLS prepended                        |\n",
    "| Add positional encoding | `[B, 197, 768]`    | Adds learnable position info         |\n",
    "| Transformer output      | `[B, 197, 768]`    | Token-wise contextual representation |\n",
    "| Select `[CLS]` output   | `[B, 768]`         | Global image representation          |\n",
    "| MLP head                | `[B, num_classes]` | Final class logits                   |\n",
    "\n",
    "---\n",
    "\n",
    "##  Final Analogy: ViT vs GPT Token Embeddings\n",
    "\n",
    "|              | GPT (Text)                    | MiniViT (Vision)            |\n",
    "| ------------ | ----------------------------- | --------------------------- |\n",
    "| Token Type   | Word/subword from vocab       | Image patch                 |\n",
    "| Token Source | `nn.Embedding(vocab_size, D)` | `Conv2d` over image patches |\n",
    "| Positional   | Learned/sinusoidal            | Learned `[1, N+1, D]`       |\n",
    "| Output Token | All or last token             | `[CLS]` token               |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c8f55-54a6-4ff5-8aa6-d8734a6dd188",
   "metadata": {},
   "source": [
    "## Why `pos_embed` is shared across all images\n",
    "\n",
    "**that‚Äôs exactly the point of positional encoding** in Vision Transformers. Let‚Äôs dive deeper to resolve the seeming contradiction you're seeing.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† First, you're absolutely right in your observation:\n",
    "\n",
    "> \"If `pos_embed` is shared across all images, doesn't that mean that all images are given the same spatial cues?\"\n",
    "\n",
    "Yes. The **same positional encoding** is added to each token position ‚Äî e.g., patch 0, patch 1, ..., patch N ‚Äî **in every image**.\n",
    "\n",
    "But that‚Äôs **not a limitation** ‚Äî it‚Äôs exactly what enables the model to learn spatial reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Let‚Äôs break it down:\n",
    "\n",
    "#### ‚úÖ **Each patch‚Äôs content differs per image**\n",
    "\n",
    "* While **position 0** (top-left patch) always gets the same positional embedding,\n",
    "* The **actual patch embedding** (`x`) differs per image, because the image content is different.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "final_input = patch_embedding + positional_embedding\n",
    "```\n",
    "\n",
    "* `patch_embedding`: image-specific\n",
    "* `positional_embedding`: shared and position-specific\n",
    "\n",
    "This tells the transformer:\n",
    "\n",
    "> \"This is **the top-left patch**, and here's what it looks like in this image.\"\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Analogy: Think of positional encoding like adding **row and column labels** in a spreadsheet\n",
    "\n",
    "* Every row (image) uses the same labels: A, B, C... for columns.\n",
    "* But the **data** in each row (image) is different.\n",
    "\n",
    "The labels tell the model:\n",
    "\n",
    "* \"This vector is **patch #5**, which is likely bottom-left,\"\n",
    "* So it can learn attention patterns like:\n",
    "\n",
    "  * \"Bottom-left often attends to center,\"\n",
    "  * Or \"the `[CLS]` token usually needs to attend to corners for global understanding.\"\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ The key idea:\n",
    "\n",
    "> We **do want the same positional embedding per position across all images**, so the transformer learns **position-aware attention behavior** ‚Äî e.g., how patch 3 attends to patch 10, or how center patches are more informative.\n",
    "\n",
    "What would go wrong if each image had its own positional encoding?\n",
    "\n",
    "* The model would no longer understand **what each position *means***.\n",
    "* Patch 5 in image A might look like top-left, and patch 5 in image B might look like center ‚Äî complete spatial chaos.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ So to summarize:\n",
    "\n",
    "| What                  | Per Image? | Shared Across Images? | Why?                                              |\n",
    "| --------------------- | ---------- | --------------------- | ------------------------------------------------- |\n",
    "| Patch Embeddings      | ‚úÖ Yes      | ‚ùå No                  | Comes from image content                          |\n",
    "| Positional Embeddings | ‚ùå No       | ‚úÖ Yes                 | Define spatial structure (position ID)            |\n",
    "| Attention             | ‚úÖ Yes      | ‚ùå No                  | Learned per image, uses both position and content |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340edbd4-e30c-4c3d-a3db-b9e8d71cd5c1",
   "metadata": {},
   "source": [
    "## For the classification task, we only use the CLS token and we discard the other N tokens\n",
    "\n",
    "\n",
    "Exactly ‚Äî in ViT **for classification**, you **only use the `[CLS]` token's output**, and **discard the other N patch tokens** after the transformer. But in **other tasks**, those N tokens are extremely useful. Here's a breakdown of where and how those tokens are used:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **Image Classification**\n",
    "\n",
    "* **Use only**: `x[:, 0, :]` ‚Üí the `[CLS]` token.\n",
    "* Other tokens: ignored.\n",
    "* Why: `[CLS]` is trained to summarize the entire image.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 2. **Semantic Segmentation**\n",
    "\n",
    "* **Use all tokens**, including patch tokens.\n",
    "* After the transformer, reshape the N patch tokens back into a 2D spatial grid.\n",
    "* Apply a small convolutional head (e.g. MLP or decoder) to generate **per-pixel predictions**.\n",
    "\n",
    "‚úÖ Example: [Segmenter](https://arxiv.org/abs/2105.05633), SETR, ViT-SEG\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 3. **Object Detection**\n",
    "\n",
    "* **Use all patch tokens**, or a selected subset.\n",
    "* Add **object query embeddings** (like in DETR), and let the model predict bounding boxes and class labels by attending to patch tokens.\n",
    "\n",
    "‚úÖ Example: [DETR](https://arxiv.org/abs/2005.12872), ViTDet, DINO\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 4. **Masked Image Modeling (Self-supervised learning)**\n",
    "\n",
    "* Use patch tokens to **reconstruct masked parts of the image**.\n",
    "* `[CLS]` may still be used, but **patch tokens are the focus**.\n",
    "* You mask out a subset of patch tokens, and the model tries to predict them.\n",
    "\n",
    "‚úÖ Example: [MAE (Masked Autoencoders)](https://arxiv.org/abs/2111.06377)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 5. **Vision-Language Tasks (e.g., CLIP, Image Captioning)**\n",
    "\n",
    "* `[CLS]` is often used as the **global image embedding** (e.g., for retrieval or alignment with text).\n",
    "* But patch tokens can be used in:\n",
    "\n",
    "  * Cross-attention with text tokens,\n",
    "  * Generating fine-grained alignments (e.g., for caption generation or grounding).\n",
    "\n",
    "‚úÖ Example: CLIP, BLIP, Flamingo, LLaVA\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 6. **Feature Extraction / Dense Predictions**\n",
    "\n",
    "* Sometimes you want features **at each patch location**, not a single global vector.\n",
    "* Patch token outputs are used for:\n",
    "\n",
    "  * Keypoint detection,\n",
    "  * Pose estimation,\n",
    "  * Saliency maps, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary Table\n",
    "\n",
    "| Task                       | Use `[CLS]` only? | Use patch tokens? | Why                                |\n",
    "| -------------------------- | ----------------- | ----------------- | ---------------------------------- |\n",
    "| Image Classification       | ‚úÖ Yes             | ‚ùå No              | Summary of image                   |\n",
    "| Semantic Segmentation      | ‚ùå No              | ‚úÖ Yes             | Dense pixel-wise output            |\n",
    "| Object Detection           | ‚ùå No              | ‚úÖ Yes             | Localize and classify objects      |\n",
    "| Masked Image Modeling      | ‚ùå No              | ‚úÖ Yes             | Reconstruct image patches          |\n",
    "| Vision-Language Embedding  | ‚úÖ (often)         | ‚úÖ (sometimes)     | Global alignment + local reasoning |\n",
    "| Keypoint / Pose Estimation | ‚ùå No              | ‚úÖ Yes             | Use local spatial features         |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c0515-36ed-4fbe-b062-241ce06f3111",
   "metadata": {},
   "source": [
    "## turn your classification-oriented ViT into a depth estimation model\n",
    "\n",
    "Great ‚Äî you're now thinking in exactly the right direction: **reusing a Vision Transformer (ViT) backbone for dense prediction**, like **monocular depth estimation**.\n",
    "\n",
    "To turn your classification-oriented ViT into a **depth estimation** model (i.e., pixel-wise regression), you'll need to modify **what you do with the output tokens**. Here‚Äôs a step-by-step guide:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ **What to change to do depth estimation with ViT**\n",
    "\n",
    "### ‚úÖ 1. **Keep all patch tokens, not just `[CLS]`**\n",
    "\n",
    "Currently:\n",
    "\n",
    "```python\n",
    "cls_out = x[:, 0]  # Only [CLS] token\n",
    "return self.mlp_head(cls_out)\n",
    "```\n",
    "\n",
    "Change to:\n",
    "\n",
    "```python\n",
    "patch_tokens = x[:, 1:, :]  # Drop CLS, keep only patches\n",
    "```\n",
    "\n",
    "We do this because:\n",
    "\n",
    "* Depth estimation is a **dense** task.\n",
    "* Each patch should predict depth for the corresponding region of the image.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **Reshape tokens into a 2D feature map**\n",
    "\n",
    "Assuming your image is split into `H x W` patches:\n",
    "\n",
    "```python\n",
    "# Suppose: x.shape = [B, N, D] with N = H * W\n",
    "H = W = int((x.size(1) - 1)**0.5)\n",
    "feat_map = patch_tokens.transpose(1, 2).reshape(B, D, H, W)  # [B, D, H, W]\n",
    "```\n",
    "\n",
    "Now you have a spatial feature map like a CNN would produce.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **Use an upsampling head to get back to full resolution**\n",
    "\n",
    "You now need to go from `H x W` (e.g., 14x14) ‚Üí back to input resolution (e.g., 224x224).\n",
    "\n",
    "Add a **decoder** head:\n",
    "\n",
    "```python\n",
    "self.depth_head = nn.Sequential(\n",
    "    nn.ConvTranspose2d(D, D//2, kernel_size=2, stride=2),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(D//2, D//4, kernel_size=2, stride=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(D//4, 1, kernel_size=1)  # Output single-channel depth map\n",
    ")\n",
    "```\n",
    "\n",
    "In `forward()`:\n",
    "\n",
    "```python\n",
    "depth = self.depth_head(feat_map)  # [B, 1, H_out, W_out]\n",
    "```\n",
    "\n",
    "You may want to resize the output to match the original input size if needed:\n",
    "\n",
    "```python\n",
    "depth = F.interpolate(depth, size=(img_size, img_size), mode='bilinear')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. **Loss Function**\n",
    "\n",
    "Use a **regression loss**, e.g.:\n",
    "\n",
    "```python\n",
    "loss = F.l1_loss(predicted_depth, ground_truth_depth)\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "* `MSELoss`, or\n",
    "* `scale-invariant loss` (used in depth estimation literature)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary of Key Changes\n",
    "\n",
    "| Step              | Change Required                      |\n",
    "| ----------------- | ------------------------------------ |\n",
    "| Output tokens     | Use all patch tokens (`x[:, 1:, :]`) |\n",
    "| Spatial reshaping | Convert to `[B, D, H, W]`            |\n",
    "| Decoder head      | Transposed conv or upsampling layers |\n",
    "| Final output      | 1-channel image (depth map)          |\n",
    "| Loss function     | Regression (e.g., L1, MSE)           |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus Tips\n",
    "\n",
    "* For better accuracy, consider **skip connections** or **hierarchical ViT variants** (e.g., Swin Transformer).\n",
    "* Look into **MAE** or **DPT** (Dense Prediction Transformer by Intel) for more ideas.\n",
    "* Use pretrained ViT weights to improve performance.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9d2d-e0fd-436f-a7f5-5c91299f8922",
   "metadata": {},
   "source": [
    "## What should be the data in the training set for this monocular depth estimation?\n",
    "\n",
    "Great question ‚Äî for **monocular depth estimation**, your training data needs to include:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **Input**: RGB Image\n",
    "\n",
    "* Shape: `[3, H, W]`\n",
    "* Example: a regular color photo from a monocular camera.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. **Target**: Ground Truth Depth Map\n",
    "\n",
    "* Shape: `[1, H, W]` ‚Äî single channel.\n",
    "* Each pixel contains the depth value (e.g., in meters or normalized units).\n",
    "* Usually stored as:\n",
    "\n",
    "  * 16-bit grayscale images (`.png`, `.tiff`, etc.), or\n",
    "  * Floating-point `.npy` arrays.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Example dataset entry:\n",
    "\n",
    "| Item        | Format                  | Description                        |\n",
    "| ----------- | ----------------------- | ---------------------------------- |\n",
    "| `image.png` | `[3, 224, 224]` (float) | RGB image                          |\n",
    "| `depth.png` | `[1, 224, 224]` (float) | Ground-truth depth map (same size) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. **Normalization and Preprocessing**\n",
    "\n",
    "### For RGB image:\n",
    "\n",
    "* Convert to float, normalize to `[0, 1]` or ImageNet stats:\n",
    "\n",
    "  ```python\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "  ```\n",
    "\n",
    "### For depth map:\n",
    "\n",
    "* Convert to float32\n",
    "* Normalize optionally (depends on your loss function and depth range)\n",
    "\n",
    "  ```python\n",
    "  depth = depth / 1000.0  # if in millimeters ‚Üí meters\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. **Popular Monocular Depth Datasets**\n",
    "\n",
    "| Dataset          | Description                        | License / Notes                    |\n",
    "| ---------------- | ---------------------------------- | ---------------------------------- |\n",
    "| **NYU Depth V2** | Indoor scenes, Kinect RGB-D images | ‚úîÔ∏è Standard for indoor depth       |\n",
    "| **KITTI**        | Outdoor driving scenes (LiDAR)     | ‚úîÔ∏è Standard for autonomous driving |\n",
    "| **Make3D**       | Outdoor stills (Stanford)          | Older, smaller                     |\n",
    "| **DIML/CVT**     | Outdoor depth from stereo          | Large and high-resolution          |\n",
    "| **TUM RGB-D**    | Indoor SLAM dataset                | ‚úîÔ∏è Camera + depth                  |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. **Custom Dataset Structure Example**\n",
    "\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00001.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00002.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ depths/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00001.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00002.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "Each `image` should match its corresponding `depth` map by filename.\n",
    "\n",
    "You can implement a PyTorch `Dataset` like this:\n",
    "\n",
    "```python\n",
    "class DepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, depth_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.depth_paths = depth_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        depth = Image.open(self.depth_paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        depth = torch.tensor(np.array(depth), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return img, depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary\n",
    "\n",
    "| Requirement     | Description                             |\n",
    "| --------------- | --------------------------------------- |\n",
    "| Input           | RGB image `[3, H, W]`                   |\n",
    "| Target          | Depth map `[1, H, W]`                   |\n",
    "| Same size?      | ‚úÖ Yes ‚Äî input and depth must be aligned |\n",
    "| Normalization   | Yes ‚Äî for both image and depth          |\n",
    "| Output of model | `[B, 1, H, W]`                          |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed95cd2-e9ef-45d0-8f56-9922da3afed1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Where do we call `forward` in `PatchEmbedding`? Do we call it at all?**\n",
    "\n",
    "Yes, `forward` is called **implicitly** in this line of the `MiniViT.forward()` method:\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    "In PyTorch, when you do `self.patch_embed(x)`, it's syntactic sugar for:\n",
    "\n",
    "```python\n",
    "self.patch_embed.forward(x)\n",
    "```\n",
    "\n",
    "So yes, `forward` is called‚Äîthis is how PyTorch modules are normally used.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why `self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))`? Shouldn't we take into account the number of batches?**\n",
    "\n",
    "Good observation, and here's why it **doesn't** take the batch size directly:\n",
    "\n",
    "* `self.cls_token` is a **learnable embedding** (like a special \"classification token\").\n",
    "* It's initialized as shape `[1, 1, embed_dim]`, because it will be **expanded at runtime** in the `forward()` method using:\n",
    "\n",
    "  ```python\n",
    "  cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "  ```\n",
    "\n",
    "So it **doesn't need to store B tokens**‚Äîwe just keep **one** and replicate it per batch during the forward pass. This saves memory and simplifies learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Why does `self.pos_embed` have `+1` in:**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "```\n",
    "\n",
    "This `+1` is **for the classification token** we just talked about.\n",
    "\n",
    "* The number of image patches is `(img_size // patch_size) ** 2`.\n",
    "* But since we also prepend a `[CLS]` token to the input sequence, the positional embedding must have **one extra position**.\n",
    "* So: `num_patches + 1` positions total.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is happening in this line:**\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    "This is where:\n",
    "\n",
    "* The image is split into non-overlapping **patches** using a `Conv2d` layer with:\n",
    "\n",
    "  * `kernel_size = patch_size`\n",
    "  * `stride = patch_size`\n",
    "\n",
    "This does both:\n",
    "\n",
    "* The patch extraction\n",
    "* The linear projection into the `embed_dim`\n",
    "\n",
    "Then:\n",
    "\n",
    "* `x.flatten(2)` flattens the spatial dimensions (`H' * W'`) into one sequence dimension (`N` = number of patches).\n",
    "* `x.transpose(1, 2)` changes shape from `[B, D, N]` to `[B, N, D]`, matching transformer expectations.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a diagram to visualize the patch embedding and token addition process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a52f8d-cca9-4776-a93a-9728cfa3d082",
   "metadata": {},
   "source": [
    "Excellent follow-up questions. Let's unpack everything clearly.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **What is `cls_token` for?**\n",
    "\n",
    "`cls_token` is a **single token** (vector) that is **prepended once per image**, **not for every patch/token**.\n",
    "\n",
    "* So: for **each image** in the batch, you add **one special token** at the beginning of the token sequence.\n",
    "* This special token is supposed to **summarize the whole image** after passing through the transformer. It‚Äôs the one used for classification.\n",
    "\n",
    "So the final input to the transformer looks like this for batch size `B`:\n",
    "\n",
    "```\n",
    "[CLS] Patch1 Patch2 Patch3 ... PatchN   ‚Üê total of N+1 tokens per image\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why is `cls_token` initialized as `[1, 1, embed_dim]` and then expanded to `[B, 1, embed_dim]`?**\n",
    "\n",
    "This is a very common PyTorch design pattern. Let's break it down:\n",
    "\n",
    "#### ‚úÖ Initialization:\n",
    "\n",
    "```python\n",
    "self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # shape = [1, 1, D]\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* One learnable embedding of shape `[1, 1, D]`, where `D = embed_dim`.\n",
    "* The extra dimensions (the two `1`s) allow you to **easily broadcast** or expand later.\n",
    "\n",
    "#### ‚úÖ Forward-time expansion:\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # shape = [B, 1, D]\n",
    "```\n",
    "\n",
    "Why expand?\n",
    "\n",
    "* So each image in the batch gets **its own copy** of the same learnable `[CLS]` token.\n",
    "* This avoids allocating `B` separate parameters ‚Äî only one is learned, and it's copied during forward.\n",
    "\n",
    "> üîÅ You don‚Äôt want to learn `B` separate tokens. You want **one token, shared across all images**, and then dynamically insert it for each image at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Why not initialize it directly as `[B, 1, D]`?**\n",
    "\n",
    "Because:\n",
    "\n",
    "1. `B` is **not known** at the time of initialization. It varies with each batch.\n",
    "2. We want to **learn just one** `cls_token` and reuse it for all batches and images.\n",
    "3. Creating it as `[B, 1, D]` would mean storing `B` tokens, which is inefficient and conceptually incorrect ‚Äî you‚Äôd be learning a separate `[CLS]` per image, which is not the goal.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Clarification of `N` and `embed_dim`:\n",
    "\n",
    "* `N = (img_size // patch_size) ** 2` ‚Äî number of patches per image.\n",
    "* `embed_dim` is a **free hyperparameter** ‚Äî it's the dimension to which each patch (flattened and projected) is mapped.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Recap with Example:\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* `img_size = 224`\n",
    "* `patch_size = 16` ‚áí `num_patches = (224//16)^2 = 14*14 = 196`\n",
    "* `embed_dim = 768`\n",
    "* `batch_size = 8`\n",
    "\n",
    "Then:\n",
    "\n",
    "* `self.cls_token` has shape `[1, 1, 768]`\n",
    "* After expansion: `cls_tokens` has shape `[8, 1, 768]`\n",
    "* `x = self.patch_embed(x)` gives `[8, 196, 768]`\n",
    "* Concatenated: `[8, 197, 768]` ‚Äî ready for the transformer\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a diagram to visualize this sequence assembly process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b50e33-9084-4fce-bf44-c70b830cce6e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784b169-cde0-46cc-9413-5deb76bfd8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254e515-8e05-4b3f-9d69-876d54d2a447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd59e8d-34ae-4334-b862-2697c9f1aee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259383f-d53c-4aec-a4dc-ef9c9c321504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025aa80f-4abd-4058-9b8b-6148e4f3943a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c752124-099d-4636-881d-d14e0cd72e92",
   "metadata": {},
   "source": [
    "**Tokens in Vision Transformers (ViT)**\n",
    "\n",
    "**tokens in LLMs (like GPT)** and **tokens in Vision Transformers (ViT)**, Even though both are called *tokens*, their roles and processing are quite different, especially in how they're embedded.\n",
    "\n",
    "---\n",
    "\n",
    "###  Tokens in LLMs (Language Models like GPT)\n",
    "- **What are they?**: Subwords or words (e.g., \"running\" ‚Üí \"run\", \"##ning\") from natural language text.\n",
    "- **Embedding**: Each token is mapped to a high-dimensional vector using a **learned embedding matrix**. This is like a dictionary mapping:  \n",
    "  ```\n",
    "  token_id ‚Üí embedding_vector\n",
    "  ```\n",
    "- **Positional encoding**: Added to token embeddings to encode the order of words.\n",
    "- **Final input**:  \n",
    "  ```\n",
    "  input = token_embedding + positional_encoding\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "###  Tokens in ViT (Vision Transformers)\n",
    "- **What are they?**: Fixed-size **image patches** (e.g., 16√ó16 pixels), flattened and projected to vectors.\n",
    "- **Embedding**:\n",
    "  - Each patch is flattened into a vector:  \n",
    "    ```\n",
    "    patch of shape [C, H, W] ‚Üí vector of shape [C*H*W]\n",
    "    ```\n",
    "  - Then it's linearly projected into a **patch embedding vector** of desired dimension using a learned linear layer (weight matrix):\n",
    "    ```\n",
    "    embedding = LinearProjection(patch_vector)\n",
    "    ```\n",
    "  - So unlike LLMs where you look up a vector from a table, in ViT you **compute it via projection**.\n",
    "\n",
    "- **Positional encoding**: Added just like in LLMs to retain spatial information.\n",
    "\n",
    "- **Final input**:  \n",
    "  ```\n",
    "  input = patch_embedding + positional_encoding\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Difference\n",
    "| Aspect | LLM Token | ViT Token |\n",
    "|-------|------------|-----------|\n",
    "| Input type | Discrete text token | Continuous image patch |\n",
    "| Embedding source | Lookup in a learned embedding table | Linear projection of patch vector |\n",
    "| Tokenization | Byte-pair encoding or similar | Splitting image into patches |\n",
    "| Positional info | Needed | Needed |\n",
    "\n",
    "---\n",
    "\n",
    "###  So what do we do with ViT tokens if there's no embedding table?\n",
    "We **learn a linear projection layer** (a dense layer without activation) that transforms each flattened image patch into the model's hidden dimension space. This acts like an embedding layer for continuous input data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc8a97-6494-4f6b-8fb3-45a563446d67",
   "metadata": {},
   "source": [
    "Let's break it down with a **concrete numeric example** and go step by step through what happens in ViT when we tokenize an image into patches and project them.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Example Setup\n",
    "\n",
    "Let's say we have:\n",
    "- An RGB image of shape **(3, 32, 32)** ‚Üí 3 channels, 32√ó32 pixels.\n",
    "- Patch size = **16 √ó 16**\n",
    "- Hidden dimension (embedding size) = **768** (typical in ViT)\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 1: Split Image into Patches\n",
    "\n",
    "Since image size is 32√ó32 and patch size is 16√ó16:\n",
    "\n",
    "$\n",
    "\\frac{32}{16} = 2 \\text{ patches along height}, \\quad \\frac{32}{16} = 2 \\text{ patches along width}\n",
    "$\n",
    "\n",
    "‚Üí Total of **2√ó2 = 4 patches**\n",
    "\n",
    "Each patch has shape:\n",
    "```\n",
    "(3, 16, 16)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 2: Flatten Each Patch\n",
    "\n",
    "Each patch is flattened into a vector:\n",
    "```\n",
    "(3, 16, 16) ‚Üí (3√ó16√ó16) = 768-dim vector\n",
    "```\n",
    "\n",
    "So now we have 4 patch vectors, each of size 768.\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 3: Linear Projection\n",
    "\n",
    "Here‚Äôs where your question hits:\n",
    "> Is the linear projection a convolution? What do we mean by this?\n",
    "\n",
    "**Linear projection** is just a **fully connected (dense) layer** applied to each patch vector. It maps the 768-dimensional vector (from the raw patch) into another **embedding space** (which can also be 768, or 512, etc., depending on model config).\n",
    "\n",
    "**Technically:**\n",
    "If you want to project a `768`-dim vector to a `D`-dim embedding:\n",
    "- You define a weight matrix `W` of shape `(D, 768)`\n",
    "- For each patch vector `x` (shape `[768]`), you compute:  \n",
    "  ```\n",
    "  embedded_patch = W @ x + b  # shape: [D]\n",
    "  ```\n",
    "\n",
    " So it's not a convolution ‚Äî it‚Äôs more like:\n",
    "```python\n",
    "nn.Linear(in_features=768, out_features=D)\n",
    "```\n",
    "\n",
    "> But... a 2D convolution with kernel size = patch size and stride = patch size **can** be used to extract all patch embeddings in one shot! ViT variants often use that for speed.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| Image | (3, 32, 32) | Original image |\n",
    "| Patching | Split into 4 patches of (3, 16, 16) | 4 patches |\n",
    "| Flattening | Each ‚Üí (768,) vector | 4 √ó 768 |\n",
    "| Linear Projection | `nn.Linear(768, D)` on each | 4 √ó D |\n",
    "\n",
    "---\n",
    "\n",
    "###  Code Snippet (PyTorch)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Image: batch of 1 RGB image, 32x32\n",
    "img = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Patch size\n",
    "patch_size = 16\n",
    "num_patches = (32 // patch_size) ** 2  # = 4\n",
    "\n",
    "# Flatten patches manually\n",
    "patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "patches = patches.contiguous().view(1, 3, 2, 2, patch_size, patch_size)\n",
    "patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()  # (1, 2, 2, 3, 16, 16)\n",
    "patches = patches.view(1, num_patches, -1)  # (1, 4, 768)\n",
    "\n",
    "# Linear projection\n",
    "embed_dim = 512\n",
    "proj = nn.Linear(768, embed_dim)\n",
    "embedded_patches = proj(patches)  # (1, 4, 512)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa380c6-28d0-4c7f-b70a-528d28c12689",
   "metadata": {},
   "source": [
    "###  Setup\n",
    "\n",
    "Let‚Äôs say we have:\n",
    "\n",
    "```python\n",
    "img = torch.randn(1, 3, 32, 32)\n",
    "```\n",
    "\n",
    "This means:\n",
    "- Batch size = 1\n",
    "- Channels = 3 (RGB)\n",
    "- Height √ó Width = 32 √ó 32\n",
    "\n",
    "We want to split this into **non-overlapping patches** of size **16√ó16**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Step-by-step Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `img.unfold(2, patch_size, patch_size)`**\n",
    "\n",
    "```python\n",
    "patches = img.unfold(2, 16, 16)\n",
    "```\n",
    "\n",
    "This unfolds the **height (dim=2)**:\n",
    "\n",
    "- Original `img` shape: `[1, 3, 32, 32]`\n",
    "- `img.unfold(2, 16, 16)` ‚Üí shape becomes:  \n",
    "  ```\n",
    "  [1, 3, 2, 32, 16]\n",
    "  ```\n",
    "  because:\n",
    "  - 32 height ‚Üí two 16x16 patches (stride = 16)\n",
    "  - Each patch has 16 rows\n",
    "\n",
    "Then apply again:\n",
    "\n",
    "```python\n",
    "patches = patches.unfold(3, 16, 16)\n",
    "```\n",
    "\n",
    "- Now shape becomes:  \n",
    "  ```\n",
    "  [1, 3, 2, 2, 16, 16]\n",
    "  ```\n",
    "\n",
    "Explanation:\n",
    "- We now have **2√ó2 patches**\n",
    "- Each patch is of shape `[3, 16, 16]`\n",
    "- So now we‚Äôve sliced the image into 4 patches\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `patches.contiguous()`**\n",
    "\n",
    "```python\n",
    "patches = patches.contiguous()\n",
    "```\n",
    "\n",
    "This ensures that the memory layout is **contiguous** in RAM. It's needed before calling `.view()` or `.reshape()` reliably. Think of it as \"cleaning up\" tensor memory before reshaping.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `patches.permute(0, 2, 3, 1, 4, 5)`**\n",
    "\n",
    "```python\n",
    "patches = patches.permute(0, 2, 3, 1, 4, 5)\n",
    "```\n",
    "\n",
    "Before permute:\n",
    "```\n",
    "shape = [1, 3, 2, 2, 16, 16]\n",
    "```\n",
    "\n",
    "After permute:\n",
    "```\n",
    "shape = [1, 2, 2, 3, 16, 16]\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- We move the **channels (3)** to be after the patch grid `(2,2)` ‚Äî so we can easily flatten each patch.\n",
    "- Axis meaning now:\n",
    "  ```\n",
    "  [batch, patch_row, patch_col, channel, patch_h, patch_w]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `patches.view(1, num_patches, -1)`**\n",
    "\n",
    "```python\n",
    "patches = patches.view(1, 4, -1)\n",
    "```\n",
    "\n",
    "Here:\n",
    "- `2 x 2 = 4` patches ‚Üí `num_patches = 4`\n",
    "- Each patch is:\n",
    "  ```\n",
    "  3 (channels) √ó 16 √ó 16 = 768 elements\n",
    "  ```\n",
    "\n",
    "So this gives:\n",
    "```\n",
    "[1, 4, 768]\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "- 1 batch\n",
    "- 4 patch tokens\n",
    "- Each of 768 dimensions\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Summary\n",
    "\n",
    "| Step | Operation | Shape | What It Does |\n",
    "|------|-----------|-------|--------------|\n",
    "| Start | `img` | `[1, 3, 32, 32]` | One RGB image |\n",
    "| `unfold(2, 16, 16)` | Unfold height | `[1, 3, 2, 32, 16]` |\n",
    "| `unfold(3, 16, 16)` | Unfold width | `[1, 3, 2, 2, 16, 16]` | Split into patches |\n",
    "| `permute(0, 2, 3, 1, 4, 5)` | Reorder axes | `[1, 2, 2, 3, 16, 16]` | Patches as `[batch, h, w, c, H, W]` |\n",
    "| `view(1, 4, 768)` | Flatten patches | `[1, 4, 768]` | Final patch tokens |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a visual diagram or want to see how to do the same with `Conv2d`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3886e-8e67-4feb-81ba-284df85406fd",
   "metadata": {},
   "source": [
    "## **Complete ViT pipeline** \n",
    "\n",
    "We had:\n",
    "\n",
    "```python\n",
    "embedded_patches = proj(patches)  # (1, 4, 512)\n",
    "```\n",
    "\n",
    "This means:\n",
    "- Batch size = 1\n",
    "- 4 embedded tokens (one for each 16√ó16 patch)\n",
    "- Each token is now a **512-dimensional embedding vector**\n",
    "\n",
    "Now let‚Äôs walk through what happens next in a Vision Transformer (ViT):\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 5: Add a [CLS] Token (Optional but common)\n",
    "\n",
    "If you're doing **classification**, ViT introduces a learnable token like BERT's `[CLS]` at the beginning:\n",
    "\n",
    "```python\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, 512))  # learnable token\n",
    "tokens = torch.cat([cls_token.expand(batch_size, -1, -1), embedded_patches], dim=1)  # (1, 5, 512)\n",
    "```\n",
    "\n",
    "Now you have:\n",
    "- 5 tokens total: `[CLS], patch_1, patch_2, patch_3, patch_4`\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 6: Add Positional Encoding\n",
    "\n",
    "Transformers are **permutation invariant**, so we add positional encoding to inject spatial structure:\n",
    "\n",
    "```python\n",
    "pos_embed = nn.Parameter(torch.randn(1, 5, 512))  # learnable positions\n",
    "tokens = tokens + pos_embed\n",
    "```\n",
    "\n",
    "Now `tokens` is ready for the transformer.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Pass Through Transformer Encoder Layers\n",
    "\n",
    "Typically several layers like:\n",
    "- Multi-head self-attention (MHSA)\n",
    "- Feedforward MLP\n",
    "- LayerNorm\n",
    "- Residual connections\n",
    "\n",
    "Let‚Äôs define a simplified encoder using PyTorch‚Äôs `nn.TransformerEncoder`:\n",
    "\n",
    "```python\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=2048)\n",
    "transformer = TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "encoded = transformer(tokens)  # shape: (1, 5, 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 8: Final Output for Classification\n",
    "\n",
    "If you added a `[CLS]` token:\n",
    "```python\n",
    "cls_output = encoded[:, 0]  # take only the [CLS] token ‚Üí shape (1, 512)\n",
    "```\n",
    "\n",
    "Then:\n",
    "```python\n",
    "head = nn.Linear(512, num_classes)\n",
    "logits = head(cls_output)  # shape: (1, num_classes)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Recap with Dimensions\n",
    "\n",
    "| Step | Shape | Notes |\n",
    "|------|-------|-------|\n",
    "| Input image | `(1, 3, 32, 32)` | RGB image |\n",
    "| Split into patches | `(1, 4, 768)` | Flattened 16√ó16 patches |\n",
    "| Project to embeddings | `(1, 4, 512)` | Linear layer |\n",
    "| Add [CLS] | `(1, 5, 512)` | 1 CLS + 4 patch embeddings |\n",
    "| Add position | `(1, 5, 512)` | Positional encoding added |\n",
    "| Transformer | `(1, 5, 512)` | Encoded via ViT layers |\n",
    "| Classify | `(1, num_classes)` | Linear head on `[CLS]` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02202454-0972-4a25-ac7a-093123b47635",
   "metadata": {},
   "source": [
    "\n",
    "##  What does a **ViT (Vision Transformer) Encoder** do?\n",
    "\n",
    "The **ViT encoder** is the **main feature extractor** in a Vision Transformer. It transforms an input image into a sequence of patch embeddings, then processes these using **self-attention layers** to produce a rich representation of the image.\n",
    "\n",
    "### Here's what it does step-by-step:\n",
    "1. **Split the image into patches** (e.g., 16x16 pixels).\n",
    "2. **Flatten each patch** and linearly embed it (turn it into a vector).\n",
    "3. **Add positional encodings** (so the transformer knows where each patch came from).\n",
    "4. **Pass the sequence of embeddings** through **Transformer encoder blocks**, which consist of:\n",
    "   - Multi-head self-attention\n",
    "   - LayerNorm\n",
    "   - MLP (feed-forward layers)\n",
    "   - Residual connections\n",
    "\n",
    "### Output:\n",
    "The encoder outputs a **sequence of feature vectors**, one for each patch (or a special [CLS] token, depending on the model). These are rich representations that can be used for classification, segmentation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "##  What about **encoders in Variational Autoencoders (VAEs)?**\n",
    "\n",
    "In **VAEs**, the encoder has a **probabilistic role**:\n",
    "- It outputs **mean** (Œº) and **log-variance** (log œÉ¬≤) of a latent variable distribution.\n",
    "- The purpose is to **sample** from this latent space and **regularize** it to be close to a prior (like a standard normal distribution).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Key Difference\n",
    "\n",
    "| Feature                   | ViT Encoder                                 | VAE Encoder                                   |\n",
    "|--------------------------|---------------------------------------------|-----------------------------------------------|\n",
    "| Purpose                  | Extract informative visual features         | Learn a probabilistic latent distribution     |\n",
    "| Output                   | Deterministic feature vectors               | Œº and log(œÉ¬≤) for sampling latent variables   |\n",
    "| Usage                    | Downstream tasks like classification        | Sampling and reconstructing input             |\n",
    "| Based on                 | Transformer blocks (self-attention)         | CNNs or MLPs usually                          |\n",
    "| Latent representation    | Deterministic (unless used in hybrid VAE)   | Probabilistic                                 |\n",
    "\n",
    "---\n",
    "\n",
    "##  Bonus Tip: Can ViTs be used in VAEs?\n",
    "\n",
    "Yes! People have created **ViT-VAEs**, where the **ViT acts as the encoder** to extract features, and then you can have a small head (e.g., linear layers) that maps those features to mean and variance, just like in standard VAEs.\n",
    "\n",
    "So, in that case:\n",
    "- ViT encoder ‚Üí feature vector\n",
    "- Then ‚Üí linear layers ‚Üí Œº and log(œÉ¬≤)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df2ad8-58c3-4379-8bb4-9f04a7f8ce25",
   "metadata": {},
   "source": [
    "The connection between the `nn.TransformerEncoder` parameters and the **Q (Query), K (Key), V (Value)** matrices lies within the **`MultiheadAttention` module** inside each `TransformerEncoderLayer`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Recap: What Are Q, K, V?\n",
    "\n",
    "In **self-attention**, each input token vector is linearly projected to:\n",
    "\n",
    "* **Q (Query)**: what the token is looking for\n",
    "* **K (Key)**: what the token offers\n",
    "* **V (Value)**: the actual content\n",
    "\n",
    "The attention is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Where Are Q, K, V in `nn.TransformerEncoder`?\n",
    "\n",
    "Each `nn.TransformerEncoderLayer` contains a `nn.MultiheadAttention` submodule. That‚Äôs where Q, K, V are computed internally using learnable projection matrices.\n",
    "\n",
    "Here‚Äôs the breakdown:\n",
    "\n",
    "#### When you define:\n",
    "\n",
    "```python\n",
    "nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "```\n",
    "\n",
    "* `d_model=512`: the input embedding dimension\n",
    "* `nhead=8`: number of attention heads\n",
    "\n",
    "Then internally, `MultiheadAttention`:\n",
    "\n",
    "* Projects the input into Q, K, and V using 3 learnable linear layers:\n",
    "\n",
    "  $$\n",
    "  Q = X W^Q,\\quad K = X W^K,\\quad V = X W^V\n",
    "  $$\n",
    "* Each head works with a lower dimension:\n",
    "\n",
    "  $$\n",
    "  d_k = \\frac{d_{model}}{n_{head}} = \\frac{512}{8} = 64\n",
    "  $$\n",
    "\n",
    "So internally:\n",
    "\n",
    "* $W^Q$, $W^K$, and $W^V$ are parameter matrices of shape `(d_model, d_model)`\n",
    "* These are split into 8 heads during attention computation\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Where Are These Parameters?\n",
    "\n",
    "If you inspect the encoder layer:\n",
    "\n",
    "```python\n",
    "layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "print(layer.self_attn)\n",
    "```\n",
    "\n",
    "You‚Äôll see:\n",
    "\n",
    "```\n",
    "MultiheadAttention(\n",
    "  embed_dim=512, num_heads=8\n",
    ")\n",
    "```\n",
    "\n",
    "And the actual projection matrices:\n",
    "\n",
    "```python\n",
    "print(layer.self_attn.in_proj_weight.shape)  # (3*embed_dim, embed_dim) => (1536, 512)\n",
    "```\n",
    "\n",
    "These are stacked versions of $W^Q$, $W^K$, and $W^V$:\n",
    "\n",
    "* First 512 rows ‚Üí $W^Q$\n",
    "* Next 512 rows ‚Üí $W^K$\n",
    "* Last 512 rows ‚Üí $W^V$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Concept   | Where it is in PyTorch                       | Shape                         |\n",
    "| --------- | -------------------------------------------- | ----------------------------- |\n",
    "| Q, K, V   | Inside `nn.MultiheadAttention`               | Computed via `in_proj_weight` |\n",
    "| `d_model` | Total embedding dimension                    | e.g., 512                     |\n",
    "| `nhead`   | Number of attention heads                    | e.g., 8                       |\n",
    "| `d_k`     | Per-head dimension (usually `d_model/nhead`) | e.g., 64                      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e9a41-161e-4d5e-90a3-dcf939194256",
   "metadata": {},
   "source": [
    "`nn.TransformerEncoder` in PyTorch is a high-level module that implements the **encoder** part of the Transformer architecture, originally introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762). It is commonly used in tasks involving **sequence modeling**, like NLP, time series, or vision transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Structure Overview\n",
    "\n",
    "```python\n",
    "torch.nn.TransformerEncoder(\n",
    "    encoder_layer,\n",
    "    num_layers,\n",
    "    norm=None\n",
    ")\n",
    "```\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "* **`encoder_layer`**: An instance of `nn.TransformerEncoderLayer`, which defines a single layer of the encoder.\n",
    "* **`num_layers`**: Number of times to stack the encoder layer.\n",
    "* **`norm`** *(optional)*: A layer normalization module applied to the final output.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Inside `TransformerEncoderLayer`\n",
    "\n",
    "Each layer consists of:\n",
    "\n",
    "1. **Multi-head self-attention**\n",
    "2. **Add & Norm**\n",
    "3. **Feedforward neural network**\n",
    "4. **Add & Norm again**\n",
    "\n",
    "It performs:\n",
    "\n",
    "```python\n",
    "x = x + self_attn(x)        # residual + self-attention\n",
    "x = norm1(x)\n",
    "x = x + feedforward(x)      # residual + feedforward\n",
    "x = norm2(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üì• Input Format\n",
    "\n",
    "* Input shape: **(sequence\\_length, batch\\_size, embedding\\_dim)**\n",
    "  This is different from standard PyTorch modules, where batch is usually first.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define one encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "\n",
    "# Stack 6 such layers\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "# Dummy input (sequence_len=10, batch_size=32, embedding_dim=512)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "# Run the transformer encoder\n",
    "output = transformer_encoder(src)  # shape: (10, 32, 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Use Cases\n",
    "\n",
    "* NLP: As the encoder in BERT-style models\n",
    "* Vision: In Vision Transformers (ViTs)\n",
    "* Time series: Forecasting with self-attention\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
