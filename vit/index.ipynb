{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7057d2de-18f1-4dff-8f4a-6cd484f7baca",
   "metadata": {},
   "source": [
    "# **1. Vision Transformer**\n",
    "\n",
    "#### **1.1. Image → Patches**\n",
    "\n",
    "You split the image (e.g., 3×224×224) into non-overlapping patches (e.g., 3×16×16). For a 224×224 image with 16×16 patches, you get **(224/16)² = 196 patches**.\n",
    "\n",
    "#### **1.2. Linear Projection**\n",
    "\n",
    "Each patch is flattened (3×16×16 = 768-dimensional vector), then passed through a **trainable linear layer** (fully connected layer) to map it to a **`D`-dimensional embedding space** (say, D = 768).\n",
    " **Learning starts here**: this linear layer has weights that are learned during training.\n",
    "\n",
    "#### **1.3. Add Positional Embeddings**\n",
    "\n",
    "Since transformers have no inherent sense of order, you add **positional embeddings** to each projected patch embedding.\n",
    "These positional embeddings are also **learnable parameters**.\n",
    "\n",
    "#### **1.4. \\[CLS] Token**\n",
    "\n",
    "You prepend a learnable **\\[CLS] token** embedding to the patch sequence. This token is used later for classification.\n",
    "\n",
    "So now your input is a sequence of `197 × 768` (196 patches + 1 CLS token).\n",
    "\n",
    "#### **1.5. Transformer Encoder (Core of Learning)**\n",
    "\n",
    "This sequence goes through multiple **Transformer encoder layers**, each consisting of:\n",
    "\n",
    "* **Multi-head self-attention**\n",
    "* **Feed-forward neural network (MLP block)**\n",
    "* **LayerNorm and residual connections**\n",
    "\n",
    " All these components have weights that are learned. This is where **most of the learning happens** — by updating weights so that attention heads and MLPs learn to extract high-level features across the entire image.\n",
    "\n",
    "#### **1.6. MLP Head**\n",
    "\n",
    "After the last Transformer layer, the \\[CLS] token goes through an MLP (fully connected) head to produce the final classification output.\n",
    " The MLP head is also trainable.\n",
    "\n",
    "---\n",
    "#### **1.7. Where's the Learning**\n",
    "\n",
    "\n",
    "| Stage                      | Learnable Parameters? | Role                             |\n",
    "| -------------------------- | --------------------- | -------------------------------- |\n",
    "| Patch Projection (Linear)  | ✅ Yes                 | Embed patches into vectors       |\n",
    "| Positional Embeddings      | ✅ Yes                 | Encode patch positions           |\n",
    "| Transformer Encoder Layers | ✅ Yes                 | Learn contextual representations |\n",
    "| \\[CLS] Token               | ✅ Yes                 | Global image representation      |\n",
    "| MLP Head                   | ✅ Yes                 | Final prediction                 |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.8. Comparison with text models like GPT**\n",
    "\n",
    "\n",
    "\n",
    "| Aspect          | NLP (GPT, LLMs)                              | ViT (PatchEmbedding)                      |\n",
    "| --------------- | -------------------------------------------- | ----------------------------------------- |\n",
    "| Tokens          | Discrete: words, subwords (e.g. 12288 vocab) | Continuous: patches of pixels             |\n",
    "| Token embedding | Look-up: `nn.Embedding(vocab_size, D)`       | Projection via `Conv2d` per patch         |\n",
    "| Token meaning   | Symbolic (cat, run, the...)                  | Visual (16×16 image patches)              |\n",
    "| Sequence length | Varies, e.g. 512 tokens                      | Fixed: depends on image size / patch size |\n",
    "| Embedding init  | Pretrained embeddings, or random             | Learnable `Conv2d` weights                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988c73c-a78c-459a-b8d2-2919d3ff1c64",
   "metadata": {},
   "source": [
    "# **2. PatchEmbedding**\n",
    "\n",
    "Difference between **vision transformers** and **language models**: how images get turned into sequences of tokens.\n",
    "`embed_dim` is not determined by image or patch size, it’s a model design choice, like hidden size in transformers\n",
    "\n",
    "\n",
    "#### **2.1. Why do we set `embed_dim` if we know `img_size` and `patch_size`?**\n",
    "\n",
    "This is a key conceptual point:\n",
    "\n",
    "* `img_size` and `patch_size` tell you **how many patches** you’ll get:\n",
    "  `n_patches = (img_size // patch_size)²`\n",
    "\n",
    "* But **`embed_dim` is not determined by image or patch size** — it’s a **model design choice**, like hidden size in transformers.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You might have 196 patches (for 224×224 image and 16×16 patches), but you can choose:\n",
    "\n",
    "  * `embed_dim = 768` (like ViT-Base)\n",
    "  * `embed_dim = 384` (smaller model)\n",
    "  * `embed_dim = 1024` (larger model)\n",
    "\n",
    "\n",
    "The choice of `embed_dim = 768` is optional and independent of the fact that a flattened `16×16x3=768` RGB patch has 768 values.\n",
    "\n",
    "\n",
    "```python\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)  # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, embed_dim]\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c03cf1-7b1f-4b6e-a3b2-30b1061d1ae5",
   "metadata": {},
   "source": [
    "**Goal of PatchEmbedding:**  Turn a **2D image** of shape `[B, 3, 224, 224]` into a **sequence of patch tokens**:\n",
    "\n",
    "```\n",
    "[B, N, D]  ← like `[batch, sequence_length, embedding_dim]`\n",
    "```\n",
    "\n",
    "This is just like in NLP, where a sentence becomes a sequence of token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.1.Image input**:\n",
    "\n",
    "* Shape: `[B, 3, 224, 224]`\n",
    "* That is: batch of RGB images\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.2. Patchifying via `Conv2d`**\n",
    "\n",
    "Here’s the trick: instead of manually slicing the image into patches, we use a `Conv2d` to do **both patch extraction and linear projection** in one step.\n",
    "\n",
    "```python\n",
    "self.proj = nn.Conv2d(\n",
    "    in_channels=3,         # RGB channels\n",
    "    out_channels=768,      # embedding dim (D)\n",
    "    kernel_size=16,        # patch size (P)\n",
    "    stride=16              # non-overlapping patches\n",
    ")\n",
    "```\n",
    "\n",
    "What this does:\n",
    "\n",
    "* The kernel slides across the image in 16×16 steps.\n",
    "* For each 16×16×3 patch, it applies a **learned linear projection** into a 768-dimensional vector.\n",
    "* The kernel weights are learnable parameters, initialized internally by PyTorch using something like **Kaiming initialization.**\n",
    "* You don’t set the kernel manually — it’s learned during training.\n",
    "* Each output channel in this Conv2D becomes a dimension in the embedding vector for each patch.\n",
    "* So you get:\n",
    "\n",
    "  ```\n",
    "  Output shape: [B, 768, 14, 14]\n",
    "  ```\n",
    "\n",
    "Why 14x14?\n",
    "\n",
    "* Because:\n",
    "\n",
    "  ```\n",
    "  224 (image size) / 16 (patch size) = 14 patches along each dimension\n",
    "  ```\n",
    "\n",
    "* Each kernel is `3 × 16 × 16` so total number of learnable weights:  = `out_channels × 3 × 16 × 16`\n",
    "---\n",
    "\n",
    "####  **2.3. Flatten and reshape**:\n",
    "\n",
    "```python\n",
    "x = x.flatten(2)       # [B, 768, 14*14] → [B, 768, 196]\n",
    "x = x.transpose(1, 2)  # [B, 196, 768]\n",
    "```\n",
    "\n",
    "Now you have:\n",
    "\n",
    "* 196 tokens (patches),\n",
    "* each of size 768 (embedding dimension),\n",
    "* just like a sentence of 196 words, each mapped to a 768-dim word embedding.\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.4. How is this different from text models like GPT?**\n",
    "\n",
    "| Aspect          | NLP (GPT, LLMs)                              | ViT (PatchEmbedding)                      |\n",
    "| --------------- | -------------------------------------------- | ----------------------------------------- |\n",
    "| Tokens          | Discrete: words, subwords (e.g. 12288 vocab) | Continuous: patches of pixels             |\n",
    "| Token embedding | Look-up: `nn.Embedding(vocab_size, D)`       | Projection via `Conv2d` per patch         |\n",
    "| Token meaning   | Symbolic (cat, run, the...)                  | Visual (16×16 image patches)              |\n",
    "| Sequence length | Varies, e.g. 512 tokens                      | Fixed: depends on image size / patch size |\n",
    "| Embedding init  | Pretrained embeddings, or random             | Learnable `Conv2d` weights                |\n",
    "\n",
    "---\n",
    "\n",
    "####  **2.5. Why `Conv2d` for projection?**\n",
    "\n",
    "Because:\n",
    "\n",
    "* It mimics the behavior of **flattening + linear projection** of each patch.\n",
    "* But it’s faster and GPU-friendly.\n",
    "* Equivalent to slicing out each 16×16 patch, flattening it into `[768]`, and applying a `Linear(3*16*16, 768)`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Dimensions**\n",
    "\n",
    "| Stage               | Shape                                    |\n",
    "| ------------------- | ---------------------------------------- |\n",
    "| Input Image         | `[B, 3, 224, 224]`                       |\n",
    "| Conv2d Output       | `[B, 768, 14, 14]`                       |\n",
    "| Flatten + Transpose | `[B, 196, 768]`                          |\n",
    "| Output Tokens       | 196 patch tokens per image, each `[768]` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b59628-2848-4bbe-97fe-15bf30af56ad",
   "metadata": {},
   "source": [
    "# **3. `MiniViT`**\n",
    "Its job is to take an image and output a **classification prediction** using a Vision Transformer.\n",
    "\n",
    "```python\n",
    "class MiniViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_classes=10, depth=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(\n",
    "            1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # [B, N, D]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "        x = x + self.pos_embed[:, :x.size(1)]  # positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # [B, N+1, D]\n",
    "        cls_out = x[:, 0]  # CLS token output\n",
    "        return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abb66e-578c-4c55-9a17-0d6ba62a2d17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **3.1. patch_embed**\n",
    "\n",
    "```python\n",
    "self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Converts the image into a sequence of **patch tokens**.\n",
    "* Output shape: `[B, N, D]`, where:\n",
    "\n",
    "  * `B` = batch size\n",
    "  * `N = (img_size // patch_size)^2` = number of patches (e.g., 14×14 = 196)\n",
    "  * `D = embed_dim` (e.g., 768)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9cc7d-1ef0-4e19-95d7-8e20784623c8",
   "metadata": {},
   "source": [
    "#### **3.2. cls_token**\n",
    "\n",
    "```python\n",
    "self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "```\n",
    "\n",
    "\n",
    "**Why `self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))`? Shouldn't we take into account the number of batches?**\n",
    "\n",
    "\n",
    "* `self.cls_token` is a **learnable embedding** (like a special \"classification token\").\n",
    "* It's initialized as shape `[1, 1, embed_dim]`, because it will be **expanded at runtime** in the `forward()` method using:\n",
    "\n",
    "  ```python\n",
    "  cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "  ```\n",
    "\n",
    "So it **doesn't need to store B tokens**—we just keep **one** and replicate it per batch during the forward pass. This saves memory and simplifies learning.\n",
    "\n",
    "\n",
    "* Defines a **learnable `[CLS]` token** — shared across all images.\n",
    "* At each forward pass, it's **expanded** to batch size `[B, 1, D=embed_dim]`.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e345af-dcd7-46eb-9d7f-1e7a3fbff273",
   "metadata": {},
   "source": [
    "#### **3.3. pos_embed**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Positional embedding for each token, including `[CLS]`.\n",
    "* Shape:\n",
    "\n",
    "  * `[1, 197, 768]` if `N=196`\n",
    "* Added to the input sequence to inject spatial info.\n",
    "\n",
    "\n",
    "\n",
    "`pos_embed` is a tensor of shape:\n",
    "\n",
    "```python\n",
    "[1, N+1, embed_dim]\n",
    "```\n",
    "\n",
    "* `N` = number of patches per image\n",
    "* `+1` = to account for the `[CLS]` token\n",
    "* `embed_dim` = same dimension as the patch embeddings\n",
    "\n",
    "Each vector in `pos_embed` is a **learnable position encoding vector**, and its job is to **inject spatial order** into the transformer’s input.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we need it?**\n",
    "\n",
    "Transformers are **permutation invariant** — they don’t care about the order of tokens unless you tell them.\n",
    "\n",
    "In vision:\n",
    "\n",
    "* Images have a clear spatial structure (top-left to bottom-right), but patch embeddings **lose this spatial information** once flattened.\n",
    "* `pos_embed` gives **each patch a unique positional tag** so the model knows \"where\" each patch came from.\n",
    "\n",
    "So even though each patch is a vector of size `embed_dim`, the **position vector** is added to it:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "This is like giving each token a GPS tag so the transformer knows where it belongs in the image.\n",
    "\n",
    "---\n",
    "\n",
    "**Does `pos_embed` allow adding or subtracting patches?**\n",
    "\n",
    "Not exactly in a literal sense — the positional embeddings:\n",
    "\n",
    "* **Don’t allow adding or subtracting patches spatially**, like in a convolution or image coordinate system.\n",
    "* But they do allow the model to **learn relationships between positions**, i.e., between patches at different locations.\n",
    "\n",
    "So in a way, yes: by giving each patch a unique positional identity, the model can **infer spatial relationships** (e.g., proximity, layout) during training.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc76a2-1554-4e16-a9bd-4f9d9c59f065",
   "metadata": {},
   "source": [
    "#### **3.4. Transformer**\n",
    "```python\n",
    "self.transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
    "    num_layers=depth\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Standard transformer encoder stack:\n",
    "\n",
    "  * Multi-head self-attention\n",
    "  * Feedforward layers\n",
    "  * LayerNorm and residual connections\n",
    "* Processes all tokens (patches + CLS)\n",
    "* Input: `[B, N+1, D]` → Output: `[B, N+1, D]`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **3.5. mlp_head**\n",
    "\n",
    "```python\n",
    "self.mlp_head = nn.Sequential(\n",
    "    nn.LayerNorm(embed_dim),\n",
    "    nn.Linear(embed_dim, num_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Post-transformer classification head\n",
    "* Takes only `[CLS]` token output: `[B, D]`\n",
    "* Maps to class scores: `[B, num_classes]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed40ae-79dc-4f54-8e43-4f08052f317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76fcc-266b-4c9f-b26c-37e7145de311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de21d46e-71cf-4e05-a823-b05a2d8de962",
   "metadata": {},
   "source": [
    "#  **4. Full Forward Pass**\n",
    "\n",
    "**Where do we call `forward` in `PatchEmbedding`? Do we call it at all?**\n",
    "\n",
    "The `forward` is called **implicitly** in this line of the `MiniViT.forward()` method:\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    "In PyTorch, when you do `self.patch_embed(x)`, it's syntactic sugar for:\n",
    "\n",
    "```python\n",
    "self.patch_embed.forward(x)\n",
    "```\n",
    "\n",
    "So, `forward` is called—this is how PyTorch modules are normally used.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **3. Why does `self.pos_embed` have `+1` in:**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "```\n",
    "\n",
    "This `+1` is **for the classification token** we just talked about.\n",
    "\n",
    "* The number of image patches is `(img_size // patch_size) ** 2`.\n",
    "* But since we also prepend a `[CLS]` token to the input sequence, the positional embedding must have **one extra position**.\n",
    "* So: `num_patches + 1` positions total.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is happening in this line:**\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    "This is where:\n",
    "\n",
    "* The image is split into non-overlapping **patches** using a `Conv2d` layer with:\n",
    "\n",
    "  * `kernel_size = patch_size`\n",
    "  * `stride = patch_size`\n",
    "\n",
    "This does both:\n",
    "\n",
    "* The patch extraction\n",
    "* The linear projection into the `embed_dim`\n",
    "\n",
    "Then:\n",
    "\n",
    "* `x.flatten(2)` flattens the spatial dimensions (`H' * W'`) into one sequence dimension (`N` = number of patches).\n",
    "* `x.transpose(1, 2)` changes shape from `[B, D, N]` to `[B, N, D]`, matching transformer expectations.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a diagram to visualize the patch embedding and token addition process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    B = x.size(0)\n",
    "```\n",
    "\n",
    "Gets batch size.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(x)  # [B, N, D]\n",
    "```\n",
    "\n",
    "Image → patch tokens\n",
    "\n",
    "---\n",
    "#### **4.1. What is `cls_token`?**\n",
    "\n",
    "\n",
    "* `self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))` is a **learnable embedding**\n",
    "* `torch.zeros(1, 1, embed_dim)` initializes a tensor filled with zeros.\n",
    "* `nn.Parameter(...)` marks it as a **learnable parameter** (its values will be updated during training).\n",
    "* It acts like a placeholder for the **\"summary\" of the image**, similar to how `[CLS]` is used in BERT for sentences.\n",
    "\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "```\n",
    "\n",
    "* It is **shared across the entire dataset** — meaning:\n",
    "\n",
    "  * Same `cls_token` is used for **every image**, in **every batch**, throughout training.\n",
    "\n",
    "\n",
    "**`expand()` in PyTorch does *not* create separate memory copies.**\n",
    "\n",
    "* It’s **not cloning** the tensor.\n",
    "* It creates a **view** on the **same underlying data** — like a broadcasted reference.\n",
    "* So even though `cls_tokens` appears to be `[B, 1, D]`, it’s still backed by **one shared parameter** (`self.cls_token`).\n",
    "\n",
    "Therefore, **during backpropagation, the gradient updates only a single shared `cls_token` parameter.**\n",
    "\n",
    "---\n",
    "\n",
    "Contrast: If you had used `.repeat()` instead of `.expand()`:\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.repeat(B, 1, 1)  # BAD if you want sharing!\n",
    "```\n",
    "\n",
    "That **would** create `B` separate memory copies — now you’d have `B` independent tokens, and gradients would not be shared. But this is **not** what we want.\n",
    "\n",
    "\n",
    "\n",
    "#### **4.2. Adding `cls_token` to the Embedding Space**\n",
    "\n",
    "```python\n",
    "x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "```\n",
    "\n",
    "\n",
    "* Prepends `[CLS]` token to the sequence of patch embeddings.\n",
    "* The `[CLS]` token will aggregate global information from all patches during transformer attention.\n",
    "* Now the transformer input sequence has length `N+1`.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0017f14-86fc-4a95-baf1-69a1e80ba8d2",
   "metadata": {},
   "source": [
    "#### **4.3. Positional Embedding**\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "We previously had:\n",
    "\n",
    "```python \n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, D))  # [1, N+1, embed_dim]\n",
    "```\n",
    "\n",
    "\n",
    "* Transformers are order-agnostic; this injects positional structure so the model knows where each patch came from.\n",
    "* The goal is to **inject positional information** into the patch + `[CLS]` token sequence, so the Transformer knows where each token comes from spatially.\n",
    "* This means: for each **position in the token sequence** (patch 0, patch 1, ..., patch N, and CLS), there’s a learnable vector of size `D`.\n",
    "\n",
    "* `pos_embed` is a **single learnable parameter of the model**, just like `cls_token`.\n",
    "* It's learned from all training images, and it is **shared** across the entire training set.\n",
    "* Every image in every batch adds these **same positional embeddings** to its tokens — the only difference is the **content** of the tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407ca9e-b0e9-41b5-b8e2-4bb30014cb3d",
   "metadata": {},
   "source": [
    "#### **4.4. Why `pos_embed` is shared across all images**\n",
    "\n",
    "\"If `pos_embed` is shared across all images, doesn't that mean that all images are given the same spatial cues?\"\n",
    "\n",
    "Yes. The **same positional encoding** is added to each token position — e.g., patch 0, patch 1, ..., patch N — **in every image**.\n",
    "\n",
    "But that’s **not a limitation** — it’s exactly what enables the model to learn spatial reasoning.\n",
    "\n",
    "\n",
    "**Each patch’s content differs per image**\n",
    "\n",
    "* While **position 0** (top-left patch) always gets the same positional embedding,\n",
    "* The **actual patch embedding** (`x`) differs per image, because the image content is different.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "final_input = patch_embedding + positional_embedding\n",
    "```\n",
    "\n",
    "* `patch_embedding`: image-specific\n",
    "* `positional_embedding`: shared and position-specific\n",
    "\n",
    "\n",
    "> We **do want the same positional embedding per position across all images**, so the transformer learns **position-aware attention behavior** — e.g., how patch 3 attends to patch 10, or how center patches are more informative.\n",
    "\n",
    "**What would go wrong if each image had its own positional encoding?**\n",
    "\n",
    "* The model would no longer understand **what each position *means***.\n",
    "* Patch 5 in image A might look like top-left, and patch 5 in image B might look like center — complete spatial chaos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64aac5-01b9-4549-b1c4-0a20f69f60a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4.5. Why do we set pos_embed  zeros and not random values**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "* **Zero is neutral**: It doesn’t bias any patch toward any direction at the start.\n",
    "* In early training steps, it allows the model to first rely more on the content of the patch embeddings (`x`), and slowly learn how position contributes.\n",
    "* If we started with random values (e.g. `torch.randn`), the model might initially be confused by meaningless noise patterns in position embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83846a10-f283-4da2-a9e8-0c0f2c62d8ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4.6. `x` and `pos_embed` Dimension**\n",
    "\n",
    "**What is `x` at this point?**\n",
    "\n",
    "Right before this line:\n",
    "\n",
    "```python\n",
    "x = torch.cat((cls_tokens, patch_embeddings), dim=1)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* `x.shape = [B, N+1, D]`\n",
    "\n",
    "  * `B` = batch size\n",
    "  * `N` = number of patches per image\n",
    "  * `+1` = for the `[CLS]` token\n",
    "  * `D` = embedding dimension\n",
    "\n",
    "\n",
    "\n",
    "**What is `self.pos_embed`?**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, D))\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* Shape = `[1, N+1, D]`\n",
    "* It holds **learnable positional encodings**, one for each token position (including the `[CLS]` token).\n",
    "* These are **shared across the batch**, just like `cls_token`.\n",
    "\n",
    "---\n",
    "\n",
    "**What does `self.pos_embed[:, :x.size(1)]` mean?**\n",
    "\n",
    "It selects a **slice** of the positional embeddings that matches the actual input sequence length.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "* To handle dynamic token lengths, e.g. if you change image size or use fewer patches (in some variants).\n",
    "\n",
    "More concretely:\n",
    "\n",
    "* `x.size(1)` = `N+1`\n",
    "* So `self.pos_embed[:, :x.size(1)]` → shape `[1, N+1, D]`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc882492-39e0-4dd2-a170-c126e28dca39",
   "metadata": {},
   "source": [
    "#  **5. Transformer Blocks**\n",
    "\n",
    "The final input to the transformer looks like this for batch size `B`:\n",
    "\n",
    "```\n",
    "[CLS] Patch1 Patch2 Patch3 ... PatchN   ← total of N+1 tokens per image\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "`x = self.transformer(x)  # [B, N+1, D]`\n",
    "\n",
    "* **What:** Feeds the token sequence (with positions) into a Transformer encoder.\n",
    "* **Why:** The transformer processes all tokens with multi-head self-attention, allowing tokens to interact and share context.\n",
    "\n",
    "---\n",
    "\n",
    "`cls_out = x[:, 0]  # [B, D]`\n",
    "\n",
    "* **What:** Extracts the output of the `[CLS]` token for each image.\n",
    "* **Why:** This token is expected to **summarize the entire image** after going through the transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  **5.1 Why  nn.TransformerEncoder and not nn.Transformer**\n",
    "\n",
    "**`nn.Transformer`**\n",
    "\n",
    "`nn.Transformer` is a **complete Transformer** model that includes both:\n",
    "\n",
    "* an **encoder**, and\n",
    "* a **decoder**.\n",
    "\n",
    "This is the original full architecture used in **sequence-to-sequence** tasks like:\n",
    "\n",
    "* Machine Translation (e.g., English → French),\n",
    "* Text summarization,\n",
    "* Image captioning.\n",
    "\n",
    "```python\n",
    "transformer = nn.Transformer(\n",
    "    d_model=768,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "The full transformer processes:\n",
    "\n",
    "* an **input sequence** (to the encoder),\n",
    "* and a **target sequence** (to the decoder).\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e6ff7-d1c1-4aee-9bb2-59797cb83031",
   "metadata": {},
   "source": [
    "**`nn.TransformerEncoder`**\n",
    "\n",
    "\n",
    "`nn.TransformerEncoder` includes **only the encoder part** of a Transformer.\n",
    "\n",
    "It consists of a stack of `nn.TransformerEncoderLayer`s.\n",
    "\n",
    "```python\n",
    "transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=768, nhead=8),\n",
    "    num_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "It's designed for tasks where:\n",
    "\n",
    "* You just need to **encode** the input sequence,\n",
    "* There's **no target sequence** to decode.\n",
    "\n",
    " **This is exactly what you need in ViT**, where:\n",
    "\n",
    "* The model receives a sequence of patch embeddings + `[CLS]` token,\n",
    "* And **outputs a representation of the image** (via the `[CLS]` token),\n",
    "* There's no need to generate a new sequence (no decoder!).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed3b49-e462-4d24-a957-dd5241dc4c07",
   "metadata": {},
   "source": [
    "#  **6. MLP Head**\n",
    "\n",
    "### `return self.mlp_head(cls_out)  # [B, num_classes]`\n",
    "\n",
    "* **What:** Passes the `[CLS]` token through a final MLP head (e.g., `LayerNorm → Linear`).\n",
    "* **Why:** Outputs classification scores per image.\n",
    "*  Output shape: `[B, num_classes]`, ready for softmax or cross-entropy loss.\n",
    "\n",
    "\n",
    "* If you have `embed_dim = 768` and `num_classes = 10`, then the final classification head is:\n",
    "\n",
    "  ```python\n",
    "  nn.Linear(embed_dim, num_classes)  # 768 → 10\n",
    "  ```\n",
    "\n",
    "* This produces output logits of shape `[B, 10]`, one row per image, each row containing scores for the 10 classes.\n",
    "\n",
    "Example from code:\n",
    "\n",
    "```python\n",
    "self.mlp_head = nn.Sequential(\n",
    "    nn.LayerNorm(embed_dim),\n",
    "    nn.Linear(embed_dim, num_classes)  # shape: [B, embed_dim] → [B, 10]\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340edbd4-e30c-4c3d-a3db-b9e8d71cd5c1",
   "metadata": {},
   "source": [
    "# **7. ViT output, CLS token**\n",
    "\n",
    "####  **7.1. Image Classification**\n",
    "\n",
    "* **Use only**: `x[:, 0, :]` → the `[CLS]` token.\n",
    "* Other tokens: ignored.\n",
    "* Why: `[CLS]` is trained to summarize the entire image.\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.2. Semantic Segmentation**\n",
    "\n",
    "* **Use all tokens**, including patch tokens.\n",
    "* After the transformer, reshape the N patch tokens back into a 2D spatial grid.\n",
    "* Apply a small convolutional head (e.g. MLP or decoder) to generate **per-pixel predictions**.\n",
    "\n",
    "Example: [Segmenter](https://arxiv.org/abs/2105.05633), SETR, ViT-SEG\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.3. Object Detection**\n",
    "\n",
    "* **Use all patch tokens**, or a selected subset.\n",
    "* Add **object query embeddings** (like in DETR), and let the model predict bounding boxes and class labels by attending to patch tokens.\n",
    "\n",
    " Example: [DETR](https://arxiv.org/abs/2005.12872), ViTDet, DINO\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.4. Masked Image Modeling (Self-supervised learning)**\n",
    "\n",
    "* Use patch tokens to **reconstruct masked parts of the image**.\n",
    "* `[CLS]` may still be used, but **patch tokens are the focus**.\n",
    "* You mask out a subset of patch tokens, and the model tries to predict them.\n",
    "\n",
    " Example: [MAE (Masked Autoencoders)](https://arxiv.org/abs/2111.06377)\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.5. Vision-Language Tasks (e.g., CLIP, Image Captioning)**\n",
    "\n",
    "* `[CLS]` is often used as the **global image embedding** (e.g., for retrieval or alignment with text).\n",
    "* But patch tokens can be used in:\n",
    "\n",
    "  * Cross-attention with text tokens,\n",
    "  * Generating fine-grained alignments (e.g., for caption generation or grounding).\n",
    "\n",
    "Example: CLIP, BLIP, Flamingo, LLaVA\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.6. Feature Extraction / Dense Predictions**\n",
    "\n",
    "* Sometimes you want features **at each patch location**, not a single global vector.\n",
    "* Patch token outputs are used for:\n",
    "\n",
    "  * Keypoint detection,\n",
    "  * Pose estimation,\n",
    "  * Saliency maps, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Task                       | Use `[CLS]` only? | Use patch tokens? | Why                                |\n",
    "| -------------------------- | ----------------- | ----------------- | ---------------------------------- |\n",
    "| Image Classification       | ✅ Yes             | ❌ No              | Summary of image                   |\n",
    "| Semantic Segmentation      | ❌ No              | ✅ Yes             | Dense pixel-wise output            |\n",
    "| Object Detection           | ❌ No              | ✅ Yes             | Localize and classify objects      |\n",
    "| Masked Image Modeling      | ❌ No              | ✅ Yes             | Reconstruct image patches          |\n",
    "| Vision-Language Embedding  | ✅ (often)         | ✅ (sometimes)     | Global alignment + local reasoning |\n",
    "| Keypoint / Pose Estimation | ❌ No              | ✅ Yes             | Use local spatial features         |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a52f8d-cca9-4776-a93a-9728cfa3d082",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b50e33-9084-4fce-bf44-c70b830cce6e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e369bd1-f2b2-4d1a-91f7-dd6d72e2e95d",
   "metadata": {},
   "source": [
    "\n",
    "| Step                    | Shape              | Description                          |\n",
    "| ----------------------- | ------------------ | ------------------------------------ |\n",
    "| Input image             | `[B, 3, 224, 224]` | RGB input                            |\n",
    "| PatchEmbedding          | `[B, 196, 768]`    | One token per 16×16 patch            |\n",
    "| Add `[CLS]` token       | `[B, 197, 768]`    | CLS prepended                        |\n",
    "| Add positional encoding | `[B, 197, 768]`    | Adds learnable position info         |\n",
    "| Transformer output      | `[B, 197, 768]`    | Token-wise contextual representation |\n",
    "| Select `[CLS]` output   | `[B, 768]`         | Global image representation          |\n",
    "| MLP head                | `[B, num_classes]` | Final class logits                   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df2ad8-58c3-4379-8bb4-9f04a7f8ce25",
   "metadata": {},
   "source": [
    "# **8. Q (Query), K (Key), V (Value) matrices and MultiheadAttention**\n",
    "\n",
    "The connection between the `nn.TransformerEncoder` parameters and the **Q (Query), K (Key), V (Value)** matrices lies within the **`MultiheadAttention` module** inside each `TransformerEncoderLayer`.\n",
    "\n",
    "\n",
    "#### **8.1. What Are Q, K, V?**\n",
    "\n",
    "In **self-attention**, each input token vector is linearly projected to:\n",
    "\n",
    "* **Q (Query)**: what the token is looking for\n",
    "* **K (Key)**: what the token offers\n",
    "* **V (Value)**: the actual content\n",
    "\n",
    "The attention is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Where Are Q, K, V in `nn.TransformerEncoder`?**\n",
    "\n",
    "Each `nn.TransformerEncoderLayer` contains a `nn.MultiheadAttention` submodule. That’s where Q, K, V are computed internally using learnable projection matrices.\n",
    "\n",
    "\n",
    "When you define:\n",
    "\n",
    "```python\n",
    "nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "```\n",
    "\n",
    "* `d_model=512`: the input embedding dimension\n",
    "* `nhead=8`: number of attention heads\n",
    "\n",
    "Then internally, `MultiheadAttention`:\n",
    "\n",
    "* Projects the input into Q, K, and V using 3 learnable linear layers:\n",
    "\n",
    "  $$\n",
    "  Q = X W^Q,\\quad K = X W^K,\\quad V = X W^V\n",
    "  $$\n",
    "* Each head works with a lower dimension:\n",
    "\n",
    "  $$\n",
    "  d_k = \\frac{d_{model}}{n_{head}} = \\frac{512}{8} = 64\n",
    "  $$\n",
    "\n",
    "So internally:\n",
    "\n",
    "* $W^Q$, $W^K$, and $W^V$ are parameter matrices of shape `(d_model, d_model)`\n",
    "* These are split into 8 heads during attention computation\n",
    "\n",
    "---\n",
    "\n",
    "####  Where Are These Parameters?\n",
    "\n",
    "If you inspect the encoder layer:\n",
    "\n",
    "```python\n",
    "layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "print(layer.self_attn)\n",
    "```\n",
    "\n",
    "You’ll see:\n",
    "\n",
    "```\n",
    "MultiheadAttention(\n",
    "  embed_dim=512, num_heads=8\n",
    ")\n",
    "```\n",
    "\n",
    "And the actual projection matrices:\n",
    "\n",
    "```python\n",
    "print(layer.self_attn.in_proj_weight.shape)  # (3*embed_dim, embed_dim) => (1536, 512)\n",
    "```\n",
    "\n",
    "These are stacked versions of $W^Q$, $W^K$, and $W^V$:\n",
    "\n",
    "* First 512 rows → $W^Q$\n",
    "* Next 512 rows → $W^K$\n",
    "* Last 512 rows → $W^V$\n",
    "\n",
    "---\n",
    "\n",
    "####  Summary\n",
    "\n",
    "| Concept   | Where it is in PyTorch                       | Shape                         |\n",
    "| --------- | -------------------------------------------- | ----------------------------- |\n",
    "| Q, K, V   | Inside `nn.MultiheadAttention`               | Computed via `in_proj_weight` |\n",
    "| `d_model` | Total embedding dimension                    | e.g., 512                     |\n",
    "| `nhead`   | Number of attention heads                    | e.g., 8                       |\n",
    "| `d_k`     | Per-head dimension (usually `d_model/nhead`) | e.g., 64                      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20bef5-b57a-4b5a-b84f-2ac8b2ef68e0",
   "metadata": {},
   "source": [
    "#### **8.3. Each head is receiving the full sequence of tokens, but a part of the token dimension**\n",
    "\n",
    "Each **attention head** receives:\n",
    "\n",
    "* The **entire sequence of tokens** (all patches in ViT, or all words in LLM),\n",
    "* But only a **slice of the embedding dimension** — i.e., **a portion of each token's features**.\n",
    "\n",
    "---\n",
    "\n",
    "####  **8.4. What Each Head Sees**\n",
    "\n",
    "Say you have:\n",
    "\n",
    "| Term                 | Example Value (ViT-B)   |\n",
    "| -------------------- | ----------------------- |\n",
    "| `B` (batch)          | 8                       |\n",
    "| `N` (tokens)         | 197 (196 patches + CLS) |\n",
    "| `D` (embed\\_dim)     | 768                     |\n",
    "| `H` (heads)          | 12                      |\n",
    "| `d_k` (per-head dim) | 768 / 12 = 64           |\n",
    "\n",
    "---\n",
    "\n",
    "So, for each head:\n",
    "\n",
    "* You split Q, K, and V into:\n",
    "\n",
    "  ```python\n",
    "  Q → [B, H, N, d_k] = [8, 12, 197, 64]\n",
    "  K → same\n",
    "  V → same\n",
    "  ```\n",
    "\n",
    "* Each of the **12 heads** receives:\n",
    "\n",
    "  * The **entire sequence** of 197 tokens\n",
    "  * Each token represented by **only 64 dimensions**, a slice of the full 768-dim\n",
    "\n",
    "> 🔸 In other words, **each head gets all tokens, but not all of each token**.\n",
    "\n",
    "---\n",
    "\n",
    "####  **8.5. Diagram of What Heads See**\n",
    "\n",
    "| Head | Sees Tokens? | Sees Full Token Embedding? | Sees Which Slice of Embedding |\n",
    "| ---- | ------------ | -------------------------- | ----------------------------- |\n",
    "| 1    | ✅ Yes        | ❌ No                       | Slice 0–63                    |\n",
    "| 2    | ✅ Yes        | ❌ No                       | Slice 64–127                  |\n",
    "| ...  | ✅ Yes        | ❌ No                       | ...                           |\n",
    "| 12   | ✅ Yes        | ❌ No                       | Slice 704–767                 |\n",
    "\n",
    "Then after computing attention independently, all heads' outputs are **concatenated back** into a full `[B, N, D]` tensor.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "* **Heads = parallel experts** each focusing on different features or relationships\n",
    "* The model learns **what to split across heads**\n",
    "* The attention mechanism helps **each token interact with every other**, but **from a different \"view\" in each head**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa49842-a426-4dca-9bd8-2f3d9128e8e5",
   "metadata": {},
   "source": [
    "#### **LLMs token Dimension and ViT Token Dimension**\n",
    "\n",
    "\n",
    "* in **LLMs**, the **token embedding dim** (e.g., 12,288) is often **much higher** than the **per-head Q/K/V projection dim**.\n",
    "* In **ViTs**, the **embedding dim** (e.g., 768) is usually the **same** as the total Q/K/V projection output — no further reduction in dimensionality.\n",
    "* In both, Q, K, V are **projected from the input**, but the total model width (`D`) is **much smaller in ViT** than in GPT-style models.\n",
    "\n",
    "---\n",
    "\n",
    "What’s Happening in Each Case?\n",
    "\n",
    "####  In ViT-B (Vision Transformer Base):\n",
    "\n",
    "* `embed_dim = 768`\n",
    "* `num_heads = 12`\n",
    "* `head_dim = 768 / 12 = 64`\n",
    "* So: **Q, K, V** are all `[B, 197, 768]` before being split into heads\n",
    "* After split: `[B, 12, 197, 64]`\n",
    "\n",
    "→ We **do not reduce** the dimension here — the model works within its `D = 768` constraint.\n",
    "\n",
    "---\n",
    "\n",
    "####  In GPT-style LLMs (e.g., GPT-4):\n",
    "\n",
    "* `token_dim = 12,288`\n",
    "* `num_heads = 96`\n",
    "* `head_dim = 128`\n",
    "* `Q`, `K`, `V` are **projected from** the 12,288-dimensional input token embedding using:\n",
    "\n",
    "```python\n",
    "W_q: [12288, 12288]  # in practice, usually packed as [12288, 3×12288] for QKV\n",
    "```\n",
    "\n",
    "* Result: Q/K/V are each `[B, seq_len, 12288]` → then reshaped to `[B, 96, seq_len, 128]`\n",
    "\n",
    " So the total Q or K table (`QK^T`) is of shape:\n",
    "\n",
    "```plaintext\n",
    "[B, 96, seq_len, seq_len]\n",
    "```\n",
    "\n",
    "→ **It doesn’t reduce dimensionality** per se, it splits into heads just like ViT, but because `token_dim` is huge (12,288), everything scales up.\n",
    "\n",
    "---\n",
    "\n",
    "| Aspect             | ViT-B                | GPT-4 (hypothetical numbers)             |\n",
    "| ------------------ | -------------------- | ---------------------------------------- |\n",
    "| Token dim          | 768                  | 12,288                                   |\n",
    "| # Heads            | 12                   | 96                                       |\n",
    "| Per-head dim       | 64                   | 128                                      |\n",
    "| Q, K, V shape      | \\[B, 197, 768]       | \\[B, seq\\_len, 12288]                    |\n",
    "| QKᵀ shape per head | \\[B, 12, 197, 197]   | \\[B, 96, seq\\_len, seq\\_len]             |\n",
    "| Dim reduction?     | ❌ Not reduced in ViT | ❌ Also not reduced, but **bigger input** |\n",
    "\n",
    "---\n",
    "\n",
    "#### So Why the Confusion?\n",
    "\n",
    "It **feels** like LLMs reduce dimensions because:\n",
    "\n",
    "* The **token size is huge**, so the per-head Q/K/V dims (like 128) seem small.\n",
    "* But they’re just **splitting**, not compressing.\n",
    "* **Both LLMs and ViTs maintain full dimensionality — just divided into heads.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddedaef-ae92-49a7-9a57-ed9cb9442679",
   "metadata": {},
   "source": [
    "#### **$W_q$ ,  $W_k$ , and $W_v$**\n",
    "\n",
    "\n",
    "There is **one** `W_q`, `W_k`, and `W_v` **per input**, but they **generate Q, K, V for all heads at once**.\n",
    "\n",
    "You **do not** have separate `W_q` matrices per head.\n",
    "\n",
    "Instead, you have **one big `W_q` matrix** (and same for `W_k` and `W_v`):\n",
    "\n",
    "```plaintext\n",
    "W_q: [embed_dim, embed_dim]  → e.g., [768, 768]\n",
    "```\n",
    "\n",
    "So when you do:\n",
    "\n",
    "```python\n",
    "Q = x @ W_q  # x: [B, N, 768] → Q: [B, N, 768]\n",
    "```\n",
    "\n",
    "You're computing **Q for all heads at once**, and **then splitting** the resulting `Q` into heads:\n",
    "\n",
    "```python\n",
    "Q → Q.view(B, N, num_heads, head_dim) → [B, 197, 12, 64]\n",
    "Q → Q.permute(0, 2, 1, 3) → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daf498-724f-493c-b73c-18e7ac4225bf",
   "metadata": {},
   "source": [
    "\n",
    "#### Why Only One $W_q$?\n",
    "\n",
    "It’s about **efficiency** and **parameter sharing**:\n",
    "\n",
    "* You use one $W_q$, one $W_k$, and one $W_v$ to project the token embeddings into a higher-dimensional tensor that you then slice into heads.\n",
    "* This allows you to train one compact linear layer per Q/K/V — and **each head learns to focus on different features through splitting**.\n",
    "\n",
    "Each head ends up with **its own portion of the projected Q, K, V**, but those projections came from a **single shared matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Concept    | Shared Across Heads? | Shape (ViT-B example) |\n",
    "| ---------- | -------------------- | --------------------- |\n",
    "| $W_q$      | ✅ Yes (1 matrix)     | `[768, 768]`          |\n",
    "| $W_k$      | ✅ Yes                | `[768, 768]`          |\n",
    "| $W_v$      | ✅ Yes                | `[768, 768]`          |\n",
    "| Per-head Q | Computed by slicing  | `[B, 12, 197, 64]`    |\n",
    "\n",
    "Then all the heads’ results are concatenated and passed through a final linear projection (`W_o`):\n",
    "\n",
    "```python\n",
    "concat_heads: [B, 197, 768]\n",
    "out = concat_heads @ W_o  # W_o: [768, 768]\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416bfc5-1111-4ffc-abcc-358a50822932",
   "metadata": {},
   "source": [
    "#### **Splitting $W_q$,  $W_k$, and $W_v$ over the heads**\n",
    "\n",
    "In **multi-head self-attention**, the output of the `Q = x @ W_q` operation is **split across multiple heads**.\n",
    "\n",
    "Let’s walk through this clearly and carefully:\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Recap: What Happens to Q, K, V\n",
    "\n",
    "You start with:\n",
    "\n",
    "```python\n",
    "x: [B, N, D]  ← e.g., [B, 197, 768]\n",
    "```\n",
    "\n",
    "Then project:\n",
    "\n",
    "```python\n",
    "Q = x @ W_q  # W_q: [D, D], so Q: [B, N, D] = [B, 197, 768]\n",
    "```\n",
    "\n",
    "So yes — the output `Q` has the **same shape as `x`**, but internally this will be **split across heads**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Splitting Q for Multi-Head Attention\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "* `D = 768`\n",
    "* `num_heads = 12`\n",
    "* Then: `head_dim = D // num_heads = 64`\n",
    "\n",
    "Now reshape:\n",
    "\n",
    "```python\n",
    "Q → Q.view(B, N, num_heads, head_dim) → [B, 197, 12, 64]\n",
    "```\n",
    "\n",
    "Then **transpose** to:\n",
    "\n",
    "```python\n",
    "Q → Q.permute(0, 2, 1, 3) → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "Now:\n",
    "\n",
    "* `B` → batch\n",
    "* `12` → number of heads\n",
    "* `197` → sequence length (tokens)\n",
    "* `64` → head dimension\n",
    "\n",
    "✅ Each head gets its own slice of Q, and attention is computed **in parallel** across all heads.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 What About K and V?\n",
    "\n",
    "Same steps:\n",
    "\n",
    "```python\n",
    "K = x @ W_k → [B, 197, 768] → [B, 12, 197, 64]\n",
    "V = x @ W_v → [B, 197, 768] → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "Then attention is computed *per head*:\n",
    "\n",
    "```python\n",
    "attn_weights = softmax(Q @ Kᵀ / sqrt(64))  → [B, 12, 197, 197]\n",
    "output = attn_weights @ V  → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "And finally, the heads are **concatenated**:\n",
    "\n",
    "```python\n",
    "output = output.permute(0, 2, 1, 3).reshape(B, 197, 768)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ So to summarize:\n",
    "\n",
    "> **Yes — `Q` is first computed as a single matrix, then reshaped/split into multiple heads** of shape `[B, num_heads, seq_len, head_dim]`.\n",
    "\n",
    "| Stage                | Shape              |\n",
    "| -------------------- | ------------------ |\n",
    "| `x` (input)          | `[B, 197, 768]`    |\n",
    "| `Q` (raw projection) | `[B, 197, 768]`    |\n",
    "| `Q` (split heads)    | `[B, 12, 197, 64]` |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a small PyTorch example showing this reshape and split step for Q, K, V?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744eb32a-5092-4968-8080-70932d3d839d",
   "metadata": {},
   "source": [
    "#### **Relation Between number of heads and Embedding Dimension**\n",
    "\n",
    "\n",
    "\n",
    "✅ Yes — that's exactly right, and very commonly used!\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 In Vision Transformers and LLMs:\n",
    "\n",
    "The **number of heads** is typically chosen such that:\n",
    "\n",
    "$$\n",
    "\\text{embed\\_dim} \\mod \\text{num\\_heads} = 0\n",
    "$$\n",
    "\n",
    "Why? Because each head gets an equal-sized slice of the embedding.\n",
    "For example:\n",
    "\n",
    "| `embed_dim` | `num_heads` | `per-head dim` = `embed_dim / num_heads` |\n",
    "| ----------- | ----------- | ---------------------------------------- |\n",
    "| 768         | 12          | 64                                       |\n",
    "| 1024        | 16          | 64                                       |\n",
    "| 1280        | 16 or 20    | 80 or 64                                 |\n",
    "| 384         | 6           | 64                                       |\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Standard Configurations\n",
    "\n",
    "| ViT Variant | `embed_dim` | `num_heads` | `per-head dim` |\n",
    "| ----------- | ----------- | ----------- | -------------- |\n",
    "| ViT-B/16    | 768         | 12          | 64             |\n",
    "| ViT-L/16    | 1024        | 16          | 64             |\n",
    "| ViT-H/14    | 1280        | 16          | 80             |\n",
    "| BERT-Base   | 768         | 12          | 64             |\n",
    "| GPT-2 Small | 768         | 12          | 64             |\n",
    "| GPT-3       | 12288       | 96          | 128            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why This Design?\n",
    "\n",
    "* **Multiple heads** let the model attend to different parts of the sequence in parallel (diverse perspectives).\n",
    "* Keeping `per-head dim = 64` is a **common practice** because:\n",
    "\n",
    "  * It balances expressiveness and compute cost.\n",
    "  * It's empirically found to work well in many architectures.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ So, Yes:\n",
    "\n",
    "> **If your embedding dimension is 768, then using 12 heads (→ 64 per head) is very common.**\n",
    "\n",
    "Would you like to explore what happens if you change the number of heads (e.g. 8 or 16) while keeping `embed_dim = 768`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8837fe4-b230-4bf7-9ca1-571d4ffb1dd6",
   "metadata": {},
   "source": [
    "#### Size of $Q$ = $x$ and $W_q$\n",
    "\n",
    "\n",
    " `Q = x @ W_q` gives the same shape as `x`, but there's an important **implementation detail** and **decomposition into attention heads** that adds clarity. Let's go step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Yes, Q Has the Same Shape as `x` — Before Splitting into Heads\n",
    "\n",
    "Assume:\n",
    "\n",
    "* `x`: `[B, 197, 768]`\n",
    "* `W_q`: `[768, 768]` (learnable parameter matrix)\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "Q = x @ W_q  → [B, 197, 768]\n",
    "```\n",
    "\n",
    "So yes — initially `Q`, `K`, and `V` **do have the same shape as `x`**, because you're projecting each token (of dimension 768) to a new 768-dimensional query vector (and similarly for keys and values).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Then Comes the Multi-Head Splitting\n",
    "\n",
    "To apply **multi-head attention**, this `[B, 197, 768]` is **reshaped** into multiple attention heads.\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* `num_heads = 12`\n",
    "* Then each head works on:\n",
    "\n",
    "  $$\n",
    "  d_k = \\frac{768}{12} = 64\n",
    "  $$\n",
    "\n",
    "Now, reshape:\n",
    "\n",
    "```python\n",
    "Q → [B, 197, 768] → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* You split the `768` dim into 12 separate heads of `64` dims.\n",
    "* Each head performs self-attention independently.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Do It This Way?\n",
    "\n",
    "* Keeping the projection output dimension = `embed_dim = 768` allows easy concatenation of all heads later.\n",
    "* After attention is computed in each head (producing `[B, 12, 197, 64]`), you **concatenate across heads**:\n",
    "\n",
    "  ```python\n",
    "  output = torch.cat(head_outputs, dim=-1)  # [B, 197, 768]\n",
    "  ```\n",
    "\n",
    "So from start to end:\n",
    "\n",
    "1. `x`: `[B, 197, 768]`\n",
    "2. `Q, K, V`: `[B, 197, 768]` (after matmul)\n",
    "3. Split: `[B, 12, 197, 64]`\n",
    "4. Attention: `[B, 12, 197, 64]`\n",
    "5. Concat: `[B, 197, 768]`\n",
    "6. Final output: `[B, 197, 768]` → passed to next layer or MLP\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Summary Table\n",
    "\n",
    "| Step                        | Shape               |\n",
    "| --------------------------- | ------------------- |\n",
    "| Input `x`                   | `[B, 197, 768]`     |\n",
    "| Q, K, V projections         | `[B, 197, 768]`     |\n",
    "| Reshaped to multi-head      | `[B, 12, 197, 64]`  |\n",
    "| Attention scores (QKᵀ)      | `[B, 12, 197, 197]` |\n",
    "| Attention output (per head) | `[B, 12, 197, 64]`  |\n",
    "| Concatenated heads          | `[B, 197, 768]`     |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0033e00-a2d4-44fc-9425-d0eedabc07eb",
   "metadata": {},
   "source": [
    "Excellent question — this goes straight into the mechanics of **self-attention** in Vision Transformers (ViT). Let's precisely explain what the dimensions of **Q, K, V**, and the **attention table (QKᵀ)** are, in terms of:\n",
    "\n",
    "* **Batch size** `B`\n",
    "* **Number of tokens** `N + 1` (patches + \\[CLS])\n",
    "* **Embedding dimension** `D = embed_dim`\n",
    "* **Number of heads** `H = num_heads`\n",
    "* **Per-head dimension** `d_k = D / H`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Notation Setup (for clarity)\n",
    "\n",
    "| Symbol | Meaning                      | Example Value in ViT-B |\n",
    "| ------ | ---------------------------- | ---------------------- |\n",
    "| `B`    | Batch size                   | e.g., 32               |\n",
    "| `N`    | Number of patches            | e.g., 196              |\n",
    "| `+1`   | CLS token                    | Total tokens = 197     |\n",
    "| `D`    | Embedding dim (`d_model`)    | 768                    |\n",
    "| `H`    | Number of heads              | 12                     |\n",
    "| `d_k`  | Per-head dimension (`D / H`) | 64                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Step-by-Step Breakdown\n",
    "\n",
    "Let’s walk through the shape of everything:\n",
    "\n",
    "### 🔹 Input to Attention Layer:\n",
    "\n",
    "```\n",
    "x: [B, 197, 768]  ← sequence of token embeddings\n",
    "```\n",
    "\n",
    "### 🔹 Linear Projection to Q, K, V\n",
    "\n",
    "Each head has its own weight matrices:\n",
    "\n",
    "```python\n",
    "Q = x @ W_q  → [B, 197, 768]\n",
    "K = x @ W_k  → [B, 197, 768]\n",
    "V = x @ W_v  → [B, 197, 768]\n",
    "```\n",
    "\n",
    "Then reshaped into heads:\n",
    "\n",
    "```\n",
    "Q, K, V → [B, H, 197, d_k]  ← split into heads\n",
    "```\n",
    "\n",
    "So for ViT-B:\n",
    "\n",
    "```\n",
    "Q, K, V → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 QKᵀ: Attention Score Matrix\n",
    "\n",
    "Now we do:\n",
    "\n",
    "```python\n",
    "attention_scores = Q @ K.transpose(-2, -1) / sqrt(d_k)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "Q: [B, 12, 197, 64]\n",
    "K: [B, 12, 197, 64]\n",
    "Kᵀ: [B, 12, 64, 197]\n",
    "\n",
    "QKᵀ → [B, 12, 197, 197]\n",
    "```\n",
    "\n",
    "✅ This gives you a **197×197 attention matrix** per head, per image.\n",
    "\n",
    "* **Rows**: queries → where attention is coming *from* (i.e., each token).\n",
    "* **Cols**: keys → where attention is going *to* (i.e., what is being attended *to*).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Multiply by V\n",
    "\n",
    "You then apply the attention weights to V:\n",
    "\n",
    "```python\n",
    "attn_output = softmax(QKᵀ) @ V  → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "Then all heads are concatenated:\n",
    "\n",
    "```\n",
    "output = concat over heads → [B, 197, 768]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary Table\n",
    "\n",
    "| Item             | Shape               | Example (ViT-B)            |\n",
    "| ---------------- | ------------------- | -------------------------- |\n",
    "| Input tokens     | `[B, 197, 768]`     | 197 = 196 patches + 1 CLS  |\n",
    "| Q, K, V          | `[B, 12, 197, 64]`  | After splitting into heads |\n",
    "| Attention table  | `[B, 12, 197, 197]` | One per head               |\n",
    "| Attention output | `[B, 197, 768]`     | Recombined from all heads  |\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 Comparison with LLMs\n",
    "\n",
    "In LLMs like GPT-4:\n",
    "\n",
    "* Token dim might be `D = 12288`\n",
    "* Sequence length could be 2048+\n",
    "* QKᵀ → `[B, H, 2048, 2048]`\n",
    "  (massive attention tables — very memory intensive)\n",
    "\n",
    "In ViT:\n",
    "\n",
    "* Sequence length is much shorter (e.g. 197)\n",
    "* Same QKᵀ logic applies, but less memory and computation cost\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see how attention maps can be visualized to interpret what the model is attending to (e.g., which patches CLS focuses on)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
