{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7057d2de-18f1-4dff-8f4a-6cd484f7baca",
   "metadata": {},
   "source": [
    "# **1. Vision Transformer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272ce71-802b-4272-8567-c5ee27fdd49e",
   "metadata": {},
   "source": [
    "\n",
    "## **2. PatchEmbedding**\n",
    "Difference between **vision transformers** and **language models**: how images get turned into sequences of tokens.\n",
    "\n",
    "**Goal of PatchEmbedding:**  Turn a **2D image** of shape `[B, 3, 224, 224]` into a **sequence of patch tokens**:\n",
    "\n",
    "```\n",
    "[B, N, D]  ← like `[batch, sequence_length, embedding_dim]`\n",
    "```\n",
    "\n",
    "\n",
    "### **2.1 Image → Patches**\n",
    "\n",
    "You split the image (e.g., 3×224×224) into non-overlapping patches (e.g., 3×16×16). For a 224×224 image with 16×16 patches, you get **(224/16)² = 196 patches**.\n",
    "\n",
    "\n",
    "**Why do we set `embed_dim` if we know `img_size` and `patch_size`?**\n",
    "\n",
    "This is a key conceptual point:\n",
    "\n",
    "* `img_size` and `patch_size` tell you **how many patches** you’ll get:\n",
    "  `n_patches = (img_size // patch_size)²`\n",
    "\n",
    "* But **`embed_dim` is not determined by image or patch size** — it’s a **model design choice**, like hidden size in transformers.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You might have 196 patches (for 224×224 image and 16×16 patches), but you can choose:\n",
    "\n",
    "  * `embed_dim = 768` (like ViT-Base)\n",
    "  * `embed_dim = 384` (smaller model)\n",
    "  * `embed_dim = 1024` (larger model)\n",
    "\n",
    "\n",
    "The choice of `embed_dim = 768` is optional and independent of the fact that a flattened `16×16x3=768` RGB patch has 768 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149b245-afd1-4dde-884d-2e60caa82309",
   "metadata": {},
   "source": [
    "### **2.2 Linear Projection**\n",
    "\n",
    "Each patch is flattened (3×16×16 = 768-dimensional vector), then passed through a **trainable linear layer** (fully connected layer) to map it to a **`D`-dimensional embedding space** (say, D = 768).\n",
    " **Learning starts here**: this linear layer has weights that are learned during training.\n",
    "\n",
    "\n",
    "####  **2.2.1 Patchifying via `Conv2d`**\n",
    "\n",
    "Here’s the trick: instead of manually slicing the image into patches, we use a `Conv2d` to do **both patch extraction and linear projection** in one step.\n",
    "\n",
    "```python\n",
    "self.proj = nn.Conv2d(\n",
    "    in_channels=3,         # RGB channels\n",
    "    out_channels=768,      # embedding dim (D)\n",
    "    kernel_size=16,        # patch size (P)\n",
    "    stride=16              # non-overlapping patches\n",
    ")\n",
    "```\n",
    "\n",
    "What this does:\n",
    "\n",
    "* The kernel slides across the image in 16×16 steps.\n",
    "* For each 16×16×3 patch, it applies a **learned linear projection** into a 768-dimensional vector.\n",
    "* The kernel weights are learnable parameters, initialized internally by PyTorch using something like **Kaiming initialization.**\n",
    "* You don’t set the kernel manually — it’s learned during training.\n",
    "* Each output channel in this Conv2D becomes a dimension in the embedding vector for each patch.\n",
    "* So you get:\n",
    "\n",
    "  ```\n",
    "  Output shape: [B, 768, 14, 14]\n",
    "  ```\n",
    "\n",
    "Why 14x14?\n",
    "\n",
    "* Because:\n",
    "\n",
    "  ```\n",
    "  224 (image size) / 16 (patch size) = 14 patches along each dimension\n",
    "  ```\n",
    "\n",
    "* Each kernel is `3 × 16 × 16` so total number of learnable weights:  = `out_channels × 3 × 16 × 16`\n",
    "---\n",
    "\n",
    "####  **2.2.2 Flatten and reshape**:\n",
    "\n",
    "```python\n",
    "x = x.flatten(2)       # [B, 768, 14*14] → [B, 768, 196]\n",
    "x = x.transpose(1, 2)  # [B, 196, 768]\n",
    "or\n",
    "x = x.permute(0, 2, 1)  # [B, N=W'*H', embed_dim ]\n",
    "\n",
    "```\n",
    "\n",
    "Now you have:\n",
    "\n",
    "* 196 tokens (patches),\n",
    "* each of size 768 (embedding dimension),\n",
    "* just like a sentence of 196 words, each mapped to a 768-dim word embedding.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Why `Conv2d` for projection?**\n",
    "\n",
    "Because:\n",
    "\n",
    "* It mimics the behavior of **flattening + linear projection** of each patch.\n",
    "* But it’s faster and GPU-friendly.\n",
    "* Equivalent to slicing out each 16×16 patch, flattening it into `[768]`, and applying a `Linear(3*16*16, 768)`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Dimensions**\n",
    "\n",
    "| Stage               | Shape                                    |\n",
    "| ------------------- | ---------------------------------------- |\n",
    "| Input Image         | `[B, 3, 224, 224]`                       |\n",
    "| Conv2d Output       | `[B, 768, 14, 14]`                       |\n",
    "| Flatten + Transpose | `[B, 196, 768]`                          |\n",
    "| Output Tokens       | 196 patch tokens per image, each `[768]` |\n",
    "\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988c73c-a78c-459a-b8d2-2919d3ff1c64",
   "metadata": {},
   "source": [
    "```python\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)  # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, embed_dim]\n",
    "        #or\n",
    "        #x = x.permute(0, 2, 1)  # [B, N=W'*H', embed_dim ]\n",
    "\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abb66e-578c-4c55-9a17-0d6ba62a2d17",
   "metadata": {},
   "source": [
    "---\n",
    "* Converts the image into a sequence of **patch tokens**.\n",
    "* Output shape: `[B, N, D]`, where:\n",
    "\n",
    "  * `B` = batch size\n",
    "  * `N = (img_size // patch_size)^2` = number of patches (e.g., 14×14 = 196)\n",
    "  * `D = embed_dim` (e.g., 768)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b59628-2848-4bbe-97fe-15bf30af56ad",
   "metadata": {},
   "source": [
    "## **3. Full Vision Transformer**\n",
    "Its job is to take an image and output a **classification prediction** using a Vision Transformer.\n",
    "\n",
    "### **`MiniViT`**\n",
    "```python\n",
    "class MiniViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_classes=10, depth=6, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(\n",
    "            1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=num_heads, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # [B, N, D]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "        #The fully explicit and dimensionally correct version is:\n",
    "        #x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = x + self.pos_embed[:, :x.size(1)]  # positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # [B, N+1, D]\n",
    "        cls_out = x[:, 0]  # CLS token output or cls_out = x[:, 0, :]\n",
    "        return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9cc7d-1ef0-4e19-95d7-8e20784623c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. [CLS] Token**\n",
    "\n",
    "In Vision Transformers (ViT), you **prepend a learnable `[CLS]` token** embedding to the patch sequence.\n",
    "This special token is used later for **classification**,\n",
    "\n",
    "```python\n",
    "self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `torch.zeros(1, 1, embed_dim)` initializes a zero tensor of shape `[1, 1, D]`.\n",
    "* `nn.Parameter(...)` makes it a **learnable parameter**, meaning it will be updated during training.\n",
    "* Conceptually, it serves as a **summary token** that aggregates information from all patches.\n",
    "* Only **one `[CLS]` token** is stored, shared across all images in all batches.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1 Expanding the `[CLS]` Token at Runtime**\n",
    "\n",
    "During the forward pass, this single token is **replicated per batch** (without copying memory):\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "```\n",
    "\n",
    "**Details:**\n",
    "\n",
    "* `expand()` in PyTorch **does not create new memory copies**.\n",
    "  It creates a **view** on the same underlying parameter.\n",
    "* Thus, although `cls_tokens` appears to be `[B, 1, D]`, it’s still backed by a **single shared parameter** `self.cls_token`.\n",
    "* During **backpropagation**, gradients from all samples update **the same `[CLS]` embedding**, ensuring it remains globally shared.\n",
    "\n",
    "If you had instead written:\n",
    "\n",
    "```python\n",
    "cls_tokens = self.cls_token.repeat(B, 1, 1)  # BAD if you want sharing!\n",
    "```\n",
    "\n",
    "then you would create **`B` separate copies** in memory, each with independent gradients —\n",
    "which is **not** desired in this context.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad25b5d4-20be-432a-829b-8e1f2f4c80d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0232],\n",
      "        [ 1.3677]])\n",
      "tensor([[0.9768, 0.9768, 0.9768],\n",
      "        [1.3677, 1.3677, 1.3677]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 1)\n",
    "print(x)\n",
    "\n",
    "x_expand = x.expand(-1, 3)\n",
    "x_expand[0, 0] = x_expand[0, 0]+1\n",
    "print(x_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0017f14-86fc-4a95-baf1-69a1e80ba8d2",
   "metadata": {},
   "source": [
    "### **4.2 Adding the `[CLS]` Token to the Patch Embeddings**\n",
    "\n",
    "After preparing the `[CLS]` token, it’s **prepended** to the sequence of patch embeddings:\n",
    "\n",
    "```python\n",
    "x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, D]\n",
    "```\n",
    "\n",
    "**Meaning:**\n",
    "\n",
    "* `[CLS]` becomes the **first token** in the transformer’s input sequence.\n",
    "* The transformer then processes `[B, N+1, D]` tokens — one more than the original number of patches.\n",
    "* During attention, the `[CLS]` token **aggregates global information** from all patches.\n",
    "* At the output, the final embedding of `[CLS]` is used for **image classification**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407ca9e-b0e9-41b5-b8e2-4bb30014cb3d",
   "metadata": {},
   "source": [
    "## **5. Positional Embedding (`pos_embed`)**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "#### **5.1. Definition and Shape**\n",
    "\n",
    "`pos_embed` is a **learnable tensor** of shape:\n",
    "\n",
    "```python\n",
    "[1, N+1, embed_dim]\n",
    "```\n",
    "\n",
    "* `N` — number of image patches\n",
    "* `+1` — accounts for the `[CLS]` token\n",
    "* `embed_dim` — embedding dimension (same as patch embeddings)\n",
    "\n",
    "Example:\n",
    "If `N = 196` and `embed_dim = 768`, shape → `[1, 197, 768]`.\n",
    "\n",
    "Each vector in `pos_embed` is a **learnable position encoding vector** whose role is to inject **spatial order** into the Transformer’s input.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2. Why We Need It**\n",
    "\n",
    "Transformers are **permutation invariant** — they do not inherently understand the order of tokens.\n",
    "\n",
    "In images:\n",
    "\n",
    "* Flattened patch embeddings lose all spatial structure.\n",
    "* `pos_embed` restores this by giving **each patch a unique spatial identity** (e.g., “top-left”, “bottom-right”, etc.).\n",
    "\n",
    "This allows the model to reason about *where* each patch came from and how patches relate spatially.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.3. How It’s Used**\n",
    "\n",
    "At input stage:\n",
    "\n",
    "```python\n",
    "x = torch.cat((cls_tokens, patch_embeddings), dim=1)  # [B, N+1, D]\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "* Adds the positional encoding to each token (patch + `[CLS]`).\n",
    "* `self.pos_embed[:, :x.size(1)]` ensures the positional embedding slice matches the actual sequence length (useful for variable image sizes).\n",
    "* Final input shape remains `[B, N+1, D]`.\n",
    "\n",
    "Conceptually, this gives each token a **“GPS tag”**, helping the Transformer know the spatial origin of each embedding.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.4. Does It Let Us Add or Subtract Patches?**\n",
    "\n",
    "Not in a literal geometric sense — positional embeddings:\n",
    "\n",
    "* Don’t modify spatial coordinates like convolutions.\n",
    "* But allow the model to **learn relationships between positions** (e.g., proximity, layout, symmetry).\n",
    "\n",
    "Thus, they help infer spatial relations during training, even though the model operates purely on token sequences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.5. Shared Across All Images**\n",
    "\n",
    "`pos_embed` is a **single learnable parameter** shared across the entire dataset — just like `cls_token`.\n",
    "\n",
    "Every image adds the **same positional embeddings** to its patch tokens:\n",
    "\n",
    "```python\n",
    "final_input = patch_embedding + positional_embedding\n",
    "```\n",
    "\n",
    "* `patch_embedding`: depends on image content (unique per image)\n",
    "* `positional_embedding`: depends on token index (shared across all images)\n",
    "\n",
    "This sharing is crucial:\n",
    "\n",
    "* Position 0 always represents the same spatial region (e.g., top-left).\n",
    "* The model learns consistent **position-aware attention** — e.g., how patch 3 attends to patch 10.\n",
    "\n",
    "If each image had its own positional encoding, the model would lose the concept of fixed spatial meaning per position, resulting in **spatial chaos**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.6. Why Initialize with Zeros**\n",
    "\n",
    "```python\n",
    "self.pos_embed = nn.Parameter(torch.zeros(1, N+1, embed_dim))\n",
    "```\n",
    "\n",
    "Reasons:\n",
    "\n",
    "* **Zero is neutral** — no initial positional bias.\n",
    "* Early in training, the model can rely on patch content and **gradually learn** how to use positional cues.\n",
    "* Random initialization (`torch.randn`) would inject meaningless noise and hinder early learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.7. Dimension Summary**\n",
    "\n",
    "| Symbol            | Meaning                     | Example Value |\n",
    "| :---------------- | :-------------------------- | :------------ |\n",
    "| `B`               | Batch size                  | —             |\n",
    "| `N`               | Number of patches per image | 196           |\n",
    "| `+1`              | Extra token for `[CLS]`     | —             |\n",
    "| `D` / `embed_dim` | Embedding dimension         | 768           |\n",
    "\n",
    "Shapes:\n",
    "\n",
    "* `x`: `[B, N+1, D]`\n",
    "* `pos_embed`: `[1, N+1, D]`\n",
    "\n",
    "Both are added element-wise before entering the Transformer encoder.\n",
    "\n",
    "---\n",
    "\n",
    "In short: **Positional Embedding**: it’s what tells the Transformer **“where each patch came from.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64aac5-01b9-4549-b1c4-0a20f69f60a2",
   "metadata": {},
   "source": [
    "### 5.8 Why  `x = x + self.pos_embed[:, :x.size(1)]` and not `x = x + self.pos_embed`\n",
    "\n",
    "The fully explicit and dimensionally correct version is:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1), :]\n",
    "```\n",
    "\n",
    "\n",
    "This subtle slicing\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "is there **for safety and flexibility**. Let’s unpack exactly why it’s written this way instead of the simpler\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Shape alignment requirement**\n",
    "\n",
    "Before the addition, we have:\n",
    "\n",
    "* `x.shape = [B, N+1, D]`\n",
    "* `self.pos_embed.shape = [1, N+1, D]`\n",
    "\n",
    "For the addition `x + self.pos_embed` to work, these two tensors must have **identical shapes** along dimensions `1` and `2`.\n",
    "\n",
    "If you hardcode:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed\n",
    "```\n",
    "\n",
    "it **only works** when your current image (or patch sequence) has *exactly* the same number of tokens `N+1` as `self.pos_embed` was initialized with.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Handling dynamic sequence lengths**\n",
    "\n",
    "When working with **different image sizes or patch configurations**,\n",
    "`N` (the number of patches) can change — for example:\n",
    "\n",
    "* If you train with 224×224 images (N= (224/16)**2= 196)\n",
    "* But evaluate with 384×384 images (N= (384/16)**2= 576)\n",
    "\n",
    "then:\n",
    "\n",
    "```python\n",
    "x.size(1) = 577\n",
    "self.pos_embed.shape = [1, 197, D]\n",
    "```\n",
    "\n",
    "The direct addition:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed\n",
    "```\n",
    "\n",
    "would raise a **shape mismatch error**.\n",
    "\n",
    "So, by slicing:\n",
    "\n",
    "```python\n",
    "self.pos_embed[:, :x.size(1)] # self.pos_embed is [B,N=577,embed_dim]\n",
    "```\n",
    "\n",
    "✅ This works only when `x.size(1) ≤ self.pos_embed.size(1)`.\n",
    "If `x.size(1)` is greater, you must interpolate the positional embeddings first.\n",
    "\n",
    "you ensure you **take exactly as many positional embeddings** as there are tokens in `x`.\n",
    "This allows the code to remain valid even when token counts differ (for example, if you interpolate `pos_embed` later).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab573003-cb85-4e7c-9903-1728934d0224",
   "metadata": {},
   "source": [
    "### **5.9.When we say that `N` (the number of patches) can change**\n",
    "\n",
    "When we say that `N` (the number of patches) can change — that’s **primarily** about situations like fine-tuning or inference on a **different image size** than what was used for pretraining.\n",
    "\n",
    "\n",
    "#### **Fine-tuning on higher-resolution images**\n",
    "\n",
    "This is the **most common** case.\n",
    "\n",
    "* **Pretraining:**\n",
    "  Vision Transformer (ViT) pretrained on 224×224 images (e.g., ImageNet-1K).\n",
    "  → Number of patches:\n",
    "  $$ N = \\left(\\frac{224}{16}\\right)^2 = 14^2 = 196 $$\n",
    "  (assuming patch size = 16×16)\n",
    "\n",
    "* **Fine-tuning:**\n",
    "  You now fine-tune on 384×384 images for higher accuracy.\n",
    "  →\n",
    "  $$ N = \\left(\\frac{384}{16}\\right)^2 = 24^2 = 576 $$\n",
    "\n",
    "Since the pretrained model has `pos_embed` of shape `[1, 197, D]` (including `[CLS]`), but you now need `[1, 577, D]`, you must **resize/interpolate** the positional embeddings to the new spatial grid.\n",
    "\n",
    "That’s why slicing and interpolation are used:\n",
    "\n",
    "```python\n",
    "self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "self.pos_embed = interpolate_pos_encoding(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Multi-scale training or inference**\n",
    "\n",
    "Sometimes during **training itself**, we vary image resolution:\n",
    "\n",
    "* Used for **data augmentation** or **robustness**.\n",
    "* Example: randomly resize inputs between 224 and 384 during training.\n",
    "\n",
    "In this case, `N` keeps changing per batch, and the model must dynamically adapt to different sequence lengths.\n",
    "Hence, the slicing notation ensures the positional embedding matches the current `x.size(1)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Using different patch sizes**\n",
    "\n",
    "Changing patch size changes `N` too:\n",
    "\n",
    "| Image Size | Patch Size | Number of Patches (`N`) |\n",
    "| ---------- | ---------- | ----------------------- |\n",
    "| 224×224    | 16×16      | 196                     |\n",
    "| 224×224    | 8×8        | 784                     |\n",
    "\n",
    "If you modify the patch size when adapting the model, you must also **adjust or reinitialize** the positional embedding.\n",
    "(Interpolation can still help if spatial layout is preserved.)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Removing or adding special tokens**\n",
    "\n",
    "Some ViT variants modify the token sequence:\n",
    "\n",
    "* Add `[DIST]` tokens (DeiT).\n",
    "* Remove `[CLS]` (for segmentation tasks).\n",
    "* Add positional tokens for regions (e.g., masked patches, bounding boxes).\n",
    "\n",
    "If `N+1` changes to `N+2` or `N`, you must slice the positional embedding accordingly:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Vision–language or multimodal adapters**\n",
    "\n",
    "In models like **CLIP** or **ViT-GPT2 hybrids**, the visual encoder (ViT) may produce features at one resolution, and you might later plug it into another model that expects different token counts or feature map sizes.\n",
    "To adapt, you resize or slice positional embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature extraction or downstream tasks**\n",
    "\n",
    "If you extract ViT features for:\n",
    "\n",
    "* Object detection (ViTDet, DINO)\n",
    "* Segmentation (Segmenter, Mask2Former)\n",
    "* Depth prediction (DPT)\n",
    "\n",
    "then you often feed **larger feature maps** (higher resolution images).\n",
    "So again, positional embeddings are interpolated to match.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Scenario                         | Why `N` Changes          | What We Do                           |\n",
    "| -------------------------------- | ------------------------ | ------------------------------------ |\n",
    "| Fine-tuning at higher resolution | 224 → 384                | Interpolate `pos_embed`              |\n",
    "| Multi-scale training             | Vary size per batch      | Slice dynamically                    |\n",
    "| Change patch size                | 16×16 → 8×8              | Recompute or interpolate `pos_embed` |\n",
    "| Add/remove tokens                | e.g., `[DIST]`, `[MASK]` | Adjust slicing                       |\n",
    "| Multimodal or downstream task    | Different spatial grids  | Resize positional encoding           |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc76a2-1554-4e16-a9bd-4f9d9c59f065",
   "metadata": {},
   "source": [
    "## **6.Transformer**\n",
    "\n",
    "```python\n",
    "self.transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=num_heads,\n",
    "        batch_first=True\n",
    "    ),\n",
    "    num_layers=depth\n",
    ")\n",
    "```\n",
    "\n",
    "### **6.1.Overview**\n",
    "\n",
    "* A standard **Transformer encoder stack** consisting of:\n",
    "\n",
    "  * Multi-Head Self-Attention\n",
    "  * Feedforward layers\n",
    "  * Layer Normalization\n",
    "  * Residual connections\n",
    "* Processes **all tokens** (patches + `[CLS]`)\n",
    "* Shape: `[B, N+1, D] → [B, N+1, D]`\n",
    "\n",
    "---\n",
    "\n",
    "### **6.2.Input to Transformer**\n",
    "\n",
    "Each image is converted into a sequence of tokens:\n",
    "\n",
    "```\n",
    "[CLS]  Patch1  Patch2  ...  PatchN   ← total of N+1 tokens\n",
    "```\n",
    "\n",
    "Then passed through the transformer:\n",
    "\n",
    "```python\n",
    "x = self.transformer(x)  # [B, N+1, D]\n",
    "cls_out = x[:, 0]        # [B, D]\n",
    "# or full version \n",
    "# cls_out = x[:, 0, :]\n",
    "```\n",
    "\n",
    "* **What:** The transformer encodes contextual relationships among all tokens.\n",
    "* **Why:** The `[CLS]` token learns to summarize the entire image representation.\n",
    "\n",
    "---\n",
    "\n",
    "### **6.3.Why Use `nn.TransformerEncoder` Instead of `nn.Transformer`**\n",
    "\n",
    "**`nn.Transformer`** is a *full* encoder–decoder model, originally for sequence-to-sequence tasks such as:\n",
    "\n",
    "* Machine translation\n",
    "* Text summarization\n",
    "* Image captioning\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "transformer = nn.Transformer(\n",
    "    d_model=768,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "This version expects both an **input sequence** (encoder) and a **target sequence** (decoder).\n",
    "\n",
    "---\n",
    "\n",
    "**`nn.TransformerEncoder`** includes only the **encoder** part:\n",
    "\n",
    "```python\n",
    "transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=768, nhead=8),\n",
    "    num_layers=6\n",
    ")\n",
    "```\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Only the **input sequence** needs to be encoded.\n",
    "* No target sequence is required.\n",
    "\n",
    "This matches **Vision Transformers (ViT)**:\n",
    "\n",
    "* Input = patch embeddings + `[CLS]` token\n",
    "* Output = encoded representations\n",
    "* No decoding step is needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **6.4.Query, Key, Value, and Multi-Head Attention**\n",
    "\n",
    "Inside each `TransformerEncoderLayer`, a `MultiheadAttention` module computes **Q (Query)**, **K (Key)**, and **V (Value)** matrices.\n",
    "\n",
    "### **6.5.Self-Attention Computation**\n",
    "\n",
    "Each input token is projected to:\n",
    "\n",
    "* **Q** — what the token is querying\n",
    "* **K** — what the token offers\n",
    "* **V** — the token’s content\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **6.6.Where Q, K, V Come From**\n",
    "\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "```\n",
    "\n",
    "Internally:\n",
    "\n",
    "\n",
    "$$\n",
    "Q = X W^Q, \\quad K = X W^K, \\quad V = X W^V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "* `W^Q`, `W^K`, `W^V` are learnable matrices of shape `(d_model, d_model)`\n",
    "* `d_k = d_model / nhead` (per-head dimension)\n",
    "\n",
    "Inspection:\n",
    "\n",
    "```python\n",
    "print(layer.self_attn.in_proj_weight.shape)\n",
    "# (3 * embed_dim, embed_dim) → (1536, 512)\n",
    "```\n",
    "\n",
    "This stacked weight contains all three projection matrices:\n",
    "first `512` rows = `W^Q`, next `512` = `W^K`, last `512` = `W^V`.\n",
    "\n",
    "---\n",
    "\n",
    " **Shape and Head Splitting**\n",
    "\n",
    "| Symbol | Meaning             | Example (ViT-B) |\n",
    "| ------ | ------------------- | --------------- |\n",
    "| `B`    | Batch size          | 8               |\n",
    "| `N`    | Tokens per image    | 197             |\n",
    "| `D`    | Embedding dimension | 768             |\n",
    "| `H`    | Number of heads     | 12              |\n",
    "| `d_k`  | Per-head dimension  | 64              |\n",
    "\n",
    "Each head receives the full sequence of tokens, but only part of each token’s features.\n",
    "\n",
    "After projection:\n",
    "\n",
    "```\n",
    "Q, K, V: [B, N, D] → reshape → [B, H, N, d_k]\n",
    "```\n",
    "\n",
    "For ViT-B:\n",
    "\n",
    "```\n",
    "Q, K, V → [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "Each head operates independently on all tokens and produces:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) → [B, H, N, d_k]\n",
    "```\n",
    "\n",
    "The outputs from all heads are concatenated:\n",
    "\n",
    "```\n",
    "[B, H, N, d_k] → [B, N, D]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6.7.Attention Score Matrix**\n",
    "\n",
    "Attention scores per head:\n",
    "\n",
    "$$\n",
    "QK^T / \\sqrt{d_k}\n",
    "$$\n",
    "\n",
    "Shapes:\n",
    "\n",
    "```\n",
    "Q: [B, H, N, d_k]\n",
    "K: [B, H, N, d_k]\n",
    "QKᵀ: [B, H, N, N]\n",
    "```\n",
    "\n",
    "* Each head computes a 197×197 attention map.\n",
    "* Rows = queries (tokens attending from)\n",
    "* Columns = keys (tokens attended to)\n",
    "\n",
    "---\n",
    "\n",
    "### **6.8.Single Shared Projection Matrices**\n",
    "\n",
    "Each of `W_q`, `W_k`, `W_v` is a **shared** linear projection across all heads:\n",
    "\n",
    "```plaintext\n",
    "W_q: [embed_dim, embed_dim]  → e.g., [768, 768]\n",
    "```\n",
    "\n",
    "```python\n",
    "Q = x @ W_q   # [B, N, 768]\n",
    "Q = Q.view(B, N, num_heads, head_dim)  # [B, 197, 12, 64]\n",
    "Q = Q.permute(0, 2, 1, 3)              # [B, 12, 197, 64]\n",
    "```\n",
    "\n",
    "Heads are created by splitting the final dimension.\n",
    "\n",
    "| Parameter  | Shared?          | Shape (ViT-B)      |\n",
    "| ---------- | ---------------- | ------------------ |\n",
    "| `W_q`      | Yes              | `[768, 768]`       |\n",
    "| `W_k`      | Yes              | `[768, 768]`       |\n",
    "| `W_v`      | Yes              | `[768, 768]`       |\n",
    "| Per-head Q | Split from total | `[B, 12, 197, 64]` |\n",
    "\n",
    "---\n",
    "\n",
    "### **6.9.Relation Between Heads and Embedding Dimension**\n",
    "\n",
    "Embedding dimension is chosen so that:\n",
    "\n",
    "$\\text{embed\\_dim} \\bmod \\text{num\\_heads} = 0$\n",
    "\n",
    "| Model     | embed_dim | num_heads | per-head dim |\n",
    "| --------- | --------- | --------- | ------------ |\n",
    "| ViT-B/16  | 768       | 12        | 64           |\n",
    "| ViT-L/16  | 1024      | 16        | 64           |\n",
    "| ViT-H/14  | 1280      | 16        | 80           |\n",
    "| BERT-Base | 768       | 12        | 64           |\n",
    "| GPT-3     | 12288     | 96        | 128          |\n",
    "\n",
    "Each head learns to attend to different parts or relationships in the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "**End-to-End Summary**\n",
    "\n",
    "| Stage              | Operation           | Shape               |\n",
    "| ------------------ | ------------------- | ------------------- |\n",
    "| Input              | token embeddings    | `[B, 197, 768]`     |\n",
    "| Q, K, V projection | linear layers       | `[B, 197, 768]`     |\n",
    "| Split into heads   | reshape             | `[B, 12, 197, 64]`  |\n",
    "| Attention weights  | `QKᵀ`               | `[B, 12, 197, 197]` |\n",
    "| Attention outputs  | weighted sum of `V` | `[B, 12, 197, 64]`  |\n",
    "| Concatenate heads  | merge back          | `[B, 197, 768]`     |\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison with LLMs**\n",
    "\n",
    "| Aspect            | ViT-B               | GPT-like LLM             |\n",
    "| ----------------- | ------------------- | ------------------------ |\n",
    "| Token dim         | 768                 | 12,288                   |\n",
    "| Heads             | 12                  | 96                       |\n",
    "| Per-head dim      | 64                  | 128                      |\n",
    "| Sequence length   | 197                 | 2048+                    |\n",
    "| Attention map     | `[B, 12, 197, 197]` | `[B, 96, 2048, 2048]`    |\n",
    "| Q, K, V reduction | None                | None (split, not shrink) |\n",
    "\n",
    "Both architectures compute self-attention the same way; the difference is only in **scale**.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* Each head sees all tokens but focuses on different relationships or spatial patterns.\n",
    "* Splitting attention enables multiple “views” of token interactions in parallel.\n",
    "* The outputs from all heads are recombined to form richer representations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfb3ca-8303-42c3-9282-a703e7ceee5a",
   "metadata": {},
   "source": [
    "\n",
    "## **7. MLP Head**\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "```python\n",
    "self.mlp_head = nn.Sequential(\n",
    "    nn.LayerNorm(embed_dim),\n",
    "    nn.Linear(embed_dim, num_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "In the `forward()` pass:\n",
    "\n",
    "```python\n",
    "x = self.transformer(x)      # [B, N+1, D]\n",
    "cls_out = x[:, 0]            # [B, D=768], or cls_out = x[:, 0, :], extract [CLS] token output \n",
    "return self.mlp_head(cls_out)  # [B, num_classes]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose**\n",
    "\n",
    "* The **MLP head** is a **post-transformer classification layer**.\n",
    "* It operates only on the `[CLS]` token, which summarizes the entire image.\n",
    "* Maps `[B, D]` → `[B, num_classes]` to produce classification logits.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "nn.Linear(embed_dim, num_classes)  # e.g., 768 → 10\n",
    "```\n",
    "\n",
    "If `embed_dim = 768` and `num_classes = 10`,\n",
    "the output shape is `[B, 10]` — one score vector per image, suitable for softmax or cross-entropy loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **Computation Flow**\n",
    "\n",
    "| Step                    | Code      | Shape              | Description                    |\n",
    "| ----------------------- | --------- | ------------------ | ------------------------------ |\n",
    "| Input image             | —         | `[B, 3, 224, 224]` | RGB input                      |\n",
    "| PatchEmbedding          | —         | `[B, 196, 768]`    | One token per 16×16 patch      |\n",
    "| Add `[CLS]` token       | —         | `[B, 197, 768]`    | Prepended learnable token      |\n",
    "| Add positional encoding | —         | `[B, 197, 768]`    | Injects spatial info           |\n",
    "| Transformer encoder     | —         | `[B, 197, 768]`    | Token-wise contextual features |\n",
    "| Select `[CLS]` token    | `x[:, 0]== x[:, 0, :]` | `[B, 768]`         | Global image representation    |\n",
    "| MLP head                | —         | `[B, num_classes]` | Final class logits             |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. ViT Output: The Role of the `[CLS]` Token**\n",
    "\n",
    "The `[CLS]` token represents the **global image descriptor** —\n",
    "it’s trained to aggregate information from all patches via self-attention.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.1. Image Classification**\n",
    "\n",
    "* Use only `[CLS]`: `x[:, 0, :]`\n",
    "* Other patch tokens are ignored.\n",
    "* The `[CLS]` token learns to summarize the entire image.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.2. Semantic Segmentation**\n",
    "\n",
    "Semantic segmentation needs **per-pixel (dense)** predictions, not a single global class.\n",
    "\n",
    "So instead of using the `[CLS]` token, we use **all patch tokens**, reshape them into a 2D spatial grid, and apply a small **decoder head (MLP or Conv)** to get per-pixel class logits.\n",
    "\n",
    "---\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "```python\n",
    "x = self.transformer(x)       # [B, N+1, D]\n",
    "patch_tokens = x[:, 1:, :]    # drop [CLS], keep only patch tokens → [B, N, D]\n",
    "h = w = int(sqrt(N))          # e.g., 14×14 patches for 224×224 image\n",
    "patch_map = patch_tokens.reshape(B, h, w, D).permute(0, 3, 1, 2)  # [B, D, H, W]\n",
    "seg_out = self.seg_head(patch_map)  # [B, num_classes, H, W]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Head Example (simple MLP / Conv decoder)**\n",
    "\n",
    "```python\n",
    "self.seg_head = nn.Sequential(\n",
    "    nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    ")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "* Shape: `[B, num_classes, H, W]`\n",
    "* Each channel corresponds to a semantic class.\n",
    "* Can be upsampled to match the original image resolution (e.g., via `F.interpolate`).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Examples: **Segmenter**, **SETR**, **ViT-SEG**\n",
    "\n",
    "---\n",
    "\n",
    "### **7.3. Object Detection**\n",
    "\n",
    "\n",
    "In ViT-based detectors (e.g., **DETR**, **ViTDet**, **DINO**), the model uses **patch tokens** plus a set of **learnable object queries** to predict bounding boxes and class labels.\n",
    "\n",
    "---\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "```python\n",
    "x = self.transformer(x)          # [B, N+1, D]\n",
    "patch_tokens = x[:, 1:, :]       # [B, N, D]\n",
    "queries = self.query_embed.weight.unsqueeze(0).repeat(B, 1, 1)  # [B, num_queries, D]\n",
    "det_input = torch.cat([queries, patch_tokens], dim=1)           # [B, N + num_queries, D]\n",
    "det_output = self.det_transformer(det_input)                    # [B, num_queries, D]\n",
    "out = self.det_head(det_output)  # → classification + bbox\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Head Example**\n",
    "\n",
    "```python\n",
    "self.det_head = nn.ModuleDict({\n",
    "    \"class\": nn.Linear(embed_dim, num_classes + 1),  # +1 for background\n",
    "    \"bbox\": nn.Linear(embed_dim, 4)                  # [cx, cy, w, h]\n",
    "})\n",
    "```\n",
    "\n",
    "Then in forward:\n",
    "\n",
    "```python\n",
    "class_logits = self.det_head[\"class\"](det_output)  # [B, num_queries, num_classes+1]\n",
    "bbox_pred = self.det_head[\"bbox\"](det_output)      # [B, num_queries, 4]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Output Shapes**\n",
    "\n",
    "| Output         | Shape                             | Meaning                       |\n",
    "| -------------- | --------------------------------- | ----------------------------- |\n",
    "| `class_logits` | `[B, num_queries, num_classes+1]` | Class probabilities per query |\n",
    "| `bbox_pred`    | `[B, num_queries, 4]`             | Bounding box coordinates      |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary for Object Detection**\n",
    "\n",
    "| Stage                  | Operation                             | Shape                         |\n",
    "| ---------------------- | ------------------------------------- | ----------------------------- |\n",
    "| Transformer output     | `[B, N+1, D]`                         | All tokens                    |\n",
    "| Patch tokens + queries | `[B, N + Q, D]`                       | Input to detection head       |\n",
    "| Detection head         | `[B, Q, num_classes+1]` + `[B, Q, 4]` | Class logits + bounding boxes |\n",
    "\n",
    "Examples: **DETR**, **ViTDet**, **DINO**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Task                       | Use `[CLS]` Only? | Use Patch Tokens? | Purpose                            |\n",
    "| -------------------------- | ----------------- | ----------------- | ---------------------------------- |\n",
    "| Image Classification       | ✅ Yes             | ❌ No              | Global summary                     |\n",
    "| Semantic Segmentation      | ❌ No              | ✅ Yes             | Dense pixel predictions            |\n",
    "| Object Detection           | ❌ No              | ✅ Yes             | Localize and classify objects      |\n",
    "| Masked Image Modeling      | ❌ No              | ✅ Yes             | Reconstruct image patches          |\n",
    "| Vision–Language            | ✅ Often           | ✅ Sometimes       | Global alignment + local grounding |\n",
    "| Pose / Keypoint Estimation | ❌ No              | ✅ Yes             | Spatial feature extraction         |\n",
    "\n",
    "---\n",
    "\n",
    " **In summary**\n",
    "\n",
    "The **MLP head** transforms the `[CLS]` output from the Transformer into final class logits.\n",
    "Depending on the downstream task, you may:\n",
    "\n",
    "* Use **only `[CLS]`** (classification, retrieval), or\n",
    "* Use **all patch tokens** (segmentation, detection, self-supervision).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687bd1d-36ed-4e62-8f60-1965d79f25bd",
   "metadata": {},
   "source": [
    "##  **8.Difference Between CNNs** and **Vision Transformers (ViTs)**\n",
    "\n",
    "### 8.1 **Inductive Bias**\n",
    "\n",
    "* **CNNs** have strong built-in inductive biases:\n",
    "\n",
    "  * **Locality**: Convolutions only look at local patches.\n",
    "  * **Translation invariance**: Features are shared across the image.\n",
    "\n",
    "* This helps CNNs **learn well even with small datasets**, but it also **restricts their flexibility** — they \"expect\" local, hierarchical features (edges → textures → objects).\n",
    "\n",
    "* **ViTs** have **less inductive bias**. They treat an image as a sequence of patches and use self-attention to learn relationships.\n",
    "\n",
    "  * This makes ViTs more **data-hungry** but also **more flexible**, since they can learn **global interactions directly** instead of being forced into local convolutional structure.\n",
    "\n",
    "Missing in CNN: the ability to *natively* capture **global dependencies** from the start. CNNs need deeper layers, pooling, or tricks like dilated convolutions to expand receptive fields.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2. **Global Context Modeling**\n",
    "\n",
    "* **CNNs**: Each neuron only sees a small receptive field initially; global context only emerges after stacking many layers.\n",
    "* **ViTs**: Every patch can directly attend to every other patch in the very first layer.\n",
    "\n",
    "  * This is why ViTs can capture **long-range dependencies** (e.g., relation between object parts far apart in an image) much more efficiently.\n",
    "\n",
    "Missing in CNN: **direct patch-to-patch communication** across the entire image.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3. **Scalability & Transfer Learning**\n",
    "\n",
    "* **CNNs**: Scaling up depth and width improves performance, but after a point it saturates. They also don't scale as efficiently to very large datasets.\n",
    "* **ViTs**: Scale extremely well with dataset size. With massive pretraining (e.g., ImageNet-21k, JFT-300M), ViTs outperform CNNs substantially because they can leverage global attention and adapt their representations.\n",
    "\n",
    "Missing in CNN: **scaling laws** that match ViTs. Transformers just \"get better\" with more data and compute.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4. **Flexibility Beyond Vision**\n",
    "\n",
    "* **CNNs** are vision-specific, tailored to spatial hierarchies.\n",
    "* **Transformers** are modality-agnostic (work for NLP, audio, multimodal).\n",
    "\n",
    "  * ViTs benefit from this generality: architectures, pretraining tricks, and transfer learning techniques from NLP can be reused directly.\n",
    "\n",
    "Missing in CNN: **a unified architecture** across different modalities.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary: What CNNs lack that ViTs provide\n",
    "\n",
    "1. **Global attention from the start** (not restricted by local receptive fields).\n",
    "2. **Weaker inductive bias** → more flexible representations.\n",
    "3. **Better scalability with data/compute**.\n",
    "4. **Architectural generality** (can unify vision, language, audio).\n",
    "\n",
    "---\n",
    "#### Example: Detecting a Bicycle (Wheel + Handlebar far apart)\n",
    "\n",
    "Imagine a **32×32 image**, divided into **4×4 patches** (each patch = 8×8 pixels).\n",
    "We want to detect that the **wheel** (bottom left) and the **handlebar** (top right) belong to the same object.\n",
    "\n",
    "---\n",
    "\n",
    "#### **CNN**\n",
    "\n",
    "* A **3×3 convolution kernel** looks at a small local patch.\n",
    "* To connect wheel (bottom-left corner) → handlebar (top-right corner):\n",
    "\n",
    "  * The information must pass through **many layers** of convolutions + pooling.\n",
    "  * Each layer increases the receptive field slowly:\n",
    "\n",
    "    * 1st layer: sees 3×3 pixels\n",
    "    * 2nd layer: maybe 7×7 pixels\n",
    "    * After \\~5–6 layers, the receptive field finally covers the whole image.\n",
    "\n",
    " Problem: The CNN only *learns the relation between wheel & handlebar indirectly* through deep stacking. It’s biased toward local patterns (edges, corners, textures).\n",
    "\n",
    "---\n",
    "\n",
    "#### **ViT**\n",
    "\n",
    "* Split the image into **16 patches** (4×4).\n",
    "* The first **self-attention layer** compares *every patch with every other patch*.\n",
    "* That means **wheel patch** can directly attend to the **handlebar patch**, even though they are far apart.\n",
    "* Attention weight example:\n",
    "\n",
    "  * Similarity between wheel patch and handlebar patch = **0.82** (high)\n",
    "  * Similarity between wheel patch and background patch = **0.05** (low)\n",
    "\n",
    " Result: ViT **immediately learns a global relationship**: \"wheel + handlebar = likely bicycle\" — no need to wait for many layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Numerical Contrast\n",
    "\n",
    "* CNN: relation strength grows only after multiple layers, e.g.\n",
    "\n",
    "  * Layer 1 relation strength (wheel → handlebar): \\~0.0\n",
    "  * Layer 3: \\~0.2\n",
    "  * Layer 6: \\~0.7\n",
    "* ViT:\n",
    "\n",
    "  * Layer 1 relation strength (wheel → handlebar): \\~0.82 already.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**\n",
    "CNNs start local → slowly become global.\n",
    "ViTs are **global from the start**, which makes them better at modeling objects whose parts are **spatially distant**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
