{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf981a9-99ee-410c-9bb2-92ffeb0735f2",
   "metadata": {},
   "source": [
    "###  **Layer Normalization (LN)**\n",
    "\n",
    "**Where?**  \n",
    "- Used in **RNNs**, **Transformers**, **NLP models**, and **fully connected networks**\n",
    "\n",
    "**How?**  \n",
    "- LN normalizes **across features in a single sample**, **not across the batch**\n",
    "- Formula:  \n",
    "  $\n",
    "  \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $  \n",
    "  where:\n",
    "  - $ \\mu $, $ \\sigma $: mean and std over the features of one sample\n",
    "\n",
    "**Key Properties:**  \n",
    "- **Independent of batch size**\n",
    "- Always uses current sample statistics\n",
    "- Learnable **scale ($ \\gamma $)** and **shift ($ \\beta $)**\n",
    "\n",
    "**Pros:**\n",
    "- Works well with variable batch sizes\n",
    "- Stable for sequential models (RNNs, Transformers)\n",
    "\n",
    "**Cons:**\n",
    "- May be slightly slower convergence compared to BN in CNNs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865c360-a481-4ec3-a293-b1b9e0a6c338",
   "metadata": {},
   "source": [
    "## **Numeric example difference between  Batch Normalization (BN) and Layer Normalization (LN)**\n",
    "\n",
    "We’ll use your batch of size 3, each with 4 features:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "5 & 4 & 7 & 2 \\\\\n",
    "1 & 6 & 2 & 3 \\\\\n",
    "4 & 8 & 1 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Batch Normalization (BN)**\n",
    "\n",
    "BN normalizes **per feature across the batch** (rows).\n",
    "That means for each column (feature), we compute:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{N} \\sum_{i=1}^N x_{ij}, \n",
    "\\quad\n",
    "\\sigma_j^2 = \\frac{1}{N} \\sum_{i=1}^N (x_{ij} - \\mu_j)^2\n",
    "$$\n",
    "\n",
    "where $N=3$ (batch size).\n",
    "\n",
    "### Example:\n",
    "\n",
    "* Feature 1 (col 0): \\[5, 1, 4] → mean = (5+1+4)/3 = 3.33, var = 2.89\n",
    "* Feature 2 (col 1): \\[4, 6, 8] → mean = 6, var = 2.67\n",
    "* Feature 3 (col 2): \\[7, 2, 1] → mean = 3.33, var = 6.22\n",
    "* Feature 4 (col 3): \\[2, 3, 9] → mean = 4.67, var = 8.22\n",
    "\n",
    "Then normalize each entry:\n",
    "\n",
    "$$\n",
    "x'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Layer Normalization (LN)**\n",
    "\n",
    "LN normalizes **per sample across all features** (row).\n",
    "That means for each row, compute:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\frac{1}{F} \\sum_{j=1}^F x_{ij}, \n",
    "\\quad\n",
    "\\sigma_i^2 = \\frac{1}{F} \\sum_{j=1}^F (x_{ij} - \\mu_i)^2\n",
    "$$\n",
    "\n",
    "where $F=4$ (features).\n",
    "\n",
    "### Example:\n",
    "\n",
    "* Row 1: \\[5, 4, 7, 2] → mean = 4.5, var = 3.25\n",
    "* Row 2: \\[1, 6, 2, 3] → mean = 3.0, var = 3.5\n",
    "* Row 3: \\[4, 8, 1, 9] → mean = 5.5, var = 10.25\n",
    "\n",
    "Then normalize each entry:\n",
    "\n",
    "$$\n",
    "x'_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code (Numeric Example)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[5, 4, 7, 2],\n",
    "              [1, 6, 2, 3],\n",
    "              [4, 8, 1, 9]], dtype=float)\n",
    "\n",
    "# Batch Normalization (per feature across batch)\n",
    "mu_batch = X.mean(axis=0)\n",
    "var_batch = X.var(axis=0)\n",
    "bn = (X - mu_batch) / np.sqrt(var_batch + 1e-5)\n",
    "\n",
    "# Layer Normalization (per sample across features)\n",
    "mu_layer = X.mean(axis=1, keepdims=True)\n",
    "var_layer = X.var(axis=1, keepdims=True)\n",
    "ln = (X - mu_layer) / np.sqrt(var_layer + 1e-5)\n",
    "\n",
    "print(\"Original X:\\n\", X)\n",
    "print(\"\\nBatchNorm (per feature across batch):\\n\", np.round(bn, 3))\n",
    "print(\"\\nLayerNorm (per row across features):\\n\", np.round(ln, 3))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Output (rounded)\n",
    "\n",
    "```\n",
    "Original X:\n",
    " [[5. 4. 7. 2.]\n",
    "  [1. 6. 2. 3.]\n",
    "  [4. 8. 1. 9.]]\n",
    "\n",
    "BatchNorm:\n",
    " [[ 0.98 -1.22  1.49 -0.93]\n",
    "  [-1.37  0.    -0.53 -0.58]\n",
    "  [ 0.39  1.22 -0.96  1.52]]\n",
    "\n",
    "LayerNorm:\n",
    " [[ 0.28 -0.09  1.38 -1.57]\n",
    "  [-1.07  1.6  -0.53  0.0 ]\n",
    "  [-0.47  0.78 -1.4   1.09]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " **Key difference**:\n",
    "\n",
    "* **BN** normalizes **column-wise (feature-wise across batch)** → all rows share the same feature stats.\n",
    "* **LN** normalizes **row-wise (within each sample)** → each row is independent of the batch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630547ab-d6c0-4843-be62-1ae130f46e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X:\n",
      " tensor([[5., 4., 7., 2.],\n",
      "        [1., 6., 2., 3.],\n",
      "        [4., 8., 1., 9.]])\n",
      "\n",
      "BatchNorm output:\n",
      " tensor([[ 0.9810, -1.2250,  1.3970, -0.8630],\n",
      "        [-1.3730, -0.0000, -0.5080, -0.5390],\n",
      "        [ 0.3920,  1.2250, -0.8890,  1.4020]])\n",
      "\n",
      "LayerNorm output:\n",
      " tensor([[ 0.2770, -0.2770,  1.3870, -1.3870],\n",
      "        [-1.0690,  1.6040, -0.5350,  0.0000],\n",
      "        [-0.4690,  0.7810, -1.4060,  1.0930]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input batch: 3 samples, 4 features each\n",
    "X = torch.tensor([[5., 4., 7., 2.],\n",
    "                  [1., 6., 2., 3.],\n",
    "                  [4., 8., 1., 9.]])\n",
    "\n",
    "# BatchNorm1d: normalizes over batch dimension (N) for each feature\n",
    "bn = nn.BatchNorm1d(num_features=4, affine=False, track_running_stats=False)\n",
    "\n",
    "# LayerNorm: normalizes per sample (over features)\n",
    "ln = nn.LayerNorm(normalized_shape=4, elementwise_affine=False)\n",
    "\n",
    "print(\"Original X:\\n\", X)\n",
    "\n",
    "print(\"\\nBatchNorm output:\\n\", torch.round(bn(X) * 1000) / 1000)  # rounded\n",
    "print(\"\\nLayerNorm output:\\n\", torch.round(ln(X) * 1000) / 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
