{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dbc7a7-0bec-4d89-aeaa-8fa1cae861a2",
   "metadata": {},
   "source": [
    "##  **Group Normalization (GroupNorm)**\n",
    "\n",
    "* **How it works**: Divides channels into groups, then normalizes within each group.\n",
    "  Example: 32 channels split into 8 groups of 4 → normalize per group.\n",
    "* **Formula**: Similar to BN, but mean/var are computed per group instead of batch.\n",
    "* **Best for**:\n",
    "\n",
    "  * CNNs when batch size is **small** (e.g., segmentation tasks with high-res images).\n",
    "  * Often used in detection/segmentation networks (Mask R-CNN, etc.).\n",
    "* **Strengths**:\n",
    "\n",
    "  * Doesn’t depend on batch size.\n",
    "  * Bridges the gap between BN and LN.\n",
    "* **Weaknesses**:\n",
    "\n",
    "  * Slightly slower than BN.\n",
    "\n",
    "**Usage in PyTorch**:\n",
    "`nn.GroupNorm(num_groups, num_channels)`\n",
    "Example: `nn.GroupNorm(32, 128)` → 128 channels divided into 32 groups.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0fb92-75c6-4a68-a2a8-e6242bc180bd",
   "metadata": {},
   "source": [
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "5 & 4 & 7 & 2\\\\\n",
    "1 & 6 & 2 & 3\\\\\n",
    "4 & 8 & 1 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We’ll use **num\\_groups = 2** (so each sample’s 4 features are split into 2 groups of size 2):\n",
    "\n",
    "* Group 1: features $[0,1]$\n",
    "* Group 2: features $[2,3]$\n",
    "\n",
    "GroupNorm normalizes **per sample, per group**:\n",
    "\n",
    "$$\n",
    "\\mu_{i,g}=\\frac{1}{|G|}\\sum_{c\\in G}x_{i,c},\\quad\n",
    "\\sigma^2_{i,g}=\\frac{1}{|G|}\\sum_{c\\in G}(x_{i,c}-\\mu_{i,g})^2,\\quad\n",
    "\\hat x_{i,c}=\\frac{x_{i,c}-\\mu_{i,g}}{\\sqrt{\\sigma^2_{i,g}+\\epsilon}}\n",
    "$$\n",
    "\n",
    "### Hand calculation (ε≈0)\n",
    "\n",
    "**Row 1:** $[5,4,7,2]$\n",
    "\n",
    "* G1 $[5,4]$: mean $4.5$, var $0.25$ → normalized $[+1,-1]$\n",
    "* G2 $[7,2]$: mean $4.5$, var $6.25$ → normalized $[+1,-1]$\n",
    "  → Row1: $[1,-1,1,-1]$\n",
    "\n",
    "**Row 2:** $[1,6,2,3]$\n",
    "\n",
    "* G1 $[1,6]$: mean $3.5$, var $6.25$ → $[-1,+1]$\n",
    "* G2 $[2,3]$: mean $2.5$, var $0.25$ → $[-1,+1]$\n",
    "  → Row2: $[-1,1,-1,1]$\n",
    "\n",
    "**Row 3:** $[4,8,1,9]$\n",
    "\n",
    "* G1 $[4,8]$: mean $6$, var $4$ → $[-1,+1]$\n",
    "* G2 $[1,9]$: mean $5$, var $16$ → $[-1,+1]$\n",
    "  → Row3: $[-1,1,-1,1]$\n",
    "\n",
    "**Result (no affine $\\gamma,\\beta$):**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 1 & -1 &  1 & -1\\\\\n",
    "-1 &  1 & -1 &  1\\\\\n",
    "-1 &  1 & -1 &  1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> With groups of size 2, each pair always normalizes to $\\{-1,+1\\}$ (up to tiny ε).\n",
    "\n",
    "---\n",
    "\n",
    "## PyTorch code (matches the math)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# (N, C) batch with 3 samples and 4 features/channels\n",
    "X = torch.tensor([[5., 4., 7., 2.],\n",
    "                  [1., 6., 2., 3.],\n",
    "                  [4., 8., 1., 9.]])\n",
    "\n",
    "# GroupNorm expects (N, C, *). We'll add a dummy spatial dim and remove it after.\n",
    "gn = nn.GroupNorm(num_groups=2, num_channels=4, affine=False)  # no gamma/beta to see pure norm\n",
    "\n",
    "# Add dummy length dimension, apply GN, then squeeze back\n",
    "X_gn = gn(X.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "print(\"Original X:\\n\", X)\n",
    "print(\"\\nGroupNorm (G=2, no affine):\\n\", torch.round(X_gn * 1000) / 1000)\n",
    "```\n",
    "\n",
    "### (Optional) With learnable γ, β\n",
    "\n",
    "```python\n",
    "gn_affine = nn.GroupNorm(num_groups=2, num_channels=4, affine=True)\n",
    "with torch.no_grad():\n",
    "    gn_affine.weight[:] = torch.tensor([1.0, 1.0, 2.0, 0.5])  # γ per channel\n",
    "    gn_affine.bias[:]   = torch.tensor([0.0, 0.0, 1.0, -1.0]) # β per channel\n",
    "\n",
    "X_gn_affine = gn_affine(X.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"\\nGroupNorm (G=2, with affine γ,β):\\n\", torch.round(X_gn_affine * 1000) / 1000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "* **num\\_groups=1** ⇒ LayerNorm over features (per sample).\n",
    "* **num\\_groups=C (here 4)** is **not** InstanceNorm; IN normalizes per channel over *spatial* dims. With vectors (no spatial extent), IN is ill-defined (zero variance). Stick to GroupNorm for vector features or add spatial dims for conv features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
