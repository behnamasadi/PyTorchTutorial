{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf8febd-f4ba-42ab-8331-fefc859aae4f",
   "metadata": {},
   "source": [
    "#  Normalization Methods in Deep Learning\n",
    "\n",
    "| **Norm Type**                                           | **Stats Computed Over**                                       | **Learned Params**   | **Best Use Cases**                                  | **Pros**                                                                                                         | **Cons / Limitations**                                                                                                               |\n",
    "| ------------------------------------------------------- | ------------------------------------------------------------- | -------------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **BatchNorm (BN)**                                      | Across **batch + spatial dims** (for each feature channel)    | γ (scale), β (shift) | CNNs with **moderate-to-large batch sizes**         | - Stabilizes training<br>- Allows higher LR<br>- Regularizes (adds noise)<br>- Default for ResNets, EfficientNet | - Needs large batch size<br>- Bad for variable-length sequences<br>- Train/test mismatch<br>- Extra overhead in distributed training |\n",
    "| **LayerNorm (LN)**                                      | Across **features of a single sample** (independent of batch) | γ, β                 | NLP (Transformers), RNNs, Vision Transformers (ViT) | - Works with batch size = 1<br>- No train/test mismatch<br>- Standard in Transformers                            | - Less effective in CNNs<br>- Sometimes slower (per-sample calc)                                                                     |\n",
    "| **InstanceNorm (IN)**                                   | Across **spatial dims per channel per sample**                | γ, β                 | Style transfer, GANs (image generation)             | - Invariant to global contrast/illumination<br>- Works for per-instance normalization                            | - Removes global contrast info (may hurt classification)<br>- Less effective for recognition tasks                                   |\n",
    "| **GroupNorm (GN)**                                      | Across **groups of channels within a sample**                 | γ, β                 | CNNs with **small batch sizes**                     | - Works with any batch size<br>- Stable for small-batch/medical imaging<br>- No train/test mismatch              | - Requires tuning of group count<br>- Not as fast as BN with big batches                                                             |\n",
    "| **LayerScale / WeightNorm / RMSNorm (modern variants)** | Varies (weights or RMS of activations)                        | Usually scale-only   | Transformers, lightweight models                    | - Even simpler than LN<br>- Lower overhead                                                                       | - Less studied, newer, not as universal                                                                                              |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dde850-652f-468d-833d-e1997038eb47",
   "metadata": {},
   "source": [
    "##  Which is more common today?\n",
    "\n",
    "* **Vision (CNNs):**\n",
    "\n",
    "  * BatchNorm still very common (ResNet, EfficientNet).\n",
    "  * GroupNorm is popular when batch size is small (detection, segmentation).\n",
    "* **NLP / Transformers:**\n",
    "\n",
    "  * LayerNorm is the **default choice** (BERT, GPT, ViT, etc.).\n",
    "* **Generative / Style Transfer:**\n",
    "\n",
    "  * InstanceNorm is often used (CycleGAN, style transfer networks).\n",
    "* **New research directions:**\n",
    "\n",
    "  * Some models avoid normalization entirely (e.g., *Normalizer-Free ResNets*, some ViT variants use RMSNorm).\n",
    "\n",
    "---\n",
    "\n",
    "##  Rule of Thumb\n",
    "\n",
    "* **If CNN + large batch** → BatchNorm.\n",
    "* **If CNN + small batch (like segmentation)** → GroupNorm.\n",
    "* **If Transformer / NLP / sequence** → LayerNorm.\n",
    "* **If Style transfer / image generation** → InstanceNorm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
