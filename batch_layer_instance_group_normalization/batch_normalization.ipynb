{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e60d011-370f-4b3c-b5fa-a243b1c349ed",
   "metadata": {},
   "source": [
    "##  Batch Normalization (BatchNorm)\n",
    "\n",
    "Batch Normalization is a layer that:\n",
    "\n",
    "1. **Normalizes** activations (zero mean, unit variance) **within each batch**\n",
    "2. Helps **speed up training**, reduce **internal covariate shift**, and **stabilize learning**\n",
    "\n",
    "It’s commonly used **after a linear or conv layer**, before activation (like ReLU).\n",
    "\n",
    "---\n",
    "\n",
    "### **What BatchNorm Does**\n",
    "\n",
    "For each mini-batch during training, BatchNorm:\n",
    "\n",
    "1. **Normalizes activations** so that they have zero mean and unit variance:\n",
    "\n",
    "   $$\n",
    "   \\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   where $\\mu_{\\text{batch}}$ and $\\sigma_{\\text{batch}}^2$ are mean and variance computed over the current batch.\n",
    "\n",
    "2. **Learns two trainable parameters** per feature:\n",
    "\n",
    "   * **Scale ($\\gamma$)**: allows the network to stretch/compress normalized activations.\n",
    "   * **Shift ($\\beta$)**: allows the network to shift normalized activations.\n",
    "\n",
    "   So the final output is:\n",
    "\n",
    "   $$\n",
    "   y = \\gamma \\hat{x} + \\beta\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Learned**\n",
    "\n",
    "* **Not the mean and variance** (those are computed dynamically during training).\n",
    "* Instead, the learnable parameters are:\n",
    "\n",
    "  * $\\gamma$ (scaling factor)\n",
    "  * $\\beta$ (shifting factor)\n",
    "\n",
    "These parameters allow BatchNorm to preserve the representational power of the network (otherwise normalization would restrict activations too much).\n",
    "\n",
    "During inference, instead of using the batch mean/variance, **running averages** (moving averages of mean and variance accumulated during training) are used.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **How γ (scale) and β (shift) are learned**\n",
    "\n",
    "* They are **regular trainable parameters** of the network, just like weights and biases in Linear/Conv layers.\n",
    "* They start with some initialization (typically $\\gamma=1, \\beta=0$).\n",
    "* During backpropagation:\n",
    "\n",
    "  * Gradients of the loss with respect to $\\gamma$ and $\\beta$ are computed.\n",
    "  * The optimizer (SGD, Adam, etc.) updates them, exactly the same way it updates weights.\n",
    "\n",
    "So the network \"decides\" the optimal scaling and shifting for each feature/channel by minimizing the training loss.\n",
    "\n",
    "---\n",
    "\n",
    "###  **How they are used**\n",
    "\n",
    "After normalizing each activation $x$ with batch statistics:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "BatchNorm outputs:\n",
    "\n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "$$\n",
    "\n",
    "* $\\gamma$ stretches or compresses the normalized activation distribution.\n",
    "* $\\beta$ shifts it up or down.\n",
    "\n",
    " If the network wants the normalized activations to be centered at 5 instead of 0, it will learn a large $\\beta$.\n",
    " If it wants the variance bigger/smaller than 1, it adjusts $\\gamma$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Are they used in evaluation (inference)?**\n",
    "\n",
    "Yes \n",
    "\n",
    "But there’s a key difference between training and evaluation:\n",
    "\n",
    "* **Training mode**:\n",
    "\n",
    "  * Normalization uses the **current batch’s mean & variance**.\n",
    "  * Then applies $\\gamma$ and $\\beta$.\n",
    "\n",
    "* **Evaluation (inference) mode**:\n",
    "\n",
    "  * Normalization uses the **running (moving average) mean & variance** accumulated during training.\n",
    "  * Then applies the same learned $\\gamma$ and $\\beta$.\n",
    "\n",
    "So in inference you do **not depend on the current batch**, but you still apply the learned scale and shift:\n",
    "\n",
    "$$\n",
    "y = \\gamma \\cdot \\frac{x - \\mu_{\\text{running}}}{\\sqrt{\\sigma_{\\text{running}}^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  **Why keep γ and β at inference?**\n",
    "\n",
    "Without $\\gamma,\\beta$, all features would always be locked to mean=0 and variance=1.\n",
    "This would restrict the representational power of the network.\n",
    "\n",
    "$\\gamma$ and $\\beta$ restore flexibility while keeping training stable.\n",
    "Think of them as a **\"learned reparametrization\"** step.\n",
    "\n",
    "---\n",
    "\n",
    "Excellent — this is the *core confusion* about BatchNorm, and it’s worth being crystal clear.\n",
    "\n",
    "You’re absolutely right:\n",
    "\n",
    "* Step 1: BatchNorm **forces each batch of activations to mean 0 and std 1**.\n",
    "* Step 2: It immediately **undoes this** with $\\gamma$ (scale) and $\\beta$ (shift).\n",
    "\n",
    "So why bother? \n",
    "\n",
    "---\n",
    "\n",
    "###  The Key Idea\n",
    "\n",
    "We don’t *want* every layer’s output to always be exactly mean=0, std=1.\n",
    "We just want the optimization to be **stable** and not blow up due to wildly different scales across layers.\n",
    "\n",
    "So the logic is:\n",
    "\n",
    "1. **Normalize first** → remove crazy fluctuations in mean/variance between batches.\n",
    "\n",
    "   * This makes gradients more stable.\n",
    "   * Prevents exploding/vanishing activations.\n",
    "   * Allows higher learning rates.\n",
    "\n",
    "2. **Then reintroduce flexibility with $\\gamma$ and $\\beta$** → let the network **choose** the best scale and offset for each feature if pure normalization is too restrictive.\n",
    "\n",
    "   * Example: maybe ReLU works best if the distribution is shifted positive.\n",
    "   * Or maybe a feature is only useful if it’s amplified.\n",
    "\n",
    "So:\n",
    "\n",
    "* The normalization step ensures **training stability**.\n",
    "* The $\\gamma,\\beta$ step ensures **representational power is not lost**.\n",
    "\n",
    "---\n",
    "\n",
    "###  What Do We Actually Learn?\n",
    "\n",
    "We learn **the \"preferred\" mean and variance of activations** for each feature.\n",
    "\n",
    "* Without $\\gamma,\\beta$, every feature is always constrained to mean=0, var=1 → too rigid.\n",
    "* With $\\gamma,\\beta$, the network can move features to **where they’re most useful**.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "$$\n",
    "\n",
    "* $\\gamma$ tells us how “wide” the distribution should be.\n",
    "* $\\beta$ tells us where the center of the distribution should be.\n",
    "\n",
    "The network learns these values automatically to minimize loss.\n",
    "\n",
    "---\n",
    "\n",
    "###  Analogy\n",
    "\n",
    "Think of normalization like **pressing reset**: every layer gets a clean, well-behaved distribution (mean 0, var 1).\n",
    "But sometimes the model actually wants “not centered at 0” — e.g., a ReLU neuron benefits if most values are slightly positive.\n",
    "\n",
    "So $\\beta$ moves the “reset” center.\n",
    "And $\\gamma$ lets the feature spread out more or less.\n",
    "\n",
    "---\n",
    "\n",
    "###  Example\n",
    "\n",
    "Imagine after normalization we always get values roughly in $[-1, 1]$.\n",
    "\n",
    "* If the next layer’s ReLU kills all negatives, half the information is wasted.\n",
    "* But if $\\beta=+1$, the distribution shifts to $[0, 2]$, and now *everything survives ReLU*.\n",
    "* If a feature is weak, $\\gamma>1$ amplifies it.\n",
    "* If a feature is noisy, $\\gamma<1$ compresses it.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c219fb-4b06-44db-a6ee-b4aa008dda06",
   "metadata": {},
   "source": [
    "### Different Behavior in Train/Test is BatchNorm\n",
    "\n",
    "```python\n",
    "model.train()  # training mode (BN uses batch stats, Dropout active)\n",
    "...\n",
    "model.eval()   # inference mode (BN uses running averages, Dropout disabled)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de8a42-b5ce-46b0-8e4a-25d42d713218",
   "metadata": {},
   "source": [
    "## tiny BatchNorm example\n",
    "\n",
    "### Step 1: Raw activations (from previous layer)\n",
    "\n",
    "$$\n",
    "x = [1, 2, 3, 4]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Normalize (mean=0, std=1)\n",
    "\n",
    "* Mean:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1+2+3+4}{4} = 2.5\n",
    "$$\n",
    "\n",
    "* Variance:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25\n",
    "$$\n",
    "\n",
    "* Std:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{1.25} \\approx 1.118\n",
    "$$\n",
    "\n",
    "Now normalize each:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\left[ \\frac{1-2.5}{1.118}, \\frac{2-2.5}{1.118}, \\frac{3-2.5}{1.118}, \\frac{4-2.5}{1.118} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} \\approx [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "Now mean = 0, std = 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Apply learned γ (scale) and β (shift)\n",
    "\n",
    "Suppose the network has learned:\n",
    "\n",
    "$$\n",
    "\\gamma = 2, \\quad \\beta = 3\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = 2 \\cdot [-1.34, -0.45, 0.45, 1.34] + 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "y \\approx [0.32, 2.10, 3.90, 5.68]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Interpretation\n",
    "\n",
    "* Normalization gave us a **stable baseline** distribution (mean=0, std=1).\n",
    "* But the network “decided” that this feature works best if it is **shifted upward** (β=3) and **stretched wider** (γ=2).\n",
    "* Now the values are centered around \\~3 with a bigger spread.\n",
    "\n",
    "This is what the network *learns*: the “best” mean and variance for each feature, while enjoying the stability of normalization.\n",
    "\n",
    "---\n",
    " **Summary of the toy example**:\n",
    "\n",
    "* Start: $[1,2,3,4]$\n",
    "* Normalize: $[-1.34, -0.45, 0.45, 1.34]$\n",
    "* Apply γ=2, β=3 → $[0.32, 2.10, 3.90, 5.68]$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ed02e-2d38-46a1-8ad0-67ba463db19d",
   "metadata": {},
   "source": [
    "## When to Use BatchNorm\n",
    "\n",
    "BatchNorm is useful when:\n",
    "\n",
    "* You have **deep feedforward networks** (MLPs, CNNs) that are hard to train.\n",
    "* You want **faster convergence** with higher learning rates.\n",
    "* You want **regularization** (BN adds some noise due to batch statistics, reducing overfitting).\n",
    "* You’re working with **image models (CNNs)** — historically it was a game-changer (ResNet, Inception, etc.).\n",
    "\n",
    "It’s less critical in:\n",
    "\n",
    "* Very **shallow networks** (where activations don’t blow up/vanish).\n",
    "* Very **small batch sizes** (BN becomes unstable because mean/variance estimates are noisy).\n",
    "\n",
    "---\n",
    "\n",
    "##  Drawbacks of BatchNorm\n",
    "\n",
    "1. **Dependency on batch size**\n",
    "\n",
    "   * If the batch is too small (say 1–4), the estimated mean/variance is noisy → training unstable.\n",
    "   * In some domains (NLP, RL), we often use small batches, so BN struggles.\n",
    "\n",
    "2. **Training vs inference mismatch**\n",
    "\n",
    "   * During training → batch mean/variance.\n",
    "   * During inference → running averages.\n",
    "   * If not estimated well, performance can drop.\n",
    "\n",
    "3. **Extra complexity**\n",
    "\n",
    "   * Slight computational overhead.\n",
    "   * Harder to use in some architectures (RNNs, transformers with variable-length sequences).\n",
    "\n",
    "4. **Not always portable**\n",
    "\n",
    "   * In distributed training, synchronizing batch stats across GPUs can be tricky.\n",
    "\n",
    "---\n",
    "\n",
    "## Is it still common and popular?\n",
    "\n",
    "* **Yes, but… with competition.**\n",
    "* BatchNorm is still extremely common in **CNNs for vision** (e.g., ResNet, EfficientNet still use BN).\n",
    "* But in **NLP and Transformers**, **LayerNorm** became the standard (BN doesn’t work well with variable-length sequences and small batches).\n",
    "* In **GANs**, people often prefer **InstanceNorm** or **GroupNorm**.\n",
    "* In very **small batch regimes**, **GroupNorm** or **LayerNorm** is more stable.\n",
    "\n",
    "### Trends:\n",
    "\n",
    "* **2015–2018**: BatchNorm was revolutionary → default in almost every model.\n",
    "* **2019–2023**: Alternatives (LayerNorm, GroupNorm) became popular depending on the architecture.\n",
    "* **Now (2025)**:\n",
    "\n",
    "  * **CNNs → BatchNorm is still default**.\n",
    "  * **Transformers (vision & NLP) → LayerNorm dominates**.\n",
    "  * **Special cases (GANs, style transfer, medical imaging with small batches) → InstanceNorm / GroupNorm**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **When to use**: CNNs, medium/large batch training, deep models that need stable training.\n",
    "* **Drawbacks**: Needs sufficiently large batches, mismatch train/test, not great for sequential/transformer models.\n",
    "* **Popularity**: Still very popular in CNNs, but LayerNorm has taken over in Transformers, and GroupNorm/InstanceNorm are preferred in small-batch or special cases.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
