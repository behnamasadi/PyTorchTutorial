{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d9f6fd-ceb8-48e3-8eaf-30f66dfc18d2",
   "metadata": {},
   "source": [
    "Here's a **comprehensive checklist** of techniques to maximize **GPU utilization and training throughput in PyTorch**, categorized by priority and **estimated impact**.\n",
    "\n",
    "---\n",
    "\n",
    "#  **Maximizing GPU Utilization in PyTorch**\n",
    "\n",
    "|  # | Technique                                  | Description                                               | Expected Impact                           | Action                                         |\n",
    "| -: | ------------------------------------------ | --------------------------------------------------------- | ----------------------------------------- | ---------------------------------------------- |\n",
    "|  1 | `pin_memory=True` in DataLoader            | Enables faster CPU→GPU transfers via page-locked memory   | **Moderate** (\\~10–20%)                   | `DataLoader(..., pin_memory=True)`             |\n",
    "|  2 | `num_workers>0` in DataLoader              | Enables multi-threaded CPU data loading                   | **High** (\\~20–50%) if CPU bottlenecked   | `DataLoader(..., num_workers=4 or 8)`          |\n",
    "|  3 | Larger `batch_size`                        | Reduces number of GPU kernel launches per epoch           | **High**, depends on GPU memory           | Try `256`, `512`, `1024` if possible           |\n",
    "|  4 | `torch.backends.cudnn.benchmark=True`      | Optimizes cuDNN kernel selection when input size is fixed | **Moderate** (5–15%)                      | Add to script startup                          |\n",
    "|  5 | Move `loss` and criterion to GPU           | Prevents CPU→GPU transfer during training                 | **Small** (\\~1–3%)                        | `criterion = nn.CrossEntropyLoss().to(device)` |\n",
    "|  6 | Use `torch.no_grad()` in eval loop         | Avoids unnecessary graph computation                      | **Small**, but essential for memory       | Already done ✅                                 |\n",
    "|  7 | Profile with `torch.profiler` or `nvprof`  | Finds performance bottlenecks per op                      | **High** insight, indirect gain           | Use when fine-tuning                           |\n",
    "|  8 | Prefetching with `prefetch_factor`         | Loads batches in background while GPU trains              | **Moderate** (\\~10–20%)                   | `DataLoader(..., prefetch_factor=2)`           |\n",
    "|  9 | Use `torch.compile(model)` (PyTorch 2.x)   | Traces and compiles model for faster inference/training   | **High** (10–40%) in PyTorch 2.x+         | Requires PyTorch 2.0+                          |\n",
    "| 10 | Use AMP (mixed-precision) training         | Reduces memory and speeds up math ops on Tensor Cores     | **Very High** (2× speed on RTX/A100 GPUs) | Use `torch.cuda.amp` or `Lightning`            |\n",
    "| 11 | Use `torch.jit.script` / `torch.jit.trace` | Optimizes graph and inlines ops                           | **Moderate** (\\~5–15%)                    | For static models                              |\n",
    "| 12 | Use `nvprof`, `nvtop`, `nvidia-smi`        | Monitor live GPU usage & bottlenecks                      | **Essential diagnostics**                 | CLI tools                                      |\n",
    "| 13 | Reduce CPU bottlenecks                     | e.g., avoid slow disk reads, keep data in RAM             | **High** in I/O-heavy workloads           | Load data in RAM, SSD preferred                |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
