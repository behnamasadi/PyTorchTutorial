{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a248af-3534-4b8e-a477-b208f4d606ef",
   "metadata": {},
   "source": [
    "## **Syntactic sugar in nn.Module** \n",
    "\n",
    "Example of **syntactic sugar** in Python using **special (dunder) methods**.\n",
    "\n",
    "```python\n",
    "model = MyModel()\n",
    "output = model(input_tensor)\n",
    "```\n",
    "\n",
    "Even though it looks like `model` is a function being called, you're actually leveraging syntactic sugar via Pythonâ€™s `__call__` method.\n",
    "\n",
    "\n",
    "When you call an instance like a function:\n",
    "\n",
    "```python\n",
    "output = model(input_tensor)\n",
    "```\n",
    "\n",
    "Python automatically calls:\n",
    "\n",
    "```python\n",
    "output = model.__call__(input_tensor)\n",
    "```\n",
    "\n",
    "In `torch.nn.Module`, the base class defines `__call__` like this:\n",
    "\n",
    "```python\n",
    "def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "```\n",
    "\n",
    "So the `forward()` method gets called when you use function-call syntax on the object.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# This actually calls model.__call__(x), which calls model.forward(x)\n",
    "x = torch.randn(5, 10)\n",
    "output = model(x)\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215d386-e221-4693-bfba-1baf430d2f51",
   "metadata": {},
   "source": [
    "## **`nn.Parameter`**\n",
    "In PyTorch, `nn.Parameter` is used to **mark a tensor as a learnable parameter** within a `nn.Module`. This tells PyTorch that the tensor should be:\n",
    "\n",
    "1. **Registered as a parameter** (visible in `model.parameters()`).\n",
    "2. **Updated by optimizers** during training (when `.backward()` is called).\n",
    "\n",
    "---\n",
    "\n",
    "###  When to Use `nn.Parameter`\n",
    "\n",
    "Use `nn.Parameter` when:\n",
    "\n",
    "* You want a custom tensor (not from a built-in layer like `nn.Linear`) to be **learned** during training.\n",
    "* You define a module and want PyTorch to **automatically track** its parameters.\n",
    "\n",
    "---\n",
    "\n",
    "###  When Not to Use It\n",
    "\n",
    "* For constant values or buffers (use `self.register_buffer` instead).\n",
    "* When the tensor should **not** be updated by the optimizer.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6550684-0e79-46f1-a3e2-ee3553bc3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor(1., requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # A learnable scalar parameter initialized to 1.0\n",
    "        self.alpha = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.alpha * x\n",
    "\n",
    "model = MyModel()\n",
    "print(list(model.parameters()))  # Shows alpha as a learnable parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c0742b-6a9c-4294-9b3e-a8ce310e94c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9887, -0.2748, -0.8510],\n",
      "        [-0.0530,  1.7576,  0.9672]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.T + self.bias\n",
    "        \n",
    "x = torch.randn(10, 3)\n",
    "model = CustomLinear(3, 2)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.weight)  # It's being updated!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
