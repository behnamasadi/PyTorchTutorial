{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb0757a-a12c-4c72-9763-ab900bb9c1eb",
   "metadata": {},
   "source": [
    "## 1. Making Training on a Single-GPU Repeatable (Running bit-for-bit) \n",
    "\n",
    "You have to remove/lock down all randomness and forbid non-deterministic kernels. It’s slower, some ops are disallowed, and cross-hardware or cross-version determinism is not guaranteed.\n",
    "\n",
    "\n",
    "```python\n",
    "# ---- put these at the VERY top of your script, before importing numpy/torch things that spawn threads ----\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"                 # stable Python hashing (affects dict/set iteration)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"  (required for CUDA matmul determinism)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1) Seed ALL RNGs\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 2) Forbid non-deterministic algorithms\n",
    "torch.use_deterministic_algorithms(True)   # raises if a nondeterministic kernel would be used\n",
    "\n",
    "# 3) Make cuDNN + matmul deterministic and stable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False     # no auto-tuning\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# 4) Avoid AMP for strict determinism (mixed precision can introduce tiny diffs)\n",
    "use_amp = False\n",
    "\n",
    "# 5) Build your DataLoader with deterministic shuffling\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BS, \n",
    "    shuffle=True,              # OK, but make it reproducible:\n",
    "    generator=g,               # <- controls the shuffle order deterministically\n",
    "    num_workers=0,             # easiest way to avoid nondeterminism from workers\n",
    "    persistent_workers=False,  # if you later use workers>0, keep this False for determinism\n",
    "    drop_last=True,            # avoids partial-batch edge cases\n",
    "    pin_memory=False           # optional; not needed for determinism\n",
    ")\n",
    "\n",
    "# 6) Remove stochastic layers/augs or fix their RNG\n",
    "#   - Set Dropout p=0 during training (or manually seed before every forward)\n",
    "#   - Use deterministic/data-independent augmentations, or seed them from `SEED` every step\n",
    "#   - BatchNorm is fine but depends on batch content/order (which we've fixed)\n",
    "\n",
    "# 7) Training loop (no AMP, no randomness)\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # If you insist on reshuffling each epoch but still deterministic, reseed with a function of (SEED, epoch):\n",
    "    # g.manual_seed(SEED + epoch)\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "### What can still break determinism?\n",
    "\n",
    "* **Different hardware/driver/PyTorch/CUDA/cuDNN versions.** Bitwise sameness is only realistic when the full stack is identical.\n",
    "* **Ops without deterministic implementations.** With `torch.use_deterministic_algorithms(True)`, PyTorch will throw if you hit one (e.g., some pooling/atomic-based ops on certain versions). Replace those ops or move to CPU alternatives.\n",
    "* **Data augmentation randomness.** Libraries like Albumentations/torchvision transforms must be seeded per step or made non-random.\n",
    "* **Dropout.** Either disable (`p=0`) or seed before every forward so the mask is reproducible. (But then you’re not really doing stochastic regularization.)\n",
    "* **Mixed precision / TF32.** For strict reproducibility, keep FP32 only and disable TF32 (done above).\n",
    "* **Multi-GPU / DDP.** True bitwise determinism is much harder due to collective operations and scheduling. It can sometimes be approximated with careful NCCL settings and static graphs, but expect occasional drift; single-GPU is the safe path.\n",
    "\n",
    "### FAQ\n",
    "\n",
    "* **“Is it meaningful to train ‘deterministically’ if I remove randomness like dropout and data aug?”**\n",
    "  It’s useful for **debugging, ablations, and CI**. For best generalization, you usually want stochasticity (augmentations, dropout), but you can still keep runs reproducible by fixing seeds and using deterministic kernels—your randomness is then *controlled and repeatable*.\n",
    "\n",
    "* **“Can I keep per-epoch reshuffling and still be reproducible?”**\n",
    "  Yes—reseed the DataLoader generator each epoch with a deterministic function (e.g., `SEED + epoch`). You’ll get a *different* shuffle each epoch, but it will be the *same* across runs.\n",
    "\n",
    "* **“How about BatchNorm?”**\n",
    "  BN is deterministic given the same batch order/content. If you switch to `model.eval()` to disable dropout, remember that BN will then use running stats (different behavior). Prefer disabling dropout explicitly instead of flipping to eval.\n",
    "\n",
    "If you tell me your model/ops (e.g., any custom CUDA layers, special pools), I can point out known non-deterministic spots and suggest deterministic substitutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f7083-7510-4c04-b691-7d1567b57662",
   "metadata": {},
   "source": [
    "## 2. Making Training on a Multi-GPU Repeatable (Running bit-for-bit) \n",
    "\n",
    "**multi-GPU Distributed Data Parallel (DDP) adds extra sources of non-determinism**—mainly from parallel communication/reduction, process-local RNGs, and data sharding. You can still get *highly reproducible* runs, but **bit-for-bit determinism is harder and not guaranteed** the way it often is on a single GPU.\n",
    "\n",
    "Here’s what’s different + how to lock it down as much as PyTorch allows.\n",
    "\n",
    "### What’s different (and why it drifts)\n",
    "\n",
    "1. **Floating-point reduction order**\n",
    "   Gradients are summed across GPUs via collective ops (NCCL all-reduce). Different reduction *orders* (ring vs tree, bucket timings) change rounding, so results can differ by a few ULPs.\n",
    "\n",
    "2. **Asynchrony & scheduling**\n",
    "   Backward/communication overlap, CUDA stream timing, and kernel scheduling can vary slightly between runs → tiny numeric diffs.\n",
    "\n",
    "3. **Parameter bucketing & module order**\n",
    "   DDP groups params into “buckets” dynamically. If module/param registration order changes (e.g., dict/hash order), bucket order can differ → different reduction order.\n",
    "\n",
    "4. **Per-rank data & RNGs**\n",
    "   Each rank sees a different shard of data and has its own RNG stream. If you don’t seed things *per rank* (and per epoch), shuffles/augs diverge run-to-run.\n",
    "\n",
    "5. **Dataloader workers**\n",
    "   Multiple workers per rank introduce more threads & RNG streams to coordinate.\n",
    "\n",
    "6. **AMP / loss scaling**\n",
    "   Mixed precision adds nondeterminism (different overflow patterns, atomics). Static scale helps, but strict FP32 is safer.\n",
    "\n",
    "### As-deterministic-as-possible DDP recipe\n",
    "\n",
    "Use this as a template (single-node, multi-GPU; one process per GPU with `torchrun`):\n",
    "\n",
    "```python\n",
    "# ---- very top of your entry script ----\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # CUDA matmul determinism\n",
    "# Optional: pin NCCL algo to stabilize reduction order (not a hard guarantee)\n",
    "os.environ.setdefault(\"NCCL_ALGO\", \"Ring\")\n",
    "os.environ.setdefault(\"NCCL_LAUNCH_MODE\", \"GROUP\")\n",
    "# Optional: reduces timing jitter when debugging; can slow things down\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import random, numpy as np, torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def set_global_determinism(seed: int, rank: int):\n",
    "    # Per-rank seeding: base + rank so each process is distinct but reproducible\n",
    "    base = seed + rank\n",
    "    random.seed(base)\n",
    "    np.random.seed(base)\n",
    "    torch.manual_seed(base)\n",
    "    torch.cuda.manual_seed(base)\n",
    "    torch.cuda.manual_seed_all(base)\n",
    "\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cuda.matmul.allow_tf32 = False\n",
    "    torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "def seed_worker(worker_id: int, base_seed: int, rank: int):\n",
    "    wseed = base_seed + 1000 * rank + worker_id\n",
    "    random.seed(wseed); np.random.seed(wseed); torch.manual_seed(wseed)\n",
    "\n",
    "def create_loader(dataset, batch_size, seed, rank, num_workers=0):\n",
    "    sampler = DistributedSampler(dataset, shuffle=True, seed=seed, drop_last=True)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + rank)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,            # for strictness keep 0; if >0, use worker_init_fn\n",
    "        worker_init_fn=(lambda wid: seed_worker(wid, seed, rank)) if num_workers>0 else None,\n",
    "        generator=g,\n",
    "        persistent_workers=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "    ), sampler\n",
    "\n",
    "def main_worker(rank, world_size, seed):\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    set_global_determinism(seed, rank)\n",
    "\n",
    "    model = build_model().cuda(rank)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[rank], find_unused_parameters=False, static_graph=True\n",
    "    )\n",
    "\n",
    "    optimizer = build_optimizer(model.parameters())\n",
    "    criterion = build_criterion()\n",
    "\n",
    "    train_loader, train_sampler = create_loader(train_dataset, BS, seed, rank, num_workers=0)\n",
    "\n",
    "    # Disable stochastic layers or make them deterministic\n",
    "    # e.g., set Dropout p=0, or keep but accept that it will be deterministic per-run given the seeding above.\n",
    "    # Avoid AMP for strictest determinism.\n",
    "    use_amp = False\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)  # deterministic reshuffle across runs\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.cuda(rank, non_blocking=False), y.cuda(rank, non_blocking=False)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "```\n",
    "\n",
    "**Launch consistently** (same world size, same device order, same seeds):\n",
    "\n",
    "```bash\n",
    "torchrun --nproc_per_node=<NUM_GPUS> --master_port=29500 train.py\n",
    "# Ensure the same versions of PyTorch/CUDA/cuDNN/NCCL and identical GPUs/drivers\n",
    "```\n",
    "\n",
    "### Extra tips / gotchas\n",
    "\n",
    "* **Bit-for-bit?** Even with all of the above, **PyTorch does not guarantee bit-exact determinism across multiple GPUs**. You *can* get extremely close (repeatable curves/metrics), but expect rare ±1 ULP differences.\n",
    "\n",
    "* **Keep the graph static.** `static_graph=True` (PyTorch ≥1.12) avoids bucket re-builds. Also keep `find_unused_parameters=False`.\n",
    "\n",
    "* **Parameter registration order.** Build modules in a deterministic order (don’t iterate Python dicts unless `PYTHONHASHSEED` is fixed—as we did).\n",
    "\n",
    "* **Data augs.** If you use stochastic augmentations, they’ll be **reproducible per run** with the seeding above (including per-epoch reshuffles via `set_epoch`). If you want *identical* augs on each rerun, don’t add extra RNG draws elsewhere.\n",
    "\n",
    "* **AMP.** For strictest reproducibility, run FP32. If you must use AMP, prefer **static loss scale** and accept small nondeterminism.\n",
    "\n",
    "* **Multiple nodes.** Cross-node adds more variability (network fabric, clocks). Pin NCCL envs and keep machines identical; determinism becomes even less guaranteed.\n",
    "\n",
    "* **Evaluation sync.** Call `dist.barrier()` before validation to keep ranks aligned (and to avoid one rank using different running stats if you switch modes).\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
