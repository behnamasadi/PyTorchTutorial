{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb22a26-ba94-409a-bbd5-e0e41394e786",
   "metadata": {},
   "source": [
    "## Training and Validation set\n",
    "\n",
    "\n",
    "\n",
    "**What Is a Validation Set**\n",
    "we usually split our dataset into **three parts**:\n",
    "\n",
    "1. **Training set** ‚Äì used to **train** the model.\n",
    "2. **Validation set** ‚Äì used to **tune** the model.\n",
    "3. **Test set** ‚Äì used to **evaluate** the model after everything is done.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose of the Validation Set**\n",
    "\n",
    "The validation set helps us:\n",
    "\n",
    "- **Monitor model performance during training.**\n",
    "- **Tune hyperparameters** (e.g., learning rate, number of layers, regularization).\n",
    "- **Detect overfitting** ‚Äì when the model learns the training data too well and fails to generalize.\n",
    "---\n",
    "\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "Let‚Äôs say you‚Äôre training a model for 20 epochs.\n",
    "\n",
    "- After each epoch (a full pass over the training data), the model‚Äôs accuracy/loss is also calculated on the **validation set**.\n",
    "- If the **validation loss starts increasing** while training loss is decreasing, it means **overfitting** is happening.\n",
    "- You can use this to **stop training early** or pick the best version of your model (called **early stopping**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c108c2-0da1-41e8-a15f-65e37cabbb77",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "dataset = datasets.MNIST(root=data_dir, train=True,download=True, transform=transform)\n",
    "```\n",
    "\n",
    "Split it to $80%$, $20%$\n",
    "\n",
    "\n",
    "- If your dataset is small, you can use **k-fold cross-validation** instead of a fixed validation set.\n",
    "- In deep learning, many people use an **80/10/10** or **70/15/15** split for training/validation/test.\n",
    "\n",
    "\n",
    "```python\n",
    "train_size = int(0.8*len(dataset))\n",
    "validation_size = len(dataset)-train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444657-c9e1-40c5-9be3-523bb483400e",
   "metadata": {},
   "source": [
    "Let's define **MNIST** simple example, input images are $28\\times 28$, and sinc we have only digits, we have $10$ outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f61a19f-cab3-43d4-b091-e2091f5c3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # flatten the image\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6dd789-48ae-4bd3-ab74-8f1a78276505",
   "metadata": {},
   "source": [
    "To see the weight and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8e9874-77ef-4373-82f7-83a6da680df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([128, 784])\n",
      "fc1.bias: torch.Size([128])\n",
      "fc2.weight: torch.Size([64, 128])\n",
      "fc2.bias: torch.Size([64])\n",
      "fc3.weight: torch.Size([10, 64])\n",
      "fc3.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7e68ab-8166-4c19-b478-62207fe0e778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"412pt\" height=\"545pt\" viewBox=\"0.00 0.00 412.00 545.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 541)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-541 408,-541 408,4 -4,4\"/>\n",
       "<!-- 134181829415376 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>134181829415376</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"233,-31 156,-31 156,0 233,0 233,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"194.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (128, 10)</text>\n",
       "</g>\n",
       "<!-- 134181835934208 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>134181835934208</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"245,-86 144,-86 144,-67 245,-67 245,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"194.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835934208&#45;&gt;134181829415376 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>134181835934208-&gt;134181829415376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.5,-66.79C194.5,-60.07 194.5,-50.4 194.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198,-41.19 194.5,-31.19 191,-41.19 198,-41.19\"/>\n",
       "</g>\n",
       "<!-- 134181835941024 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>134181835941024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"129,-141 28,-141 28,-122 129,-122 129,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835941024&#45;&gt;134181835934208 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>134181835941024-&gt;134181835934208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.14,-121.98C115.8,-113.46 144.75,-100.23 166.24,-90.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.88,-93.51 175.52,-86.17 164.97,-87.14 167.88,-93.51\"/>\n",
       "</g>\n",
       "<!-- 134181829415216 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>134181829415216</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"111,-207 46,-207 46,-177 111,-177 111,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"78.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">fc3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"78.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 134181829415216&#45;&gt;134181835941024 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>134181829415216-&gt;134181835941024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.5,-176.84C78.5,-169.21 78.5,-159.7 78.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82,-151.27 78.5,-141.27 75,-151.27 82,-151.27\"/>\n",
       "</g>\n",
       "<!-- 134181835938624 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>134181835938624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"242,-141 147,-141 147,-122 242,-122 242,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"194.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835938624&#45;&gt;134181835934208 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>134181835938624-&gt;134181835934208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.5,-121.75C194.5,-114.8 194.5,-104.85 194.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198,-96.09 194.5,-86.09 191,-96.09 198,-96.09\"/>\n",
       "</g>\n",
       "<!-- 134181835937904 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>134181835937904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"237,-201.5 136,-201.5 136,-182.5 237,-182.5 237,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835937904&#45;&gt;134181835938624 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>134181835937904-&gt;134181835938624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.68,-182.37C188.79,-174.25 190.49,-161.81 191.92,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.43,-151.55 193.31,-141.17 188.49,-150.6 195.43,-151.55\"/>\n",
       "</g>\n",
       "<!-- 134181835938336 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>134181835938336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-267.5 0,-267.5 0,-248.5 101,-248.5 101,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835938336&#45;&gt;134181835937904 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>134181835938336-&gt;134181835937904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.84,-248.37C91.96,-237.49 131.91,-218.69 158.77,-206.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.46,-209.12 168.01,-201.7 157.48,-202.79 160.46,-209.12\"/>\n",
       "</g>\n",
       "<!-- 134181829412496 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>134181829412496</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"83,-339 18,-339 18,-309 83,-309 83,-339\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\">fc2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 134181829412496&#45;&gt;134181835938336 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>134181829412496-&gt;134181835938336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-308.8C50.5,-299.7 50.5,-287.79 50.5,-277.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-277.84 50.5,-267.84 47,-277.84 54,-277.84\"/>\n",
       "</g>\n",
       "<!-- 134181835936656 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>134181835936656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-267.5 119,-267.5 119,-248.5 214,-248.5 214,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835936656&#45;&gt;134181835937904 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>134181835936656-&gt;134181835937904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.2,-248.37C172.13,-238.97 176.92,-223.67 180.71,-211.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.07,-212.49 183.72,-201.91 177.39,-210.41 184.07,-212.49\"/>\n",
       "</g>\n",
       "<!-- 134181835936320 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>134181835936320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"209,-333.5 108,-333.5 108,-314.5 209,-314.5 209,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"158.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835936320&#45;&gt;134181835936656 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>134181835936320-&gt;134181835936656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159.58,-314.37C160.74,-305.07 162.63,-289.98 164.14,-277.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.62,-278.26 165.39,-267.91 160.67,-277.39 167.62,-278.26\"/>\n",
       "</g>\n",
       "<!-- 134181835941552 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>134181835941552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-399.5 13,-399.5 13,-380.5 114,-380.5 114,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835941552&#45;&gt;134181835936320 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>134181835941552-&gt;134181835936320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M76.31,-380.37C91.89,-369.87 118.43,-352 137.12,-339.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.25,-342.19 145.59,-333.7 135.34,-336.38 139.25,-342.19\"/>\n",
       "</g>\n",
       "<!-- 134181829412256 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>134181829412256</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"96,-471 31,-471 31,-441 96,-441 96,-471\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\">fc1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\"> (128)</text>\n",
       "</g>\n",
       "<!-- 134181829412256&#45;&gt;134181835941552 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>134181829412256-&gt;134181835941552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63.5,-440.8C63.5,-431.7 63.5,-419.79 63.5,-409.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"67,-409.84 63.5,-399.84 60,-409.84 67,-409.84\"/>\n",
       "</g>\n",
       "<!-- 134181835938720 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>134181835938720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"209,-399.5 132,-399.5 132,-380.5 209,-380.5 209,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835938720&#45;&gt;134181835936320 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>134181835938720-&gt;134181835936320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168.88,-380.37C167.14,-371.07 164.31,-355.98 162.04,-343.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.45,-343.09 160.17,-333.91 158.57,-344.38 165.45,-343.09\"/>\n",
       "</g>\n",
       "<!-- 134181835934736 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>134181835934736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"221,-465.5 120,-465.5 120,-446.5 221,-446.5 221,-465.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835934736&#45;&gt;134181835938720 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>134181835934736-&gt;134181835938720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.5,-446.37C170.5,-437.16 170.5,-422.29 170.5,-410.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174,-409.91 170.5,-399.91 167,-409.91 174,-409.91\"/>\n",
       "</g>\n",
       "<!-- 134181829412016 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>134181829412016</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"212,-537 129,-537 129,-507 212,-507 212,-537\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\">fc1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\"> (128, 784)</text>\n",
       "</g>\n",
       "<!-- 134181829412016&#45;&gt;134181835934736 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>134181829412016-&gt;134181835934736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.5,-506.8C170.5,-497.7 170.5,-485.79 170.5,-475.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174,-475.84 170.5,-465.84 167,-475.84 174,-475.84\"/>\n",
       "</g>\n",
       "<!-- 134181835946880 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>134181835946880</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"309,-267.5 232,-267.5 232,-248.5 309,-248.5 309,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"270.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835946880&#45;&gt;134181835937904 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>134181835946880-&gt;134181835937904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.17,-248.37C245.64,-238.06 222.78,-220.65 206.31,-208.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.99,-204.98 197.92,-201.7 203.75,-210.54 207.99,-204.98\"/>\n",
       "</g>\n",
       "<!-- 134181835934688 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>134181835934688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"328,-333.5 227,-333.5 227,-314.5 328,-314.5 328,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835934688&#45;&gt;134181835946880 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>134181835934688-&gt;134181835946880</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276.56,-314.37C275.54,-305.07 273.89,-289.98 272.57,-277.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.04,-277.47 271.47,-267.91 269.08,-278.23 276.04,-277.47\"/>\n",
       "</g>\n",
       "<!-- 134181829412096 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>134181829412096</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"316,-405 239,-405 239,-375 316,-375 316,-405\"/>\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\">fc2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\"> (64, 128)</text>\n",
       "</g>\n",
       "<!-- 134181829412096&#45;&gt;134181835934688 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>134181829412096-&gt;134181835934688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.5,-374.8C277.5,-365.7 277.5,-353.79 277.5,-343.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281,-343.84 277.5,-333.84 274,-343.84 281,-343.84\"/>\n",
       "</g>\n",
       "<!-- 134181835938240 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>134181835938240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"360,-141 283,-141 283,-122 360,-122 360,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"321.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 134181835938240&#45;&gt;134181835934208 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>134181835938240-&gt;134181835934208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M301.1,-121.98C280.48,-113.38 248.39,-99.99 224.81,-90.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"225.85,-86.79 215.28,-86.17 223.16,-93.25 225.85,-86.79\"/>\n",
       "</g>\n",
       "<!-- 134181835941792 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>134181835941792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"396,-201.5 295,-201.5 295,-182.5 396,-182.5 396,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"345.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 134181835941792&#45;&gt;134181835938240 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>134181835941792-&gt;134181835938240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M341.96,-182.37C338.56,-174.07 333.31,-161.28 328.98,-150.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.09,-149.09 325.06,-141.17 325.61,-151.75 332.09,-149.09\"/>\n",
       "</g>\n",
       "<!-- 134181829415136 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>134181829415136</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"404,-273 327,-273 327,-243 404,-243 404,-273\"/>\n",
       "<text text-anchor=\"middle\" x=\"365.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">fc3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"365.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (10, 64)</text>\n",
       "</g>\n",
       "<!-- 134181829415136&#45;&gt;134181835941792 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>134181829415136-&gt;134181835941792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M361.06,-242.8C358.19,-233.6 354.42,-221.53 351.31,-211.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.59,-210.35 348.26,-201.84 347.91,-212.43 354.59,-210.35\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchviz\n",
    "from IPython.display import display, SVG  # Import SVG display\n",
    "\n",
    "model=SimpleNN()\n",
    "input=torch.randn(size=[28*28, 128])\n",
    "\n",
    "# dot=torchviz.make_dot(model(input),params=dict(model.named_parameters()),show_attrs=True, show_saved=True )\n",
    "dot=torchviz.make_dot(model(input),params=dict(model.named_parameters()) )\n",
    "\n",
    "# Render as SVG and display directly in the notebook\n",
    "dot.format = 'svg'\n",
    "svg_string = dot.pipe().decode('utf-8')  # Get the SVG string\n",
    "display(SVG(svg_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba7c3b-d1cf-4334-83e6-3ada403e3f6c",
   "metadata": {},
   "source": [
    "And we pass the to optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "These are the ones affected by `loss.backward()` and `optimizer.step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db22c5-0aff-4813-b978-9cee57b781a5",
   "metadata": {},
   "source": [
    "### Test Set\n",
    "\n",
    "```python\n",
    "test_set = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23cde6-76e2-40cf-9161-16ab54def64d",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fa005-8d08-42c9-907c-9d66018bd11a",
   "metadata": {},
   "source": [
    "```python\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0995030-f88b-4a49-9b55-42b363156733",
   "metadata": {},
   "source": [
    "### `autograd` and `torch.no_grad()`:\n",
    "In PyTorch, **autograd (automatic differentiation)** is enabled by default. This means:\n",
    "- Every operation is tracked in a **computational graph**\n",
    "- Gradients can be calculated with `.backward()`\n",
    "\n",
    "But during **evaluation** (validation or test), we don‚Äôt need to compute gradients. So:\n",
    "\n",
    "\n",
    "- Temporarily **disables autograd**\n",
    "- Reduces **memory usage**\n",
    "- Speeds up **inference**\n",
    "- Prevents unnecessary **graph building**\n",
    "\n",
    "You should always use `with torch.no_grad():` during **validation** or **inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d8ba8-037d-4d1a-a070-27b103957b1b",
   "metadata": {},
   "source": [
    "`correct += (outputs.argmax(1) == labels).sum().item()` \n",
    "\n",
    "Let‚Äôs break this line into pieces.\n",
    "\n",
    "Assume:\n",
    "```python\n",
    "outputs.shape = (64, 10)  # 64 samples, 10 class scores per sample\n",
    "labels.shape = (64,)      # true class labels (0 to 9)\n",
    "```\n",
    "\n",
    "- `outputs.argmax(1)` ‚Üí returns the **predicted class index** per sample\n",
    "  - Shape: `[64]`\n",
    "- `== labels` ‚Üí element-wise comparison: True if prediction is correct\n",
    "  - Shape: `[64]` of `True/False`\n",
    "- `.sum()` ‚Üí counts how many predictions are correct\n",
    "- `.item()` ‚Üí converts the single scalar tensor to a Python `int`\n",
    "- `correct += ...` ‚Üí accumulate total correct predictions across all batches\n",
    "\n",
    "### üîç Full Example:\n",
    "```python\n",
    "# Suppose predictions are [2, 1, 0, 3]\n",
    "# and true labels are  [2, 0, 0, 3]\n",
    "# Comparison: [T, F, T, T] ‚Üí sum = 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66605da-dbc0-4731-95a0-3fd1053fdae8",
   "metadata": {},
   "source": [
    "## What Is Stored in `loss`?\n",
    "\n",
    "Let‚Äôs say you have:\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, labels)\n",
    "```\n",
    "\n",
    "- `outputs` shape: `[batch_size, num_classes]`\n",
    "- `labels` shape: `[batch_size]` (class indices)\n",
    "\n",
    "\n",
    "`CrossEntropyLoss` combines:\n",
    "1. **`LogSoftmax`** over output logits\n",
    "2. **Negative log-likelihood**\n",
    "\n",
    "So it computes:\n",
    "$ \\text{loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\text{softmax}(x_i)_{y_i} \\right) $\n",
    "\n",
    "**What‚Äôs inside `loss`**?\n",
    "- A **scalar tensor** (e.g., `tensor(1.8634, grad_fn=<NllLossBackward>)`)\n",
    "- It represents the **average loss** over the batch\n",
    "- It tracks the **gradient graph**, so calling `.backward()` will compute gradients\n",
    "\n",
    "To inspect:\n",
    "```python\n",
    "print(loss)\n",
    "print(loss.item())  # to get float\n",
    "```\n",
    "\n",
    "If you want **individual per-sample losses**:\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "losses = criterion(outputs, labels)  # tensor of shape [batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633b684-e110-40cd-a0c1-5ec49502e0c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "loss: it is sum of `CrossEntropyLoss`  over all outputs and labels (we calculate it for all batches and make an average over the batch size)\n",
    "```\n",
    "loss:  tensor(2.3085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "loss.item()\n",
    "```\n",
    "loss.item():  2.3084824085235596\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```\n",
    "outputs.shape:  torch.Size([64, 10])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "outputs:  tensor([[-0.0878,  0.0970,  0.0720,  0.1041, -0.0197, -0.0194, -0.0781, -0.0617,0.0722, -0.0009],\n",
    "                 [...],\n",
    "                 [-0.0541,  0.0800,  0.0789,  0.0715,  0.0250, -0.0123, -0.0937, -0.0662, 0.1120,  0.0342]],\n",
    "device='cuda:0', grad_fn=<AddmmBackward0>)\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "outputs.argmax(1): we have $64$ batches (output is $64 \\times 10$), it finds the max in each row, so we have 64 values here, since we are classifying numbers between $0$ to $9$ each max value point exactly to the same class of digit, so we can just compare them with labels to check if the output of network is correct:\n",
    "\n",
    "```\n",
    "outputs.argmax(1):  tensor([3, 8, 2, 8, 8, 8, 8, 8, 1, 1, 2, 8, 8, 8, 8, 8, 8, 2, 8, 1, 3, 1, 3, 2,\n",
    "        2, 8, 8, 8, 2, 2, 8, 1, 8, 8, 2, 2, 8, 8, 3, 2, 1, 8, 3, 8, 2, 8, 3, 3,\n",
    "        8, 2, 1, 8, 3, 1, 8, 8, 3, 8, 8, 1, 2, 8, 8, 8], device='cuda:0')\n",
    "```\n",
    "---\n",
    "\n",
    "true class value, size is $64$ ($64$ batches):\n",
    "```\n",
    "labels:  tensor([7, 3, 9, 3, 2, 0, 6, 6, 4, 7, 1, 4, 6, 5, 3, 3, 0, 6, 7, 9, 4, 1, 7, 2,\n",
    "        9, 0, 5, 1, 1, 3, 9, 7, 4, 4, 4, 1, 3, 0, 8, 9, 7, 2, 8, 4, 1, 8, 0, 5,\n",
    "        7, 2, 7, 4, 2, 3, 6, 3, 4, 5, 1, 7, 8, 3, 8, 3], device='cuda:0')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "size is $64$ ($64$ batches):\n",
    "```        \n",
    "outputs.argmax(1) == labels:  tensor([False, False, False, False, False, False, False, False, False, False,\n",
    "        False, False, False, False, False, False, False, False, False, False,\n",
    "        False,  True, False,  True, False, False, False, False, False, False,\n",
    "        False, False, False, False, False, False, False, False, False, False,\n",
    "        False, False, False, False, False,  True, False, False, False,  True,\n",
    "        False, False, False, False, False, False, False, False, False, False,\n",
    "        False, False,  True, False], device='cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd6098-32df-49bb-958a-8e0294a428c5",
   "metadata": {},
   "source": [
    "### `model.eval()` and `model.train()`\n",
    "`model.eval()` and `model.train()` are both methods implemented by the `nn.Module` class in PyTorch. They are used to **switch the behavior** of certain layers in your model **between training and evaluation modes**.\n",
    "\n",
    "---\n",
    "\n",
    "**`model.train()`**\n",
    "\n",
    "- Sets the model to **training mode**.\n",
    "- It **affects layers like:**\n",
    "  - `Dropout`: Randomly drops units to prevent overfitting (during training only).\n",
    "  - `BatchNorm`: Uses **batch statistics** (mean and variance) during training.\n",
    "\n",
    "  **What it does under the hood:**\n",
    "- Sets an internal flag (`self.training = True`) on the model and all its submodules.\n",
    "- This tells layers like Dropout and BatchNorm to behave in \"training\" mode.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**`model.eval()`**\n",
    "\n",
    "- Sets the model to **evaluation mode**.\n",
    "- It **disables behaviors** that are only useful during training.\n",
    "\n",
    "üîß Under the hood:\n",
    "- Sets `self.training = False` recursively.\n",
    "- Affects modules like:\n",
    "  - `Dropout`: Now passes data through unchanged (no dropout).\n",
    "  - `BatchNorm`: Uses **running averages** (not batch stats).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "If you forget to call `.eval()` during validation/testing, your results may be **inconsistent** and **less accurate**, especially with Dropout/BatchNorm.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17b36d-8fb9-4d40-bb91-ff9dc2c0959a",
   "metadata": {},
   "source": [
    "##  What Can You Do With `evaluate()` Results?\n",
    "\n",
    "While the `evaluate()` function gives us **metrics**, we haven‚Äôt yet done anything *with* those metrics. So let‚Äôs talk about what you **should do** with the evaluation results in real training workflows.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You can use the validation **loss** and **accuracy** to:\n",
    "\n",
    "### 1. **Monitor for Overfitting**\n",
    "- Compare `train_loss` vs `val_loss`\n",
    "- If `val_loss` starts increasing while `train_loss` is still decreasing ‚Üí model is overfitting\n",
    "- You might want to:\n",
    "  - Stop training early\n",
    "  - Add regularization\n",
    "  - Use dropout or data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Implement Early Stopping**\n",
    "Stop training if validation loss **stops improving** for N epochs.\n",
    "\n",
    "```python\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(...)\n",
    "    val_loss, val_acc = evaluate(...)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Select the Best Model**\n",
    "- Save the model whenever `val_accuracy` (or `val_loss`) improves\n",
    "- Load it later for testing or deployment\n",
    "\n",
    "```python\n",
    "# Save best-performing model\n",
    "if val_accuracy > best_val_acc:\n",
    "    best_val_acc = val_accuracy\n",
    "    torch.save(model.state_dict(), \"best_model.pth\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Visualize Metrics**\n",
    "Store and plot:\n",
    "- `train_loss`, `val_loss`\n",
    "- `val_accuracy`\n",
    "\n",
    "It helps you:\n",
    "- Understand model learning\n",
    "- Identify underfitting/overfitting\n",
    "- Tune learning rate and architecture\n",
    "\n",
    "```python\n",
    "train_losses.append(train_loss)\n",
    "val_losses.append(val_loss)\n",
    "val_accuracies.append(val_accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Adjust Learning Rate Dynamically**\n",
    "Some schedulers (e.g., `ReduceLROnPlateau`) require validation loss to decide whether to **reduce learning rate**.\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Log Metrics for Monitoring Tools**\n",
    "- TensorBoard\n",
    "- WandB\n",
    "- CSV logs\n",
    "\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Summary of What to Do with `evaluate()`:\n",
    "\n",
    "| Goal                    | What to Do                              |\n",
    "|-------------------------|------------------------------------------|\n",
    "| Monitor progress        | Print/plot loss & accuracy               |\n",
    "| Avoid overfitting       | Use early stopping                      |\n",
    "| Keep best model         | Save model if val loss improves         |\n",
    "| Fine-tune hyperparams   | Adjust based on val performance         |\n",
    "| Dynamic LR adjustment   | Feed val loss to learning rate scheduler |\n",
    "| Visualization/logging   | Use TensorBoard or logging tools        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb679fe-cac3-49f4-b26d-f1a7a10f0075",
   "metadata": {},
   "source": [
    "## Add a Test Set to Your Pipeline\n",
    "\n",
    "###  1. Load the Test Set\n",
    "```python\n",
    "# Load the test set\n",
    "test_set = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Add a `test()` Function (similar to `evaluate()`)\n",
    "\n",
    "```python\n",
    "def test(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Run `test()` After Training is Done\n",
    "\n",
    "Usually after training (and possibly loading the best model checkpoint):\n",
    "\n",
    "```python\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Run on test set\n",
    "test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  What Do You Do With the Test Results?\n",
    "\n",
    "###  If **Test Accuracy is Good**:\n",
    "- üéâ Your model generalizes well!\n",
    "- You‚Äôre ready to:\n",
    "  - Deploy the model\n",
    "  - Use it in a product or service\n",
    "  - Publish the results\n",
    "  - Tune for inference efficiency (e.g., quantization, pruning)\n",
    "\n",
    "###  If **Test Accuracy is Bad**:\n",
    "- You might be **overfitting** (check training/val loss)\n",
    "- You might have:\n",
    "  - Insufficient or unbalanced training data\n",
    "  - Model architecture that is too simple or too complex\n",
    "  - Noisy or mislabeled data\n",
    "- Possible fixes:\n",
    "  - Add regularization (dropout, weight decay)\n",
    "  - Get more data or augment it\n",
    "  - Try transfer learning (pretrained models)\n",
    "  - Tune learning rate, batch size, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| Load test set | `train=False` |\n",
    "| Run `test()` | after training (and loading best model) |\n",
    "| Evaluate | Print & compare with val accuracy |\n",
    "| If bad | Analyze and retrain with improvements |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
