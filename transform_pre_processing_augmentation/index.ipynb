{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Normalization in PyTorch\n",
    "\n",
    "In PyTorch, preprocessing transforms prepare raw images for model input. A typical pipeline includes:\n",
    "\n",
    "* **Resizing**\n",
    "  Ensures all images share consistent dimensions.\n",
    "\n",
    "  ```python\n",
    "  transforms.Resize((256, 256))       # Fixed size  \n",
    "  transforms.Resize(256)              # Maintains aspect ratio\n",
    "  ```\n",
    "\n",
    "* **Cropping**\n",
    "\n",
    "  * **CenterCrop:** For consistent evaluation crops.\n",
    "  * **RandomCrop / RandomResizedCrop:** For training augmentation.\n",
    "\n",
    "* **Data Augmentation** *(training only)*\n",
    "  Examples: `RandomHorizontalFlip`, `RandomRotation`, `ColorJitter`, `RandomErasing`, `RandomAffine`, etc., to improve generalization.\n",
    "\n",
    "* **ToTensor**\n",
    "  Converts a PIL image or NumPy array to a `torch.Tensor` in `[C, H, W]` format, scaling pixel values from `[0, 255]` to `[0.0, 1.0]`.\n",
    "\n",
    "  ```python\n",
    "  transforms.ToTensor()\n",
    "  ```\n",
    "\n",
    "  **Note:** This does **not** normalize by mean/std â€” it only rescales.\n",
    "\n",
    "* **Normalization (Standardization)**\n",
    "  Applies per-channel Z-score normalization:\n",
    "\n",
    "  $$\n",
    "  X' = \\frac{X - \\mu}{\\sigma}\n",
    "  $$\n",
    "\n",
    "  where `mean = Î¼` and `std = Ïƒ` are computed **from the training set only**.\n",
    "\n",
    "  ```python\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "  ```\n",
    "\n",
    "  Use dataset-specific stats unless working with pretrained models, in which case match the modelâ€™s training stats (e.g., ImageNet).\n",
    "\n",
    "---\n",
    "\n",
    "## Building a Transform Pipeline\n",
    "\n",
    "**Golden rule:**\n",
    "\n",
    "```\n",
    "Resize â†’ Augment â†’ ToTensor â†’ Normalize\n",
    "```\n",
    "\n",
    "This order ensures:\n",
    "\n",
    "* Resizing/augmentation happens on PIL images.\n",
    "* Conversion to tensors happens before normalization.\n",
    "* Normalization matches the way statistics were computed.\n",
    "\n",
    "**Example: Training vs Evaluation**\n",
    "\n",
    "```python\n",
    "# Training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Evaluation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "* **ToTensor** only rescales to `[0, 1]` â€” normalization is a separate step.\n",
    "* Mean/std are **always computed from the training set only** to avoid data leakage.\n",
    "* Apply the same normalization to training, validation, and test sets.\n",
    "* Keep the order of transforms consistent with how you computed your stats.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization vs. Normalization\n",
    "\n",
    "### 1.1 Normalization (Min-Max Scaling or Feature Scaling)\n",
    "Normalization rescales the feature values to a fixed range, usually $[0, 1]$. The formula is:\n",
    "\n",
    "$\n",
    "X_{\\text{norm}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$\n",
    "\n",
    "- **Purpose**: Useful when you want all features to contribute equally to the model, especially when they are on different scales.\n",
    "- **Assumptions**: Does **not** assume any particular distribution of the data.\n",
    "- **Sensitive to outliers**: Yes â€” since it relies on the minimum and maximum values, outliers can significantly affect the scaling.\n",
    "- **Use cases**: Algorithms that rely on distances or assume bounded input features, such as:\n",
    "  - K-Nearest Neighbors (KNN)\n",
    "  - Neural Networks (e.g., when using sigmoid/tanh activations)\n",
    "  - Principal Component Analysis (PCA), when interpretability is not affected by bounded scale\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Standardization (Z-score Normalization)\n",
    "Standardization transforms the data to have zero mean and unit variance. The formula is:\n",
    "\n",
    "$\n",
    "X_{\\text{standard}} = \\frac{X - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "- **Purpose**: Useful when features have different means and variances and you want to center them around 0.\n",
    "- **Assumptions**: Works well if the data is approximately normally distributed, but this is **not a strict requirement**.\n",
    "- **Sensitive to outliers**: Less than min-max normalization, but outliers still affect mean and standard deviation.\n",
    "- **Use cases**: Algorithms that assume data is centered or use covariance:\n",
    "  - Linear Regression\n",
    "  - Logistic Regression\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Principal Component Analysis (PCA) (for preserving variance direction)\n",
    "  - K-Means Clustering\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Aspect               | Normalization \\([0,1]\\)                  | Standardization ($\\mu=0$, $\\sigma=1$) |\n",
    "|----------------------|-------------------------------------------|--------------------------------------------|\n",
    "| Range                | Bounded $([0,1]$)                      | Unbounded                                 |\n",
    "| Sensitive to outliers | High                                    | Medium                                     |\n",
    "| Assumes normality    | No                                       | No (but benefits from it)                 |\n",
    "| Preserves outliers   | No                                       | Yes (to an extent)                        |\n",
    "| Use cases            | KNN, Neural Nets                         | SVM, Linear Models, PCA, K-Means          |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example From MNIST and ImageNet\n",
    "\n",
    "\n",
    "**1. MNIST mean & std**\n",
    "\n",
    "* The commonly used values `mean=[0.1307]` and `std=[0.3081]` are **precomputed** from the **training split** of MNIST (grayscale, so only one channel).\n",
    "* These statistics are used to normalize **both** training and test images, but they are **calculated only from the training set** to avoid \"data leakage.\"\n",
    "\n",
    "**2. ImageNet mean & std**\n",
    "\n",
    "```python\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "```\n",
    "\n",
    "* These values are computed from the **ImageNet training split** (RGB channels).\n",
    "* Same rule: we compute the stats on the training set, then apply them to **both** training and validation/test sets.\n",
    "\n",
    "**3. Dataset separation**\n",
    "\n",
    "* Yes â€” for datasets like MNIST, CIFAR-10, and ImageNet, the **training and test (or validation) splits are separate**.\n",
    "* When you download them with `torchvision.datasets.MNIST` or `ImageNet`, you can explicitly choose `train=True` or `train=False` (for MNIST/CIFAR), or use the appropriate folder (`train/` vs `val/`) for ImageNet.\n",
    "\n",
    "**4. Why fix the mean/std?**\n",
    "\n",
    "* Once the dataset split is fixed, we donâ€™t recompute the mean/std every time â€” we use the same fixed numbers so experiments are reproducible.\n",
    "* Computing them separately for train and test would be incorrect because it leaks test information.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "When you evaluate a model, you must apply **the exact same preprocessing** to the test (or validation) data as you did to the training data.\n",
    "\n",
    "So for ImageNet:\n",
    "\n",
    "```python\n",
    "transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "```\n",
    "\n",
    "is applied **both**:\n",
    "\n",
    "* During training (on training images, after augmentation like crop/flip)\n",
    "* During testing/validation (on test images, without augmentations except resizing/cropping)\n",
    "\n",
    "**Reason:**\n",
    "\n",
    "* The model learns to work with inputs that are normalized in this way.\n",
    "* Feeding it unnormalized test data would cause a distribution mismatch, hurting performance.\n",
    "\n",
    "ðŸ“Œ In short:\n",
    "\n",
    "* Mean/std are computed **only** from the training split (no leakage).\n",
    "* Normalization with those values is applied to **all splits** when feeding the model.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also show **a minimal PyTorch example** where we normalize both train and test splits for MNIST or ImageNet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing your own Data set\n",
    "Unlike pre-packaged datasets such as `ImageNet` or `MNIST`, which come with fixed training and test splits and precomputed normalization statistics, working with your own custom dataset requires you to decide how to split the data and how to compute normalization values. In this case, you have two main approaches:\n",
    "\n",
    "\n",
    "\n",
    "#### A) Random split each run\n",
    "\n",
    "You have to, compute mean/std on **train only** each time\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "\n",
    "root = \"/path/to/images\"  # class-subfolder structure for ImageFolder\n",
    "\n",
    "# Base transform WITHOUT Normalize for stats computation\n",
    "base_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),          # -> [0,1]\n",
    "])\n",
    "\n",
    "full = datasets.ImageFolder(root, transform=base_tf)\n",
    "\n",
    "# Fixed-seed split\n",
    "n = len(full)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.15 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "train_raw, val_raw, test_raw = random_split(full, [n_train, n_val, n_test])\n",
    "\n",
    "# Compute mean/std on TRAIN ONLY (this run)\n",
    "def mean_std(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    n_pix, mean, M2 = 0, torch.zeros(3), torch.zeros(3)\n",
    "    for x, _ in loader:\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1)\n",
    "        bm = x.mean(dim=(0, 2))\n",
    "        bv = x.var(dim=(0, 2), unbiased=False)\n",
    "        bp = b*h*w\n",
    "        delta = bm - mean\n",
    "        tot = n_pix + bp\n",
    "        mean += delta * (bp / tot)\n",
    "        M2 += bv*bp + (delta**2) * (n_pix*bp/tot)\n",
    "        n_pix = tot\n",
    "    std = torch.sqrt(M2 / n_pix)\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "mean, std = mean_std(train_raw)\n",
    "\n",
    "# Rebuild datasets WITH Normalize, preserving indices\n",
    "norm_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "full_norm = datasets.ImageFolder(root, transform=norm_tf)\n",
    "train = Subset(full_norm, train_raw.indices)\n",
    "val   = Subset(full_norm,  val_raw.indices)\n",
    "test  = Subset(full_norm,  test_raw.indices)\n",
    "```\n",
    "\n",
    "#### B) Fixed Split\n",
    "\n",
    "Using fix seed will create the same random set so we can precalculate the mean and std.\n",
    "```python\n",
    "seed = 42\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "train_raw, val_raw, test_raw = random_split(full, [n_train, n_val, n_test], generator=g)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
