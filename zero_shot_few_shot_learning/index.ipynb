{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927d5905-8f9d-4876-8a07-e56c11fabb06",
   "metadata": {},
   "source": [
    "#  1. Zero-Shot Learning (ZSL)\n",
    "\n",
    "**Definition:**\n",
    "A model is asked to perform a task or recognize a class **it has never seen during training**, without any labeled examples of that class. Instead, it relies on **auxiliary information** such as natural language descriptions, attributes, or embeddings.\n",
    "\n",
    "**Key Idea:**\n",
    " The model transfers knowledge from known tasks/classes to unseen ones using **semantic relationships**.\n",
    "\n",
    "**How it works (intuition):**\n",
    "\n",
    "* During training, the model learns a joint representation of **inputs** (e.g., images, text) and **descriptions/labels** (e.g., word embeddings, prompts).\n",
    "* At test time, it gets a new class (e.g., \"zebra\") that wasn‚Äôt in training data, but it knows \"zebra = an animal with black and white stripes\".\n",
    "* The model uses its learned embedding space to match the input image with the description.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Train on animals like **cats, dogs, horses**.\n",
    "* At inference, ask it to classify a **zebra**.\n",
    "* Even though \"zebra\" images were never seen, the model matches \"zebra\" with its **textual description or attributes**.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Image classification:** CLIP (OpenAI) ‚Üí Given text like \"a photo of a zebra\", it matches unseen images.\n",
    "* **Machine translation:** Translate English ‚Üí German without training directly, by pivoting through embeddings.\n",
    "* **Text-to-SQL, text-to-code:** LLMs answering queries in domains they were not explicitly trained for.\n",
    "\n",
    "---\n",
    "\n",
    "#  2. Few-Shot Learning (FSL)\n",
    "\n",
    "**Definition:**\n",
    "A model is asked to perform a task or recognize a class with **only a handful of labeled examples per class** (e.g., 1‚Äì10 samples).\n",
    "\n",
    "**Key Idea:**\n",
    " Learn to **generalize from very few examples** by leveraging **meta-learning** or pretrained representations.\n",
    "\n",
    "**How it works (intuition):**\n",
    "\n",
    "* The model is pretrained on a large dataset (like ImageNet or massive text).\n",
    "* During fine-tuning or prompting, it adapts to a new task with very few labeled examples.\n",
    "* Often done with **meta-learning approaches** (like Prototypical Networks, Matching Networks, MAML).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You want a model to recognize **penguins**, but you only have 5 penguin images.\n",
    "* A pretrained CNN + metric-learning approach can cluster embeddings, so even 5 samples are enough to define a \"penguin class\".\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Medical imaging:** Often, only a few labeled disease samples are available.\n",
    "* **LLMs:** Few-shot prompting in GPT ‚Äì give it 3‚Äì4 examples in your prompt, and it generalizes to new queries.\n",
    "* **Speech recognition:** Adapt to a new speaker with a few audio clips.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Zero-Shot vs. Few-Shot (Comparison)\n",
    "\n",
    "| Aspect                                | Zero-Shot                                               | Few-Shot                             |\n",
    "| ------------------------------------- | ------------------------------------------------------- | ------------------------------------ |\n",
    "| **Training exposure to target class** | ‚ùå Never seen                                            | ‚úÖ Seen a few labeled examples        |\n",
    "| **Auxiliary info needed**             | Semantic knowledge (text, attributes, prompts)          | Small labeled dataset                |\n",
    "| **Generalization**                    | From **descriptions/embeddings**                        | From **few examples**                |\n",
    "| **Example**                           | Classify \"zebra\" with no zebra images, only description | Classify \"zebra\" with 5 zebra images |\n",
    "| **Common in**                         | CLIP, LLMs (zero-shot QA, translation)                  | Meta-learning, LLM few-shot prompts  |\n",
    "\n",
    "---\n",
    "\n",
    "#  4. Intuitive Analogy\n",
    "\n",
    "* **Zero-Shot:** You‚Äôve never eaten \"ramen\" but someone tells you *\"it‚Äôs like noodles in soup with toppings.\"* You recognize it the first time you see it.\n",
    "* **Few-Shot:** You try \"ramen\" only 3 times ‚Üí Now you can reliably recognize ramen in the future.\n",
    "\n",
    "---\n",
    "\n",
    "In deep learning, **pretrained models + embeddings (CLIP, LLMs, transformers)** made **zero-shot and few-shot learning practical**.\n",
    "Instead of training from scratch, we now rely on **transfer learning + prompt engineering + meta-learning**.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d8016-4fa6-4c7c-ae44-e9e0195930ed",
   "metadata": {},
   "source": [
    "#  1. Zero-Shot Learning Networks\n",
    "\n",
    "These rely heavily on **pretraining + semantic embeddings** (text, attributes, prompts).\n",
    "\n",
    "###  Vision (Images)\n",
    "\n",
    "* **CLIP (OpenAI, 2021)**\n",
    "  Learns a joint embedding of image and text ‚Üí enables zero-shot image classification with natural language prompts.\n",
    "* **ALIGN (Google, 2021)**\n",
    "  Similar to CLIP, large-scale image‚Äìtext contrastive training.\n",
    "* **DeViSE (2013)**\n",
    "  Maps images into a word embedding space (Word2Vec/GloVe) for zero-shot classification.\n",
    "* **Zero-shot GANs** (e.g., *T2F, StyleGAN adaptations*)\n",
    "  Generate unseen categories based on textual descriptions.\n",
    "\n",
    "### üìñ NLP (Text)\n",
    "\n",
    "* **GPT family (GPT-3, GPT-4, etc.)**\n",
    "  Zero-shot text generation & QA with only a prompt (no labeled data).\n",
    "* **BERT + prompt-based learning**\n",
    "  With \"masked language modeling\" can adapt to unseen labels when reframed as text prediction.\n",
    "* **T5 (Text-to-Text Transfer Transformer)**\n",
    "  Zero-shot across tasks (e.g., summarization, translation) by framing everything as text-to-text.\n",
    "\n",
    "###  Multimodal\n",
    "\n",
    "* **Florence (Microsoft)** ‚Äì large multimodal foundation model.\n",
    "* **X-CLIP, BLIP, Flamingo (DeepMind)** ‚Äì multimodal models with strong zero-shot transfer.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2. Few-Shot Learning Networks\n",
    "\n",
    "These are designed to generalize from **very few examples**, often using **meta-learning** or **metric learning**.\n",
    "\n",
    "###  Vision (Images)\n",
    "\n",
    "* **Matching Networks (2016)**\n",
    "  Learn to compare query images to few labeled support examples.\n",
    "* **Prototypical Networks (2017)**\n",
    "  Represent each class by the mean embedding (\"prototype\") ‚Üí classify queries by distance to prototypes.\n",
    "* **Relation Networks (2018)**\n",
    "  Learn a similarity function between query and support samples.\n",
    "* **MAML (Model-Agnostic Meta-Learning, 2017)**\n",
    "  Learns an initialization that quickly adapts to new tasks with few gradient steps.\n",
    "* **Siamese Networks (2015)**\n",
    "  Pairwise comparison of embeddings ‚Üí good for 1-shot classification.\n",
    "\n",
    "###  NLP (Text)\n",
    "\n",
    "* **GPT-3/4 with Few-Shot Prompting**\n",
    "  Provide a few Q\\&A examples in the prompt ‚Üí model generalizes to new queries.\n",
    "* **PET (Pattern-Exploiting Training)**\n",
    "  Fine-tunes masked language models with a few examples.\n",
    "* **Meta-learning for NLP (e.g., ProtoBERT, FewShotBERT)**.\n",
    "\n",
    "###  Speech & Multimodal\n",
    "\n",
    "* **Speech few-shot**: Adaptation of wav2vec2.0 or HuBERT to a new speaker with few samples.\n",
    "* **Vision-Language few-shot**: CLIP + Prototypical networks for few-shot classification.\n",
    "\n",
    "---\n",
    "\n",
    "#  3. Summary (Networks by Type)\n",
    "\n",
    "| Type          | Networks / Models                                                                                           | Core Idea                                                                        |\n",
    "| ------------- | ----------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| **Zero-Shot** | CLIP, ALIGN, DeViSE, GPT-3/4, T5, BERT (prompted), Flamingo                                                 | Use pretrained embeddings + semantic info (text/attributes)                      |\n",
    "| **Few-Shot**  | Prototypical Networks, Matching Nets, Relation Nets, MAML, Siamese Nets, Few-Shot BERT, GPT-3 with examples | Learn to generalize from few labeled samples via meta-learning / metric learning |\n",
    "\n",
    "---\n",
    "\n",
    " **Rule of thumb:**\n",
    "\n",
    "* If you have **no samples** of the new task/class ‚Üí **Zero-Shot (CLIP, GPT-3, T5)**.\n",
    "* If you have **1‚Äì10 samples per class** ‚Üí **Few-Shot (ProtoNets, Matching Nets, MAML, or LLM prompting with examples)**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
