{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e60d011-370f-4b3c-b5fa-a243b1c349ed",
   "metadata": {},
   "source": [
    "##  What is Batch Normalization (BatchNorm)?\n",
    "\n",
    "Batch Normalization is a layer that:\n",
    "\n",
    "1. **Normalizes** activations (zero mean, unit variance) **within each batch**\n",
    "2. Helps **speed up training**, reduce **internal covariate shift**, and **stabilize learning**\n",
    "\n",
    "It‚Äôs commonly used **after a linear or conv layer**, before activation (like ReLU).\n",
    "\n",
    "---\n",
    "\n",
    "##  When Is BatchNorm Used?\n",
    "\n",
    "| Phase       | Is BatchNorm used? | How it behaves                                              |\n",
    "|-------------|---------------------|--------------------------------------------------------------|\n",
    "| **Training**    | ‚úÖ Yes                | Uses **mean and variance of current batch** for normalization. Also updates running statistics (running mean & variance). |\n",
    "| **Validation**  | ‚úÖ Yes (with `.eval()`) | Uses the **running (moving average) statistics** collected during training. |\n",
    "| **Test**        | ‚úÖ Yes (with `.eval()`) | Same as validation ‚Äì uses running stats from training.     |\n",
    "\n",
    "---\n",
    "\n",
    "###  `model.train()` ‚ûú Training Mode\n",
    "\n",
    "In this mode, **BatchNorm**:\n",
    "\n",
    "- Normalizes using **batch statistics** (mean & variance of the current mini-batch).\n",
    "- Updates its **running averages** (for later use during evaluation/test).\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "# x -> BatchNorm -> Normalize with batch mean/var\n",
    "#                  Update running_mean and running_var\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  `model.eval()` ‚ûú Evaluation (Validation/Test) Mode\n",
    "\n",
    "In this mode, **BatchNorm**:\n",
    "\n",
    "- Normalizes using the **stored running statistics**.\n",
    "- **Does NOT** update running stats.\n",
    "- Gives consistent output for the same input (no randomness).\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "# x -> BatchNorm -> Normalize with running_mean/var (from training)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Why This Matters\n",
    "\n",
    "Let‚Äôs say your batch size is small. Then batch statistics can vary a lot from batch to batch during training ‚Äî that‚Äôs fine. But during validation or test, we want stable and **consistent behavior** ‚Äî hence we use running averages.\n",
    "\n",
    "If you forget to switch to `.eval()`, BatchNorm may behave inconsistently ‚Äî using tiny validation/test batches' mean/variance ‚Äî and results will be wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Mode         | Use BatchNorm? | Uses What Stats?            | Updates Running Stats? |\n",
    "|--------------|----------------|------------------------------|-------------------------|\n",
    "| Training     | Yes            | Current batch mean/variance | ‚úÖ Yes                  |\n",
    "| Validation   | Yes            | Running mean/variance       | ‚ùå No                   |\n",
    "| Testing      | Yes            | Running mean/variance       | ‚ùå No                   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa5bc1-3987-4f62-9520-c9380f85f1db",
   "metadata": {},
   "source": [
    "\n",
    "##  **BatchNorm ‚Äì Equation (Training Phase)**\n",
    "\n",
    "Given an input batch $ x = [x_1, x_2, ..., x_m] $, BatchNorm does the following:\n",
    "\n",
    "1. **Compute batch mean and variance**:\n",
    "   $\n",
    "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i,\\quad\n",
    "   \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "   $\n",
    "\n",
    "2. **Normalize each input**:\n",
    "   $\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "   $\n",
    "\n",
    "3. **Scale and shift** (learnable parameters $ \\gamma $, $ \\beta $):\n",
    "   $   y_i = \\gamma \\hat{x}_i + \\beta    $\n",
    "\n",
    "  During training, $ \\mu_B $ and $ \\sigma_B^2 $ come **from the current mini-batch**, and we **update the running estimates**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Running Statistics (for Evaluation)\n",
    "\n",
    "To make the model deterministic during validation/testing, we maintain a **running average** of the batch statistics:\n",
    "\n",
    "- Running mean:\n",
    "  $\n",
    "  \\mu_{running} \\leftarrow (1 - \\alpha) \\cdot \\mu_{running} + \\alpha \\cdot \\mu_B\n",
    "  $\n",
    "\n",
    "- Running variance:\n",
    "  $\n",
    "  \\sigma^2_{running} \\leftarrow (1 - \\alpha) \\cdot \\sigma^2_{running} + \\alpha \\cdot \\sigma_B^2\n",
    "  $\n",
    "\n",
    "Where:\n",
    "- $ \\alpha $ is a momentum term (e.g., 0.1)\n",
    "- These are **not learned**, but updated during training\n",
    "\n",
    "---\n",
    "\n",
    "##  **During Validation / Testing**\n",
    "\n",
    "We do not use the current batch statistics. Instead:\n",
    "\n",
    "$ \\hat{x}_i = \\frac{x_i - \\mu_{running}}{\\sqrt{\\sigma^2_{running} + \\epsilon}} $\n",
    "\n",
    "$ y_i = \\gamma \\hat{x}_i + \\beta $\n",
    "\n",
    "> ‚ùå No updates to mean/variance  \n",
    "> ‚úÖ Use consistent stats collected during training\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Side-by-Side Comparison\n",
    "\n",
    "| Step               | Training Mode                              | Eval Mode                        |\n",
    "|--------------------|---------------------------------------------|-----------------------------------|\n",
    "| Normalization mean | Batch mean \\( \\mu_B \\)                      | Running mean \\( \\mu_{running} \\) |\n",
    "| Normalization var  | Batch variance \\( \\sigma_B^2 \\)             | Running var \\( \\sigma^2_{running} \\) |\n",
    "| Updates stats?     | ‚úÖ Yes (for running mean/var)               | ‚ùå No                             |\n",
    "| Behavior           | Noisy (batch-dependent)                     | Stable (global stats)            |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example (Python-style, simplified)\n",
    "\n",
    "Here‚Äôs a small NumPy example to simulate this behavior:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Simulated inputs: batch of size 4\n",
    "x_batch = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# --- Training phase ---\n",
    "mu_B = x_batch.mean()\n",
    "var_B = x_batch.var()\n",
    "epsilon = 1e-5\n",
    "x_hat = (x_batch - mu_B) / np.sqrt(var_B + epsilon)\n",
    "\n",
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "y_batch_train = gamma * x_hat + beta\n",
    "\n",
    "# Running statistics update\n",
    "mu_running = 0.0\n",
    "var_running = 1.0\n",
    "momentum = 0.1\n",
    "mu_running = (1 - momentum) * mu_running + momentum * mu_B\n",
    "var_running = (1 - momentum) * var_running + momentum * var_B\n",
    "\n",
    "# --- Evaluation phase ---\n",
    "x_hat_eval = (x_batch - mu_running) / np.sqrt(var_running + epsilon)\n",
    "y_batch_eval = gamma * x_hat_eval + beta\n",
    "\n",
    "print(\"Training output:\", y_batch_train)\n",
    "print(\"Evaluation output:\", y_batch_eval)\n",
    "```\n",
    "\n",
    "### Sample Output:\n",
    "```\n",
    "Training output: [-1.3416 -0.4472  0.4472  1.3416]\n",
    "Evaluation output: [-0.4629  0.2314  0.9258  1.6202]\n",
    "```\n",
    "\n",
    "You can see the **difference in output** due to different normalization stats.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5412de-4386-44cc-ba3e-3fff7ff68d2a",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "We want to keep the output of activation function unit gaussian, so we make them gaussian. In the process we are not normalizing weight but the output of activations.\n",
    "\n",
    "\n",
    "Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.\n",
    "\n",
    "You want to be able how much saturation you have:\n",
    "bth of them are learnable \n",
    "$\\gamma$ and $\\beta$\n",
    "\n",
    "[1](https://youtu.be/wEoyxE0GP2M?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&t=2933),\n",
    "[2](https://arxiv.org/abs/1502.03167),\n",
    "[3](https://www.youtube.com/watch?v=DtEq44FTPM4), [4](https://www.youtube.com/watch?v=dXB-KQYkzNU), [5](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "\n",
    "## Batch Norm with PyTorch\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
