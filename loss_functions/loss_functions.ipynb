{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring function: $|\\textbf{W}.\\textbf{X}_i+b|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean Absolute Error Loss\n",
    "Treats outlier like anyother point so it won't go out of its way for outliers which might lead to poor prediction from time to time. Also it is not optimized for gradient descent (due to incontinuty on zero)Support vector regression use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Squared Error Loss\n",
    "The adventage is we can easily compute gradient for ML. Good for regression but outlier will make it a poor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pseudo-Huber Loss\n",
    "Mean Absolute Error doenst comply with outlier and Mean Squared Error freaks out with outlier. If your data is 70% on one side and 30% on the other side, both will reult in a poor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Welsch (Leclerc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Geman McClure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Causchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Cross-Entropy Loss\n",
    "When we use <b>sigmoid</b> as activation function and <b>Mean Squared Error Loss</b>, if our neuron output is very wrong ,since $\\sigma(z)$ s saturated and it is almost a flat line ($z \\to \\infty$, $\\sigma(z) \\to 1$ and $z \\to -\\infty$, $\\sigma(z) \\to 0$) , $\\sigma'(z)$ gives us a small value so learning will be very slow. \n",
    "<b>Cross-Entropy Loss</b> designed for binary classifiers. In a binary classifier,  $y \\in \\{0,1\\}$ is the class lable and $a$ is the output of the activation function (could be any number).\n",
    "\n",
    "\n",
    "$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z)\\end{eqnarray}$\n",
    "  \n",
    "$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z),\\end{eqnarray}$\n",
    "\n",
    "Let say we have binary classifier, so we have 1 output node and the class labeles are either 1 or 0, \n",
    "$C=-\\frac{1}{n} \\sum_{x} y\\ln(a)+ (1-y)\\ln(1-a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Multi-Class Cross-Entropy Loss\n",
    "If we have $j$ output in the last layer $L$, the total cost for all inputs is:\n",
    "\n",
    "$\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\n",
    "  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\n",
    "\\end{eqnarray} $\n",
    "\n",
    "One way to remmeber that the order is  $ y_j \\ln a^L_j$ and not other way around is to remmeber that $y_j$ could be \n",
    "absoulute zero so it can't be the parameter for logarithm function, but $a^L_j$ can $\\lim a^L_j \\to 0 $, so it is the parameter of $\\ln$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the the $\\delta$ when the loss function is **Multi-Class Cross-Entropy Loss** and activation function is **sigmoid**:\n",
    "\n",
    "\n",
    "$\\delta_j=\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial z_j} &=\\frac{\\partial C}{\\partial a_j}\\frac{\\partial a_j}{\\partial z_j}   = & -\\frac{1}{n} \\sum_x \\left(\n",
    "    \\frac{y }{\\sigma(z_j)} -\\frac{(1-y)}{1-\\sigma(z_j)} \\right)\\sigma'(z_j) \n",
    "& = & -\\frac{1}{n} \\sum_x \\left( \n",
    "    \\frac{y -\\sigma(z_j)}{\\sigma(z_j)(1-\\sigma(z_j))}\n",
    "    \\right).\\sigma(z_j)(1-\\sigma(z_j))\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Reminder: for sigmoid activation function:\n",
    "\n",
    "$\\sigma'(z_j)=\\sigma(z_j)(1-\\sigma(z_j))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we would have:\n",
    "\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "    \\delta^L = a^L-y.\\tag 1\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Reminder: in quadratic loss function, $\\delta$ was:\n",
    "\n",
    "$\\delta^L= 2(\\textbf{a}^{(L)}-\\textbf{y}) \\odot   \\sigma^{\\prime}(\\textbf{z}^{(L)})$\n",
    "\n",
    "For previous layer $L-1, L-2,...$, the $\\delta^{(L-1)}$ is same as wath we computed before in network with quadratic loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{eqnarray} \n",
    "      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \n",
    "      a^{L-1}_k  (a^L_j-y_j) \\tag 2\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the rate at which the weight learns is controlled by $\\sigma(z)-y$ (by the error in the output). The larger the error, the faster the neuron will learn. This avoids the learning slowdown caused by the $\\sigma '(z)$ term in the analogous equation for the quadratic cost,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{eqnarray}\n",
    "      \\frac{\\partial C}{\\partial b^L_{j}} & = & \\frac{1}{n} \\sum_x \n",
    "      (a^L_j-y_j).\n",
    "  \\tag{3}\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Negative log likelihood\n",
    "\n",
    "The <b>Negative Log-Likelihood Loss function (NLL)</b> is applied only on models with the softmax function as an output activation layer. We can think a softmax output layer with log-likelihood cost as being quite similar to a sigmoid output layer with cross-entropy cost.\n",
    "\n",
    "The Softmax function is :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}}$\n",
    "\n",
    "\n",
    "The softmax function takes an input vector of size $N$, and then modifies the values such that every one of them falls between 0 and 1.\n",
    "\n",
    "\n",
    "$NLL$ uses a negative connotation since the probabilities (or likelihoods) vary between zero and one, and the logarithms of values in this range are negative. In the end, the loss value becomes positive. The negative log likelihood is retrieved from approximating the maximum likelihood estimation $MLE$. This means that we try to maximize the model’s log likelihood, and as a result, minimize the $NLL$.  \n",
    "\n",
    "Neural network estimate: \n",
    "$f(X)_c=p(y=c|X)$\n",
    "\n",
    "Which means NN estimate the probability that the output of network, $y$ for the given vector $X$ is the correct class $c$.\n",
    "For instance we have 3 classes, which means 3 nodes at the output layer, and for a given input we get:\n",
    "$(0.1,0.85,0.05 )$, which means which means our network belives with the probability of $0.85$ this input belong to the second class. Since the inputs are independent of each other the joint probability of them would be teh multipication of them just like $MLE$. Since output probabilities are betwen zero and one the $ln$ of them would be a negative value, so we turn them into positive number by multiplying tem by a negative number and we turn maximisation problem into minimization.\n",
    "\n",
    "$l(\\textbf{f}(X),y)=-ln f(X)_c=-\\Sigma_c 1_{y=c}ln(f(X)_c)$\n",
    "\n",
    "\n",
    "Refs:\n",
    "\n",
    "[1](https://www.youtube.com/watch?v=PpFTODTztsU), [2](https://neptune.ai/blog/pytorch-loss-functions),\n",
    "[3](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Hinge Loss\n",
    "Let's say we have binary classifier with two labels, $-1,+1$ to measure the misclassification error:\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq  sign(f(x_i))] = \\frac{1}{n}\\sum_{i=1}^{n}[y_i f(x_i)] \\leq \\frac{1}{n}\\sum_{i=1}^{n}L(y_i f(x_i)) $\n",
    "\n",
    "minimizing this error function is computationaly complicated so \n",
    "\n",
    "Mainly used in SVMm which set a boundry as far as possible between all data points (maximizes the minimum margin).\n",
    "It will penalized the points even they are in the margin.\n",
    "\n",
    "\n",
    "scoring function: $|\\textbf{W}.\\textbf{X}_i+b|$\n",
    "\n",
    "Training test label, $y_i = ±1$\n",
    "\n",
    "$L_i=max(0,1-y_i|\\textbf{W}.\\textbf{X}_i+b|)$\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=r-vYJqcFxBI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/hinge.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Loss Functions\n",
    "All the abve loss functions can be generalized by:\n",
    "\n",
    "$p(x,\\alpha)=\\frac{|2- \\alpha|}{\\alpha} \\left (   \\left ( \\frac{x^2}{|2-\\alpha|}+1  \\right )^{\\alpha/2}  -1 \\right )  $\n",
    "\n",
    "\n",
    "$NLL(\\theta,\\alpha)=min \\rho_{\\theta,\\alpha} (x,\\alpha )+logZ(\\alpha) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://arxiv.org/abs/1701.03077), [2](https://www.youtube.com/watch?v=QBbC3Cjsnjg), [3](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7), [4](https://rohanvarma.me/Loss-Functions/), [5](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23), [6](https://neptune.ai/blog/pytorch-loss-functions), [7](https://www.youtube.com/watch?v=ErfnhcEV1O8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
