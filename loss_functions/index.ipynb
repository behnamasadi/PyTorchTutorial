{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  loss tensor:\n",
    "\n",
    "`nn.CrossEntropyLoss()` by default computes the **mean** of the per-sample losses:\n",
    "\n",
    "```python\n",
    "loss = torch.nn.CrossEntropyLoss()  # default: reduction='mean'\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "```python\n",
    "loss.item() = sum of individual losses / batch size\n",
    "```\n",
    "\n",
    "So if your batch has 10 samples, and the individual losses are:\n",
    "\n",
    "```python\n",
    "[1.1, 0.8, 1.0, ..., 0.9]  # 10 values\n",
    "```\n",
    "\n",
    "Then the returned `loss` will be the **average**, e.g. `0.95`.\n",
    "\n",
    "---\n",
    "\n",
    "**If You Want Total Loss Over Epoch**\n",
    "\n",
    "You can choose:\n",
    "\n",
    "* **Use `reduction='sum'`** and accumulate directly\n",
    "* **Stick with mean**, but multiply by batch size to compute total loss:\n",
    "\n",
    "```python\n",
    "loss = criterion(outputs_batch, labels_batch)\n",
    "running_loss += loss.item() * inputs_batch.size(0)  # batch_size\n",
    "```\n",
    "\n",
    "This is useful when you want total loss across all samples in the epoch.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice for Epoch-Level Loss:**\n",
    "\n",
    "```python\n",
    "epoch_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "for inputs_batch, labels_batch in train_loader:\n",
    "    outputs_batch = model(inputs_batch)\n",
    "    loss = criterion(outputs_batch, labels_batch)\n",
    "    \n",
    "    batch_size = inputs_batch.size(0)\n",
    "    epoch_loss += loss.item() * batch_size  # accumulate total loss\n",
    "    total_samples += batch_size\n",
    "\n",
    "# average loss for the epoch\n",
    "epoch_loss = epoch_loss / total_samples\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_batch:\n",
      " tensor([[ 0.0249, -0.3460],\n",
      "        [ 2.2181,  0.5232],\n",
      "        [ 0.3125, -0.0335],\n",
      "        [ 0.1748, -1.0939],\n",
      "        [ 0.0109, -0.3387],\n",
      "        [-0.6867,  0.6368],\n",
      "        [ 0.9007, -2.1055],\n",
      "        [ 1.0608,  0.2083],\n",
      "        [ 1.0950,  0.3399],\n",
      "        [-0.7521,  1.6487]])\n",
      "labels_batch:\n",
      " tensor([2, 2, 3, 3, 2, 0, 0, 3, 0, 2])\n",
      "------------------------------------------------------\n",
      "Loss: 1.4052388668060303\n",
      "Loss: tensor(1.4052, grad_fn=<NllLossBackward0>)\n",
      "------------------------------------------------------\n",
      "outputs_batch.shape:  torch.Size([10, 4])\n",
      "outputs_batch:\n",
      " tensor([[-0.2916, -0.1549,  0.1629, -0.0074],\n",
      "        [ 0.2972,  0.5286,  0.1931, -0.1649],\n",
      "        [-0.2981, -0.0999,  0.1152, -0.0366],\n",
      "        [-0.1561, -0.2563,  0.3386,  0.0204],\n",
      "        [-0.2915, -0.1556,  0.1632, -0.0069],\n",
      "        [-0.3373, -0.1296,  0.0648,  0.0202],\n",
      "        [-0.0698, -0.2705,  0.3611,  0.1934],\n",
      "        [-0.0946,  0.1462,  0.1615, -0.1537],\n",
      "        [-0.0589,  0.1862,  0.1649, -0.1639],\n",
      "        [-0.1454,  0.0332,  0.0704,  0.0489]], grad_fn=<AddmmBackward0>)\n",
      "------------------------------------------------------\n",
      "labels_batch:\n",
      " tensor([2, 2, 3, 3, 2, 0, 0, 3, 0, 2])\n",
      "predicted:\n",
      " tensor([2, 1, 2, 2, 2, 2, 2, 2, 1, 2])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class MyCustomDataSet(Dataset):\n",
    "    def __init__(self, data, lables):\n",
    "        self.data = data\n",
    "        self.lables = lables\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.lables[idx]\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "sample_size = 100\n",
    "number_of_classes = 4\n",
    "data_dim = 2\n",
    "data = torch.randn(sample_size, data_dim)\n",
    "lables = torch.randint(0, number_of_classes, (sample_size,))\n",
    "\n",
    "dataset = MyCustomDataSet(data=data, lables=lables)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "val_size = int(0.15*len(dataset))\n",
    "test_size = len(dataset)-train_size-val_size\n",
    "\n",
    "dataset = MyCustomDataSet(data=data, lables=lables)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.relu(self.fc1(input))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = MyCustomModel()\n",
    "\n",
    "optimizer = optim.AdamW(params=model.parameters(),\n",
    "                        lr=1e-3,   weight_decay=1e-2)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs_batch, labels_batch) in enumerate(train_loader):\n",
    "        print(\"inputs_batch:\\n\", inputs_batch)\n",
    "        print(\"labels_batch:\\n\", labels_batch)\n",
    "\n",
    "        batch_size = inputs_batch.size(0)\n",
    "\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "        outputs_batch = model(inputs_batch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(outputs_batch, labels_batch)\n",
    "        print(\"Loss:\", loss.item())\n",
    "        print(\"Loss:\", loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"outputs_batch.shape: \", outputs_batch.shape)\n",
    "\n",
    "        print(\"outputs_batch:\\n\", outputs_batch)\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"labels_batch:\\n\", labels_batch)\n",
    "        _, predicted = torch.max(outputs_batch, 1)\n",
    "\n",
    "        print(\"predicted:\\n\", predicted)\n",
    "\n",
    "        print((predicted == labels_batch).sum().item())\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Classification Loss Functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Task Type                  | PyTorch Loss Function             | Target Shape          | Output Shape          |\n",
    "| -------------------------- | --------------------------------- | --------------------- | --------------------- |\n",
    "| Binary Classification      | `nn.BCEWithLogitsLoss`            | `[B, 1]` or `[B]`     | Sigmoid/logits, `[B]` |\n",
    "| Multi-Class (single label) | `nn.CrossEntropyLoss`             | `[B]` (class indices) | Raw logits, `[B, C]`  |\n",
    "| Multi-Label (multi-class)  | `nn.BCEWithLogitsLoss`            | `[B, C]`              | Logits, `[B, C]`      |\n",
    "| Probabilistic models       | `nn.NLLLoss` (with `log_softmax`) | `[B]`                 | Log-probabilities     |\n",
    "| Custom metric learning     | `nn.TripletMarginLoss`, etc.      | depends               | depends               |\n",
    "\n",
    "---\n",
    "\n",
    "### Inside the Training/Eval Loop\n",
    "\n",
    "\n",
    "```python\n",
    "# TRAINING\n",
    "model.train()\n",
    "for images, labels in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)         # shape [B, C] for multi-class\n",
    "    loss = criterion(outputs, labels)  # criterion = e.g., nn.CrossEntropyLoss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# EVALUATION\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # collect predictions for accuracy, precision, etc.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The **loss** is a scalar value computed by comparing the model’s output (predictions) with the ground truth (labels). The choice of loss function depends on the task type (binary, multi-class, multi-label).\n",
    "\n",
    "\n",
    "\n",
    "* `images.size(0)` is the **batch size**, i.e., the number of samples in the current mini-batch.\n",
    "* `loss.item()` gives the **scalar average loss per sample** in that batch (computed by PyTorch internally).\n",
    "* So `loss.item() * images.size(0)` gives the **total loss for that batch** (i.e., sum over all samples in the batch).\n",
    "\n",
    "**Therefore:**\n",
    "\n",
    "```python\n",
    "total_loss += loss.item() * images.size(0)\n",
    "```\n",
    "\n",
    "is **accumulating the total loss over the entire dataset**, not averaging yet.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Average Loss for the Epoch**\n",
    "\n",
    "To compute the **epoch average**, you divide at the end:\n",
    "\n",
    "```python\n",
    "epoch_loss = total_loss / dataset_size  # dataset_size = len(train_dataset)\n",
    "```\n",
    "\n",
    "###  Summary\n",
    "\n",
    "| Variable         | Type   | Meaning                           |\n",
    "| ---------------- | ------ | --------------------------------- |\n",
    "| `loss`           | Tensor | Mean loss for current batch       |\n",
    "| `loss.item()`    | float  | Scalar value of batch mean loss   |\n",
    "| `images.size(0)` | int    | Number of samples in the batch    |\n",
    "| `total_loss`     | float  | Accumulated (summed) batch losses |\n",
    "| `epoch_loss`     | float  | Average loss over the whole epoch |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Best Practices for Loss Calculation\n",
    "\n",
    "1. **Use correct output format:**\n",
    "\n",
    "   * For `CrossEntropyLoss`: model must output *raw logits* (no softmax needed).\n",
    "   * For `BCEWithLogitsLoss`: model must output *raw logits* (no sigmoid).\n",
    "\n",
    "2. **Do not use `.item()` inside training loop** unless you’re logging.\n",
    "\n",
    "   ```python\n",
    "   total_loss += loss.item() * images.size(0)  # accumulate loss for avg\n",
    "   ```\n",
    "\n",
    "3. **Track batch-wise loss properly** (for logging or early stopping):\n",
    "\n",
    "   ```python\n",
    "   running_loss += loss.item() * batch_size\n",
    "   epoch_loss = running_loss / dataset_size\n",
    "   ```\n",
    "\n",
    "4. **Early stopping/monitoring:**\n",
    "\n",
    "   * Use validation loss to monitor overfitting.\n",
    "   * Save the best model using:\n",
    "\n",
    "     ```python\n",
    "     if val_loss < best_val_loss:\n",
    "         torch.save(model.state_dict(), \"best_model.pth\")\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### Custom Loss \n",
    "\n",
    "Example: Label smoothing for classification\n",
    "\n",
    "```python\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_class = pred.size(1)\n",
    "        log_probs = F.log_softmax(pred, dim=1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Working Example (PyTorch)\n",
    "\n",
    "* Model definition\n",
    "* Training & validation loop\n",
    "* Calculation of:\n",
    "\n",
    "  * Loss\n",
    "  * Accuracy\n",
    "  * Precision, Recall, F1-score (per epoch)\n",
    "\n",
    "We'll use **multi-class classification** as it's more general (e.g., 3 classes).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 🔹 Create synthetic dataset: 100 samples, 5 features, 3 classes\n",
    "num_samples = 100\n",
    "num_features = 5\n",
    "num_classes = 3\n",
    "\n",
    "X = torch.randn(num_samples, num_features)\n",
    "y = torch.randint(0, num_classes, (num_samples,))  # Labels: 0, 1, 2\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "train_set, val_set = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "# 🔹 Define a simple model\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleClassifier(num_features, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Training + Validation with Metrics\n",
    "\n",
    "```python\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds, train_labels = [], []\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)  # shape [B, C]\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * y_batch.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_preds.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate train metrics\n",
    "    train_loss /= len(train_set)\n",
    "    train_acc = sum(torch.tensor(train_preds) == torch.tensor(train_labels)) / len(train_labels)\n",
    "    train_precision = precision_score(train_labels, train_preds, average='macro')\n",
    "    train_recall = recall_score(train_labels, train_preds, average='macro')\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    # 🔹 Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * y_batch.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_set)\n",
    "    val_acc = sum(torch.tensor(val_preds) == torch.tensor(val_labels)) / len(val_labels)\n",
    "    val_precision = precision_score(val_labels, val_preds, average='macro')\n",
    "    val_recall = recall_score(val_labels, val_preds, average='macro')\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "    # 🔸 Print results\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f} | Prec: {train_precision:.2f} | Rec: {train_recall:.2f} | F1: {train_f1:.2f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.2f} | Prec: {val_precision:.2f} | Rec: {val_recall:.2f} | F1: {val_f1:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This code will:\n",
    "\n",
    "* Train a simple linear classifier\n",
    "* Evaluate on both training and validation sets\n",
    "* Compute and print:\n",
    "\n",
    "  * Loss (mean)\n",
    "  * Accuracy\n",
    "  * Precision, Recall, F1-score (macro-averaged across classes)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this line:\n",
    "\n",
    "```python\n",
    "_, predicted = torch.max(preds, 1)\n",
    "```\n",
    "\n",
    "is **standard for multi-class classification** where `preds` is of shape `[B, C]`, with **raw logits** per class.\n",
    "\n",
    "Let me explain and then contrast with binary classification:\n",
    "\n",
    "---\n",
    "\n",
    "###  `torch.max(preds, 1)` — what it does:\n",
    "\n",
    "* `preds`: Tensor of shape `[B, C]` (batch size × number of classes).\n",
    "* `torch.max(preds, 1)` returns a tuple:\n",
    "\n",
    "  * `values`: max logit per sample → not used (that's the `_`)\n",
    "  * `indices`: **predicted class index** for each sample.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "_, predicted = torch.max(preds, 1)  # predicted shape: [B]\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "correct += (predicted == y).sum().item()\n",
    "```\n",
    "\n",
    "compares predicted class indices to true labels and counts the number of correct predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Does it change for **multi-class** vs **binary**?\n",
    "\n",
    "| Case                      | Model Output Shape | Criterion                | Prediction logic        |\n",
    "| ------------------------- | ------------------ | ------------------------ | ----------------------- |\n",
    "| **Multi-class** (e.g. 3+) | `[B, C]`           | `nn.CrossEntropyLoss()`  | `torch.max(preds, 1)`   |\n",
    "| **Binary** (2 classes)    | `[B]` or `[B, 1]`  | `nn.BCEWithLogitsLoss()` | `preds.sigmoid() > 0.5` |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 For Binary Classification (with `BCEWithLogitsLoss`)\n",
    "\n",
    "```python\n",
    "# Model output is [B], raw logits\n",
    "preds = model(x)               # shape [B]\n",
    "probs = torch.sigmoid(preds)   # shape [B]\n",
    "predicted = (probs > 0.5).long()\n",
    "correct += (predicted == y).sum().item()\n",
    "```\n",
    "\n",
    "> Note: `y` should be float/int with shape `[B]` or `[B, 1]` matching the output.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "\n",
    "| Type              | Output (from model) | Target `y`          | Prediction Rule                             |\n",
    "| ----------------- | ------------------- | ------------------- | ------------------------------------------- |\n",
    "| Multi-class       | `[B, C]` logits     | class indices `[B]` | `_, predicted = torch.max(preds, 1)`        |\n",
    "| Binary (1 output) | `[B]` logits        | 0 or 1              | `predicted = (sigmoid(preds) > 0.5).long()` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodesic loss\n",
    "\n",
    "**Geodesic loss** is a loss function used in deep learning when predicting **rotations**, especially **3D rotations**, like in camera pose estimation, object orientation prediction, or visual odometry. It measures the \"shortest distance\" between two rotations on the **manifold of 3D rotations**, i.e., **SO(3)** (the Special Orthogonal group of 3×3 rotation matrices).\n",
    "\n",
    "---\n",
    "\n",
    "##  What Is Geodesic Loss?\n",
    "\n",
    "###  Problem:\n",
    "\n",
    "Suppose you have:\n",
    "\n",
    "* Ground truth rotation: $R_{\\text{gt}} \\in SO(3)$\n",
    "* Predicted rotation: $R_{\\text{pred}} \\in SO(3)$\n",
    "\n",
    "You want to measure how \"far\" these two are on the **rotation manifold**.\n",
    "\n",
    "###  Geodesic Loss (in radians):\n",
    "\n",
    "The geodesic distance $\\theta$ between two rotation matrices is:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1} \\left( \\frac{\\text{trace}(R_{\\text{gt}}^\\top R_{\\text{pred}}) - 1}{2} \\right)\n",
    "$$\n",
    "\n",
    "This computes the **angle** between the two rotations.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Not Use L2 Loss?\n",
    "\n",
    "Rotation matrices (or quaternions) lie on a curved manifold (non-Euclidean space). Using naive L2 loss between matrices or quaternions can:\n",
    "\n",
    "* Be misleading,\n",
    "* Fail to respect rotation constraints,\n",
    "* Lead to worse convergence.\n",
    "\n",
    "---\n",
    "\n",
    "##  PyTorch Implementation (Rotation Matrices)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    \"\"\"\n",
    "    Computes geodesic loss (in radians) between two rotation matrices.\n",
    "    R_pred, R_gt: shape (..., 3, 3)\n",
    "    Returns: loss value (mean geodesic distance)\n",
    "    \"\"\"\n",
    "    batch_size = R_pred.size(0)\n",
    "    R_diff = torch.matmul(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "    # Clamp for numerical stability\n",
    "    cos_theta = (trace - 1) / 2\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n",
    "    theta = torch.acos(cos_theta)\n",
    "    return theta.mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  If You Use Quaternions Instead\n",
    "\n",
    "If rotations are represented as **unit quaternions** $q_1, q_2$, you can use:\n",
    "\n",
    "$$\n",
    "\\theta = 2 \\cos^{-1}(|q_1 \\cdot q_2|)\n",
    "$$\n",
    "\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "def geodesic_loss_quaternion(q_pred, q_gt):\n",
    "    \"\"\"\n",
    "    q_pred, q_gt: shape (B, 4), assumed normalized\n",
    "    \"\"\"\n",
    "    dot = torch.sum(q_pred * q_gt, dim=1)\n",
    "    dot = torch.abs(dot)  # account for double cover: q and -q are same\n",
    "    dot = torch.clamp(dot, -1.0, 1.0)\n",
    "    theta = 2 * torch.acos(dot)\n",
    "    return theta.mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  When to Use Geodesic Loss\n",
    "\n",
    "Use geodesic loss when your network **outputs a rotation**, and:\n",
    "\n",
    "* You care about **angular difference** between predicted and ground truth rotation.\n",
    "* You use **rotation matrices**, **quaternions**, or **axis-angle** representations.\n",
    "* Tasks like:\n",
    "\n",
    "  * Visual odometry (rotation error),\n",
    "  * Camera pose regression,\n",
    "  * Orientation prediction in robotics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  **Rotations and Relative Rotation**\n",
    "\n",
    "Given two rotation matrices:\n",
    "\n",
    "* $R_{\\text{gt}}$: ground truth\n",
    "* $R_{\\text{pred}}$: predicted\n",
    "\n",
    "Then the **relative rotation** from prediction to ground truth is:\n",
    "\n",
    "$$\n",
    "R_{\\text{rel}} = R_{\\text{gt}}^\\top R_{\\text{pred}}\n",
    "$$\n",
    "\n",
    "If $R_{\\text{pred}} = R_{\\text{gt}}$, then:\n",
    "\n",
    "$$\n",
    "R_{\\text{rel}} = R_{\\text{gt}}^\\top R_{\\text{gt}} = I\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\text{trace}(R_{\\text{rel}}) = \\text{trace}(I) = 3\n",
    "$$\n",
    "\n",
    "So yes, **when two rotations are close, the relative rotation is close to identity**, and the trace is close to 3.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Geodesic Angle from Trace**\n",
    "\n",
    "From that relative rotation, the **angle** $\\theta$ between rotations is:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1} \\left( \\frac{\\text{trace}(R_{\\text{rel}}) - 1}{2} \\right)\n",
    "$$\n",
    "\n",
    "This comes from the Rodrigues' rotation formula, which states:\n",
    "\n",
    "$$\n",
    "\\text{trace}(R) = 1 + 2 \\cos(\\theta)\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\cos(\\theta) = \\frac{\\text{trace}(R) - 1}{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Example\n",
    "\n",
    "If:\n",
    "\n",
    "* $\\text{trace}(R_{\\text{gt}}^\\top R_{\\text{pred}}) = 2.95$\n",
    "* then:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{2.95 - 1}{2} = 0.975\n",
    "\\Rightarrow \\theta \\approx \\cos^{-1}(0.975) \\approx 12.9^\\circ\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Intuition\n",
    "\n",
    "* **Trace close to 3** → $R_{\\text{pred}}$ ≈ $R_{\\text{gt}}$\n",
    "* **Trace = -1** → 180° rotation difference\n",
    "* **Trace = 1** → 90° rotation difference\n",
    "* The geodesic loss converts this trace into a **rotation angle**, which is a proper metric on **SO(3)**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Why is $\\theta = 0$ when the trace is 3?\n",
    "\n",
    "Actually, **it *is* zero**.\n",
    "\n",
    "Let’s walk through it **step by step**.\n",
    "\n",
    "---\n",
    "\n",
    "###  1. If the two rotations are exactly equal:\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "R_{\\text{gt}}^\\top R_{\\text{pred}} = R^\\top R = I\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\text{trace}(I) = 3\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Plug into geodesic loss:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1}\\left(\\frac{\\text{trace}(R_{\\text{gt}}^\\top R_{\\text{pred}}) - 1}{2}\\right)\n",
    "= \\cos^{-1}\\left(\\frac{3 - 1}{2}\\right) = \\cos^{-1}(1) = 0\n",
    "$$\n",
    "\n",
    " **So yes — when the matrices are equal, $\\theta = 0$**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Recap of the formula:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1} \\left( \\frac{\\text{trace}(R_{\\text{gt}}^\\top R_{\\text{pred}}) - 1}{2} \\right)\n",
    "$$\n",
    "\n",
    "* If rotations are **identical**, trace = 3 → $\\theta = \\cos^{-1}(1) = 0$\n",
    "* If rotations differ by 180°, trace = –1 → $\\theta = \\cos^{-1}(-1) = \\pi$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##  Geodesic Distance Between Quaternions\n",
    "\n",
    "Given:\n",
    "\n",
    "* $q_{\\text{gt}} \\in \\mathbb{R}^4$: ground truth unit quaternion\n",
    "* $q_{\\text{pred}} \\in \\mathbb{R}^4$: predicted unit quaternion\n",
    "\n",
    "###  The geodesic distance is defined as:\n",
    "\n",
    "$$\n",
    "\\theta = 2 \\cos^{-1}(|q_{\\text{gt}} \\cdot q_{\\text{pred}}|)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $q_{\\text{gt}} \\cdot q_{\\text{pred}}$ is the **dot product**\n",
    "* The **absolute value** accounts for the double-cover property (explained below)\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Absolute Value?\n",
    "\n",
    "Quaternions have a **double cover** of SO(3), meaning:\n",
    "\n",
    "* $q$ and $-q$ represent the **same** rotation.\n",
    "\n",
    "So to get the minimal rotation angle between two quaternions, we take:\n",
    "\n",
    "$$\n",
    "|q_{\\text{gt}} \\cdot q_{\\text{pred}}|\n",
    "$$\n",
    "\n",
    "This ensures the angle is in $[0, \\pi]$.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Intuition\n",
    "\n",
    "Let’s examine some special cases:\n",
    "\n",
    "###  Case 1: Perfect Match\n",
    "\n",
    "If $q_{\\text{pred}} = q_{\\text{gt}}$, then:\n",
    "\n",
    "$$\n",
    "q_{\\text{gt}} \\cdot q_{\\text{pred}} = 1 \\Rightarrow \\theta = 2 \\cos^{-1}(1) = 0\n",
    "$$\n",
    "\n",
    " So yes — **zero angle when quaternions are identical**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Case 2: Opposite Quaternions (still same rotation)\n",
    "\n",
    "If $q_{\\text{pred}} = -q_{\\text{gt}}$, then:\n",
    "\n",
    "$$\n",
    "q_{\\text{gt}} \\cdot q_{\\text{pred}} = -1 \\Rightarrow \\theta = 2 \\cos^{-1}(|-1|) = 2 \\cos^{-1}(1) = 0\n",
    "$$\n",
    "\n",
    " Still zero — this reflects that $q$ and $-q$ are **physically the same rotation**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Case 3: 90° Rotation Between\n",
    "\n",
    "Suppose the angle between them is 90°:\n",
    "\n",
    "$$\n",
    "|q_{\\text{gt}} \\cdot q_{\\text{pred}}| = \\cos(45^\\circ) = \\frac{\\sqrt{2}}{2}\n",
    "\\Rightarrow \\theta = 2 \\cos^{-1}\\left(\\frac{\\sqrt{2}}{2}\\right) = 90^\\circ = \\frac{\\pi}{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  PyTorch Code\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def geodesic_loss_quaternion(q_pred, q_gt):\n",
    "    \"\"\"\n",
    "    Compute geodesic loss between predicted and ground-truth quaternions.\n",
    "    q_pred, q_gt: (B, 4), normalized unit quaternions\n",
    "    \"\"\"\n",
    "    dot = torch.sum(q_pred * q_gt, dim=1)\n",
    "    dot = torch.abs(dot)  # account for double-cover\n",
    "    dot = torch.clamp(dot, -1.0, 1.0)  # numerical stability\n",
    "    theta = 2 * torch.acos(dot)\n",
    "    return theta.mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "\n",
    "| Case                         | Dot Product       | $\\theta = 2 \\cos^{-1}(|\\cdot|)$ |\n",
    "|-----------------------------|-------------------|-------------------------------------|\n",
    "| $q_{\\text{pred}} = q_{\\text{gt}}$ | 1                 | $0$                             |\n",
    "| $q_{\\text{pred}} = -q_{\\text{gt}}$ | –1                | $0$                             |\n",
    "| 90° apart                   | $\\frac{\\sqrt{2}}{2}$ | $\\frac{\\pi}{2}$                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Case 2 Revisited: Are Opposite Quaternions Opposite Rotations?\n",
    "\n",
    "###  Short Answer:\n",
    "\n",
    "**No**, they are *not* opposite rotations — they are **the same rotation**.\n",
    "\n",
    "###  Why?\n",
    "\n",
    "Unit quaternions represent 3D rotations via a double cover of the rotation group SO(3). That means:\n",
    "\n",
    "$$\n",
    "q \\text{ and } -q \\text{ represent the same rotation in 3D space}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Visual Intuition\n",
    "\n",
    "A quaternion is often written as:\n",
    "\n",
    "$$\n",
    "q = \\left[\\cos\\left(\\frac{\\theta}{2}\\right), \\, \\sin\\left(\\frac{\\theta}{2}\\right) \\hat{u} \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\theta$ is the rotation angle\n",
    "* $\\hat{u}$ is the unit rotation axis\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "-q = \\left[-\\cos\\left(\\frac{\\theta}{2}\\right), \\, -\\sin\\left(\\frac{\\theta}{2}\\right) \\hat{u} \\right]\n",
    "$$\n",
    "\n",
    "Which has the **same rotation effect**, because both correspond to the same 3D rotation matrix.\n",
    "\n",
    "---\n",
    "\n",
    "##  So When Are Quaternions \"Opposite Rotations\"?\n",
    "\n",
    "They aren't. If you want to describe a rotation in the **opposite direction**, you **don’t negate the quaternion** — instead you:\n",
    "\n",
    "* Keep the same axis $\\hat{u}$\n",
    "* Negate the angle: $\\theta \\to -\\theta$\n",
    "\n",
    "This yields a **different quaternion**, not simply the negation.\n",
    "\n",
    "---\n",
    "\n",
    "##  What Would Be a 180° Opposite Rotation?\n",
    "\n",
    "Say the ground truth is rotation by +90° around Z-axis. The \"opposite rotation\" would be –90° around Z. The quaternion for:\n",
    "\n",
    "* +90°: $q = [\\cos(45^\\circ), 0, 0, \\sin(45^\\circ)]$\n",
    "* –90°: $q' = [\\cos(-45^\\circ), 0, 0, \\sin(-45^\\circ)]$\n",
    "\n",
    "These are **not negatives of each other**, and their geodesic distance is:\n",
    "\n",
    "$$\n",
    "\\theta = 2 \\cos^{-1}(|q \\cdot q'|) = \\text{non-zero}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "\n",
    "| Quaternions                                     | Same Rotation? | Geodesic Distance |\n",
    "| ----------------------------------------------- | -------------- | ----------------- |\n",
    "| $q$ vs $-q$                                     | ✅ Yes          | $\\theta = 0$      |\n",
    "| $q_1$, $q_2$ with same axis but opposite angles | ❌ No           | $\\theta > 0$      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring function: $|\\textbf{W}.\\textbf{X}_i+b|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean Absolute Error Loss\n",
    "Treats outlier like anyother point so it won't go out of its way for outliers which might lead to poor prediction from time to time. Also it is not optimized for gradient descent (due to incontinuty on zero)Support vector regression use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Squared Error Loss\n",
    "The adventage is we can easily compute gradient for ML. Good for regression but outlier will make it a poor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pseudo-Huber Loss\n",
    "Mean Absolute Error doenst comply with outlier and Mean Squared Error freaks out with outlier. If your data is 70% on one side and 30% on the other side, both will reult in a poor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Welsch (Leclerc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Geman McClure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Causchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Cross-Entropy Loss\n",
    "When we use <b>sigmoid</b> as activation function and <b>Mean Squared Error Loss</b>, if our neuron output is very wrong ,since $\\sigma(z)$ s saturated and it is almost a flat line ($z \\to \\infty$, $\\sigma(z) \\to 1$ and $z \\to -\\infty$, $\\sigma(z) \\to 0$) , $\\sigma'(z)$ gives us a small value so learning will be very slow. \n",
    "<b>Cross-Entropy Loss</b> designed for binary classifiers. In a binary classifier,  $y \\in \\{0,1\\}$ is the class lable and $a$ is the output of the activation function (could be any number).\n",
    "\n",
    "\n",
    "$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} & = & (a-y)\\sigma'(z) x = a \\sigma'(z)\\end{eqnarray}$\n",
    "  \n",
    "$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} & = & (a-y)\\sigma'(z) = a \\sigma'(z),\\end{eqnarray}$\n",
    "\n",
    "Let say we have binary classifier, so we have 1 output node and the class labeles are either 1 or 0, \n",
    "$C=-\\frac{1}{n} \\sum_{x} y\\ln(a)+ (1-y)\\ln(1-a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Multi-Class Cross-Entropy Loss\n",
    "If we have $j$ output in the last layer $L$, the total cost for all inputs is:\n",
    "\n",
    "$\\begin{eqnarray}  C = -\\frac{1}{n} \\sum_x\n",
    "  \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\n",
    "\\end{eqnarray} $\n",
    "\n",
    "One way to remmeber that the order is  $ y_j \\ln a^L_j$ and not other way around is to remmeber that $y_j$ could be \n",
    "absoulute zero so it can't be the parameter for logarithm function, but $a^L_j$ can $\\lim a^L_j \\to 0 $, so it is the parameter of $\\ln$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the the $\\delta$ when the loss function is **Multi-Class Cross-Entropy Loss** and activation function is **sigmoid**:\n",
    "\n",
    "\n",
    "$\\delta_j=\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial z_j} &=\\frac{\\partial C}{\\partial a_j}\\frac{\\partial a_j}{\\partial z_j}   = & -\\frac{1}{n} \\sum_x \\left(\n",
    "    \\frac{y }{\\sigma(z_j)} -\\frac{(1-y)}{1-\\sigma(z_j)} \\right)\\sigma'(z_j) \n",
    "& = & -\\frac{1}{n} \\sum_x \\left( \n",
    "    \\frac{y -\\sigma(z_j)}{\\sigma(z_j)(1-\\sigma(z_j))}\n",
    "    \\right).\\sigma(z_j)(1-\\sigma(z_j))\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Reminder: for sigmoid activation function:\n",
    "\n",
    "$\\sigma'(z_j)=\\sigma(z_j)(1-\\sigma(z_j))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we would have:\n",
    "\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "    \\delta^L = a^L-y.\\tag 1\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Reminder: in quadratic loss function, $\\delta$ was:\n",
    "\n",
    "$\\delta^L= 2(\\textbf{a}^{(L)}-\\textbf{y}) \\odot   \\sigma^{\\prime}(\\textbf{z}^{(L)})$\n",
    "\n",
    "For previous layer $L-1, L-2,...$, the $\\delta^{(L-1)}$ is same as wath we computed before in network with quadratic loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{eqnarray} \n",
    "      \\frac{\\partial C}{\\partial w^L_{jk}} & = & \\frac{1}{n} \\sum_x \n",
    "      a^{L-1}_k  (a^L_j-y_j) \\tag 2\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the rate at which the weight learns is controlled by $\\sigma(z)-y$ (by the error in the output). The larger the error, the faster the neuron will learn. This avoids the learning slowdown caused by the $\\sigma '(z)$ term in the analogous equation for the quadratic cost,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{eqnarray}\n",
    "      \\frac{\\partial C}{\\partial b^L_{j}} & = & \\frac{1}{n} \\sum_x \n",
    "      (a^L_j-y_j).\n",
    "  \\tag{3}\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Negative log likelihood\n",
    "\n",
    "The <b>Negative Log-Likelihood Loss function (NLL)</b> is applied only on models with the softmax function as an output activation layer. We can think a softmax output layer with log-likelihood cost as being quite similar to a sigmoid output layer with cross-entropy cost.\n",
    "\n",
    "The Softmax function is :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}}$\n",
    "\n",
    "\n",
    "The softmax function takes an input vector of size $N$, and then modifies the values such that every one of them falls between 0 and 1.\n",
    "\n",
    "\n",
    "$NLL$ uses a negative connotation since the probabilities (or likelihoods) vary between zero and one, and the logarithms of values in this range are negative. In the end, the loss value becomes positive. The negative log likelihood is retrieved from approximating the maximum likelihood estimation $MLE$. This means that we try to maximize the model’s log likelihood, and as a result, minimize the $NLL$.  \n",
    "\n",
    "Neural network estimate: \n",
    "$f(X)_c=p(y=c|X)$\n",
    "\n",
    "Which means NN estimate the probability that the output of network, $y$ for the given vector $X$ is the correct class $c$.\n",
    "For instance we have 3 classes, which means 3 nodes at the output layer, and for a given input we get:\n",
    "$(0.1,0.85,0.05 )$, which means which means our network belives with the probability of $0.85$ this input belong to the second class. Since the inputs are independent of each other the joint probability of them would be teh multipication of them just like $MLE$. Since output probabilities are betwen zero and one the $ln$ of them would be a negative value, so we turn them into positive number by multiplying tem by a negative number and we turn maximisation problem into minimization.\n",
    "\n",
    "$l(\\textbf{f}(X),y)=-ln f(X)_c=-\\Sigma_c 1_{y=c}ln(f(X)_c)$\n",
    "\n",
    "\n",
    "Refs:\n",
    "\n",
    "[1](https://www.youtube.com/watch?v=PpFTODTztsU), [2](https://neptune.ai/blog/pytorch-loss-functions),\n",
    "[3](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " tensor([[-0.3319,  1.6391,  0.8178,  1.8935, -0.0741],\n",
      "        [ 0.1611,  0.1806,  2.6024,  0.9387, -1.5747],\n",
      "        [ 0.5691, -0.7668, -0.9221,  1.5614,  0.6988]], requires_grad=True)\n",
      "target: \n",
      " tensor([4, 3, 2])\n",
      "output: \n",
      " tensor(2.6592, grad_fn=<NllLossBackward0>)\n",
      "m(input).exp()\n",
      " tensor([[0.0457, 0.3280, 0.1443, 0.4230, 0.0591],\n",
      "        [0.0630, 0.0643, 0.7243, 0.1372, 0.0111],\n",
      "        [0.1878, 0.0494, 0.0423, 0.5067, 0.2138]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# size of input (N x C) is = 3 x 5\n",
    "#N\n",
    "number_of_item_in_training_set=3\n",
    "\n",
    "#C\n",
    "number_of_neuron_in_output_layer=5\n",
    "\n",
    "input = torch.randn(number_of_item_in_training_set, number_of_neuron_in_output_layer, requires_grad=True)\n",
    "\n",
    "# target should have only 3 elements (N=3) and every element in target should have 0 <= value < C=5\n",
    "# because these are target lables. Target: (N)\n",
    "target = torch.tensor([4, 3, 2])\n",
    "\n",
    "m = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "# NLLLoss expects the inputs to be log probabilities\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "output = nll_loss(m(input), target)\n",
    "output.backward()\n",
    "\n",
    "print('input: \\n', input)\n",
    "print('target: \\n', target)\n",
    "print('output: \\n', output)\n",
    "print('m(input).exp()\\n',m(input).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3968)\n",
      "tensor(1.3968)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "a = torch.tensor(([0.88, 0.12], [0.51, 0.49]), dtype = torch.float)\n",
    "target = torch.tensor([1, 0])\n",
    "output = loss(torch.log(a), target)\n",
    "print(output)\n",
    "\n",
    "print((-torch.log(a[0, 1]) - torch.log(a[1, 0])) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Hinge Loss\n",
    "Let's say we have binary classifier with two labels, $-1,+1$ to measure the misclassification error:\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq  sign(f(x_i))] = \\frac{1}{n}\\sum_{i=1}^{n}[y_i f(x_i)] \\leq \\frac{1}{n}\\sum_{i=1}^{n}L(y_i f(x_i)) $\n",
    "\n",
    "minimizing this error function is computationaly complicated so \n",
    "\n",
    "Mainly used in SVMm which set a boundry as far as possible between all data points (maximizes the minimum margin).\n",
    "It will penalized the points even they are in the margin.\n",
    "\n",
    "\n",
    "scoring function: $|\\textbf{W}.\\textbf{X}_i+b|$\n",
    "\n",
    "Training test label, $y_i = ±1$\n",
    "\n",
    "$L_i=max(0,1-y_i|\\textbf{W}.\\textbf{X}_i+b|)$\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=r-vYJqcFxBI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/hinge.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Loss Functions\n",
    "All the abve loss functions can be generalized by:\n",
    "\n",
    "$p(x,\\alpha)=\\frac{|2- \\alpha|}{\\alpha} \\left (   \\left ( \\frac{x^2}{|2-\\alpha|}+1  \\right )^{\\alpha/2}  -1 \\right )  $\n",
    "\n",
    "\n",
    "$NLL(\\theta,\\alpha)=min \\rho_{\\theta,\\alpha} (x,\\alpha )+logZ(\\alpha) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://arxiv.org/abs/1701.03077), [2](https://www.youtube.com/watch?v=QBbC3Cjsnjg), [3](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7), [4](https://rohanvarma.me/Loss-Functions/), [5](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23), [6](https://neptune.ai/blog/pytorch-loss-functions), [7](https://www.youtube.com/watch?v=ErfnhcEV1O8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
