{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification Loss Functions\n",
    "\n",
    "## 1.1. Motivation for Probabilistic Outputs\n",
    "\n",
    "In a classification task, we want to assign an input \\$x\\$ to one of \\$K\\$ classes.\n",
    "\n",
    "A naïve idea is to design a neural network with \\$K\\$ output neurons and set the correct one to **1** and all others to **0**.\n",
    "But this “hard assignment” has two problems:\n",
    "\n",
    "* It does not express **uncertainty** (e.g., the model might be 70% dog, 30% cat).\n",
    "* It prevents us from interpreting outputs as **probabilities**, which are essential for downstream decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. From Logits to Probabilities: Softmax\n",
    "\n",
    "To produce probabilities, we apply the **softmax** function to logits \\$z\\_k\\$:\n",
    "\n",
    "$$\n",
    "Q(k) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}.\n",
    "$$\n",
    "\n",
    "* Each \\$Q(k)\\in\\[0,1]\\$\n",
    "* \\$\\sum\\_k Q(k)=1\\$\n",
    "* The highest logit → highest probability\n",
    "\n",
    "Thus, softmax converts arbitrary real values into a probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3. Why Not Use MSE?\n",
    "\n",
    "If we compare predicted \\$Q\\$ with one-hot targets \\$P\\$ using MSE:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_k} = (Q(k)-P(k)) \\cdot Q(k)(1-Q(k)).\n",
    "$$\n",
    "\n",
    "The extra factor \\$Q(k)(1-Q(k))\\$ (derivative of softmax) makes gradients **tiny** whenever logits saturate.\n",
    "→ **Vanishing gradients** → slow learning.\n",
    "\n",
    "This motivates using **Cross-Entropy Loss** instead.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4. Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy directly measures how close predicted \\$Q\\$ is to true \\$P\\$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{k=1}^K P(k)\\log Q(k).\n",
    "$$\n",
    "\n",
    "For one-hot targets (true class \\$y\\$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\log Q(y).\n",
    "$$\n",
    "\n",
    "* If \\$Q(y)\\$ is high → small loss.\n",
    "* If \\$Q(y)\\to 0\\$ → loss \\$\\to\\infty\\$ (strong penalty).\n",
    "\n",
    "**Gradient simplification:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_k} = Q(k) - P(k).\n",
    "$$\n",
    "\n",
    "No vanishing softmax derivative → stable training.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5. Information-Theoretic Roots (KL Divergence)\n",
    "\n",
    "Cross-Entropy connects to **KL divergence**:\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P\\|Q) = \\sum_k P(k)\\log\\frac{P(k)}{Q(k)}.\n",
    "$$\n",
    "\n",
    "Expands to:\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P\\|Q) = H(P,Q) - H(P).\n",
    "$$\n",
    "\n",
    "Since \\$H(P)\\$ is constant, minimizing \\$D\\_{\\mathrm{KL}}\\$ ≡ minimizing **cross-entropy**.\n",
    "Thus, **cross-entropy loss = maximum likelihood learning** for categorical data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6. Label Distributions in Practice\n",
    "\n",
    "1. **Predicted (\\$Q\\$):** from logits via softmax.\n",
    "2. **True (\\$P\\$):**\n",
    "\n",
    "   * Normally one-hot (e.g., label “dog” → \\$P=\\[0,1,0]\\$).\n",
    "   * Can be soft:\n",
    "\n",
    "     * **Label smoothing**: e.g. $\\[0.05,0.9,0.05]\\$\n",
    "     * **Knowledge distillation**: soft targets from a teacher model\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7. Worked Example\n",
    "\n",
    "* 3 classes: cat, dog, rabbit\n",
    "* True label = dog → \\$P=\\[0,1,0]\\$\n",
    "* Model predicts \\$Q=\\[0.2,0.7,0.1]\\$\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "H(P,Q) = -\\log 0.7.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1.8. Cross-Entropy Derivation from KL (Explicit)\n",
    "\n",
    "Setup:\n",
    "\n",
    "* Logits \\$z\\_k\\$, softmax \\$Q(k|x)\\$.\n",
    "* One-hot target \\$P(k|x)=\\mathbf{1}\\[k=y]\\$.\n",
    "\n",
    "KL divergence:\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P\\|Q) = -\\sum_k P(k)\\log Q(k) + \\text{const}.\n",
    "$$\n",
    "\n",
    "With one-hot \\$P\\$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x,y) = -\\log Q(y|x).\n",
    "$$\n",
    "\n",
    "For a batch:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{batch}} = -\\frac{1}{n}\\sum_{i=1}^n \\log Q(y_i|x_i).\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_k} = Q(k) - \\mathbf{1}[k=y].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9. Negative Log-Likelihood (NLL)\n",
    "\n",
    "The **Negative Log-Likelihood Loss (NLL)** is another way to write maximum likelihood:\n",
    "\n",
    "* Likelihood:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n Q_\\theta(y_i|x_i).\n",
    "$$\n",
    "\n",
    "* Log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^n \\log Q_\\theta(y_i|x_i).\n",
    "$$\n",
    "\n",
    "* Negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = -\\sum_{i=1}^n \\log Q(y_i|x_i).\n",
    "$$\n",
    "\n",
    "Example: for output \\$(0.1,0.85,0.05)\\$, target = class 2 →\n",
    "Loss = \\$-\\log(0.85)\\$.\n",
    "\n",
    "NLL appears negative because \\$\\log(p)\\leq 0\\$ for \\$0\\<p\\leq 1\\$, so we negate it to make loss positive.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.10. Cross-Entropy vs NLL\n",
    "\n",
    "**Cross-Entropy (general):**\n",
    "\n",
    "$$\n",
    "H(P,Q) = -\\sum_k P(k)\\log Q(k).\n",
    "$$\n",
    "\n",
    "* Works for **any target distribution \\$P\\$**.\n",
    "* Handles one-hot, smoothed labels, or soft teacher distributions.\n",
    "\n",
    "**NLL (special case):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = -\\log Q(y).\n",
    "$$\n",
    "\n",
    "* Tied to **single-class labels** (one-hot).\n",
    "* Directly maximizes likelihood of observed labels.\n",
    "\n",
    "**When they coincide:**\n",
    "\n",
    "If \\$P\\$ is one-hot → CE and NLL are identical.\n",
    "That’s why in PyTorch:\n",
    "\n",
    "* `nn.CrossEntropyLoss` = `nn.LogSoftmax + nn.NLLLoss`.\n",
    "\n",
    "**When they differ:**\n",
    "\n",
    "* CE works with soft targets (label smoothing, distillation).\n",
    "* NLL requires a single observed class.\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=Pwgpl9mKars), [2](https://www.youtube.com/watch?v=PpFTODTztsU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.11 Examples** \n",
    "\n",
    "### **Numeric Example** \n",
    "\n",
    "* True class: **Dog** ($y=2$)\n",
    "* Classes: Cat (0), Dog (1), Rabbit (2)\n",
    "* Predicted softmax distribution:\n",
    "\n",
    "  $$\n",
    "  Q = [0.1, \\; 0.7, \\; 0.2]\n",
    "  $$\n",
    "* Target one-hot vector:\n",
    "\n",
    "  $$\n",
    "  P = [0, \\; 1, \\; 0]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**1. Negative Log-Likelihood (NLL)**\n",
    "\n",
    "NLL looks only at the probability of the **true class** (Dog = index 1):\n",
    "\n",
    "$$\n",
    "\\text{NLL} = -\\log Q(y) = -\\log(0.7) \\approx 0.357\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Cross-Entropy**\n",
    "\n",
    "Cross-Entropy is:\n",
    "\n",
    "$$\n",
    "H(P,Q) = -\\sum_k P(k)\\log Q(k)\n",
    "$$\n",
    "\n",
    "Since $P$ is one-hot ($P(1)=1$):\n",
    "\n",
    "$$\n",
    "H(P,Q) = -\\log Q(1) = -\\log(0.7) \\approx 0.357\n",
    "$$\n",
    "\n",
    " With one-hot labels, **CE = NLL**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Mean Squared Error (MSE)**\n",
    "\n",
    "MSE compares probability vectors:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\tfrac{1}{2}\\sum_k (Q(k)-P(k))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}\\big[(0.1-0)^2 + (0.7-1)^2 + (0.2-0)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}(0.01 + 0.09 + 0.04) = \\tfrac{1}{2}(0.14) = 0.07\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Table**\n",
    "\n",
    "| Loss Type         | Formula                            | Value in example | Notes                                            |\n",
    "| ----------------- | ---------------------------------- | ---------------- | ------------------------------------------------ |\n",
    "| **NLL**           | $-\\log Q(y)$                       | **0.357**        | Looks only at true class probability.            |\n",
    "| **Cross-Entropy** | $-\\sum_k P(k)\\log Q(k)$            | **0.357**        | Equals NLL if $P$ is one-hot.                    |\n",
    "| **MSE**           | $\\tfrac{1}{2}\\sum_k (Q(k)-P(k))^2$ | **0.07**         | Much smaller, but gradients weaker (can vanish). |\n",
    "\n",
    "---\n",
    "\n",
    "**4. What if labels are not one-hot?**\n",
    "\n",
    "Suppose we use **label smoothing**:\n",
    "\n",
    "$$\n",
    "P = [0.05, \\; 0.9, \\; 0.05]\n",
    "$$\n",
    "\n",
    "* **Cross-Entropy**:\n",
    "\n",
    "  $$\n",
    "  H(P,Q) = -(0.05\\log 0.1 + 0.9\\log 0.7 + 0.05\\log 0.2)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\approx -(0.05 \\cdot -2.302 + 0.9\\cdot -0.357 + 0.05\\cdot -1.609)\n",
    "  \\approx 0.453\n",
    "  $$\n",
    "\n",
    "* **NLL**: Not defined (since it expects a single class label).\n",
    "\n",
    "* **MSE**: Still computable, but less meaningful.\n",
    "\n",
    "This is where **Cross-Entropy ≠ NLL**, and CE is more general.\n",
    "\n",
    "---\n",
    "\n",
    "This small example shows:\n",
    "\n",
    "* With one-hot labels → **CE = NLL**.\n",
    "* MSE gives a much smaller number (and weaker gradients).\n",
    "* With soft labels → only **CE** works properly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with 2 Classes and comparing **MSE vs Cross-Entropy** Example: \n",
    "\n",
    "\n",
    "* True class: $y=0$ (so target distribution is $P = [1,0]$)\n",
    "* Model logits:\n",
    "\n",
    "  $$\n",
    "  z = [3, -2]\n",
    "  $$\n",
    "* Apply softmax to get predicted probabilities:\n",
    "\n",
    "  $$\n",
    "  Q(0) = \\frac{e^3}{e^3 + e^{-2}}, \\quad Q(1) = \\frac{e^{-2}}{e^3 + e^{-2}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**Compute probabilities**\n",
    "\n",
    "$$\n",
    "e^3 \\approx 20.085, \\quad e^{-2} \\approx 0.135\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(0) = \\frac{20.085}{20.085 + 0.135} \\approx 0.9933\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(1) = \\frac{0.135}{20.085 + 0.135} \\approx 0.0067\n",
    "$$\n",
    "\n",
    "So the model predicts class 0 with **99.3% probability**, class 1 with **0.7% probability**.\n",
    "\n",
    "---\n",
    "\n",
    "**MSE Loss**\n",
    "\n",
    "MSE compares probabilities $Q$ to the target one-hot vector $P = [1,0]$:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\tfrac{1}{2}\\big[(Q(0)-1)^2 + (Q(1)-0)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}\\big[(0.9933-1)^2 + (0.0067-0)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}\\big[( -0.0067)^2 + (0.0067)^2\\big] \\approx \\tfrac{1}{2}(0.000045+0.000045)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.000045\n",
    "$$\n",
    "\n",
    "Pretty small loss.\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient with MSE**\n",
    "\n",
    "General gradient with MSE and softmax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_k} = (Q(k)-P(k)) \\cdot Q(k)(1-Q(k))\n",
    "$$\n",
    "\n",
    "For class 0:\n",
    "\n",
    "$$\n",
    "(Q(0)-P(0)) \\cdot Q(0)(1-Q(0)) = (0.9933-1)\\cdot 0.9933\\cdot(0.0067)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (-0.0067)\\cdot(0.00665) \\approx -0.000044\n",
    "$$\n",
    "\n",
    "For class 1:\n",
    "\n",
    "$$\n",
    "(Q(1)-P(1)) \\cdot Q(1)(1-Q(1)) = (0.0067-0)\\cdot(0.0067)(0.9933)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.0067)\\cdot(0.00665) \\approx 0.000044\n",
    "$$\n",
    "\n",
    "Gradient is **tiny** (\\~$10^{-5}$).\n",
    "Even though the model is slightly wrong, updates will be **very slow**.\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Entropy Loss**\n",
    "\n",
    "Cross-entropy loss for true class $y=0$:\n",
    "\n",
    "$$\n",
    "\\text{CE} = -\\log Q(0) = -\\log(0.9933) \\approx 0.0067\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient with Cross-Entropy**\n",
    "\n",
    "General gradient with CE + softmax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_k} = Q(k) - P(k)\n",
    "$$\n",
    "\n",
    "For class 0:\n",
    "\n",
    "$$\n",
    "Q(0)-P(0) = 0.9933-1 = -0.0067\n",
    "$$\n",
    "\n",
    "For class 1:\n",
    "\n",
    "$$\n",
    "Q(1)-P(1) = 0.0067-0 = 0.0067\n",
    "$$\n",
    "\n",
    "Gradient is **\\~100× larger** than with MSE.\n",
    "So updates will be meaningful, not negligible.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "* With **MSE**, gradients are dampened by the softmax derivative $Q(1-Q)$.\n",
    "  → Tiny updates → slow learning.\n",
    "* With **Cross-Entropy**, gradients simplify to $(Q-P)$.\n",
    "  → Strong updates proportional to the prediction error.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Model is Confident But it is Wrong Example\n",
    "\n",
    "**Setup**\n",
    "\n",
    "* True class: $y=0$ → target $P = [1,0]$.\n",
    "* Model logits:\n",
    "\n",
    "  $$\n",
    "  z = [-2, \\; 3]\n",
    "  $$\n",
    "* This time the network strongly favors class 1 instead of class 0.\n",
    "\n",
    "---\n",
    "\n",
    "**Compute probabilities (softmax)**\n",
    "\n",
    "$$\n",
    "e^{-2} \\approx 0.135, \\quad e^{3} \\approx 20.085\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(0) = \\frac{0.135}{0.135+20.085} \\approx 0.0067\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(1) = \\frac{20.085}{0.135+20.085} \\approx 0.9933\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Model says class 0 (true class) → **0.67%**\n",
    "* Model says class 1 (wrong class) → **99.3%**\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: MSE Loss**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\tfrac{1}{2}\\big[(Q(0)-1)^2 + (Q(1)-0)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}\\big[(0.0067-1)^2 + (0.9933-0)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tfrac{1}{2}\\big[(-0.9933)^2 + (0.9933)^2\\big] \n",
    "= \\tfrac{1}{2}(0.9866+0.9866) \\approx 0.9866\n",
    "$$\n",
    "\n",
    "So the loss is **≈ 1**. Not small, but also not huge.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Gradient with MSE**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_k} = (Q(k)-P(k)) \\cdot Q(k)(1-Q(k))\n",
    "$$\n",
    "\n",
    "For class 0:\n",
    "\n",
    "$$\n",
    "(Q(0)-P(0)) \\cdot Q(0)(1-Q(0)) = (0.0067-1)\\cdot 0.0067\\cdot 0.9933\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (-0.9933)\\cdot (0.00665) \\approx -0.0066\n",
    "$$\n",
    "\n",
    "For class 1:\n",
    "\n",
    "$$\n",
    "(Q(1)-P(1)) \\cdot Q(1)(1-Q(1)) = (0.9933-0)\\cdot 0.9933\\cdot 0.0067\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.9933)\\cdot (0.00665) \\approx 0.0066\n",
    "$$\n",
    "\n",
    "Gradient is still **tiny (\\~0.006)**.\n",
    "Even though the model is *very wrong*, MSE barely pushes it to update.\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Entropy Loss**\n",
    "\n",
    "For true class $y=0$:\n",
    "\n",
    "$$\n",
    "\\text{CE} = -\\log Q(0) = -\\log(0.0067) \\approx 5.01\n",
    "$$\n",
    "\n",
    "This is **much larger** than MSE’s \\~1.\n",
    "The model is heavily penalized for being confidently wrong.\n",
    "\n",
    "---\n",
    "\n",
    "**Gradient with Cross-Entropy**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_k} = Q(k) - P(k)\n",
    "$$\n",
    "\n",
    "For class 0:\n",
    "\n",
    "$$\n",
    "Q(0)-P(0) = 0.0067 - 1 = -0.9933\n",
    "$$\n",
    "\n",
    "For class 1:\n",
    "\n",
    "$$\n",
    "Q(1)-P(1) = 0.9933 - 0 = 0.9933\n",
    "$$\n",
    "\n",
    "Gradient is **\\~150× larger** than with MSE. \n",
    "The network will be strongly corrected.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* **MSE**: even if the model is **confident but wrong**, gradients remain small → weak correction.\n",
    "* **Cross-Entropy**: the loss **explodes** when the model is confident in the wrong class → strong correction.\n",
    "\n",
    "This behavior is exactly what we want:\n",
    "\n",
    "* Reward confident correct predictions (loss → 0).\n",
    "* Severely punish confident wrong predictions (loss → ∞).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Loss Functions\n",
    "\n",
    "* **MSELoss**\n",
    "* **L1Loss**\n",
    "* **SmoothL1Loss (Huber)**\n",
    "Scoring function: $|\\textbf{W}.\\textbf{X}_i+b|$\n",
    "\n",
    "### 2.1. Mean Absolute Error Loss\n",
    "Treats outlier like another point so it won't go out of its way for outliers which might lead to poor prediction from time to time. Also, it is not optimized for gradient descent (due to discontinuity on zero)Support vector regression use this.\n",
    "\n",
    "### 2.2. Mean Squared Error Loss\n",
    "The advantage is we can easily compute gradient for ML. Good for regression but outlier will make it a poor model\n",
    "\n",
    "### 2.3. Pseudo-Huber Loss\n",
    "Mean Absolute Error don't comply with outlier and Mean Squared Error freaks out with outlier. If your data is 70% on one side and 30% on the other side, both will result in a poor model.\n",
    "\n",
    "### 2.4. Welsch (Leclerc)\n",
    "### 2.5. Geman McClure\n",
    "### 2.6. Causchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geodesic & Geometry-Aware Loss Functions\n",
    "\n",
    "Geodesic loss is commonly used in deep learning when predicting **rotations** (e.g., in camera pose estimation, object orientation prediction, or visual odometry).\n",
    "It measures the **shortest distance** between two rotations on the rotation manifold **SO(3)**, ensuring predictions respect rotation geometry.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1 Why Not Use L2 Loss?\n",
    "\n",
    "Rotations live on a **curved manifold** (SO(3)), not Euclidean space.\n",
    "Naive L2 loss between matrices or quaternions can:\n",
    "\n",
    "* Misrepresent distances,\n",
    "* Violate rotation constraints,\n",
    "* Hinder convergence.\n",
    "\n",
    "Instead, we use **geodesic distance**, which correctly measures the angular distance between two rotations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2 Geodesic Loss with Rotation Matrices\n",
    "\n",
    "Given:\n",
    "\n",
    "* Ground truth: \\$R\\_{\\text{gt}} \\in SO(3)\\$\n",
    "* Prediction: \\$R\\_{\\text{pred}} \\in SO(3)\\$\n",
    "\n",
    "The **relative rotation** is:\n",
    "\n",
    "$$\n",
    "R_{\\text{rel}} = R_{\\text{gt}}^\\top R_{\\text{pred}}\n",
    "$$\n",
    "\n",
    "The **geodesic angle** is:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1} \\left( \\frac{\\text{trace}(R_{\\text{rel}}) - 1}{2} \\right)\n",
    "$$\n",
    "\n",
    "* \\$\\text{trace}(R\\_{\\text{rel}}) = 3\\$ → identical rotations (\\$\\theta = 0\\$)\n",
    "* \\$\\text{trace}(R\\_{\\text{rel}}) = -1\\$ → 180° difference (\\$\\theta = \\pi\\$)\n",
    "\n",
    "This comes directly from Rodrigues’ rotation formula:\n",
    "\\$\\text{trace}(R) = 1 + 2 \\cos(\\theta)\\$\n",
    "\n",
    "---\n",
    "\n",
    "#### PyTorch Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    \"\"\"\n",
    "    Computes geodesic loss (radians) between two rotation matrices.\n",
    "    R_pred, R_gt: (B, 3, 3)\n",
    "    \"\"\"\n",
    "    R_diff = torch.matmul(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "    cos_theta = (trace - 1) / 2\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)  # stability\n",
    "    theta = torch.acos(cos_theta)\n",
    "    return theta.mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3 Geodesic Loss with Quaternions\n",
    "\n",
    "If rotations are represented as **unit quaternions** \\$q\\_{\\text{gt}}, q\\_{\\text{pred}} \\in \\mathbb{R}^4\\$:\n",
    "\n",
    "$$\n",
    "\\theta = 2 \\cos^{-1}\\!\\Big(|q_{\\text{gt}} \\cdot q_{\\text{pred}}|\\Big)\n",
    "$$\n",
    "\n",
    "* The dot product measures similarity.\n",
    "* The **absolute value** accounts for the double-cover property (\\$q\\$ and \\$-q\\$ represent the same rotation).\n",
    "\n",
    "---\n",
    "\n",
    "#### PyTorch Implementation\n",
    "\n",
    "```python\n",
    "def geodesic_loss_quaternion(q_pred, q_gt):\n",
    "    \"\"\"\n",
    "    Compute geodesic loss between predicted & ground-truth quaternions.\n",
    "    q_pred, q_gt: (B, 4), normalized\n",
    "    \"\"\"\n",
    "    dot = torch.sum(q_pred * q_gt, dim=1)\n",
    "    dot = torch.abs(dot)                  # handle double cover\n",
    "    dot = torch.clamp(dot, -1.0, 1.0)     # stability\n",
    "    theta = 2 * torch.acos(dot)\n",
    "    return theta.mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Intuition & Special Cases\n",
    "\n",
    "#### Case 1: Perfect Match\n",
    "\n",
    "\\$q\\_{\\text{pred}} = q\\_{\\text{gt}} ;;\\Rightarrow;; \\theta = 0\\$\n",
    "\n",
    "#### Case 2: Opposite Quaternions (same rotation)\n",
    "\n",
    "\\$q\\_{\\text{pred}} = -q\\_{\\text{gt}} ;;\\Rightarrow;; \\theta = 0\\$\n",
    "(because quaternions double-cover SO(3))\n",
    "\n",
    "#### Case 3: 90° Difference\n",
    "\n",
    "\\$|q\\_{\\text{gt}} \\cdot q\\_{\\text{pred}}| = \\cos(45^\\circ) = \\tfrac{\\sqrt{2}}{2}\\$\n",
    "\\$\\Rightarrow \\theta = 90^\\circ = \\tfrac{\\pi}{2}\\$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Clarification: Opposite Rotations vs Opposite Quaternions\n",
    "\n",
    "* \\$q\\$ vs \\$-q\\$ →  same rotation (\\$\\theta = 0\\$)\n",
    "* Rotations of +90° and –90° around the same axis →  different rotations (\\$\\theta > 0\\$)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.6 Summary Table\n",
    "\n",
    "| Case                                   | Dot Product             | Geodesic Distance  |\n",
    "| -------------------------------------- | ----------------------- | ------------------ |\n",
    "| \\$q\\_{\\text{pred}} = q\\_{\\text{gt}}\\$  | \\$1\\$                   | \\$0\\$              |\n",
    "| \\$q\\_{\\text{pred}} = -q\\_{\\text{gt}}\\$ | \\$-1\\$ (abs → \\$1\\$)    | \\$0\\$              |\n",
    "| 90° apart                              | \\$\\tfrac{\\sqrt{2}}{2}\\$ | \\$\\tfrac{\\pi}{2}\\$ |\n",
    "| 180° apart                             | \\$0\\$                   | \\$\\pi\\$            |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PyTorch Example** \n",
    "\n",
    "\n",
    "####  `nn.CrossEntropyLoss`\n",
    "\n",
    "* **Expected input**: raw logits (no softmax).\n",
    "* **Expected target**: class index (integer), tensor of indices for each item in the batch.\n",
    "* Internally:\n",
    "\n",
    "  * Applies `F.log_softmax(logits, dim=1)`\n",
    "  * Then applies `nn.NLLLoss`.\n",
    "\n",
    "So `CrossEntropyLoss = log_softmax + NLLLoss`.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Batch of size 2, 3 classes → logits\n",
    "outputs = torch.tensor([[0.1, 0.3, 0.2], [0.4, 0.1, 0.2]])  # shape (2,3)\n",
    "\n",
    "# True class index (not one-hot!)\n",
    "label = torch.tensor([2, 0])  # shape (2,)\n",
    "\n",
    "loss = criterion(outputs, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  **label smoothing**\n",
    "\n",
    "* PyTorch’s `nn.CrossEntropyLoss` by default does **not** accept one-hot or smoothed probability vectors — it expects class indices.\n",
    "\n",
    "* To use label smoothing, PyTorch added `label_smoothing` argument:\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "```\n",
    "\n",
    "Under the hood, it converts the hard label into a smoothed distribution and computes generalized CE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  loss tensor:\n",
    "\n",
    "`nn.CrossEntropyLoss()` by default, computes the **mean** of the per-sample losses:\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.CrossEntropyLoss()  # default: reduction='mean'\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "```python\n",
    "loss.item() = sum of individual losses/batch size\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.041774034500122"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loss = criterion(outputs, label)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020887017250061"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\") \n",
    "loss = criterion(outputs, label)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If You Want Total Loss Over Epoch**\n",
    "\n",
    "You can choose:\n",
    "\n",
    "* **Use `reduction='sum'`** and accumulate directly\n",
    "* **Stick with mean**, but multiply by batch size to compute total loss:\n",
    "\n",
    "```python\n",
    "loss = criterion(outputs_batch, labels_batch) # default: reduction='mean'\n",
    "running_loss += loss.item() * inputs_batch.size(0)  # batch_size\n",
    "```\n",
    "\n",
    "This is useful when you want the total loss across all samples in the epoch.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice for Epoch-Level Loss:**\n",
    "\n",
    "```python\n",
    "epoch_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "for inputs_batch, labels_batch in train_loader:\n",
    "    outputs_batch = model(inputs_batch)\n",
    "    loss = criterion(outputs_batch, labels_batch)\n",
    "    \n",
    "    batch_size = inputs_batch.size(0)\n",
    "    epoch_loss += loss.item() * batch_size  # accumulate total loss\n",
    "    total_samples += batch_size\n",
    "\n",
    "# average loss for the epoch\n",
    "epoch_loss = epoch_loss / total_samples\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `torch.max(outputs, 1)` or `outputs.max(1)`\n",
    "\n",
    "```python\n",
    "_, predicted = outputs.max(1)\n",
    "# or \n",
    "_, predicted = torch.max(outputs, 1)\n",
    "```\n",
    "Is **standard for multi-class classification** where `outputs` is of shape `[B, C]`, with **raw logits** per class.\n",
    "\n",
    "In our case  Batch of size `2, 3` classes → `logits` \n",
    "\n",
    "```python\n",
    "outputs = torch.tensor([[0.1, 0.3, 0.2], [0.4, 0.1, 0.2]])  # shape (2,3)\n",
    "```\n",
    "\n",
    "**what it does:**\n",
    "\n",
    "* `outputs`: Tensor of shape `[B, C]` (batch size × number of classes). \n",
    "\n",
    "* `torch.max(outputs, 1)` returns a tuple:\n",
    "\n",
    "  * `values` (that's the `_`) : max logit per sample → not used \n",
    "  * `indices` (that's the `predicted`): **predicted class index** for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "_, predicted = outputs.max(1)\n",
    "print(\"predicted\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predicted.eq(label).sum().item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "correct += predicted.eq(label).sum().item()\n",
    "# or \n",
    "# correct += (predicted == label).sum().item()\n",
    "print(correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares predicted class indices to true labels and counts the number of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "total_loss += loss.item() * images.size(0)\n",
    "```\n",
    "\n",
    "is **accumulating the total loss over the entire dataset**, not averaging yet.\n",
    "\n",
    "---\n",
    "\n",
    "To compute the **epoch average**, you divide at the end:\n",
    "\n",
    "```python\n",
    "epoch_loss = total_loss / dataset_size  # dataset_size = len(train_dataset)\n",
    "```\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Variable         | Type   | Meaning                           |\n",
    "| ---------------- | ------ | --------------------------------- |\n",
    "| `loss`           | Tensor | Mean loss for current batch       |\n",
    "| `loss.item()`    | float  | Scalar value of batch mean loss   |\n",
    "| `images.size(0)` | int    | Number of samples in the batch    |\n",
    "| `total_loss`     | float  | Accumulated (summed) batch losses |\n",
    "| `epoch_loss`     | float  | Average loss over the whole epoch |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
