{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2348b9d4-a29f-4158-90a8-cd441e9d780b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1. Motivation\n",
    "\n",
    "ConvNeXt (CVPR 2022, Facebook/Meta) aims to answer a question:\n",
    "\n",
    "**If we modernize classical CNNs with all the training tricks used in Vision Transformers (ViT), could a pure CNN match or surpass ViT?**\n",
    "\n",
    "ConvNeXt = **ResNet-50/ResNet-200 rewritten with modern design choices inspired by ViT**, without using attention.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "* Replace all **3×3 convolutions** in bottleneck blocks with **depthwise 7×7 convolutions**.\n",
    "* Use **LayerNorm** instead of BatchNorm.\n",
    "* Use very large **conv kernels** (7×7 DW).\n",
    "* Use **ConvNeXt blocks** that structurally resemble **MLP blocks in ViT**.\n",
    "* Use **inverted bottlenecks** with very large expansion (just like MLP ratios in ViT).\n",
    "\n",
    "ConvNeXt is therefore a **CNN that behaves like a Vision Transformer**, but runs faster and often performs better.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Overall Architecture\n",
    "\n",
    "ConvNeXt uses a **4-stage hierarchy** similar to ResNet / Swin Transformer:\n",
    "\n",
    "| Stage   | Resolution      | Channels |\n",
    "| ------- | --------------- | -------- |\n",
    "| Stage 1 | 224×224 → 56×56 | C        |\n",
    "| Stage 2 | 56×56 → 28×28   | 2C       |\n",
    "| Stage 3 | 28×28 → 14×14   | 4C       |\n",
    "| Stage 4 | 14×14 → 7×7     | 8C       |\n",
    "\n",
    "A Tiny model uses C=96.\n",
    "A Base model uses C=128.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ConvNeXt Block\n",
    "\n",
    "The ConvNeXt block has **four main parts**:\n",
    "\n",
    "1. **Depthwise convolution**\n",
    "   Kernel size **7×7** (very large), per-channel:\n",
    "\n",
    "   $$\n",
    "   y_{c} = x_{c} * k^{(c)}\n",
    "   $$\n",
    "\n",
    "2. **LayerNorm**\n",
    "   Applied channel-wise:\n",
    "\n",
    "   $$\n",
    "   \\hat{x} = \\frac{x - \\mu}{\\sigma}\n",
    "   $$\n",
    "\n",
    "3. **Pointwise MLP (two linear layers using 1×1 convs)**\n",
    "   Expansion ratio = **4×**:\n",
    "\n",
    "   $$\n",
    "   u = W_1 \\hat{x}\n",
    "   $$\n",
    "   $$\n",
    "   z = \\text{GELU}(u)\n",
    "   $$\n",
    "   $$\n",
    "   y = W_2 z\n",
    "   $$\n",
    "\n",
    "4. **Residual connection**:\n",
    "\n",
    "   $$\n",
    "   \\text{Block}(x) = x + y\n",
    "   $$\n",
    "\n",
    "This is **almost identical** to a ViT MLP block, except the attention is replaced by a **depthwise convolution**.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Comparison with a ResNet Bottleneck\n",
    "\n",
    "ResNet bottleneck block:\n",
    "\n",
    "* 1×1 → 3×3 → 1×1\n",
    "* BatchNorm everywhere\n",
    "* Activation after each conv\n",
    "* Small expansion ratio (4× internally but shrinks back)\n",
    "\n",
    "ConvNeXt block:\n",
    "\n",
    "* No BatchNorm → LayerNorm only\n",
    "* **7×7 depthwise conv**\n",
    "* **Inverted bottleneck** (expand → shrink), exactly like a ViT MLP block\n",
    "* Very few activations (GELU only once)\n",
    "* No ReLU after every conv\n",
    "\n",
    "This makes ConvNeXt much closer to ViT.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Stage Downsampling Design\n",
    "\n",
    "Between stages, ConvNeXt uses a **simple downsampling layer**:\n",
    "\n",
    "* LayerNorm\n",
    "* 2×2 stride-2 convolution\n",
    "\n",
    "This mimics ViT patch embeddings and Swin Transformer patch merging.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Full Block Diagram (Text)\n",
    "\n",
    "```\n",
    "Input\n",
    " │\n",
    " ▼\n",
    "Depthwise Conv (7×7)\n",
    " │\n",
    " ▼\n",
    "LayerNorm\n",
    " │\n",
    " ▼\n",
    "Pointwise 1×1 Conv (4× expansion)\n",
    " │\n",
    " ▼\n",
    "GELU\n",
    " │\n",
    " ▼\n",
    "Pointwise 1×1 Conv (projection)\n",
    " │\n",
    " ▼\n",
    "Add residual\n",
    " │\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Mathematical Form of a Single Block\n",
    "\n",
    "Let the input be a tensor\n",
    "$$ x \\in \\mathbb{R}^{H \\times W \\times C} $$\n",
    "\n",
    "Step 1: Depthwise convolution\n",
    "$$\n",
    "u_{h,w,c} = (x_{:, :, c} * k_c)(h,w)\n",
    "$$\n",
    "\n",
    "Step 2: LayerNorm\n",
    "$$\n",
    "\\hat{u}*{h,w,c} = \\frac{u*{h,w,c} - \\mu_c}{\\sigma_c}\n",
    "$$\n",
    "\n",
    "Step 3: Expansion\n",
    "$$\n",
    "v_{h,w,c'} = \\sum_{c} W^{(1)}*{c,c'},\\hat{u}*{h,w,c}\n",
    "$$\n",
    "with expansion ratio 4, so $c' = 4C$.\n",
    "\n",
    "Step 4: GELU\n",
    "$$\n",
    "g_{h,w,c'} = \\text{GELU}(v_{h,w,c'})\n",
    "$$\n",
    "\n",
    "Step 5: Projection\n",
    "$$\n",
    "p_{h,w,c} = \\sum_{c'} W^{(2)}*{c',c}, g*{h,w,c'}\n",
    "$$\n",
    "\n",
    "Step 6: Residual\n",
    "$$\n",
    "y = x + p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Why ConvNeXt Works So Well\n",
    "\n",
    "ConvNeXt integrates the best ideas from both CNNs and Transformers:\n",
    "\n",
    "### From Transformers:\n",
    "\n",
    "* LayerNorm instead of BatchNorm\n",
    "* GELU activation\n",
    "* Large MLP expansion\n",
    "* Fewer nonlinearities\n",
    "* Simple stage transitions\n",
    "\n",
    "### From CNNs:\n",
    "\n",
    "* Efficient convolutions\n",
    "* Local receptive fields\n",
    "* No attention (faster, cheaper)\n",
    "\n",
    "The result is a **ViT-level accuracy with CNN-level speed**.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Model Variants\n",
    "\n",
    "ConvNeXt models follow Swin-style naming:\n",
    "\n",
    "* **ConvNeXt-T** (Tiny)\n",
    "* **ConvNeXt-S** (Small)\n",
    "* **ConvNeXt-B** (Base)\n",
    "* **ConvNeXt-L** (Large)\n",
    "* **ConvNeXt-XL** (Extra Large)\n",
    "* **ConvNeXt-v2** (2023, improved training + LayerScale)\n",
    "\n",
    "---\n",
    "\n",
    "# 10. How it Performs\n",
    "\n",
    "ConvNeXt-Base (pure CNN) ≈ ViT-Base\n",
    "ConvNeXt-Large ≈ ViT-Large\n",
    "ConvNeXt-XL beats Swin-B and ViT-L in some tasks\n",
    "\n",
    "Used widely for:\n",
    "\n",
    "* Classification\n",
    "* Detection (with FPN/Mask-RCNN)\n",
    "* Segmentation (with UperNet / FPN)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515c38f-9040-4a19-a6b6-b0767d30323d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PyTorch implementation of a ConvNeXt block\n",
    "\n",
    "This is a **standalone, minimal but faithful** ConvNeXt block:\n",
    "\n",
    "* Depthwise 7×7 convolution\n",
    "* LayerNorm (channels-last)\n",
    "* 1×1 conv MLP with 4× expansion\n",
    "* GELU\n",
    "* Optional LayerScale\n",
    "* Optional stochastic depth (DropPath)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "```\n",
    "\n",
    "### 1.1 Utility: DropPath (stochastic depth)\n",
    "\n",
    "```python\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-sample stochastic depth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob: float = 0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        # shape: (batch, 1, 1, 1) so it is broadcast across H, W, C\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor = torch.floor(random_tensor)  # 0 or 1\n",
    "        return x / keep_prob * random_tensor\n",
    "```\n",
    "\n",
    "### 1.2 Utility: LayerNorm for channels-last\n",
    "\n",
    "ConvNeXt uses channels-last (`N, H, W, C`) internally for LayerNorm efficiency.\n",
    "\n",
    "```python\n",
    "class LayerNormChannelsLast(nn.LayerNorm):\n",
    "    \"\"\"\n",
    "    LayerNorm expecting input in (B, H, W, C) format.\n",
    "    Inherits nn.LayerNorm but just documents expected layout.\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__(normalized_shape, eps=eps)\n",
    "```\n",
    "\n",
    "### 1.3 ConvNeXt block (single stage block)\n",
    "\n",
    "```python\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop_path: float = 0.0,\n",
    "        layer_scale_init_value: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dim: number of channels (C)\n",
    "        mlp_ratio: expansion ratio in the 1x1 conv MLP\n",
    "        drop_path: stochastic depth rate\n",
    "        layer_scale_init_value: if > 0, uses a learnable gamma vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            dim, dim,\n",
    "            kernel_size=7,\n",
    "            padding=3,\n",
    "            groups=dim  # depthwise\n",
    "        )\n",
    "\n",
    "        self.norm = LayerNormChannelsLast(dim, eps=1e-6)\n",
    "\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.pwconv1 = nn.Linear(dim, hidden_dim)  # channels-last, so use Linear\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.gamma = nn.Parameter(\n",
    "                layer_scale_init_value * torch.ones(dim),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        shortcut = x\n",
    "\n",
    "        # 1) depthwise conv in NCHW\n",
    "        x = self.dwconv(x)  # (B, C, H, W)\n",
    "\n",
    "        # 2) convert to NHWC for LayerNorm + MLP\n",
    "        x = x.permute(0, 2, 3, 1)  # (B, H, W, C)\n",
    "\n",
    "        # 3) LayerNorm\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 4) MLP: Linear -> GELU -> Linear\n",
    "        x = self.pwconv1(x)       # (B, H, W, hidden_dim)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)       # (B, H, W, C)\n",
    "\n",
    "        # 5) LayerScale (optional)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "\n",
    "        # 6) back to NCHW\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "        # 7) residual + drop_path\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "You can drop this into a stage like:\n",
    "\n",
    "```python\n",
    "class ConvNeXtStage(nn.Module):\n",
    "    def __init__(self, dim, depth, mlp_ratio=4.0, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        dpr = torch.linspace(0, drop_path_rate, depth).tolist()  # different drop_rates\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvNeXtBlock(\n",
    "                    dim=dim,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    drop_path=dpr[i],\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Step-by-step flow diagram of the ConvNeXt block\n",
    "\n",
    "Assume input tensor\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times C \\times H \\times W}.\n",
    "$$\n",
    "\n",
    "High-level diagram:\n",
    "\n",
    "```text\n",
    "        ┌─────────────────────────────┐\n",
    "        │         Input x            │\n",
    "        │     (B, C, H, W)           │\n",
    "        └────────────┬───────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "          Depthwise Conv 7×7 (groups=C)\n",
    "          (B, C, H, W)\n",
    "                     │\n",
    "                     ▼\n",
    "          Permute → (B, H, W, C)\n",
    "                     │\n",
    "                     ▼\n",
    "                LayerNorm\n",
    "               (per-channel)\n",
    "                     │\n",
    "                     ▼\n",
    "              Linear (C → 4C)\n",
    "                     │\n",
    "                     ▼\n",
    "                    GELU\n",
    "                     │\n",
    "                     ▼\n",
    "              Linear (4C → C)\n",
    "                     │\n",
    "                     ▼\n",
    "              (optional) γ ⊙ x\n",
    "                LayerScale\n",
    "                     │\n",
    "                     ▼\n",
    "          Permute → (B, C, H, W)\n",
    "                     │\n",
    "                     ▼\n",
    "            DropPath (stochastic)\n",
    "                     │\n",
    "                     ▼\n",
    "          Residual add with input\n",
    "          y = x_input + Δx\n",
    "                     │\n",
    "                     ▼\n",
    "                Output y\n",
    "            (B, C, H, W)\n",
    "```\n",
    "\n",
    "Step-by-step narrative:\n",
    "\n",
    "1. **Input**\n",
    "   Take input feature map\n",
    "   $$x \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "2. **Depthwise 7×7 convolution**\n",
    "   Apply depthwise conv, one kernel per channel:\n",
    "   $$u = \\text{DWConv}_{7\\times 7}(x) \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "3. **Change layout**\n",
    "   Permute to channels-last:\n",
    "   $$u' = \\text{permute}(u) \\in \\mathbb{R}^{B \\times H \\times W \\times C}.$$\n",
    "\n",
    "4. **LayerNorm**\n",
    "   Normalize each channel:\n",
    "   $$\\hat{u} = \\text{LayerNorm}(u').$$\n",
    "\n",
    "5. **First linear (expansion)**\n",
    "   $$v = \\hat{u} W_1 + b_1,$$\n",
    "   where\n",
    "   $$W_1 \\in \\mathbb{R}^{C \\times 4C}, \\quad v \\in \\mathbb{R}^{B \\times H \\times W \\times 4C}.$$\n",
    "\n",
    "6. **Nonlinearity**\n",
    "   $$g = \\text{GELU}(v).$$\n",
    "\n",
    "7. **Second linear (projection)**\n",
    "   $$p = g W_2 + b_2,$$\n",
    "   where\n",
    "   $$W_2 \\in \\mathbb{R}^{4C \\times C}, \\quad p \\in \\mathbb{R}^{B \\times H \\times W \\times C}.$$\n",
    "\n",
    "8. **LayerScale (optional)**\n",
    "   If using gamma:\n",
    "   $$p' = \\gamma \\odot p,$$\n",
    "   where\n",
    "   $$\\gamma \\in \\mathbb{R}^{C}.$$\n",
    "\n",
    "9. **Back to NCHW**\n",
    "   $$p'' = \\text{permute}(p') \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "10. **DropPath**\n",
    "    $$\\Delta x = \\text{DropPath}(p'').$$\n",
    "\n",
    "11. **Residual add**\n",
    "    $$y = x + \\Delta x.$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ConvNeXt training recipe (classification)\n",
    "\n",
    "This is a standard, ImageNet-style training recipe adapted from common ConvNeXt usage.\n",
    "\n",
    "### 3.1 Data preprocessing\n",
    "\n",
    "**Input size**: 224×224 (for standard models)\n",
    "\n",
    "**Training transforms**:\n",
    "\n",
    "* RandomResizedCrop(224, interpolation=bilinear)\n",
    "* RandomHorizontalFlip(0.5)\n",
    "* Color jitter (optional, light)\n",
    "* AutoAugment or RandAugment (recommended)\n",
    "* Mixup + CutMix\n",
    "* Random Erasing\n",
    "* Normalize with ImageNet mean/std:\n",
    "\n",
    "  * mean = [0.485, 0.456, 0.406]\n",
    "  * std = [0.229, 0.224, 0.225]\n",
    "\n",
    "**Validation transforms**:\n",
    "\n",
    "* Resize shorter side to 256\n",
    "* CenterCrop(224×224)\n",
    "* Normalize with same mean/std\n",
    "\n",
    "### 3.2 Optimizer and schedule\n",
    "\n",
    "* Optimizer: **AdamW**\n",
    "* Base learning rate for large batch (e.g. 4096 global):\n",
    "  $$\\text{lr}_{\\text{base}} = 4 \\times 10^{-3}.$$\n",
    "* If your global batch size is smaller, scale linearly:\n",
    "$$\\text{lr} = \\text{lr}_{\\text{base}} \\times \\frac{\\text{batch\\_size}}{4096}.$$\n",
    "\n",
    "* Weight decay:\n",
    "  $$\\text{wd} = 0.05.$$\n",
    "* Betas: (0.9, 0.999)\n",
    "\n",
    "**Scheduler**:\n",
    "\n",
    "* Warmup: 20 epochs of linear warmup to peak lr\n",
    "* Then cosine decay down to a small value\n",
    "  $$\\text{lr}*{\\text{final}} \\approx \\text{lr}*{\\text{max}} \\times 10^{-2}.$$\n",
    "\n",
    "**Epochs**:\n",
    "\n",
    "* 300 epochs for ImageNet from scratch (common setting)\n",
    "* You can do 100–150 epochs for smaller experiments.\n",
    "\n",
    "**Regularization details**:\n",
    "\n",
    "* Label smoothing:\n",
    "  $$\\epsilon = 0.1.$$\n",
    "* Mixup: alpha = 0.8\n",
    "* CutMix: alpha = 1.0\n",
    "* DropPath (stochastic depth): linearly increased with depth, e.g. max 0.1–0.3 for deeper models.\n",
    "\n",
    "### 3.3 Simple PyTorch training skeleton\n",
    "\n",
    "Below is a **minimal** training loop skeleton for classification using a ConvNeXt backbone (you can plug in your own model):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch, device, scaler=None, criterion=None):\n",
    "    model.train()\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "\n",
    "    for step, (images, targets) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # optional mixed precision\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct1, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item() * images.size(0)\n",
    "\n",
    "            _, pred = outputs.topk(1, dim=1)\n",
    "            correct1 += (pred.squeeze(1) == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    top1 = 100.0 * correct1 / total\n",
    "    avg_loss = loss_sum / total\n",
    "    print(f\"Validation: Loss={avg_loss:.4f}, Top-1 Acc={top1:.2f}%\")\n",
    "    return avg_loss, top1\n",
    "```\n",
    "\n",
    "And a high-level setup:\n",
    "\n",
    "```python\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "model = YourConvNeXtModel(num_classes=1000).to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=4e-3,      # adjust for your batch size\n",
    "    weight_decay=0.05,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # update lr via scheduler here if you use cosine\n",
    "    train_one_epoch(model, train_loader, optimizer, epoch, device, scaler=scaler)\n",
    "    validate(model, val_loader, device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
