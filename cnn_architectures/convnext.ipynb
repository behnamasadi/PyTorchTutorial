{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2348b9d4-a29f-4158-90a8-cd441e9d780b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1. Motivation\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/convnext1.png\" width=\"40%\" height=\"40%\" />\n",
    "\n",
    "ConvNeXt (CVPR 2022, Facebook/Meta) aims to answer a question:\n",
    "\n",
    "**If we modernize classical CNNs with all the training tricks used in Vision Transformers (ViT), could a pure CNN match or surpass ViT?**\n",
    "\n",
    "ConvNeXt = **ResNet-50/ResNet-200 rewritten with modern design choices inspired by ViT**, without using attention.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "* Replace all **3√ó3 convolutions** in bottleneck blocks with **depthwise 7√ó7 convolutions**.\n",
    "* Use **LayerNorm** instead of BatchNorm.\n",
    "* Use very large **conv kernels** (7√ó7 DW).\n",
    "* Use **ConvNeXt blocks** that structurally resemble **MLP blocks in ViT**.\n",
    "* Use **inverted bottlenecks** with very large expansion (just like MLP ratios in ViT).\n",
    "\n",
    "ConvNeXt is therefore a **CNN that behaves like a Vision Transformer**, but runs faster and often performs better.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Overall Architecture\n",
    "\n",
    "ConvNeXt uses a **4-stage hierarchy** similar to ResNet / Swin Transformer:\n",
    "\n",
    "| Stage   | Resolution      | Channels |\n",
    "| ------- | --------------- | -------- |\n",
    "| Stage 1 | 224√ó224 ‚Üí 56√ó56 | C        |\n",
    "| Stage 2 | 56√ó56 ‚Üí 28√ó28   | 2C       |\n",
    "| Stage 3 | 28√ó28 ‚Üí 14√ó14   | 4C       |\n",
    "| Stage 4 | 14√ó14 ‚Üí 7√ó7     | 8C       |\n",
    "\n",
    "A Tiny model uses C=96.\n",
    "A Base model uses C=128.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ConvNeXt Block\n",
    "\n",
    "The ConvNeXt block has **four main parts**:\n",
    "\n",
    "1. **Depthwise convolution**\n",
    "   Kernel size **7√ó7** (very large), per-channel:\n",
    "\n",
    "   $$\n",
    "   y_{c} = x_{c} * k^{(c)}\n",
    "   $$\n",
    "\n",
    "2. **LayerNorm**\n",
    "   Applied channel-wise:\n",
    "\n",
    "   $$\n",
    "   \\hat{x} = \\frac{x - \\mu}{\\sigma}\n",
    "   $$\n",
    "\n",
    "3. **Pointwise MLP (two linear layers using 1√ó1 convs)**\n",
    "   Expansion ratio = **4√ó**:\n",
    "\n",
    "   $$\n",
    "   u = W_1 \\hat{x}\n",
    "   $$\n",
    "   $$\n",
    "   z = \\text{GELU}(u)\n",
    "   $$\n",
    "   $$\n",
    "   y = W_2 z\n",
    "   $$\n",
    "\n",
    "4. **Residual connection**:\n",
    "\n",
    "   $$\n",
    "   \\text{Block}(x) = x + y\n",
    "   $$\n",
    "\n",
    "This is **almost identical** to a ViT MLP block, except the attention is replaced by a **depthwise convolution**.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Comparison with a ResNet Bottleneck\n",
    "\n",
    "ResNet bottleneck block:\n",
    "\n",
    "* 1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1\n",
    "* BatchNorm everywhere\n",
    "* Activation after each conv\n",
    "* Small expansion ratio (4√ó internally but shrinks back)\n",
    "\n",
    "ConvNeXt block:\n",
    "\n",
    "* No BatchNorm ‚Üí LayerNorm only\n",
    "* **7√ó7 depthwise conv**\n",
    "* **Inverted bottleneck** (expand ‚Üí shrink), exactly like a ViT MLP block\n",
    "* Very few activations (GELU only once)\n",
    "* No ReLU after every conv\n",
    "\n",
    "This makes ConvNeXt much closer to ViT.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Stage Downsampling Design\n",
    "\n",
    "Between stages, ConvNeXt uses a **simple downsampling layer**:\n",
    "\n",
    "* LayerNorm\n",
    "* 2√ó2 stride-2 convolution\n",
    "\n",
    "This mimics ViT patch embeddings and Swin Transformer patch merging.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Full Block Diagram (Text)\n",
    "\n",
    "```\n",
    "Input\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "Depthwise Conv (7√ó7)\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "LayerNorm\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "Pointwise 1√ó1 Conv (4√ó expansion)\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "GELU\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "Pointwise 1√ó1 Conv (projection)\n",
    " ‚îÇ\n",
    " ‚ñº\n",
    "Add residual\n",
    " ‚îÇ\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Mathematical Form of a Single Block\n",
    "\n",
    "Let the input be a tensor\n",
    "$$ x \\in \\mathbb{R}^{H \\times W \\times C} $$\n",
    "\n",
    "Step 1: Depthwise convolution\n",
    "$$\n",
    "u_{h,w,c} = (x_{:, :, c} * k_c)(h,w)\n",
    "$$\n",
    "\n",
    "Step 2: LayerNorm\n",
    "$$\n",
    "\\hat{u}_{h,w,c} = \\frac{u_{h,w,c} - \\mu_c}{\\sigma_c}\n",
    "$$\n",
    "\n",
    "Step 3: Expansion\n",
    "$$\n",
    "v_{h,w,c'} = \\sum_{c} W^{(1)}_{c,c'},\\hat{u}_{h,w,c}\n",
    "$$\n",
    "with expansion ratio 4, so $c' = 4C$.\n",
    "\n",
    "Step 4: GELU\n",
    "$$\n",
    "g_{h,w,c'} = \\text{GELU}(v_{h,w,c'})\n",
    "$$\n",
    "\n",
    "Step 5: Projection\n",
    "$$\n",
    "p_{h,w,c} = \\sum_{c'} W^{(2)}_{c',c}, g_{h,w,c'}\n",
    "$$\n",
    "\n",
    "Step 6: Residual\n",
    "$$\n",
    "y = x + p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Why ConvNeXt Works So Well\n",
    "\n",
    "ConvNeXt integrates the best ideas from both CNNs and Transformers:\n",
    "\n",
    "### From Transformers:\n",
    "\n",
    "* LayerNorm instead of BatchNorm\n",
    "* GELU activation\n",
    "* Large MLP expansion\n",
    "* Fewer nonlinearities\n",
    "* Simple stage transitions\n",
    "\n",
    "### From CNNs:\n",
    "\n",
    "* Efficient convolutions\n",
    "* Local receptive fields\n",
    "* No attention (faster, cheaper)\n",
    "\n",
    "The result is a **ViT-level accuracy with CNN-level speed**.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Model Variants\n",
    "\n",
    "ConvNeXt models follow Swin-style naming:\n",
    "\n",
    "* **ConvNeXt-T** (Tiny)\n",
    "* **ConvNeXt-S** (Small)\n",
    "* **ConvNeXt-B** (Base)\n",
    "* **ConvNeXt-L** (Large)\n",
    "* **ConvNeXt-XL** (Extra Large)\n",
    "* **ConvNeXt-v2** (2023, improved training + LayerScale)\n",
    "\n",
    "---\n",
    "\n",
    "# 10. How it Performs\n",
    "\n",
    "ConvNeXt-Base (pure CNN) ‚âà ViT-Base\n",
    "ConvNeXt-Large ‚âà ViT-Large\n",
    "ConvNeXt-XL beats Swin-B and ViT-L in some tasks\n",
    "\n",
    "Used widely for:\n",
    "\n",
    "* Classification\n",
    "* Detection (with FPN/Mask-RCNN)\n",
    "* Segmentation (with UperNet / FPN)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515c38f-9040-4a19-a6b6-b0767d30323d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PyTorch implementation of a ConvNeXt block\n",
    "\n",
    "This is a **standalone, minimal but faithful** ConvNeXt block:\n",
    "\n",
    "* Depthwise 7√ó7 convolution\n",
    "* LayerNorm (channels-last)\n",
    "* 1√ó1 conv MLP with 4√ó expansion\n",
    "* GELU\n",
    "* Optional LayerScale\n",
    "* Optional stochastic depth (DropPath)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "```\n",
    "\n",
    "### 1.1 Utility: DropPath (stochastic depth)\n",
    "\n",
    "```python\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-sample stochastic depth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob: float = 0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        # shape: (batch, 1, 1, 1) so it is broadcast across H, W, C\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor = torch.floor(random_tensor)  # 0 or 1\n",
    "        return x / keep_prob * random_tensor\n",
    "```\n",
    "\n",
    "### 1.2 Utility: LayerNorm for channels-last\n",
    "\n",
    "ConvNeXt uses channels-last (`N, H, W, C`) internally for LayerNorm efficiency.\n",
    "\n",
    "```python\n",
    "class LayerNormChannelsLast(nn.LayerNorm):\n",
    "    \"\"\"\n",
    "    LayerNorm expecting input in (B, H, W, C) format.\n",
    "    Inherits nn.LayerNorm but just documents expected layout.\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__(normalized_shape, eps=eps)\n",
    "```\n",
    "\n",
    "### 1.3 ConvNeXt block (single stage block)\n",
    "\n",
    "```python\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop_path: float = 0.0,\n",
    "        layer_scale_init_value: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dim: number of channels (C)\n",
    "        mlp_ratio: expansion ratio in the 1x1 conv MLP\n",
    "        drop_path: stochastic depth rate\n",
    "        layer_scale_init_value: if > 0, uses a learnable gamma vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            dim, dim,\n",
    "            kernel_size=7,\n",
    "            padding=3,\n",
    "            groups=dim  # depthwise\n",
    "        )\n",
    "\n",
    "        self.norm = LayerNormChannelsLast(dim, eps=1e-6)\n",
    "\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.pwconv1 = nn.Linear(dim, hidden_dim)  # channels-last, so use Linear\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "        if layer_scale_init_value > 0:\n",
    "            self.gamma = nn.Parameter(\n",
    "                layer_scale_init_value * torch.ones(dim),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        shortcut = x\n",
    "\n",
    "        # 1) depthwise conv in NCHW\n",
    "        x = self.dwconv(x)  # (B, C, H, W)\n",
    "\n",
    "        # 2) convert to NHWC for LayerNorm + MLP\n",
    "        x = x.permute(0, 2, 3, 1)  # (B, H, W, C)\n",
    "\n",
    "        # 3) LayerNorm\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 4) MLP: Linear -> GELU -> Linear\n",
    "        x = self.pwconv1(x)       # (B, H, W, hidden_dim)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)       # (B, H, W, C)\n",
    "\n",
    "        # 5) LayerScale (optional)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "\n",
    "        # 6) back to NCHW\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "        # 7) residual + drop_path\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "You can drop this into a stage like:\n",
    "\n",
    "```python\n",
    "class ConvNeXtStage(nn.Module):\n",
    "    def __init__(self, dim, depth, mlp_ratio=4.0, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        dpr = torch.linspace(0, drop_path_rate, depth).tolist()  # different drop_rates\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvNeXtBlock(\n",
    "                    dim=dim,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    drop_path=dpr[i],\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Step-by-step flow diagram of the ConvNeXt block\n",
    "\n",
    "Assume input tensor\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times C \\times H \\times W}.\n",
    "$$\n",
    "\n",
    "High-level diagram:\n",
    "\n",
    "```text\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ         Input x            ‚îÇ\n",
    "        ‚îÇ     (B, C, H, W)           ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "          Depthwise Conv 7√ó7 (groups=C)\n",
    "          (B, C, H, W)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "          Permute ‚Üí (B, H, W, C)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "                LayerNorm\n",
    "               (per-channel)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "              Linear (C ‚Üí 4C)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "                    GELU\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "              Linear (4C ‚Üí C)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "              (optional) Œ≥ ‚äô x\n",
    "                LayerScale\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "          Permute ‚Üí (B, C, H, W)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "            DropPath (stochastic)\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "          Residual add with input\n",
    "          y = x_input + Œîx\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "                Output y\n",
    "            (B, C, H, W)\n",
    "```\n",
    "\n",
    "Step-by-step narrative:\n",
    "\n",
    "1. **Input**\n",
    "   Take input feature map\n",
    "   $$x \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "2. **Depthwise 7√ó7 convolution**\n",
    "   Apply depthwise conv, one kernel per channel:\n",
    "   $$u = \\text{DWConv}_{7\\times 7}(x) \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "3. **Change layout**\n",
    "   Permute to channels-last:\n",
    "   $$u' = \\text{permute}(u) \\in \\mathbb{R}^{B \\times H \\times W \\times C}.$$\n",
    "\n",
    "4. **LayerNorm**\n",
    "   Normalize each channel:\n",
    "   $$\\hat{u} = \\text{LayerNorm}(u').$$\n",
    "\n",
    "5. **First linear (expansion)**\n",
    "   $$v = \\hat{u} W_1 + b_1,$$\n",
    "   where\n",
    "   $$W_1 \\in \\mathbb{R}^{C \\times 4C}, \\quad v \\in \\mathbb{R}^{B \\times H \\times W \\times 4C}.$$\n",
    "\n",
    "6. **Nonlinearity**\n",
    "   $$g = \\text{GELU}(v).$$\n",
    "\n",
    "7. **Second linear (projection)**\n",
    "   $$p = g W_2 + b_2,$$\n",
    "   where\n",
    "   $$W_2 \\in \\mathbb{R}^{4C \\times C}, \\quad p \\in \\mathbb{R}^{B \\times H \\times W \\times C}.$$\n",
    "\n",
    "8. **LayerScale (optional)**\n",
    "   If using gamma:\n",
    "   $$p' = \\gamma \\odot p,$$\n",
    "   where\n",
    "   $$\\gamma \\in \\mathbb{R}^{C}.$$\n",
    "\n",
    "9. **Back to NCHW**\n",
    "   $$p'' = \\text{permute}(p') \\in \\mathbb{R}^{B \\times C \\times H \\times W}.$$\n",
    "\n",
    "10. **DropPath**\n",
    "    $$\\Delta x = \\text{DropPath}(p'').$$\n",
    "\n",
    "11. **Residual add**\n",
    "    $$y = x + \\Delta x.$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ConvNeXt training recipe (classification)\n",
    "\n",
    "This is a standard, ImageNet-style training recipe adapted from common ConvNeXt usage.\n",
    "\n",
    "### 3.1 Data preprocessing\n",
    "\n",
    "**Input size**: 224√ó224 (for standard models)\n",
    "\n",
    "**Training transforms**:\n",
    "\n",
    "* RandomResizedCrop(224, interpolation=bilinear)\n",
    "* RandomHorizontalFlip(0.5)\n",
    "* Color jitter (optional, light)\n",
    "* AutoAugment or RandAugment (recommended)\n",
    "* Mixup + CutMix\n",
    "* Random Erasing\n",
    "* Normalize with ImageNet mean/std:\n",
    "\n",
    "  * mean = [0.485, 0.456, 0.406]\n",
    "  * std = [0.229, 0.224, 0.225]\n",
    "\n",
    "**Validation transforms**:\n",
    "\n",
    "* Resize shorter side to 256\n",
    "* CenterCrop(224√ó224)\n",
    "* Normalize with same mean/std\n",
    "\n",
    "### 3.2 Optimizer and schedule\n",
    "\n",
    "* Optimizer: **AdamW**\n",
    "* Base learning rate for large batch (e.g. 4096 global):\n",
    "  $$\\text{lr}_{\\text{base}} = 4 \\times 10^{-3}.$$\n",
    "* If your global batch size is smaller, scale linearly:\n",
    "$$\\text{lr} = \\text{lr}_{\\text{base}} \\times \\frac{\\text{batch\\_size}}{4096}.$$\n",
    "\n",
    "* Weight decay:\n",
    "  $$\\text{wd} = 0.05.$$\n",
    "* Betas: (0.9, 0.999)\n",
    "\n",
    "**Scheduler**:\n",
    "\n",
    "* Warmup: 20 epochs of linear warmup to peak lr\n",
    "* Then cosine decay down to a small value\n",
    "  $$\\text{lr}*{\\text{final}} \\approx \\text{lr}*{\\text{max}} \\times 10^{-2}.$$\n",
    "\n",
    "**Epochs**:\n",
    "\n",
    "* 300 epochs for ImageNet from scratch (common setting)\n",
    "* You can do 100‚Äì150 epochs for smaller experiments.\n",
    "\n",
    "**Regularization details**:\n",
    "\n",
    "* Label smoothing:\n",
    "  $$\\epsilon = 0.1.$$\n",
    "* Mixup: alpha = 0.8\n",
    "* CutMix: alpha = 1.0\n",
    "* DropPath (stochastic depth): linearly increased with depth, e.g. max 0.1‚Äì0.3 for deeper models.\n",
    "\n",
    "### 3.3 Simple PyTorch training skeleton\n",
    "\n",
    "Below is a **minimal** training loop skeleton for classification using a ConvNeXt backbone (you can plug in your own model):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch, device, scaler=None, criterion=None):\n",
    "    model.train()\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "\n",
    "    for step, (images, targets) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # optional mixed precision\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct1, total = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item() * images.size(0)\n",
    "\n",
    "            _, pred = outputs.topk(1, dim=1)\n",
    "            correct1 += (pred.squeeze(1) == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    top1 = 100.0 * correct1 / total\n",
    "    avg_loss = loss_sum / total\n",
    "    print(f\"Validation: Loss={avg_loss:.4f}, Top-1 Acc={top1:.2f}%\")\n",
    "    return avg_loss, top1\n",
    "```\n",
    "\n",
    "And a high-level setup:\n",
    "\n",
    "```python\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "model = YourConvNeXtModel(num_classes=1000).to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=4e-3,      # adjust for your batch size\n",
    "    weight_decay=0.05,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # update lr via scheduler here if you use cosine\n",
    "    train_one_epoch(model, train_loader, optimizer, epoch, device, scaler=scaler)\n",
    "    validate(model, val_loader, device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f94125-52bb-4a8c-b7e7-52fbf2925182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_atto.d2_in1k\n",
      "convnext_atto_ols.a2_in1k\n",
      "convnext_base.clip_laion2b\n",
      "convnext_base.clip_laion2b_augreg\n",
      "convnext_base.clip_laion2b_augreg_ft_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\n",
      "convnext_base.clip_laiona\n",
      "convnext_base.clip_laiona_320\n",
      "convnext_base.clip_laiona_augreg_320\n",
      "convnext_base.clip_laiona_augreg_ft_in1k_384\n",
      "convnext_base.fb_in1k\n",
      "convnext_base.fb_in22k\n",
      "convnext_base.fb_in22k_ft_in1k\n",
      "convnext_base.fb_in22k_ft_in1k_384\n",
      "convnext_femto.d1_in1k\n",
      "convnext_femto_ols.d1_in1k\n",
      "convnext_large.fb_in1k\n",
      "convnext_large.fb_in22k\n",
      "convnext_large.fb_in22k_ft_in1k\n",
      "convnext_large.fb_in22k_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_ft_320\n",
      "convnext_large_mlp.clip_laion2b_ft_soup_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384\n",
      "convnext_nano.d1h_in1k\n",
      "convnext_nano.in12k\n",
      "convnext_nano.in12k_ft_in1k\n",
      "convnext_nano.r384_ad_in12k\n",
      "convnext_nano.r384_in12k\n",
      "convnext_nano.r384_in12k_ft_in1k\n",
      "convnext_nano_ols.d1h_in1k\n",
      "convnext_pico.d1_in1k\n",
      "convnext_pico_ols.d1_in1k\n",
      "convnext_small.fb_in1k\n",
      "convnext_small.fb_in22k\n",
      "convnext_small.fb_in22k_ft_in1k\n",
      "convnext_small.fb_in22k_ft_in1k_384\n",
      "convnext_small.in12k\n",
      "convnext_small.in12k_ft_in1k\n",
      "convnext_small.in12k_ft_in1k_384\n",
      "convnext_tiny.fb_in1k\n",
      "convnext_tiny.fb_in22k\n",
      "convnext_tiny.fb_in22k_ft_in1k\n",
      "convnext_tiny.fb_in22k_ft_in1k_384\n",
      "convnext_tiny.in12k\n",
      "convnext_tiny.in12k_ft_in1k\n",
      "convnext_tiny.in12k_ft_in1k_384\n",
      "convnext_tiny_hnf.a2h_in1k\n",
      "convnext_xlarge.fb_in22k\n",
      "convnext_xlarge.fb_in22k_ft_in1k\n",
      "convnext_xlarge.fb_in22k_ft_in1k_384\n",
      "convnext_xxlarge.clip_laion2b_rewind\n",
      "convnext_xxlarge.clip_laion2b_soup\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in12k\n",
      "convnext_zepto_rms.ra4_e3600_r224_in1k\n",
      "convnext_zepto_rms_ols.ra4_e3600_r224_in1k\n",
      "convnextv2_atto.fcmae\n",
      "convnextv2_atto.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae\n",
      "convnextv2_base.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_femto.fcmae\n",
      "convnextv2_femto.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae\n",
      "convnextv2_huge.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_512\n",
      "convnextv2_large.fcmae\n",
      "convnextv2_large.fcmae_ft_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_nano.fcmae\n",
      "convnextv2_nano.fcmae_ft_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_pico.fcmae\n",
      "convnextv2_pico.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae\n",
      "convnextv2_tiny.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k_384\n",
      "test_convnext2.r160_in1k\n",
      "test_convnext3.r160_in1k\n",
      "test_convnext.r160_in1k\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "all_convnext = timm.list_models(\"*convnext*\", pretrained=True)\n",
    "for m in all_convnext:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894cefa4-fe52-4888-8559-1dbac8cd3961",
   "metadata": {},
   "source": [
    "# **ConvNeXt / ConvNeXt-v2 Model Name in timm**\n",
    "Below is a **complete, clean explanation** of all ConvNeXt names in **timm**, including:\n",
    "\n",
    "1. **Naming scheme (atto, pico, femto, nano...)**\n",
    "2. **Parameters, depth, width, FLOPs** (approx).\n",
    "3. **Differences between ConvNeXt and ConvNeXt-v2**\n",
    "4. **Meaning of OLS, RMS, HNF, MLP variants**\n",
    "5. **A table summarizing all models**\n",
    "\n",
    "Everything is structured so you can quickly choose the right backbone.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. ConvNeXt naming scheme in `timm`\n",
    "\n",
    "timm uses very small ‚Äúscientific scale‚Äù names for tiny models:\n",
    "\n",
    "| Name        | Meaning        | Typical Params |\n",
    "| ----------- | -------------- | -------------- |\n",
    "| **zepto**   | extremely tiny | ~1M‚Äì1.5M       |\n",
    "| **atto**    | very tiny      | ~2M‚Äì3M         |\n",
    "| **femto**   | tiny           | ~3M‚Äì5M         |\n",
    "| **pico**    | small-ish tiny | ~5M‚Äì7M         |\n",
    "| **nano**    | small          | ~7M‚Äì10M        |\n",
    "| **tiny**    | medium-small   | ~28M           |\n",
    "| **small**   | medium         | ~50M           |\n",
    "| **base**    | large-ish      | ~88M           |\n",
    "| **large**   | very large     | ~197M          |\n",
    "| **xlarge**  | bigger         | ~350M          |\n",
    "| **xxlarge** | huge           | ~600M+         |\n",
    "\n",
    "These correspond to the same conceptual hierarchy as MobileNet/ViT sizes (tiny ‚Üí small ‚Üí base ‚Üí large).\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Full explanation of model strings\n",
    "\n",
    "## **convnext_atto**\n",
    "\n",
    "* ConvNeXt-v1 ‚ÄúAtto‚Äù\n",
    "* ~2.0M params\n",
    "* Extreme lightweight model (MobileNet-like level)\n",
    "\n",
    "## **convnext_atto_ols**\n",
    "\n",
    "* Same as above, but **OLS = Omni Lite Scaling**, an experimental `timm` scaling method modifying depth/width for better FLOPs/accuracy tradeoff.\n",
    "\n",
    "## **convnext_atto_rms**\n",
    "\n",
    "* Same model, but **RMSNorm** instead of LayerNorm.\n",
    "* RMSNorm:\n",
    "  $$\n",
    "  y = \\frac{x}{\\sqrt{\\text{mean}(x^2)}}\\cdot g\n",
    "  $$\n",
    "  No bias; faster on some hardware.\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_base**\n",
    "\n",
    "* Standard ConvNeXt-B (Base)\n",
    "* 88M params\n",
    "* 224x224 FLOPs ‚âà 15.4 GFLOPs\n",
    "\n",
    "Equivalent to ViT-B in accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_femto**\n",
    "\n",
    "* ConvNeXt-v1 ‚ÄúFemto‚Äù\n",
    "* ~4M params\n",
    "* A bit larger than Atto\n",
    "\n",
    "## **convnext_femto_ols**\n",
    "\n",
    "* Same model with Omni Lite Scaling\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_large**\n",
    "\n",
    "* ConvNeXt-L (Large)\n",
    "* 197M params\n",
    "* ‚âà 34 GFLOPs\n",
    "* Large-scale backbone (ImageNet ~86‚Äì87% top-1)\n",
    "\n",
    "## **convnext_large_mlp**\n",
    "\n",
    "* Same ConvNeXt large, but classifier head is changed to a bigger MLP-style head.\n",
    "* Useful for some classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_nano**\n",
    "\n",
    "* Nano: ~7M params\n",
    "* Good compromise for edge devices\n",
    "* Faster than EfficientNet-B0 with similar accuracy\n",
    "\n",
    "## **convnext_nano_ols**\n",
    "\n",
    "* OLS scaling version\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_pico**\n",
    "\n",
    "* Pico: ~6M params\n",
    "* Very small but slightly more width than femto\n",
    "\n",
    "## **convnext_pico_ols**\n",
    "\n",
    "* OLS version\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_small**\n",
    "\n",
    "* ConvNeXt-S (Small)\n",
    "* 50M params\n",
    "* ‚âà 8.7 GFLOPs\n",
    "* Good for moderate GPU training\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_tiny**\n",
    "\n",
    "* ConvNeXt-T (Tiny)\n",
    "* 28M params\n",
    "* Sold as ‚ÄúResNet50 replacement‚Äù\n",
    "* ‚âà 4.5 GFLOPs\n",
    "\n",
    "## **convnext_tiny_hnf**\n",
    "\n",
    "* **hnf = High Norm Frequency** variant\n",
    "* Another experimental normalization variant in timm.\n",
    "* Uses alternative norm placement.\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_xlarge**\n",
    "\n",
    "* ConvNeXt-XL (Extra Large)\n",
    "* 350M params\n",
    "* ~60 GFLOPs\n",
    "* Very heavy backbone\n",
    "\n",
    "## **convnext_xxlarge**\n",
    "\n",
    "* ConvNeXt-XXL\n",
    "* ~600M params\n",
    "* More than 100 GFLOPs\n",
    "* Research-grade only\n",
    "\n",
    "---\n",
    "\n",
    "## **convnext_zepto_rms**\n",
    "\n",
    "* ‚ÄúZepto‚Äù size (~1‚Äì1.5M params)\n",
    "* Extreme low-resource\n",
    "* RMSNorm version\n",
    "\n",
    "## **convnext_zepto_rms_ols**\n",
    "\n",
    "* RMSNorm + OLS scaling.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ConvNeXt-v2 models\n",
    "\n",
    "ConvNeXt-v2 (Meta FAIR 2023) is an improved version:\n",
    "\n",
    "* Uses **FCN-based ConvNeXt block**\n",
    "* Adds **Global Response Normalization (GRN)**:\n",
    "  $$\n",
    "  G = \\frac{x}{\\sqrt{\\sum x^2 + \\epsilon}}\n",
    "  $$\n",
    "* Stronger performance at same parameter count\n",
    "\n",
    "### Names\n",
    "\n",
    "## **convnextv2_atto**\n",
    "\n",
    "* ~2.7M params\n",
    "* New v2 block\n",
    "* Much stronger than ConvNeXt-v1 Atto\n",
    "\n",
    "## **convnextv2_base**\n",
    "\n",
    "* 88M params (same as v1)\n",
    "* Higher accuracy\n",
    "\n",
    "## **convnextv2_femto**\n",
    "\n",
    "* ~4M params\n",
    "\n",
    "## **convnextv2_huge**\n",
    "\n",
    "* ~1B params\n",
    "* Very heavy research model\n",
    "\n",
    "## **convnextv2_large**\n",
    "\n",
    "* 197M params\n",
    "\n",
    "## **convnextv2_nano**\n",
    "\n",
    "* ~7M params\n",
    "\n",
    "## **convnextv2_pico**\n",
    "\n",
    "* ~5M params\n",
    "\n",
    "## **convnextv2_small**\n",
    "\n",
    "* ~50M params\n",
    "\n",
    "## **convnextv2_tiny**\n",
    "\n",
    "* ~28M params\n",
    "\n",
    "---\n",
    "\n",
    "# 4. test_convnext, test_convnext2, test_convnext3\n",
    "\n",
    "These are **internal timm experimental models** (not for normal use):\n",
    "\n",
    "* Used for architecture testing\n",
    "* Unstable\n",
    "* Do not load pretrained weights\n",
    "* Should be ignored unless debugging timm source code\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Summary table\n",
    "\n",
    "### ConvNeXt-v1\n",
    "\n",
    "| Model                  | Params | Notes               |\n",
    "| ---------------------- | ------ | ------------------- |\n",
    "| convnext_zepto_rms     | ~1M    | RMSNorm             |\n",
    "| convnext_zepto_rms_ols | ~1M    | RMSNorm + OLS       |\n",
    "| convnext_atto          | ~2M    | standard            |\n",
    "| convnext_atto_ols      | ~2M    | OLS                 |\n",
    "| convnext_atto_rms      | ~2M    | RMSNorm             |\n",
    "| convnext_femto         | ~4M    | tiny                |\n",
    "| convnext_femto_ols     | ~4M    | OLS                 |\n",
    "| convnext_pico          | ~6M    |                     |\n",
    "| convnext_pico_ols      | ~6M    | OLS                 |\n",
    "| convnext_nano          | ~7‚Äì8M  | good small backbone |\n",
    "| convnext_nano_ols      | ~7‚Äì8M  |                     |\n",
    "| convnext_tiny          | 28M    | standard Tiny       |\n",
    "| convnext_tiny_hnf      | 28M    | HNF norm            |\n",
    "| convnext_small         | 50M    |                     |\n",
    "| convnext_base          | 88M    |                     |\n",
    "| convnext_large         | 197M   |                     |\n",
    "| convnext_large_mlp     | 197M   | bigger MLP head     |\n",
    "| convnext_xlarge        | 350M   |                     |\n",
    "| convnext_xxlarge       | 600M+  |                     |\n",
    "\n",
    "### ConvNeXt-v2\n",
    "\n",
    "| Model            | Params | Notes          |\n",
    "| ---------------- | ------ | -------------- |\n",
    "| convnextv2_atto  | ~2.7M  | v2 block + GRN |\n",
    "| convnextv2_femto | ~4M    |                |\n",
    "| convnextv2_pico  | ~5M    |                |\n",
    "| convnextv2_nano  | ~7M    |                |\n",
    "| convnextv2_tiny  | 28M    |                |\n",
    "| convnextv2_small | 50M    |                |\n",
    "| convnextv2_base  | 88M    |                |\n",
    "| convnextv2_large | 197M   |                |\n",
    "| convnextv2_huge  | ~1B    |                |\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Encoding the names (if you want to detect/parse automatically)\n",
    "\n",
    "A regex that encodes the naming scheme:\n",
    "\n",
    "```\n",
    "^(convnextv2|convnext)_\n",
    "(zepto|atto|femto|pico|nano|tiny|small|base|large|xlarge|xxlarge)\n",
    "(_rms|_ols|_hnf|_mlp)?\n",
    "$\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* The first group = **family**\n",
    "* Second = **model size**\n",
    "* Third = **modifier** (optional)\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also:\n",
    "\n",
    "* Produce a CSV / JSON summary of all models\n",
    "* Compare ConvNeXt vs EfficientNet vs MobileNet vs ViT in FLOPs/params\n",
    "* Recommend which model you should use for your dataset (your images look ~chest X-ray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea5a12-1152-41b4-a17d-c0fea478fba9",
   "metadata": {},
   "source": [
    "# **ConvNeXt / ConvNeXt-v2 Models Ranking**\n",
    "Below is a **clean, practical ranking** of all ConvNeXt / ConvNeXt-v2 models from **timm**, ordered by what *you should actually use in real projects*, not just theoretical size.\n",
    "\n",
    "This ranking considers:\n",
    "\n",
    "* Accuracy\n",
    "* Stability\n",
    "* Training difficulty\n",
    "* Parameter efficiency\n",
    "* GPU memory\n",
    "* Usefulness for medical images (your case)\n",
    "* Real-world performance vs FLOPs\n",
    "\n",
    "It is the **definitive recommendation list**.\n",
    "\n",
    "---\n",
    "\n",
    "#  Final Ranking (Best ‚Üí Worst for real use)\n",
    "\n",
    "### **Tier 1 ‚Äî The best overall models (use these first)**\n",
    "\n",
    "1. **convnextv2_base**\n",
    "2. **convnextv2_small**\n",
    "3. **convnextv2_tiny**\n",
    "4. **convnext_tiny** (v1, still extremely strong)\n",
    "\n",
    "These four offer the best *accuracy-to-compute ratio*.\n",
    "They train easily and work on most GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "#  Tier 2 ‚Äî Very good models (if you want faster models)\n",
    "\n",
    "5. **convnextv2_nano**\n",
    "6. **convnextv2_pico**\n",
    "7. **convnext_nano**\n",
    "8. **convnext_small**\n",
    "\n",
    "These are perfect for:\n",
    "\n",
    "* Medium datasets\n",
    "* Edge devices\n",
    "* When you want high accuracy with low compute\n",
    "\n",
    "---\n",
    "\n",
    "#  Tier 3 ‚Äî Large research-grade / heavy models (avoid unless you need max accuracy)\n",
    "\n",
    "9. **convnextv2_large**\n",
    "10. **convnextv2_huge**\n",
    "11. **convnext_large**\n",
    "12. **convnext_xlarge**\n",
    "13. **convnext_xxlarge**\n",
    "\n",
    "Reasons:\n",
    "\n",
    "* Require huge batch sizes\n",
    "* Hard to train\n",
    "* Will **not** give you meaningful gain on a small dataset (like X-rays)\n",
    "* Mainly for ImageNet-1k/22k or huge datasets\n",
    "* Require multi-GPU or A100/H100 hardware\n",
    "\n",
    "---\n",
    "\n",
    "#  Tier 4 ‚Äî Very tiny models (OK but lower accuracy)\n",
    "\n",
    "14. **convnextv2_femto**\n",
    "15. **convnextv2_atto**\n",
    "16. **convnext_femto**\n",
    "17. **convnext_pico**\n",
    "18. **convnext_atto**\n",
    "19. **convnext_zepto_rms**\n",
    "\n",
    "Use these **only** for ultra-low-compute environments or mobile apps.\n",
    "Not good for deep, subtle medical image patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#  Tier 5 ‚Äî Experimental / special variants (avoid)\n",
    "\n",
    "20. **convnext_‚Ä¶_ols** (OLS variants)\n",
    "21. **convnext_‚Ä¶_rms** (RMSNorm variants)\n",
    "22. **convnext_tiny_hnf**\n",
    "23. **convnext_large_mlp**\n",
    "\n",
    "They are:\n",
    "\n",
    "* Experimental\n",
    "* No strong community benchmarks\n",
    "* Mostly useful for profiling, not production\n",
    "\n",
    "---\n",
    "\n",
    "#  Tier 6 ‚Äî Do not use (internal debugging models)\n",
    "\n",
    "24. **test_convnext**\n",
    "25. **test_convnext2**\n",
    "26. **test_convnext3**\n",
    "\n",
    "These exist **only for timm internal testing**.\n",
    "\n",
    "---\n",
    "\n",
    "# ü•á Best Choice for *You* (Medical Images, 4-class classification, dataset size ~6k)\n",
    "\n",
    "### **Use these in order:**\n",
    "\n",
    "1. **convnextv2_tiny** ‚Üê Best for your dataset + single GPU\n",
    "2. **convnextv2_small** ‚Üê If you have 12‚Äì24 GB VRAM\n",
    "3. **convnext_tiny** ‚Üê If you want classic ConvNeXt\n",
    "4. **convnextv2_nano** ‚Üê If you want fast & light\n",
    "5. **convnext_base** ‚Üê Only if you have strong GPU and enough data (‚â•50k samples)\n",
    "\n",
    "Anything larger than ‚Äúbase‚Äù will **overfit your chest X-ray dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "#  Simple rule of thumb\n",
    "\n",
    "* **Small dataset (<20k images)** ‚Üí Use **Tiny**, **Nano**, or **Small**\n",
    "* **Medium dataset (50k‚Äì200k)** ‚Üí Use **Small** or **Base**\n",
    "* **Large dataset (>1M)** ‚Üí Use **Large** or **Huge**\n",
    "\n",
    "For **medical imaging**, Tiny and Small ConvNeXt models consistently outperform bigger ones due to less overfitting.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09939d08-6f6a-43cf-ac19-604c0ae906f9",
   "metadata": {},
   "source": [
    "# **ConvNext Training Recipe**\n",
    "Below is a **clear, model-size‚Äìspecific recipe** for every ConvNeXt / ConvNeXt-v2 model in timm.\n",
    "These recommendations come from official Meta papers, timm defaults, and empirical results.\n",
    "Just plug and play.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Core Rule (Most Important)\n",
    "\n",
    "ConvNeXt **does NOT like SGD**.\n",
    "ConvNeXt **strongly prefers AdamW**, cosine decay, and long warmup.\n",
    "\n",
    "ConvNeXt = Transformer-style training.\n",
    "\n",
    "So:\n",
    "\n",
    "* Optimizer = **AdamW** (always)\n",
    "* Weight decay = **0.05**\n",
    "* Betas = **(0.9, 0.999)**\n",
    "* Learning rate = **scaled by batch size**\n",
    "* Scheduler = **cosine + warmup**\n",
    "\n",
    "This is true for all sizes: from atto ‚Üí tiny ‚Üí base ‚Üí xxlarge.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Learning Rate Scaling\n",
    "\n",
    "The official rule:\n",
    "\n",
    "$$\\text{lr} = 4 \\times 10^{-3} \\cdot \\frac{\\text{batch\\_size}}{4096}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "| Global batch | LR      |\n",
    "| ------------ | ------- |\n",
    "| 4096         | 4e-3    |\n",
    "| 2048         | 2e-3    |\n",
    "| 1024         | 1e-3    |\n",
    "| 512          | 5e-4    |\n",
    "| 256          | 2.5e-4  |\n",
    "| 128          | 1.25e-4 |\n",
    "\n",
    "If you train on **one GPU**, your batch may be 32 ‚Üí 128.\n",
    "Use LR between **1e-4 ‚Üí 3e-4**.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Recommended settings *per ConvNeXt model size*\n",
    "\n",
    "Below I provide **the best settings** for each name category.\n",
    "\n",
    "---\n",
    "\n",
    "# 3.1. For extremely tiny models\n",
    "\n",
    "**convnext_zepto*, convnext_atto*, convnext_femto*, convnext_pico*, convnext_nano***\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **2e-4 ‚Üí 4e-4** (for batch 128‚Äì256)\n",
    "* weight decay = **0.05**\n",
    "* betas = **(0.9, 0.999)**\n",
    "* drop_path = **0.0 ‚Üí 0.1**\n",
    "* epochs = **300**\n",
    "* warmup = **20 epochs**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* These models underfit quickly ‚Üí smaller DropPath\n",
    "* No need for strong regularization like mixup/cutmix\n",
    "* Good for medical images (X-ray)\n",
    "\n",
    "---\n",
    "\n",
    "# 3.2. convnext_tiny (28M) and convnext_tiny_hnf\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **3e-4** (batch 128)\n",
    "* weight decay = **0.05**\n",
    "* betas = **(0.9, 0.999)**\n",
    "* drop_path = **0.1 ‚Üí 0.2**\n",
    "* epochs = **300**\n",
    "* warmup = **20 epochs**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* This is the ‚Äúentry-level strong model‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# 3.3. convnext_small (50M)\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **2e-4** (batch 128)\n",
    "* weight decay = **0.05**\n",
    "* drop_path = **0.2 ‚Üí 0.4**\n",
    "* epochs = **300**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Start using stronger regularization\n",
    "* Mixup = 0.8, CutMix = 1.0 recommended\n",
    "\n",
    "---\n",
    "\n",
    "# 3.4. convnext_base (88M)\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **1e-4 ‚Üí 1.5e-4**\n",
    "* weight decay = **0.05**\n",
    "* betas = **(0.9, 0.999)**\n",
    "* drop_path = **0.3 ‚Üí 0.5**\n",
    "* epochs = **300**\n",
    "* warmup = **20**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Needs larger stochastic depth\n",
    "* Sensitive to lr > 2e-4 ‚Üí avoid too large lr\n",
    "\n",
    "---\n",
    "\n",
    "# 3.5. convnext_large (197M) / convnext_large_mlp\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **8e-5 ‚Üí 1e-4**\n",
    "* weight decay = **0.05**\n",
    "* drop_path = **0.4 ‚Üí 0.6**\n",
    "* epochs = **300**\n",
    "* warmup = **20**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* If training from scratch:\n",
    "\n",
    "  * GPU memory heavy\n",
    "  * Requires gradient checkpointing\n",
    "\n",
    "---\n",
    "\n",
    "# 3.6. convnext_xlarge / convnext_xxlarge\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **5e-5 ‚Üí 8e-5**\n",
    "* wd = **0.05**\n",
    "* drop_path = **0.6 ‚Üí 0.8**\n",
    "* epochs = **300**\n",
    "* warmup = **20**\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Only realistic with multi-GPU training\n",
    "* Very sensitive to learning rate\n",
    "\n",
    "---\n",
    "\n",
    "# 3.7. ConvNeXt-v2 variants\n",
    "\n",
    "**convnextv2_atto ‚Äî convnextv2_huge**\n",
    "\n",
    "ConvNeXt-v2 uses **GRN**, but the training setup stays similar.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "* AdamW\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "* lr = **3e-4** for small models\n",
    "* lr = **2e-4** for tiny‚Üísmall\n",
    "* lr = **1e-4 ‚Üí 8e-5** for base‚Üílarge‚Üíhuge\n",
    "* weight decay = **0.05**\n",
    "* drop_path = **0.1 ‚Üí 0.6 depending on size**\n",
    "* epochs = **300**\n",
    "* warmup = **20**\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Special model modifiers: OLS / RMS / HNF\n",
    "\n",
    "### If the model name includes:\n",
    "\n",
    "| Modifier | Meaning                     | Training impact                                          |\n",
    "| -------- | --------------------------- | -------------------------------------------------------- |\n",
    "| **_ols** | Omni Lite Scaling           | Same optimizer, identical hyperparameters                |\n",
    "| **_rms** | RMSNorm                     | Same optimizer, but allows slightly larger lr, e.g. +20% |\n",
    "| **_hnf** | High Norm Frequency variant | Same training; slightly more stable with Mixup/CutMix    |\n",
    "\n",
    "So:\n",
    "You do **NOT** need to change optimizer or wd for RMS/OLS/HNF.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Which optimizer + params should YOU use (recommended per complexity)\n",
    "\n",
    "Below is a clean shortlist depending on your dataset size:\n",
    "\n",
    "---\n",
    "\n",
    "## Small datasets (medical images, 5k‚Äì20k samples)\n",
    "\n",
    "**Recommended models:**\n",
    "\n",
    "* convnext_femto\n",
    "* convnext_pico\n",
    "* convnext_nano\n",
    "* convnext_tiny\n",
    "\n",
    "**Settings:**\n",
    "\n",
    "```\n",
    "optimizer = AdamW\n",
    "lr = 1e-4 ‚Üí 2e-4\n",
    "weight_decay = 0.05\n",
    "drop_path = 0.0 ‚Üí 0.1\n",
    "epochs = 100 ‚Üí 150\n",
    "warmup = 5 ‚Üí 10\n",
    "no Mixup / CutMix (medical data)\n",
    "label smoothing = 0.05\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Medium datasets (50k‚Äì200k)\n",
    "\n",
    "**Recommended:**\n",
    "\n",
    "* convnext_tiny\n",
    "* convnext_small\n",
    "* convnext_base\n",
    "\n",
    "**Settings:**\n",
    "\n",
    "```\n",
    "optimizer = AdamW\n",
    "lr = 2e-4 ‚Üí 3e-4\n",
    "wd = 0.05\n",
    "drop_path = 0.2\n",
    "epochs = 200 ‚Üí 300\n",
    "warmup = 10 ‚Üí 20\n",
    "mixup = 0.8\n",
    "cutmix = 1.0\n",
    "label smoothing = 0.1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Large datasets (ImageNet-scale, >1M images)\n",
    "\n",
    "**Recommended:**\n",
    "\n",
    "* convnext_base\n",
    "* convnext_large\n",
    "* convnext_xlarge\n",
    "\n",
    "**Settings:**\n",
    "\n",
    "```\n",
    "optimizer = AdamW\n",
    "lr = scaled by batch (1e-4 ‚Üí 4e-4)\n",
    "wd = 0.05\n",
    "drop_path = 0.3 ‚Üí 0.7\n",
    "epochs = 300\n",
    "warmup = 20\n",
    "mixup = 0.8\n",
    "cutmix = 1.0\n",
    "label smoothing = 0.1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Simple code snippet to load the recommended optimizer\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "model_name = \"convnext_tiny\"  # change here\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=4).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,              # adjust for your batch size\n",
    "    weight_decay=0.05,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,\n",
    "    eta_min=1e-6,\n",
    ")\n",
    "```\n",
    "\n",
    "Warmup can be done with a custom warmup scheduler (linear warmup).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4cb49-dc45-4fd0-a56a-2ec0caa9b96d",
   "metadata": {},
   "source": [
    "## Fine Tune a Pretrained \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071fa8c5-eeff-4bb0-943a-33fb315ae33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 224, 224)\n",
      "features shape: torch.Size([1, 768, 7, 7])\n",
      "output shape:  torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "model_name = \"convnextv2_tiny\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model_config = model.pretrained_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaaf38d-cc69-40f2-bf37-8f6345673815",
   "metadata": {},
   "source": [
    "## Getting Input, Feature, and output size\n",
    "To fine tune we need to know the output of last stage, we can only get the number of channels, but the resolution we can't, since in `Conv2d` we only specify input and output channel, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c67a50d-c53f-4bcd-9377-8dc3dd7a02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 224, 224)\n",
      "features shape: torch.Size([1, 768, 7, 7])\n",
      "output shape:  torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "#print(type(model_config[\"input_size\"]))\n",
    "print(\"input shape: \", model_config[\"input_size\"])\n",
    "(C, H, W) = model_config[\"input_size\"]\n",
    "\n",
    "x = torch.randn(1, C, H, W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = model.forward_features(x)\n",
    "    print(\"features shape:\", features.shape)\n",
    "    \n",
    "    out = model(x)\n",
    "    print(\"output shape: \",out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f7b5e-6a4c-40c6-a73f-bed3d3dc2dbf",
   "metadata": {},
   "source": [
    "## Setting Head\n",
    "\n",
    "\n",
    "In **ConvNeXt / ConvNeXtV2**, the **correct and standard way** is:\n",
    "\n",
    "### ‚úÖ **Apply global average pooling ‚Üí get a 768-dim vector ‚Üí feed to your final FC layer**\n",
    "\n",
    "Not the flattened 7√ó7√ó768 tensor.\n",
    "\n",
    "Below is the reasoning and comparison.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. **What the backbone outputs**\n",
    "\n",
    "You observed:\n",
    "\n",
    "* Output shape from last stage:\n",
    "  **[B, 768, 7, 7]**\n",
    "\n",
    "This is the **feature map** produced by ConvNeXtV2-Tiny.\n",
    "\n",
    "To convert this into class predictions, you have **two options**:\n",
    "\n",
    "---\n",
    "\n",
    "# OPTION A ‚Äî **Global Average Pooling (GAP)**\n",
    "\n",
    "### Output:\n",
    "\n",
    "* Input: [B, 768, 7, 7]\n",
    "* After GAP: [B, 768]\n",
    "\n",
    "### Classification head:\n",
    "\n",
    "* FC(768 ‚Üí 4)\n",
    "\n",
    "### This is what ConvNeXt is designed for.\n",
    "\n",
    "This is what timm does internally.\n",
    "\n",
    "### Why it‚Äôs preferred:\n",
    "\n",
    "1. **Reduces parameters dramatically**\n",
    "   FC with flat features would require:\n",
    "\n",
    "   $$768 \\times 7 \\times 7 = 37632$$\n",
    "\n",
    "   So FC would be\n",
    "   **37,632 ‚Üí 4**\n",
    "   versus\n",
    "   **768 ‚Üí 4**\n",
    "   Massive difference in number of learnable weights.\n",
    "\n",
    "2. **Prevents overfitting**\n",
    "   You have **6k images**. A head with ~150k parameters will overfit quickly.\n",
    "\n",
    "3. **GAP is translation-invariant**\n",
    "   For medical images (like X-rays), location can vary slightly due to patient misalignment.\n",
    "\n",
    "4. **Every modern CNN uses GAP:**\n",
    "\n",
    "   * ResNet\n",
    "   * EfficientNet\n",
    "   * MobileNet\n",
    "   * ConvNeXt + ConvNeXtV2\n",
    "   * RegNet\n",
    "   * DenseNet\n",
    "\n",
    "It is the standard because it turns feature maps into semantic vectors without exploding parameter count.\n",
    "\n",
    "---\n",
    "\n",
    "# OPTION B ‚Äî **Flatten (7√ó7√ó768) and feed through a large FC**\n",
    "\n",
    "### Output:\n",
    "\n",
    "* Flatten: [B, 37,632]\n",
    "\n",
    "### Head:\n",
    "\n",
    "* FC(37632 ‚Üí 4)\n",
    "\n",
    "### Problems:\n",
    "\n",
    "1. Much larger parameter count\n",
    "2. Overfitting risk increases significantly\n",
    "3. Slower training\n",
    "4. Ignores spatial pooling benefits\n",
    "5. Not how ConvNeXt was designed to be used\n",
    "\n",
    "### When is flattening used?\n",
    "\n",
    "Rarely in modern CNNs.\n",
    "Only architectures like AlexNet (very old) used it.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úîÔ∏è Conclusion: Use Global Pooling\n",
    "\n",
    "### **Use this:**\n",
    "\n",
    "```python\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=4)\n",
    "```\n",
    "\n",
    "Timm will automatically:\n",
    "\n",
    "* apply global average pooling\n",
    "* replace the FC head with a 768 ‚Üí 4 classifier\n",
    "\n",
    "If you want manual control:\n",
    "\n",
    "```python\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "in_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(in_features, 4)\n",
    "```\n",
    "\n",
    "Or if ConvNeXtV2 uses `model.head = nn.Linear(768, 4)` directly:\n",
    "\n",
    "```python\n",
    "model.head = nn.Linear(768, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîç Summary (which one and why)\n",
    "\n",
    "| Method                 | Input to classifier | Params    | Overfitting risk | Standard? | Recommended? |\n",
    "| ---------------------- | ------------------- | --------- | ---------------- | --------- | ------------ |\n",
    "| **Global Avg Pooling** | 768                 | Very low  | Low              | ‚úîÔ∏è Yes    | **‚úîÔ∏è Yes**   |\n",
    "| **Flatten 7√ó7√ó768**    | 37,632              | Very high | High             | ‚ùå No      | **‚ùå No**     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b481f3-650b-438f-b024-2ab897aec863",
   "metadata": {},
   "source": [
    "# ‚úîÔ∏è **Custom Head ‚Äî Structure Review**\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "custom_head_v1_b = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),     # [B, 768, 7, 7] -> [B, 768, 1, 1]\n",
    "    nn.Flatten(1),               # -> [B, 768]\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "This is a **valid, well-structured classifier head**.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úîÔ∏è Why this head is correct\n",
    "\n",
    "### 1. **Correct handling of ConvNeXtV2 output**\n",
    "\n",
    "ConvNeXtV2-Tiny outputs:\n",
    "\n",
    "* features: `[B, 768, 7, 7]`\n",
    "\n",
    "You apply:\n",
    "\n",
    "* global pooling ‚Üí `[B, 768, 1, 1]`\n",
    "* flatten ‚Üí `[B, 768]`\n",
    "\n",
    "This is **exactly** how the official ConvNeXt head works.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úîÔ∏è Why your head design is GOOD\n",
    "\n",
    "### 1. **Small MLP avoids overfitting**\n",
    "\n",
    "Your dataset is ~6k images ‚Üí very small.\n",
    "A simple 1‚Äì2 layer MLP is ideal.\n",
    "\n",
    "### 2. **Dropout is useful**\n",
    "\n",
    "Chest X-ray datasets often have:\n",
    "\n",
    "* class imbalance\n",
    "* noisy labels\n",
    "* small dataset size\n",
    "\n",
    "Dropout 0.3 ‚Üí 0.25 is **healthy** without hurting accuracy.\n",
    "\n",
    "### 3. **128 hidden units is a good bottleneck**\n",
    "\n",
    "A larger head (512 or 1024) would overfit.\n",
    "A smaller head (<64) might underfit.\n",
    "\n",
    "128 works extremely well in practice.\n",
    "\n",
    "### 4. **This head adds nonlinearity**\n",
    "\n",
    "Which improves decision boundaries (important for subtle findings).\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úîÔ∏è Recommended improvement (not required)\n",
    "\n",
    "### Add LayerNorm before the MLP\n",
    "\n",
    "ConvNeXt models benefit from normalization before an FC layer:\n",
    "\n",
    "```python\n",
    "nn.LayerNorm(768)\n",
    "```\n",
    "\n",
    "### Improved version:\n",
    "\n",
    "```python\n",
    "custom_head_v1_b = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.LayerNorm(768),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(768, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "```\n",
    "\n",
    "LayerNorm stabilizes fine-tuning and is used extensively in ConvNeXt blocks.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
