{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b43c47-1458-4b4a-9178-8f731c3c01c7",
   "metadata": {},
   "source": [
    "# **DenseNet201 — Architecture, Motivation, Properties**\n",
    "\n",
    "## 1. Core Idea\n",
    "\n",
    "DenseNet stands for **Densely Connected Convolutional Network**.\n",
    "\n",
    "Instead of each layer receiving only the output of the previous layer, **each layer receives the concatenation of all preceding feature maps** in the block.\n",
    "\n",
    "If the block has layers\n",
    "$$x_0, x_1, x_2, \\dots, x_L,$$\n",
    "then each layer does\n",
    "$$x_l = H_l([x_0, x_1, \\dots, x_{l-1}]).$$\n",
    "\n",
    "So inside a dense block, **the channel dimension grows** as you go deeper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575a3aa-5d06-45fd-a05c-43b67af0dd2b",
   "metadata": {},
   "source": [
    "\n",
    "## **2. DenseNet Architecture (e.g., DenseNet-121 / 169 / 201)**\n",
    "\n",
    "The global structure is:\n",
    "\n",
    "1. **Stem**\n",
    "2. **Dense Block 1**\n",
    "3. **Transition Layer 1**\n",
    "4. **Dense Block 2**\n",
    "5. **Transition Layer 2**\n",
    "6. **Dense Block 3**\n",
    "7. **Transition Layer 3**\n",
    "8. **Dense Block 4**\n",
    "9. **Classifier Head**\n",
    "\n",
    "So yes — it mirrors the ResNet “stem → 4 stages → head” structure, but each “stage” is a dense block instead of a stack of residual blocks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Stem**\n",
    "\n",
    "The stem is:\n",
    "\n",
    "* $7 \\times 7$ conv, stride 2\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* $3 \\times 3$ max-pool, stride 2\n",
    "\n",
    "Input example:\n",
    "$$B \\times 3 \\times 224 \\times 224$$\n",
    "Output:\n",
    "$$B \\times 64 \\times 56 \\times 56.$$\n",
    "\n",
    "This matches ResNet’s stem exactly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Dense Blocks (the 4 “stages”)\n",
    "\n",
    "DenseNet has **4 dense blocks**, analogous to ResNet’s 4 stages.\n",
    "\n",
    "But instead of residual blocks, a dense block contains **L dense layers**, where L depends on the model:\n",
    "\n",
    "| Model        | Layers in Dense Block 1 | 2  | 3  | 4  |\n",
    "| ------------ | ----------------------- | -- | -- | -- |\n",
    "| DenseNet-121 | 6                       | 12 | 24 | 16 |\n",
    "| DenseNet-169 | 6                       | 12 | 32 | 32 |\n",
    "| DenseNet-201 | 6                       | 12 | 48 | 32 |\n",
    "| DenseNet-264 | 6                       | 12 | 64 | 48 |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Transition Layers (between stages)\n",
    "\n",
    "These are placed between dense blocks and do:\n",
    "\n",
    "1. $1 \\times 1$ convolution (channel compression): $C \\to \\theta C$, usually $\\theta = 0.5)$\n",
    "2. $2 \\times 2$ average pooling $stride 2$\n",
    "\n",
    "Their job is exactly like ResNet downsampling blocks:\n",
    "\n",
    "* reduce spatial size\n",
    "* reduce channels\n",
    "* forward features to the next stage\n",
    "\n",
    "But the mechanism is different:\n",
    "\n",
    "* ResNet uses stride-2 conv in residual blocks\n",
    "* DenseNet uses pooling + compression\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Classifier Head\n",
    "\n",
    "At the end:\n",
    "\n",
    "* Global average pooling\n",
    "* Fully connected layer (num_classes)\n",
    "\n",
    "Input example:\n",
    "$$B \\times C_{\\text{final}} \\times 7 \\times 7$$\n",
    "Output:\n",
    "$$B \\times C_{\\text{final}}$$\n",
    "Then a linear classifier.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Clear Comparison: ResNet vs DenseNet Structure\n",
    "\n",
    "| Component    | ResNet                 | DenseNet                           |\n",
    "| ------------ | ---------------------- | ---------------------------------- |\n",
    "| Stem         | Conv + BN + MaxPool    | Same                               |\n",
    "| Stage 1      | Residual blocks × N    | Dense layers × L1                  |\n",
    "| Downsampling | Stride 2 conv in block | Transition layer (pool + 1×1 conv) |\n",
    "| Stage 2      | Residual blocks × N    | Dense layers × L2                  |\n",
    "| Stage 3      | Residual blocks × N    | Dense layers × L3                  |\n",
    "| Stage 4      | Residual blocks × N    | Dense layers × L4                  |\n",
    "| Head         | GAP + FC               | Same                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76d39a-087e-42ff-bd0e-51db8f741265",
   "metadata": {},
   "source": [
    "## **3. DenseNet-201 Dense Block (Numeric Walkthrough, k = 32, L = 6)**\n",
    "\n",
    "This is a complete, unified explanation of the shapes, architecture, and per-layer operations inside a DenseNet block, including bottleneck behavior and the transition layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Input Setup**\n",
    "\n",
    "Dense Block 1 receives:\n",
    "\n",
    "* Batch size\n",
    "  $$B = 4$$\n",
    "* Input channels\n",
    "  $$C_0 = 64$$\n",
    "* Spatial dimensions\n",
    "  $$H = W = 56$$\n",
    "\n",
    "Input tensor:\n",
    "\n",
    "$$x_0 \\in \\mathbb{R}^{4 \\times 64 \\times 56 \\times 56}.$$\n",
    "\n",
    "Dense block hyperparameters:\n",
    "\n",
    "* Number of layers\n",
    "  $$L = 6$$\n",
    "* Growth rate\n",
    "  $$k = 32$$\n",
    "* Bottleneck channels\n",
    "  $$4k = 128$$\n",
    "\n",
    "Each dense layer adds **32 channels** to the global feature map.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. DenseNet Layer Architecture (L1–L6)**\n",
    "\n",
    "Every dense layer has *identical internal structure*:\n",
    "\n",
    "```\n",
    "Input: concat([x0, x1, ..., x_{l-1}])\n",
    "↓ BN\n",
    "↓ ReLU\n",
    "↓ 1×1 Conv  (output = 4k = 128 channels)\n",
    "↓ BN\n",
    "↓ ReLU\n",
    "↓ 3×3 Conv  (output = k = 32 channels)\n",
    "↓ Output = x_l  (32 channels)\n",
    "```\n",
    "\n",
    "Formal expression:\n",
    "\n",
    "$$\n",
    "H_l = \\text{Conv}_{3\\times3}(\n",
    "\\text{ReLU}(\n",
    "\\text{BN}(\n",
    "\\text{Conv}_{1\\times1}(\n",
    "\\text{ReLU}(\n",
    "\\text{BN}([x_0,\\dots,x_{l-1}])\n",
    ")\n",
    ")\n",
    ")\n",
    ")\n",
    ")\n",
    "$$\n",
    "\n",
    "### Important:\n",
    "\n",
    "The **1×1 bottleneck always receives all concatenated inputs**, not just the previous layer’s output.\n",
    "\n",
    "So at each layer:\n",
    "\n",
    "* Input channel count **increases**\n",
    "* 1×1 conv **compresses** to 128 channels\n",
    "* 3×3 conv **creates** 32 new channels\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Layer-by-Layer Numeric Shapes (Full Dense Block)**\n",
    "\n",
    "Each layer’s **input** = concatenation of all previous outputs.\n",
    "\n",
    "**Layer 1**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 64 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$64 \\rightarrow 128$$\n",
    "Output:\n",
    "$$4 \\times 128 \\times 56 \\times 56$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "Output:\n",
    "$$x_1 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Concatenated for next layer:\n",
    "$$C = 64 + 32 = 96$$\n",
    "\n",
    "---\n",
    "\n",
    " **Layer 2**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 96 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$96 \\rightarrow 128$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "Output:\n",
    "$$x_2 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Concatenated:\n",
    "$$C = 64 + 2\\cdot32 = 128$$\n",
    "\n",
    "---\n",
    "\n",
    " **Layer 3**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 128 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$128 \\rightarrow 128$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "Output:\n",
    "$$x_3 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Concatenated:\n",
    "$$C = 160$$\n",
    "\n",
    "---\n",
    "\n",
    " **Layer 4**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 160 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$160 \\rightarrow 128$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "\n",
    "Output:\n",
    "$$x_4 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Concatenated:\n",
    "$$C = 192$$\n",
    "\n",
    "---\n",
    "\n",
    " **Layer 5**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 192 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$192 \\rightarrow 128$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "\n",
    "Output:\n",
    "$$x_5 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Concatenated:\n",
    "$$C = 224$$\n",
    "\n",
    "---\n",
    "\n",
    " **Layer 6**\n",
    "\n",
    "Input:\n",
    "$$4 \\times 224 \\times 56 \\times 56$$\n",
    "\n",
    "1×1 conv:\n",
    "$$224 \\rightarrow 128$$\n",
    "\n",
    "3×3 conv:\n",
    "$$128 \\rightarrow 32$$\n",
    "\n",
    "Output:\n",
    "$$x_6 \\in \\mathbb{R}^{4 \\times 32 \\times 56 \\times 56}$$\n",
    "\n",
    "Final concatenated output:\n",
    "$$C_{\\text{out}} = 64 + 6\\cdot32 = 256$$\n",
    "\n",
    "So Dense Block 1 outputs:\n",
    "\n",
    "$$\\text{DB1 out} \\in \\mathbb{R}^{4 \\times 256 \\times 56 \\times 56}.$$\n",
    "\n",
    "General formula:\n",
    "\n",
    "$$C_{\\text{out}} = C_0 + Lk.$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Transition Layer After Dense Block**\n",
    "\n",
    "DenseNet uses a transition layer:\n",
    "\n",
    "* $1 \\times 1$ convolution (channel compression)\n",
    "* $2 \\times 2$ average pooling (spatial downsampling)\n",
    "\n",
    "Compression factor:\n",
    "$$\\theta = 0.5$$\n",
    "\n",
    "Input:\n",
    "$$4 \\times 256 \\times 56 \\times 56$$\n",
    "\n",
    "After $1 \\times 1$ conv:\n",
    "$$C' = \\theta \\cdot 256 = 128$$\n",
    "Shape:\n",
    "$$4 \\times 128 \\times 56 \\times 56$$\n",
    "\n",
    "After $2 \\times 2$ avg pooling:\n",
    "$$56 \\rightarrow 28$$\n",
    "Final shape:\n",
    "$$4 \\times 128 \\times 28 \\times 28$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "* Input to block:\n",
    "  $$4 \\times 64 \\times 56 \\times 56$$\n",
    "* Output of block:\n",
    "  $$4 \\times 256 \\times 56 \\times 56$$\n",
    "* Output after transition:\n",
    "  $$4 \\times 128 \\times 28 \\times 28$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Final Combined Table (All Layers)**\n",
    "\n",
    "| Layer | Input Channels | 1×1 Conv (bottleneck) | 3×3 Conv (growth) |\n",
    "| ----- | -------------- | --------------------- | ----------------- |\n",
    "| L1    | 64             | 64 → 128              | 128 → 32          |\n",
    "| L2    | 96             | 96 → 128              | 128 → 32          |\n",
    "| L3    | 128            | 128 → 128             | 128 → 32          |\n",
    "| L4    | 160            | 160 → 128             | 128 → 32          |\n",
    "| L5    | 192            | 192 → 128             | 128 → 32          |\n",
    "| L6    | 224            | 224 → 128             | 128 → 32          |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc2711-1123-42ce-af85-df2f3f4481b8",
   "metadata": {},
   "source": [
    "## **4. Parameter Count**\n",
    "\n",
    "DenseNet-201 has **≈ 20 million parameters**, which is very small compared to:\n",
    "\n",
    "* ResNet-152 → 60M\n",
    "* VGG-16 → 138M\n",
    "* EfficientNet-B4 → 19M\n",
    "* ConvNeXt-Tiny → 28M\n",
    "\n",
    "You get high accuracy with much smaller memory footprint.\n",
    "\n",
    "\n",
    "#### DenseNet-121 Depth Breakdown\n",
    "\n",
    "DenseNet counts:\n",
    "\n",
    "1. Every **1×1 conv** in each dense layer\n",
    "2. Every **3×3 conv** in each dense layer\n",
    "3. Every **conv** in the stem\n",
    "4. Every **1×1 conv** in the transition layers\n",
    "5. The final classifier FC layer is *not* counted in the depth\n",
    "\n",
    "DenseNet-121 uses **DenseNet-BC** (Bottleneck + Compression), so each dense layer has **two convolutions**:\n",
    "\n",
    "* 1×1 conv\n",
    "* 3×3 conv\n",
    "\n",
    "Thus:\n",
    "\n",
    "#### Each dense layer = 2 conv layers\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Count dense block layers\n",
    "\n",
    "Dense block layer counts:\n",
    "\n",
    "* Block 1: 6\n",
    "* Block 2: 12\n",
    "* Block 3: 24\n",
    "* Block 4: 16\n",
    "\n",
    "Total dense layers:\n",
    "\n",
    "$$6 + 12 + 24 + 16 = 58.$$\n",
    "\n",
    "Each of these 58 layers contains **2 convs**:\n",
    "\n",
    "Total convs inside dense blocks:\n",
    "\n",
    "$$58 \\times 2 = 116.$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Add the stem convolution\n",
    "\n",
    "DenseNet stem has one convolution:\n",
    "\n",
    "* $7 \\times 7$ conv → 1 layer\n",
    "\n",
    "So far:\n",
    "\n",
    "$$116 + 1 = 117.$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Add transition-layer convolutions\n",
    "\n",
    "There are **3 transition layers**, and each has:\n",
    "\n",
    "* **1×1 conv** → 1 conv per transition\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$+ 3 = 120.$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Count: 121 Layers\n",
    "\n",
    "DenseNet-121 includes:\n",
    "\n",
    "* 116 convs in dense blocks\n",
    "* 1 conv in stem\n",
    "* 3 convs in transitions\n",
    "* 1 classification layer is *not counted*\n",
    "* Total:\n",
    "\n",
    "$$121.$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Component              | Count        |\n",
    "| ---------------------- | ------------ |\n",
    "| Dense layers           | 58           |\n",
    "| Conv per dense layer   | ×2           |\n",
    "| Convs in dense blocks  | 58 × 2 = 116 |\n",
    "| Stem conv              | +1           |\n",
    "| Transition layer convs | +3           |\n",
    "| **Total**              | **121**      |\n",
    "\n",
    "That is why it's called **DenseNet-121**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d2243-b09a-442c-bc8d-cdab3894c31c",
   "metadata": {},
   "source": [
    "## **5. When DenseNet201 Performs Well**\n",
    "\n",
    "DenseNet201 is **excellent for**:\n",
    "\n",
    "#### ✔ Medical imaging (X-ray, CT, MRI)\n",
    "\n",
    "Dense multi-scale features and stable gradients help a lot.\n",
    "\n",
    "#### ✔ Small datasets\n",
    "\n",
    "Because it has fewer parameters and strong feature reuse.\n",
    "\n",
    "#### ✔ Tasks requiring very deep effective receptive fields\n",
    "\n",
    "But without massive compute (compared to large ResNets).\n",
    "\n",
    "#### ✔ Training from scratch or transfer learning\n",
    "\n",
    "Dense connections help train even with limited data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. When It Is Not Ideal**\n",
    "\n",
    "DenseNet201 is **not great** when:\n",
    "\n",
    "**❌ You need very large input images**\n",
    "\n",
    "Channel concatenation grows memory quickly.\n",
    "\n",
    "**❌ You use extremely large batch sizes**\n",
    "\n",
    "High memory usage in dense blocks limits scaling.\n",
    "\n",
    "**❌ You need high inference speed**\n",
    "\n",
    "Dense concatenation makes it slower than EfficientNet/ConvNeXt.\n",
    "\n",
    "For your **Lung Disease Dataset** project, DenseNet201 is a top-tier choice.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. How to Use DenseNet201 in PyTorch (timm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddf01df-e5c1-4c58-a8c6-ffb551531244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34182f1549ba40fc8f4827664268a817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/81.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "model = timm.create_model(\n",
    "    'densenet201',\n",
    "    pretrained=True,\n",
    "    num_classes=5\n",
    ")\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9af5e-f8c8-4658-9a44-910715467a19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 8. Optimizer Recommendation\n",
    "\n",
    "For DenseNet201:\n",
    "\n",
    "**Stage 1 (frozen backbone)**\n",
    "\n",
    "Use:\n",
    "\n",
    "* **Adam**, lr ≈ 1e-3 to 3e-4\n",
    "\n",
    "Reason: Adam handles the random initial classifier head well.\n",
    "\n",
    "**Stage 2 (full fine-tuning)**\n",
    "\n",
    "Use:\n",
    "\n",
    "* **AdamW**, lr ≈ 1e-4 to 3e-5\n",
    "* Weight decay between **0.01 and 0.05**\n",
    "\n",
    "DenseNet benefits strongly from weight decay because of concatenation-driven feature growth.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. Compared to Other Models\n",
    "\n",
    "**Accuracy vs Parameter Count (rough)**\n",
    "\n",
    "| Model           | Params | Accuracy (ImageNet) |\n",
    "| --------------- | ------ | ------------------- |\n",
    "| DenseNet201     | ~20M   | ~77%                |\n",
    "| ResNet50        | 25M    | ~76%                |\n",
    "| EfficientNet-B4 | 19M    | ~82%                |\n",
    "| ConvNeXt-Tiny   | 28M    | ~82%                |\n",
    "| Swin-Tiny       | 28M    | ~81%                |\n",
    "\n",
    "DenseNet isn't SOTA anymore, but remains **very strong for medical imaging** because of feature reuse and stable gradients.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
