{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "Example:\n",
    "\n",
    "\n",
    "$f(w,x)=\\frac{1}{1+e^{-(w_{0}x_{0}+w_{1}x_{1}+w_{2})}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "<img src='images/auto.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Functions\n",
    "\n",
    "$a=w_{0}*x_{0}$\n",
    "\n",
    "$b=w_{1}*x_{1}$\n",
    "\n",
    "$c=a+b$\n",
    "\n",
    "$d=c+w_{2}$\n",
    "\n",
    "$e=-d$\n",
    "\n",
    "$f=exp(e)$\n",
    "\n",
    "$g=1+f$\n",
    "\n",
    "$h=1/g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Values\n",
    "$w_0=2.0$\n",
    "\n",
    "$x_0=-1.0$\n",
    "\n",
    "$w_1=-3.0$\n",
    "\n",
    "$x_1=-2.0$\n",
    "\n",
    "$w_2=-3.0$\n",
    "\n",
    "## Forward\n",
    "$a= -2.0$\n",
    "\n",
    "$b= 6.0$\n",
    "\n",
    "$c= 4.0$\n",
    "\n",
    "$d= 1.0$\n",
    "\n",
    "$e= -1.0$\n",
    "\n",
    "$f= 0.36$\n",
    "\n",
    "$g= 1.36$\n",
    "\n",
    "$h= 0.73$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Gradients\n",
    "1) $\\frac{\\partial h  }{\\partial h }=1$\n",
    "\n",
    "2) $\\frac{\\partial h }{\\partial g }=\\frac{-1}{g^2}=-0.53$\n",
    "\n",
    "$\\frac{\\partial g }{\\partial f }=1$\n",
    "\n",
    "3) $\\frac{\\partial h }{\\partial f }=\\frac{\\partial h }{\\partial g }\\frac{\\partial g }{\\partial f }=-0.53$\n",
    "\n",
    "$\\frac{\\partial f }{\\partial e}=exp(e)=0.36$\n",
    "\n",
    "4) $\\frac{\\partial h }{\\partial e}=\\frac{\\partial h }{\\partial f}\\frac{\\partial f }{\\partial e}=-0.53* 0.36=-0.19$\n",
    "\n",
    "$\\frac{\\partial e }{\\partial d}=-1$\n",
    "\n",
    "5) $\\frac{\\partial h }{\\partial d}=\\frac{\\partial h }{\\partial e}\\frac{\\partial e }{\\partial d}=-0.19*-1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial c}=1$\n",
    "\n",
    "6) $\\frac{\\partial h }{\\partial c}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial c}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial w_2}=1$\n",
    "\n",
    "7) $\\frac{\\partial h }{\\partial w_2}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial w_2}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial a}=1$\n",
    "\n",
    "8) $\\frac{\\partial h }{\\partial a}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial a}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial b}=1$\n",
    "\n",
    "9) $\\frac{\\partial h }{\\partial b}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial b}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial w_0}=x_0$\n",
    "\n",
    "10) $\\frac{\\partial h }{\\partial w_0}=\\frac{\\partial h }{\\partial a}=\\frac{\\partial a }{\\partial w_0}=0.19*x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial x_0 }=w_0$\n",
    "\n",
    "11) $\\frac{\\partial h }{\\partial x_0 }=\\frac{\\partial h }{\\partial a }\\frac{\\partial a}{\\partial x_0 }=0.19*w_0=0.38$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial w_1}=x_0$\n",
    "\n",
    "12) $\\frac{\\partial h }{\\partial w_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial w_1}=0.19* x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial x_1}=w_1$\n",
    "\n",
    "13) $\\frac{\\partial h }{\\partial x_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial x_1}=0.19*w_1=-0.57$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph \"DAG\" consisting of Function objects. \n",
    "\n",
    "\n",
    "## torch.autograd\n",
    "`torch.autograd` is PyTorch’s automatic differentiation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([2.0 ,3.0],requires_grad=True)\n",
    "y=torch.tensor([6.0 ,4.0],requires_grad=True)\n",
    "z=3*x**3-y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `.backward()` on `z`, autograd calculates these gradients and stores them in\n",
    "the respective tensors’ `.grad` attribute.\n",
    "We need to explicitly pass a gradient argument in `z.backward()` because it is a vector.\n",
    "gradient is a tensor of the same shape as Q, and it represents the gradient of z w.r.t. itself, i.e. `dz\\dz=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.6740e-29,  3.7179e-39])\n",
      "torch.Size([2])\n",
      "tensor([ 8.9135e-30, -3.6720e-40])\n"
     ]
    }
   ],
   "source": [
    "external_grad=torch.empty(2,requires_grad=True)\n",
    "z.backward(external_grad)\n",
    "print(x.grad)\n",
    "print(x.shape)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAGs are dynamic in PyTorch. An important thing to note is: **the graph is recreated from scratch, after each `.backward()` call**, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model. You can change the shape, size and operations at every iteration if needed.\n",
    "\n",
    "In this DAG: \n",
    "\n",
    "1. **arrows**: are in the direction of the forward pass. \n",
    "2. **nodes**: represent the backward functions of each operation in the forward pass.\n",
    "3. **leaf**: A leaf Variable is a variable that is at the beginning of the graph. That means that no operation tracked by the autograd engine created it. nodes in blue represent our leaf tensors.\n",
    "4. **roots** are the output tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor(-2., grad_fn=<MulBackward0>)\n",
      "b= tensor(6., grad_fn=<MulBackward0>)\n",
      "c= tensor(4., grad_fn=<AddBackward0>)\n",
      "d= tensor(1., grad_fn=<AddBackward0>)\n",
      "e= tensor(-1., grad_fn=<NegBackward>)\n",
      "f= tensor(0.3679, grad_fn=<ExpBackward>)\n",
      "g= tensor(1.3679, grad_fn=<AddBackward0>)\n",
      "h= tensor(0.7311, grad_fn=<MulBackward0>)\n",
      "dh/dw0= tensor(-0.1966)\n",
      "dh/dx0= tensor(0.3932)\n",
      "dh/dw1= tensor(-0.3932)\n",
      "dh/dx1= tensor(-0.3932)\n",
      "dh/dw2= tensor(0.1966)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'images/graph.svg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "w0=torch.tensor(2.0,requires_grad=True )\n",
    "x0=torch.tensor(-1.0,requires_grad=True)\n",
    "\n",
    "w1=torch.tensor(-3.0,requires_grad=True)\n",
    "x1=torch.tensor(-2.0,requires_grad=True)\n",
    "\n",
    "w2=torch.tensor(-3.0,requires_grad=True)\n",
    "\n",
    "\n",
    "a=w0*x0\n",
    "b=w1*x1\n",
    "c=a+b\n",
    "d=c+w2\n",
    "e=-d\n",
    "f=torch.exp(e)\n",
    "g=1+f\n",
    "h=1/g\n",
    "\n",
    "print(\"a=\",a)\n",
    "print(\"b=\",b)\n",
    "print(\"c=\",c)\n",
    "print(\"d=\",d)\n",
    "print(\"e=\",e)\n",
    "print(\"f=\",f)\n",
    "print(\"g=\",g)\n",
    "print(\"h=\",h)\n",
    "\n",
    "\n",
    "h.backward()\n",
    "print(\"dh/dw0=\",w0.grad)\n",
    "print(\"dh/dx0=\",x0.grad)\n",
    "print(\"dh/dw1=\",w1.grad)\n",
    "print(\"dh/dx1=\",w1.grad)\n",
    "print(\"dh/dw2=\",w2.grad)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchviz\n",
    "\n",
    "\n",
    "h_params={'w0':w0,'x0':x0,'w1':w1,'x1':x1,'w2':w2,\n",
    "          'a':a ,'b':b, 'c':c, 'd':d, 'e':e, 'f':f, 'g':g, 'h':h }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dot=torchviz.make_dot(h,params=h_params)\n",
    "dot.format='svg'\n",
    "dot.render(filename='graph', directory='images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/graph.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion from the DAG and turning off computation of gradients\n",
    "\n",
    "In a NN, parameters that don’t compute gradients are usually called **frozen parameters**. It is useful to “freeze” part of your model if you know in advance that you won’t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "so the two reason two use froze parameters:\n",
    "1. Performance: if you know in advance that you won’t need the gradients of those parameters\n",
    "2. Finetuning a pretrained network\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for **finetuning** a pretrained network (i.e. keeping the covnet layer fiex and only train the fully connected layer).\n",
    "\n",
    "Also during evaluating/validation you need to turn off computation of gradients with `torch.no_grad()` in pair with `model.eval()` to turn off batch normilization and drop out layers as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad()\n",
    "`torch.no_grad()` works in context manager. In the `__enter__()` method, it calls the `set_grad_enabled(False)`\n",
    " so for all tensor objects `requires_grad` will turn into False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8074, -1.7852,  0.8007],\n",
      "        [-0.1867, -0.2302, -0.9471]], requires_grad=True)\n",
      "tensor([[ 1.6147, -3.5705,  1.6014],\n",
      "        [-0.3734, -0.4605, -1.8943]], grad_fn=<MulBackward0>)\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.randn([2,3], requires_grad=True)\n",
    "print(x)\n",
    "y=2*x\n",
    "print(y)\n",
    "print(y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y=2*x\n",
    "    print(y.requires_grad)\n",
    "print(y.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detach()\n",
    "The *detach()* function, detaches the output from the computationnal graph, so no gradient will be backproped along this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/graph_after_detach.svg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "import torch\n",
    "\n",
    "a=torch.tensor([1.2],requires_grad=True)\n",
    "b=torch.tensor([2.2],requires_grad=True)\n",
    "c=torch.tensor([0.2],requires_grad=True)\n",
    "d=torch.tensor([0.8],requires_grad=True)\n",
    "e=torch.tensor([7.],requires_grad=True)\n",
    "f_params={'a':a,'b':b,'c':c,'d':d,'e':e}\n",
    "\n",
    "# Graph before detach\n",
    "f=d*((a+b)*(c))+e\n",
    "f.backward()\n",
    "dot=torchviz.make_dot(f,params=f_params)\n",
    "dot.format='svg'\n",
    "dot.render(filename='graph_before_detach', directory='images')\n",
    "\n",
    "# Graph after detach\n",
    "f=d*((a+b)*(c.detach()))+e\n",
    "dot=torchviz.make_dot(f,params=f_params)\n",
    "dot.format='svg'\n",
    "dot.render(filename='graph_after_detach', directory='images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph before detach:\n",
    "<img src='images/graph_before_detach.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after detach:\n",
    "<img src='images/graph_after_detach.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the gradients to zero\n",
    "\n",
    "PyTorch **accumulates** the gradients on subsequent backward passes. This is convenient while training RNNs. \n",
    "\n",
    "- `zero_grad` clears old gradients from the last step (otherwise you’d just accumulate the gradients from all loss.backward() calls).\n",
    "- `loss.backward()` computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n",
    "- `opt.step()` causes the optimizer to take a step based on the gradients of the parameters.\n",
    "\n",
    "\n",
    "\n",
    "We explicitly need to call `zero_grad()` because, after `loss.backward()` (when gradients are computed), we need to use `optimizer.step()` to proceed gradient descent. More specifically, the gradients are not automatically zeroed because these two operations, `loss.backward()` and `optimizer.step()`, are separated, and `optimizer.step()` requires the just computed gradients.\n",
    "\n",
    "In addition, sometimes, we need to accumulate gradient among some batches; to do that, we can simply call backward multiple times and optimize once.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Refs: [1](https://stackoverflow.com/questions/44732217/why-do-we-need-to-explicitly-call-zero-grad?noredirect=1&lq=1), [2](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev=torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "learning_rate=1e-6\n",
    "N,D_in,H,D_out=64,1000,100,10\n",
    "x=torch.randn(N,D_in)\n",
    "y=torch.randn(H,D_out)\n",
    "\n",
    "model=torch.nn.Sequential()\n",
    "model.add_module('w0',torch.nn.Linear(D_in,H))\n",
    "model.add_module('relu',torch.nn.ReLU())\n",
    "model.add_module('w0',torch.nn.Linear(H,D_out))\n",
    "\n",
    "loss_function=torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "number_of_iterations=500\n",
    "for i in range(number_of_iterations):\n",
    "    y_predict=model(x)\n",
    "    loss=loss_function(y_predict,y)\n",
    "    print(i, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
