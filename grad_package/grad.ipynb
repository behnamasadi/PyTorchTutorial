{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "Example:\n",
    "\n",
    "\n",
    "$f(w,x)=\\frac{1}{1+e^{-(w_{0}x_{0}+w_{1}x_{1}+w_{2})}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "<img src='images/auto.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Functions\n",
    "\n",
    "$a=w_{0}*x_{0}$\n",
    "\n",
    "$b=w_{1}*x_{1}$\n",
    "\n",
    "$c=a+b$\n",
    "\n",
    "$d=c+w_{2}$\n",
    "\n",
    "$e=-d$\n",
    "\n",
    "$f=exp(e)$\n",
    "\n",
    "$g=1+f$\n",
    "\n",
    "$h=1/g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Values\n",
    "$w_0=2.0$\n",
    "\n",
    "$x_0=-1.0$\n",
    "\n",
    "$w_1=-3.0$\n",
    "\n",
    "$x_1=-2.0$\n",
    "\n",
    "$w_2=-3.0$\n",
    "\n",
    "## Forward\n",
    "$a= -2.0$\n",
    "\n",
    "$b= 6.0$\n",
    "\n",
    "$c= 4.0$\n",
    "\n",
    "$d= 1.0$\n",
    "\n",
    "$e= -1.0$\n",
    "\n",
    "$f= 0.36$\n",
    "\n",
    "$g= 1.36$\n",
    "\n",
    "$h= 0.73$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Gradients\n",
    "1) $\\frac{\\partial h  }{\\partial h }=1$\n",
    "\n",
    "2) $\\frac{\\partial h }{\\partial g }=\\frac{-1}{g^2}=-0.53$\n",
    "\n",
    "$\\frac{\\partial g }{\\partial f }=1$\n",
    "\n",
    "3) $\\frac{\\partial h }{\\partial f }=\\frac{\\partial h }{\\partial g }\\frac{\\partial g }{\\partial f }=-0.53$\n",
    "\n",
    "$\\frac{\\partial f }{\\partial e}=exp(e)=0.36$\n",
    "\n",
    "4) $\\frac{\\partial h }{\\partial e}=\\frac{\\partial h }{\\partial f}\\frac{\\partial f }{\\partial e}=-0.53* 0.36=-0.19$\n",
    "\n",
    "$\\frac{\\partial e }{\\partial d}=-1$\n",
    "\n",
    "5) $\\frac{\\partial h }{\\partial d}=\\frac{\\partial h }{\\partial e}\\frac{\\partial e }{\\partial d}=-0.19*-1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial c}=1$\n",
    "\n",
    "6) $\\frac{\\partial h }{\\partial c}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial c}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial w_2}=1$\n",
    "\n",
    "7) $\\frac{\\partial h }{\\partial w_2}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial w_2}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial a}=1$\n",
    "\n",
    "8) $\\frac{\\partial h }{\\partial a}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial a}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial b}=1$\n",
    "\n",
    "9) $\\frac{\\partial h }{\\partial b}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial b}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial w_0}=x_0$\n",
    "\n",
    "10) $\\frac{\\partial h }{\\partial w_0}=\\frac{\\partial h }{\\partial a}=\\frac{\\partial a }{\\partial w_0}=0.19*x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial x_0 }=w_0$\n",
    "\n",
    "11) $\\frac{\\partial h }{\\partial x_0 }=\\frac{\\partial h }{\\partial a }\\frac{\\partial a}{\\partial x_0 }=0.19*w_0=0.38$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial w_1}=x_0$\n",
    "\n",
    "12) $\\frac{\\partial h }{\\partial w_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial w_1}=0.19* x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial x_1}=w_1$\n",
    "\n",
    "13) $\\frac{\\partial h }{\\partial x_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial x_1}=0.19*w_1=-0.57$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor(-2., grad_fn=<MulBackward0>)\n",
      "b= tensor(6., grad_fn=<MulBackward0>)\n",
      "c= tensor(4., grad_fn=<AddBackward0>)\n",
      "d= tensor(1., grad_fn=<AddBackward0>)\n",
      "e= tensor(-1., grad_fn=<NegBackward>)\n",
      "f= tensor(0.3679, grad_fn=<ExpBackward>)\n",
      "g= tensor(1.3679, grad_fn=<AddBackward0>)\n",
      "h= tensor(0.7311, grad_fn=<MulBackward0>)\n",
      "dh/dw0= tensor(-0.1966)\n",
      "dh/dx0= tensor(0.3932)\n",
      "dh/dw1= tensor(-0.3932)\n",
      "dh/dx1= tensor(-0.3932)\n",
      "dh/dw2= tensor(0.1966)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w0=torch.tensor(2.0,requires_grad=True )\n",
    "x0=torch.tensor(-1.0,requires_grad=True)\n",
    "\n",
    "w1=torch.tensor(-3.0,requires_grad=True)\n",
    "x1=torch.tensor(-2.0,requires_grad=True)\n",
    "\n",
    "w2=torch.tensor(-3.0,requires_grad=True)\n",
    "\n",
    "\n",
    "a=w0*x0\n",
    "b=w1*x1\n",
    "c=a+b\n",
    "d=c+w2\n",
    "e=-d\n",
    "f=torch.exp(e)\n",
    "g=1+f\n",
    "h=1/g\n",
    "\n",
    "print(\"a=\",a)\n",
    "print(\"b=\",b)\n",
    "print(\"c=\",c)\n",
    "print(\"d=\",d)\n",
    "print(\"e=\",e)\n",
    "print(\"f=\",f)\n",
    "print(\"g=\",g)\n",
    "print(\"h=\",h)\n",
    "\n",
    "\n",
    "h.backward()\n",
    "print(\"dh/dw0=\",w0.grad)\n",
    "print(\"dh/dx0=\",x0.grad)\n",
    "print(\"dh/dw1=\",w1.grad)\n",
    "print(\"dh/dx1=\",w1.grad)\n",
    "print(\"dh/dw2=\",w2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, **leaves** are the input tensors, **roots** are the output tensors.\n",
    "  \n",
    "# Leaf\n",
    "A leaf Variable is a variable that is at the beginning of the graph. That means that no operation tracked by the autograd engine created it. \n",
    "\n",
    "\n",
    "Below is a visual representation of the **DAG** in our example. In the graph, the *arrows* are in the direction of the forward pass. The *nodes* represent the backward functions of each operation in the forward pass. The *leaf* nodes in blue represent our leaf tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "\n",
    "a=torch.tensor([1.2],requires_grad=True)\n",
    "b=torch.tensor([2.2],requires_grad=True)\n",
    "c=torch.tensor([0.2],requires_grad=True)\n",
    "d=torch.tensor([0.8],requires_grad=True)\n",
    "e=torch.tensor([7.],requires_grad=True)\n",
    "\n",
    "f=d*((a+b)*(c))+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/graph.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion from the DAG\n",
    "\n",
    "In a NN, parameters that don’t compute gradients are usually called **frozen parameters**. It is useful to “freeze” part of your model if you know in advance that you won’t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for **finetuning** a pretrained network\n",
    "\n",
    "- requires_grad=False\n",
    "- no_grad()\n",
    "- detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no_grad()\n",
    "*torch.no_grad()*: in this context manager in the **__enter__()** method, *set_grad_enabled(False)*\n",
    " so for tensor object *requires_grad* will turn into False.\n",
    " \n",
    "when you use no_grad(), you can control the new w1 and new w2 have no gradients since\n",
    " they are generated by operations, which means you only change the value of w1 and w2,\n",
    " not gradient part, they still have previous defined variable gradient information and back propagation can continue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detach\n",
    "*detach()*: detaches the output from the computationnal graph. So no gradient will be backproped along this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=d*((a+b)*(c.detach()))+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/graph_detach.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero_grad\n",
    "\n",
    "PyTorch accumulates the gradients on subsequent backward passes. \n",
    "\n",
    "We explicitly need to call zero_grad() because, after loss.backward() (when gradients are computed), we need to use optimizer.step() to proceed gradient descent. More specifically, the gradients are not automatically zeroed because these two operations, loss.backward() and optimizer.step(), are separated, and optimizer.step() requires the just computed gradients.\n",
    "\n",
    "In addition, sometimes, we need to accumulate gradient among some batches; to do that, we can simply call backward multiple times and optimize once.\n",
    "\n",
    "Refs: [1](https://stackoverflow.com/questions/44732217/why-do-we-need-to-explicitly-call-zero-grad?noredirect=1&lq=1), [2](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N,D_in,H, D_out=64, 1000, 100, 10\n",
    "learning_rate=1e-6\n",
    "number_of_iterations=500\n",
    "\n",
    "x=torch.randn(N,D_in)\n",
    "y=torch.randn(N,D_out)\n",
    "\n",
    "\n",
    "model1=torch.nn.Sequential(torch.nn.Linear(D_in,H),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(H,D_out))\n",
    "\n",
    "loss_function=torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    y_predict=model1(x)\n",
    "    loss = loss_function(y_predict, y)\n",
    "    #print(i, loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model1.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    model1.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
