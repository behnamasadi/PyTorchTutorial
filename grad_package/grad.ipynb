{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "Example:\n",
    "\n",
    "\n",
    "$f(w,x)=\\frac{1}{1+e^{-(w_{0}x_{0}+w_{1}x_{1}+w_{2})}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "<img src='images/auto.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Functions\n",
    "\n",
    "$a=w_{0}*x_{0}$\n",
    "\n",
    "$b=w_{1}*x_{1}$\n",
    "\n",
    "$c=a+b$\n",
    "\n",
    "$d=c+w_{2}$\n",
    "\n",
    "$e=-d$\n",
    "\n",
    "$f=exp(e)$\n",
    "\n",
    "$g=1+f$\n",
    "\n",
    "$h=1/g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Values\n",
    "$w_0=2.0$\n",
    "\n",
    "$x_0=-1.0$\n",
    "\n",
    "$w_1=-3.0$\n",
    "\n",
    "$x_1=-2.0$\n",
    "\n",
    "$w_2=-3.0$\n",
    "\n",
    "## Forward\n",
    "$a= -2.0$\n",
    "\n",
    "$b= 6.0$\n",
    "\n",
    "$c= 4.0$\n",
    "\n",
    "$d= 1.0$\n",
    "\n",
    "$e= -1.0$\n",
    "\n",
    "$f= 0.36$\n",
    "\n",
    "$g= 1.36$\n",
    "\n",
    "$h= 0.73$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Gradients\n",
    "1) $\\frac{\\partial h  }{\\partial h }=1$\n",
    "\n",
    "2) $\\frac{\\partial h }{\\partial g }=\\frac{-1}{g^2}=-0.53$\n",
    "\n",
    "$\\frac{\\partial g }{\\partial f }=1$\n",
    "\n",
    "3) $\\frac{\\partial h }{\\partial f }=\\frac{\\partial h }{\\partial g }\\frac{\\partial g }{\\partial f }=-0.53$\n",
    "\n",
    "$\\frac{\\partial f }{\\partial e}=exp(e)=0.36$\n",
    "\n",
    "4) $\\frac{\\partial h }{\\partial e}=\\frac{\\partial h }{\\partial f}\\frac{\\partial f }{\\partial e}=-0.53* 0.36=-0.19$\n",
    "\n",
    "$\\frac{\\partial e }{\\partial d}=-1$\n",
    "\n",
    "5) $\\frac{\\partial h }{\\partial d}=\\frac{\\partial h }{\\partial e}\\frac{\\partial e }{\\partial d}=-0.19*-1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial c}=1$\n",
    "\n",
    "6) $\\frac{\\partial h }{\\partial c}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial c}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial d }{\\partial w_2}=1$\n",
    "\n",
    "7) $\\frac{\\partial h }{\\partial w_2}=\\frac{\\partial h }{\\partial d}\\frac{\\partial d }{\\partial w_2}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial a}=1$\n",
    "\n",
    "8) $\\frac{\\partial h }{\\partial a}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial a}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial c }{\\partial b}=1$\n",
    "\n",
    "9) $\\frac{\\partial h }{\\partial b}=\\frac{\\partial h }{\\partial c}\\frac{\\partial c }{\\partial b}=0.19*1=0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial w_0}=x_0$\n",
    "\n",
    "10) $\\frac{\\partial h }{\\partial w_0}=\\frac{\\partial h }{\\partial a}=\\frac{\\partial a }{\\partial w_0}=0.19*x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial a }{\\partial x_0 }=w_0$\n",
    "\n",
    "11) $\\frac{\\partial h }{\\partial x_0 }=\\frac{\\partial h }{\\partial a }\\frac{\\partial a}{\\partial x_0 }=0.19*w_0=0.38$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial w_1}=x_0$\n",
    "\n",
    "12) $\\frac{\\partial h }{\\partial w_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial w_1}=0.19* x_0=-0.19$\n",
    "\n",
    "$\\frac{\\partial b }{\\partial x_1}=w_1$\n",
    "\n",
    "13) $\\frac{\\partial h }{\\partial x_1}=\\frac{\\partial h }{\\partial b}\\frac{\\partial b }{\\partial x_1}=0.19*w_1=-0.57$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph \"DAG\" consisting of Function objects. \n",
    "\n",
    "\n",
    "## torch.autograd\n",
    "`torch.autograd` is PyTorch’s automatic differentiation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "x=torch.tensor([2.0 ,3.0],requires_grad=True)\n",
    "y=torch.tensor([6.0 ,4.0],requires_grad=True)\n",
    "z=3*x**3-y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `.backward()` on `z`, autograd calculates these gradients and stores them in\n",
    "the respective tensors’ `.grad` attribute.\n",
    "We need to explicitly pass a gradient argument in `z.backward()` because it is a vector.\n",
    "gradient is a tensor of the same shape as Q, and it represents the gradient of z w.r.t. itself, i.e. `dz\\dz=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1252e-13,  3.6158e-39])\n",
      "torch.Size([2])\n",
      "tensor([ 1.7084e-13, -3.5712e-40])\n"
     ]
    }
   ],
   "source": [
    "external_grad=torch.empty(2,requires_grad=True)\n",
    "z.backward(external_grad)\n",
    "print(x.grad)\n",
    "print(x.shape)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAGs are dynamic in PyTorch. An important thing to note is: **the graph is recreated from scratch, after each `.backward()` call**, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model. You can change the shape, size and operations at every iteration if needed.\n",
    "\n",
    "In this DAG: \n",
    "\n",
    "1. **arrows**: are in the direction of the forward pass. \n",
    "2. **nodes**: represent the backward functions of each operation in the forward pass.\n",
    "3. **leaf**: A leaf Variable is a variable that is at the beginning of the graph. That means that no operation tracked by the autograd engine created it. nodes in blue represent our leaf tensors.\n",
    "4. **roots** are the output tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor(-2., grad_fn=<MulBackward0>)\n",
      "b= tensor(6., grad_fn=<MulBackward0>)\n",
      "c= tensor(4., grad_fn=<AddBackward0>)\n",
      "d= tensor(1., grad_fn=<AddBackward0>)\n",
      "e= tensor(-1., grad_fn=<NegBackward0>)\n",
      "f= tensor(0.3679, grad_fn=<ExpBackward0>)\n",
      "g= tensor(1.3679, grad_fn=<AddBackward0>)\n",
      "h= tensor(0.7311, grad_fn=<MulBackward0>)\n",
      "dh/dw0= tensor(-0.1966)\n",
      "dh/dx0= tensor(0.3932)\n",
      "dh/dw1= tensor(-0.3932)\n",
      "dh/dx1= tensor(-0.3932)\n",
      "dh/dw2= tensor(0.1966)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'images/graph.svg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "w0=torch.tensor(2.0,requires_grad=True )\n",
    "x0=torch.tensor(-1.0,requires_grad=True)\n",
    "\n",
    "w1=torch.tensor(-3.0,requires_grad=True)\n",
    "x1=torch.tensor(-2.0,requires_grad=True)\n",
    "\n",
    "w2=torch.tensor(-3.0,requires_grad=True)\n",
    "\n",
    "\n",
    "a=w0*x0\n",
    "b=w1*x1\n",
    "c=a+b\n",
    "d=c+w2\n",
    "e=-d\n",
    "f=torch.exp(e)\n",
    "g=1+f\n",
    "h=1/g\n",
    "\n",
    "print(\"a=\",a)\n",
    "print(\"b=\",b)\n",
    "print(\"c=\",c)\n",
    "print(\"d=\",d)\n",
    "print(\"e=\",e)\n",
    "print(\"f=\",f)\n",
    "print(\"g=\",g)\n",
    "print(\"h=\",h)\n",
    "\n",
    "\n",
    "h.backward()\n",
    "print(\"dh/dw0=\",w0.grad)\n",
    "print(\"dh/dx0=\",x0.grad)\n",
    "print(\"dh/dw1=\",w1.grad)\n",
    "print(\"dh/dx1=\",w1.grad)\n",
    "print(\"dh/dw2=\",w2.grad)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchviz\n",
    "\n",
    "\n",
    "h_params={'w0':w0,'x0':x0,'w1':w1,'x1':x1,'w2':w2,\n",
    "          'a':a ,'b':b, 'c':c, 'd':d, 'e':e, 'f':f, 'g':g, 'h':h }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dot=torchviz.make_dot(h,params=h_params)\n",
    "dot.format='svg'\n",
    "dot.render(filename='graph', directory='images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/graph.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.no_grad()`\n",
    "\n",
    "\n",
    "By default, PyTorch tracks every operation on tensors with `requires_grad=True` in order to build a **computational graph** for backpropagation.\n",
    "\n",
    "But in some cases, we **don’t need gradients**:\n",
    "\n",
    "* **Evaluation / Inference** (we don’t train, just forward pass).\n",
    "* **Testing** (computing accuracy, loss, etc. without updating weights).\n",
    "* **When inspecting intermediate values** (we don’t want PyTorch to waste memory tracking them).\n",
    "* **Freezing parameters** during fine-tuning.\n",
    "\n",
    "Using `torch.no_grad()` disables gradient tracking, making things:\n",
    "\n",
    "* Faster\n",
    "* Use less memory\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 1 — Evaluation / Inference\n",
    "\n",
    "```python\n",
    "model.eval()  # set dropout/batchnorm to eval mode\n",
    "\n",
    "with torch.no_grad():   # disable gradient tracking\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        # we can compute accuracy without gradients\n",
    "```\n",
    "\n",
    "Without `no_grad()`, PyTorch would build a computation graph for outputs, wasting memory.\n",
    "\n",
    "---\n",
    "\n",
    "####  Example 2 — Just checking model predictions\n",
    "\n",
    "```python\n",
    "x = torch.randn(1, 3, 224, 224)  # dummy input\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "print(y_pred)\n",
    "```\n",
    "\n",
    "Here, we’re just predicting — no need to track gradients.\n",
    "\n",
    "---\n",
    "\n",
    "####  Example 3 — Freezing part of the model\n",
    "\n",
    "```python\n",
    "# Suppose we want to freeze the feature extractor\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Forward pass without gradients for frozen layers\n",
    "with torch.no_grad():\n",
    "    features = model.features(x)  # won't track grads\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `zero_grad()`\n",
    "\n",
    "PyTorch **accumulates** the gradients on subsequent backward passes. This is convenient while training RNNs. \n",
    "\n",
    "- `zero_grad` clears old gradients from the last step (otherwise you’d just accumulate the gradients from all loss.backward() calls).\n",
    "- `loss.backward()` computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n",
    "- `opt.step()` causes the optimizer to take a step based on the gradients of the parameters.\n",
    "\n",
    "\n",
    "\n",
    "We explicitly need to call `zero_grad()` because, after `loss.backward()` (when gradients are computed), we need to use `optimizer.step()` to proceed with gradient descent. More specifically, the gradients are not automatically zeroed because these two operations, `loss.backward()` and `optimizer.step()`, are separated, and `optimizer.step()` requires the just computed gradients.\n",
    "\n",
    "In addition, sometimes, we need to accumulate the gradient among some batches; to do that, we can simply call backward multiple times and optimize once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "number_of_iterations=500\n",
    "for i in range(number_of_iterations):\n",
    "    y_predict=model(x)\n",
    "    loss=loss_function(y_predict,y)\n",
    "    print(i, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requires_grad\n",
    "\n",
    "**Why do we need `requires_grad=True` for tensors we create manually?**\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1, 3], dtype=torch.float32, requires_grad=True)\n",
    "```\n",
    "\n",
    "you are saying:\n",
    " “I want autograd to track operations on this tensor, so later I can compute gradients wrt this tensor.”\n",
    "\n",
    "If you omit `requires_grad=True`, PyTorch treats the tensor as a constant, not as a variable to optimize.\n",
    "So in your example:\n",
    "\n",
    "```python\n",
    "z = x**2 + y**3\n",
    "z.backward(torch.ones_like(z))\n",
    "```\n",
    "\n",
    "Will only give you gradients if `x` and `y` were created with `requires_grad=True`.\n",
    "\n",
    "---\n",
    "\n",
    "**Why don’t we need to say `requires_grad=True` in `nn.Module` models?**\n",
    "\n",
    "When you create layers like:\n",
    "\n",
    "```python\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "```\n",
    "\n",
    "The **parameters (weights and biases)** inside `torch.nn.Linear` are created as `nn.Parameter`.\n",
    "And `nn.Parameter` is just a tensor with `requires_grad=True` by default:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "linear = nn.Linear(2, 3)\n",
    "print(linear.weight.requires_grad)  # True\n",
    "print(linear.bias.requires_grad)    # True\n",
    "```\n",
    "\n",
    "So, the trainable parameters are automatically set up for gradient computation.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
