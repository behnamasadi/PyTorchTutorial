import torch

x=torch.randn([2,3], requires_grad=True)
print(x)
y=2*x
print(y)
print(y.requires_grad)


# torch.no_grad() in this context manager in the __enter__() method, set_grad_enabled(False)
# so for tensor object requires_grad will turn into False

with torch.no_grad():
    y=2*x
    print(y.requires_grad)

# if you use this no_grad(), you can control the new w1 and new w2 have no gradients since
# they are generated by operations, which means you only change the value of w1 and w2,
# not gradient part, they still have previous defined variable gradient information and back propagation can continue.