{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f84572-dd6d-4705-b083-a56634ee367e",
   "metadata": {},
   "source": [
    "\n",
    "# **1. Pooling in CNNs**\n",
    "\n",
    "Pooling layers reduce the **spatial resolution** of feature maps (height and width), allowing networks to:\n",
    "\n",
    "* Increase **receptive field**\n",
    "* Reduce **computation and memory cost**\n",
    "* Introduce **translation invariance**\n",
    "* Prevent **overfitting** by summarizing local features\n",
    "\n",
    "---\n",
    "\n",
    "## **1.1. Types of Pooling Layers in CNNs**\n",
    "\n",
    "| Type                             | Formula                                                             | Intuition                                            | When to Use                                                                | Impact                                                                 |                                                            |                                            |\n",
    "| -------------------------------- | ------------------------------------------------------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------ |\n",
    "| **Max Pooling**                  | $$y = \\max(x_{i,j})$$                                               | Keeps the strongest activation in each region        | Most common (default) choice in CNNs                                       | Keeps salient edges/features; robust to noise; may discard subtle info |                                                            |                                            |\n",
    "| **Average Pooling**              | $$y = \\frac{1}{N}\\sum_{i,j}x_{i,j}$$                                | Averages features in each region                     | When smooth feature maps are desired (e.g., before classification)         | Reduces noise, but blurs sharp features                                |                                                            |                                            |\n",
    "| **Global Average Pooling (GAP)** | $$y_c = \\frac{1}{HW}\\sum_{i,j}x_{i,j,c}$$                           | Average over the whole feature map per channel       | Replaces fully connected layer at the end (e.g., ResNet)                   | Forces each channel to represent a class concept; fewer parameters     |                                                            |                                            |\n",
    "| **Global Max Pooling (GMP)**     | $$y_c = \\max_{i,j}(x_{i,j,c})$$                                     | Take the max across the entire spatial map           | Used when only the most prominent feature matters (e.g., object detection) | High selectivity; ignores global context                               |                                                            |                                            |\n",
    "| **Lp Pooling**                   | $$y = \\left(\\frac{1}{N}\\sum                                         | x                                                    | ^p\\right)^{1/p}$$                                                          | Generalization of average/max pooling                                  | Can control between average ($p=1$) and max ($p\\to\\infty$) | Adjustable sensitivity to high activations |\n",
    "| **Stochastic Pooling**           | Select random value from region proportional to activation strength | Adds stochasticity for regularization                | Used occasionally for regularization (rare today)                          | Acts as a regularizer; less deterministic                              |                                                            |                                            |\n",
    "| **Adaptive Pooling**             | Output a target size regardless of input shape                      | Used in PyTorch `AdaptiveAvgPool2d` before FC layers | Ensures consistent output shape                                            | Flexible and shape-agnostic                                            |                                                            |                                            |\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2. When to Use Which (CNN)**\n",
    "\n",
    "| Scenario                                     | Recommended Pooling                    | Reason                                          |\n",
    "| -------------------------------------------- | -------------------------------------- | ----------------------------------------------- |\n",
    "| Image classification (ResNet, VGG)           | Max pooling early, then GAP at the end | Keeps strong features, ends with global context |\n",
    "| Smooth/semantic tasks (segmentation)         | Average pooling                        | Avoid losing weak signals                       |\n",
    "| Detection or localization                    | Max pooling                            | Preserves strong activations                    |\n",
    "| Architecture flexibility (input size varies) | Adaptive Avg Pool                      | Consistent final feature vector                 |\n",
    "| Regularization or stochastic effects         | Stochastic Pooling                     | Adds randomness; rarely used today              |\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Pooling in Vision Transformers (ViTs)**\n",
    "\n",
    "Transformers **don’t use pooling layers** in the classical sense — but they still **downsample** or **aggregate tokens** to reduce sequence length or extract global representation.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1. Pooling Analogues in ViTs**\n",
    "\n",
    "| Pooling Equivalent                       | Where It Happens                  | Formula/Mechanism                                  | Impact                                                  | When to Use                                                             |                                                   |\n",
    "| ---------------------------------------- | --------------------------------- | -------------------------------------------------- | ------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| **CLS Token pooling**                    | End of ViT (BERT-style)           | Output of `[CLS]` token represents the whole image | Learns a global representation during training          | Default ViT (classification tasks)                                      |                                                   |\n",
    "| **Global Average Pooling over tokens**   | Alternative to CLS                | $$y = \\frac{1}{N}\\sum_i T_i$$                      | Averages all patch tokens                               | More stable; used in ConvNeXt-ViT hybrids                               |                                                   |\n",
    "| **Patch Merging (Hierarchical Pooling)** | Swin Transformer                  | Concatenates and linearly projects nearby patches  | $$X' = W [X_{i,j}, X_{i+1,j}, X_{i,j+1}, X_{i+1,j+1}]$$ | Reduces resolution and increases feature dim; like CNN stride-2 pooling | When building hierarchical (pyramid) Transformers |\n",
    "| **Token Pooling / Attention Pooling**    | DeiT, PoolFormer, Class-Attention | Attention weights aggregate tokens adaptively      | $$y = \\text{softmax}(A)X$$                              | Adaptive pooling via attention weights                                  | Used in hybrid attention architectures            |\n",
    "| **Spatial Reduction Attention (SRA)**    | PVT (Pyramid Vision Transformer)  | Reduces K,V token count by strided pooling         | Reduces attention cost (quadratic → linear)             | Used for efficiency on large images                                     |                                                   |\n",
    "| **Adaptive Token Sampling / Pooling**    | TokenLearner, MobileViT           | Selects most informative tokens                    | Dynamic spatial pooling                                 | Used for efficient ViTs on edge devices                                 |                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2. When to Use Which (ViT)**\n",
    "\n",
    "| Scenario                        | Recommended Pooling / Downsampling       | Reason                                                     |\n",
    "| ------------------------------- | ---------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Standard ViT (224×224)**      | CLS Token                                | Simple and effective for classification                    |\n",
    "| **More stable representation**  | Global Average Pool over tokens          | Reduces dependency on CLS token learning                   |\n",
    "| **Hierarchical representation** | Patch Merging (Swin, PVT)                | Enables pyramid features (good for detection/segmentation) |\n",
    "| **Lightweight / Mobile ViTs**   | Token Pooling (TokenLearner, PoolFormer) | Efficient adaptive token reduction                         |\n",
    "| **Large-resolution inputs**     | Spatial Reduction Attention              | Reduces memory and compute cost                            |\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Key Insights (CNN vs ViT)**\n",
    "\n",
    "| Aspect                       | CNN                             | ViT                              |\n",
    "| ---------------------------- | ------------------------------- | -------------------------------- |\n",
    "| Pooling type                 | Fixed (max/avg)                 | Learnable / attention-based      |\n",
    "| Purpose                      | Reduce spatial size             | Reduce token sequence length     |\n",
    "| Global representation        | Global Average Pool             | CLS or mean token pooling        |\n",
    "| Hierarchical feature pyramid | Natural (due to stride/pooling) | Explicitly added (Patch Merging) |\n",
    "| Inductive bias               | Translation invariance          | None (learned)                   |\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Summary of Best Practices**\n",
    "\n",
    "✅ **CNNs**\n",
    "\n",
    "* Use **MaxPool** in early layers for invariance.\n",
    "* Use **GlobalAvgPool** instead of FC layers for compactness.\n",
    "* Use **AdaptiveAvgPool** to handle variable input sizes.\n",
    "\n",
    "✅ **ViTs**\n",
    "\n",
    "* Use **CLS token** for classification tasks.\n",
    "* Use **Patch Merging** (Swin-like) for detection/segmentation.\n",
    "* Use **Global Average pooling** over tokens for more stable performance.\n",
    "* Use **Attention-based pooling** for adaptive or efficient ViT variants.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **PyTorch code comparison** between\n",
    "\n",
    "1. CNN pooling (MaxPool2d, AvgPool2d, GlobalAvgPool2d)\n",
    "   and\n",
    "2. ViT token pooling (CLS, mean pooling, patch merging)?\n",
    "   It would help visualize the differences concretely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf58b17-5782-49cb-8ed7-15c28a330362",
   "metadata": {},
   "source": [
    "## **small numeric example**\n",
    "Let’s make a **small numeric example** that directly contrasts **pooling in CNNs** vs **token pooling in ViTs**, so you can *see* how each works.\n",
    "\n",
    "We’ll use very small numbers for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. CNN Pooling Example**\n",
    "\n",
    "Assume we have a **feature map** (1 channel, 4×4):\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 & 4 \\\n",
    "5 & 6 & 7 & 8 \\\n",
    "3 & 2 & 9 & 1 \\\n",
    "0 & 4 & 5 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1. Max Pooling (2×2, stride=2)**\n",
    "\n",
    "We divide the image into non-overlapping 2×2 windows:\n",
    "\n",
    "| Region       | Values    | Max |\n",
    "| ------------ | --------- | --- |\n",
    "| Top-left     | [1,3,5,6] | 6   |\n",
    "| Top-right    | [2,4,7,8] | 8   |\n",
    "| Bottom-left  | [3,2,0,4] | 4   |\n",
    "| Bottom-right | [9,1,5,2] | 9   |\n",
    "\n",
    "Resulting pooled feature map:\n",
    "\n",
    "$$\n",
    "Y_{\\text{max}} =\n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\n",
    "4 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Impact**: Keeps only strongest features (edges, corners).\n",
    "❌ Loses weak but possibly useful information.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2. Average Pooling (2×2, stride=2)**\n",
    "\n",
    "Average each 2×2 region instead of taking max:\n",
    "\n",
    "| Region       | Values    | Mean |\n",
    "| ------------ | --------- | ---- |\n",
    "| Top-left     | [1,3,5,6] | 3.75 |\n",
    "| Top-right    | [2,4,7,8] | 5.25 |\n",
    "| Bottom-left  | [3,2,0,4] | 2.25 |\n",
    "| Bottom-right | [9,1,5,2] | 4.25 |\n",
    "\n",
    "Result:\n",
    "\n",
    "$$\n",
    "Y_{\\text{avg}} =\n",
    "\\begin{bmatrix}\n",
    "3.75 & 5.25 \\\n",
    "2.25 & 4.25\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Impact**: Smooths the representation.\n",
    "❌ Reduces contrast between regions.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3. Global Average Pooling (entire 4×4)**\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{16}\\sum X = \\frac{68}{16} = 4.25\n",
    "$$\n",
    "\n",
    "✅ **Impact**: Converts full spatial map into a single scalar (per channel).\n",
    "Used before classification heads.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. ViT Pooling Example**\n",
    "\n",
    "Now let’s simulate Vision Transformer pooling with **4 patch tokens + CLS token**.\n",
    "\n",
    "Assume:\n",
    "$$\n",
    "\\text{CLS} = [1, 1, 1], \\quad\n",
    "T_1 = [2, 0, 1], \\quad\n",
    "T_2 = [0, 3, 1], \\quad\n",
    "T_3 = [1, 2, 2], \\quad\n",
    "T_4 = [3, 1, 0]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2.1. CLS Token Pooling (standard ViT)**\n",
    "\n",
    "The CLS token attends to all patches during training, and we take only its embedding after the final transformer block.\n",
    "\n",
    "So output = `[CLS]` = `[1, 1, 1]` → later becomes `[y₁, y₂, y₃]` (after MLP head).\n",
    "\n",
    "✅ **Impact**: Learns a *weighted summary* of all tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2. Mean Pooling over Tokens**\n",
    "\n",
    "We can compute the mean over **all patch tokens (not CLS)**:\n",
    "\n",
    "$$\n",
    "\\bar{T} = \\frac{1}{4}(T_1 + T_2 + T_3 + T_4)\n",
    "$$\n",
    "\n",
    "Compute elementwise:\n",
    "\n",
    "$$\n",
    "T_1 + T_2 + T_3 + T_4 =\n",
    "[2+0+1+3, ; 0+3+2+1, ; 1+1+2+0] = [6, 6, 4]\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\bar{T} = [1.5, 1.5, 1.0]\n",
    "$$\n",
    "\n",
    "✅ **Impact**: Similar to Global Average Pooling in CNNs.\n",
    "More stable than CLS-based pooling for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3. Patch Merging (Swin Transformer–style)**\n",
    "\n",
    "Imagine tokens come from 2×2 grid → [T₁, T₂, T₃, T₄].\n",
    "\n",
    "Patch merging concatenates and projects them:\n",
    "\n",
    "$$\n",
    "\\text{Concat}(T_1, T_2, T_3, T_4) =\n",
    "[2,0,1, ; 0,3,1, ; 1,2,2, ; 3,1,0]\n",
    "$$\n",
    "\n",
    "Then we apply a linear layer (say, sum all values for simplicity):\n",
    "\n",
    "$$\n",
    "Y = 2+0+1+0+3+1+1+2+2+3+1+0 = 16\n",
    "$$\n",
    "\n",
    "Result: one merged token that represents the 2×2 region.\n",
    "\n",
    "✅ **Impact**: Reduces token count, increases feature dimension (hierarchical).\n",
    "❌ May lose fine-grained detail.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Summary Table**\n",
    "\n",
    "| Operation           | Input Size | Output Size | Formula               | Effect                  |\n",
    "| ------------------- | ---------- | ----------- | --------------------- | ----------------------- |\n",
    "| Max Pool (CNN)      | 4×4        | 2×2         | $\\max$ per region     | Keeps strongest signal  |\n",
    "| Avg Pool (CNN)      | 4×4        | 2×2         | Mean per region       | Smooths representation  |\n",
    "| Global Avg (CNN)    | 4×4        | Scalar      | $\\frac{1}{HW}\\sum x$  | Compresses spatial info |\n",
    "| CLS Pool (ViT)      | N tokens   | 1 token     | Learned summary       | Learns global rep       |\n",
    "| Mean Pool (ViT)     | N tokens   | 1 token     | $\\frac{1}{N}\\sum T_i$ | Simple global summary   |\n",
    "| Patch Merging (ViT) | 4 tokens   | 1 token     | Concat + Linear       | Downsamples hierarchy   |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63c9f8-c573-436e-adc6-87c42ac684e1",
   "metadata": {},
   "source": [
    "#  What is Max Pooling?\n",
    "Max pooling is a key concept in **deep learning**, especially in **Convolutional Neural Networks (CNNs)** used for image processing and computer vision. \n",
    "\n",
    "\n",
    "**Max pooling** is a **downsampling** operation that reduces the spatial dimensions (width and height) of an input feature map while retaining the most important information.\n",
    "\n",
    "It works by sliding a window (typically 2×2) over the input and **taking the maximum value** in each region.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you have a 4×4 feature map:\n",
    "\n",
    "```\n",
    "1  3  2  4  \n",
    "5  6  1  2  \n",
    "3  2  0  1  \n",
    "1  2  4  3  \n",
    "```\n",
    "\n",
    "Applying 2×2 max pooling with stride 2 gives:\n",
    "\n",
    "```\n",
    "6  4  \n",
    "3  4  \n",
    "```\n",
    "\n",
    "We took the **maximum** from each 2×2 block:\n",
    "- max(1, 3, 5, 6) = 6\n",
    "- max(2, 4, 1, 2) = 4\n",
    "- etc.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Do We Use Max Pooling?\n",
    "\n",
    "1. **Dimensionality Reduction**\n",
    "   - Reduces the number of computations in later layers.\n",
    "   - Helps with overfitting by summarizing regions.\n",
    "\n",
    "2. **Translation Invariance**\n",
    "   - Small shifts or movements in the image don’t change the pooled value.\n",
    "   - Useful for recognizing features regardless of their exact position.\n",
    "\n",
    "3. **Highlighting Strong Features**\n",
    "   - Max pooling keeps only the **strongest activation** (most important signal) in each region.\n",
    "\n",
    "---\n",
    "\n",
    "###  Common Pooling Types\n",
    "\n",
    "| Type           | What it does                         |\n",
    "|----------------|--------------------------------------|\n",
    "| Max Pooling    | Takes the **maximum** value          |\n",
    "| Average Pooling| Takes the **average** value          |\n",
    "| Global Pooling | Takes max/average over entire map    |\n",
    "\n",
    "---\n",
    "\n",
    "###  Typical Parameters\n",
    "\n",
    "- **Kernel Size**: Size of the window (e.g., 2×2)\n",
    "- **Stride**: How far the window moves (e.g., 2 skips every other pixel)\n",
    "- **Padding**: Whether to pad the input to keep the same size (usually not used in pooling)\n",
    "\n",
    "---\n",
    "\n",
    "###  Is Max Pooling Always Good?\n",
    "\n",
    "- **Pros**:\n",
    "  - Reduces memory and computation\n",
    "  - Adds robustness to small changes\n",
    "  - Helps generalization\n",
    "\n",
    "- **Cons**:\n",
    "  - Can lose spatial precision\n",
    "  - Not learnable (fixed operation)\n",
    "\n",
    "---\n",
    "\n",
    "###  Alternatives to Max Pooling\n",
    "\n",
    "- **Strided Convolutions**: Learnable and can replace pooling\n",
    "- **Global Average Pooling**: Often used before fully connected layers\n",
    "- **Attention Mechanisms**: Learn what to focus on instead of blindly pooling\n",
    "\n",
    "---\n",
    "\n",
    "###  Intuition\n",
    "\n",
    "Imagine scanning a patch of an image: max pooling keeps **only the strongest signal** (like the brightest pixel or most confident feature), making the model focus on **what matters most** while ignoring noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf0747-c107-4705-8c59-30fbb5a3a2e4",
   "metadata": {},
   "source": [
    "# The Order of Relu and Max Pooling \n",
    "\n",
    "\n",
    "The order **does matter**, and typically we use **ReLU → MaxPooling (most common)**, not the other way around.\n",
    "\n",
    "\n",
    "With **ReLU** and **MaxPool**, the **forward result cannot differ**—for any pooling window $S$,\n",
    "\n",
    "$$\n",
    "\\max(\\operatorname{ReLU}(S)) \\;=\\; \\operatorname{ReLU}(\\max(S)).\n",
    "$$\n",
    "\n",
    "ReLU is monotone, and max is monotone, so the two orders give the same pooled value.\n",
    "\n",
    "But you *can* get **different arg-max indices** (and thus different backprop routes / unpooling behavior) even though the numeric output matches. Here’s a concrete numeric case:\n",
    "\n",
    "### Example (2×2 window)\n",
    "\n",
    "Let the conv output in a pooling window be\n",
    "\n",
    "$$\n",
    "W=\\begin{bmatrix}\n",
    "-4 & -4\\\\\n",
    "-4 & \\mathbf{-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Order A: Conv → ReLU → MaxPool**\n",
    "  After ReLU:\n",
    "\n",
    "  $$\n",
    "  \\operatorname{ReLU}(W)=\\begin{bmatrix}\n",
    "  0 & 0\\\\\n",
    "  0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  All entries tie at 0. Many libraries break ties by picking the **first** index (e.g., top-left).\n",
    "  **Pooled value:** $0$. **Index chosen:** (top-left).\n",
    "\n",
    "* **Order B: Conv → MaxPool → ReLU**\n",
    "  MaxPool (pre-ReLU) picks the **least negative** (i.e., the maximum) which is $-1$ at **bottom-right**.\n",
    "  After ReLU: $\\operatorname{ReLU}(-1)=0$.\n",
    "  **Pooled value:** $0$. **Index chosen:** (bottom-right).\n",
    "\n",
    "So:\n",
    "\n",
    "* **Forward pooled value** is $0$ in both orders (identical).\n",
    "* **Selected index differs** (top-left vs bottom-right).\n",
    "  This can change **which spatial location receives gradient** (or the stored indices used by MaxUnpool), even though the scalar output is the same. (In this specific all-nonpositive case, the gradient magnitude still becomes 0 due to ReLU, but the *index* recorded by pooling differs.)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Learnable parameters\n",
    "\n",
    "* **Conv layers** have learnable parameters:\n",
    "\n",
    "  * **weights** (the filter kernels)\n",
    "  * **biases** (if enabled)\n",
    "* **ReLU** has no parameters (it’s just a fixed non-linearity).\n",
    "* **MaxPool** has no parameters either — it’s just an operation that selects the maximum in each window.\n",
    "\n",
    " So the *only* learnable parameters are in the convolution. Pooling never has learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### What MaxPool does “remember”\n",
    "\n",
    "MaxPool does not learn, but it **remembers the index of the max element** in each pooling window during the forward pass.\n",
    "\n",
    "* In the **backward pass**, the gradient is sent only to that max location, all other positions get zero gradient.\n",
    "* This is what we saw in the PyTorch experiment: the gradient routes depend on which index was picked.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why order matters\n",
    "\n",
    "Even though forward outputs from `Conv→ReLU→MaxPool` and `Conv→MaxPool→ReLU` are the same, the **chosen indices can differ** (especially in windows with negatives).\n",
    "\n",
    "That means:\n",
    "\n",
    "* **Gradient routing can differ**:\n",
    "\n",
    "  * `Conv→ReLU→MaxPool`: negatives are zeroed first, so only positive activations can receive gradient.\n",
    "  * `Conv→MaxPool→ReLU`: MaxPool might pick a negative as the “max” (if all are negative). After ReLU, the output becomes 0, but the index is still recorded → during backprop, the gradient will flow to that negative conv output before being squashed.\n",
    "* This affects how the optimizer updates the conv **weights**, since gradients are computed with respect to those weights.\n",
    "\n",
    "So, **the difference is not because MaxPool has learnable parameters** (it doesn’t).\n",
    "The difference is in **which conv weights get updated**, because gradient flow is determined by the pooling index selection.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "* The learnable parameters are **conv weights & biases only**.\n",
    "* Order matters because **gradient paths differ** due to how MaxPool selects indices **before or after ReLU**.\n",
    "* Over many updates, this can slightly change how the network learns — which is why almost all architectures standardize on **Conv → ReLU → MaxPool**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81e37e-5af6-4777-9059-003d44899eb9",
   "metadata": {},
   "source": [
    "a **step-by-step backprop** with tiny 2×2 windows to show exactly what happens. We’ll use a single conv feature map (so we can ignore multi-channel complications), a **2×2 MaxPool** (so it reduces to a single scalar), and the loss $L$ is just the pooled output (so $\\partial L/\\partial(\\text{pooled})=1$).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) All-negative window → gradients die in both orders\n",
    "\n",
    "Let the **conv output** (pre-ReLU) be\n",
    "\n",
    "$$\n",
    "Z=\\begin{bmatrix}\n",
    "-4 & -2\\\\\n",
    "-3 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Order A: Conv → ReLU → MaxPool**\n",
    "\n",
    "1. **ReLU**: $A=\\operatorname{ReLU}(Z)=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}$\n",
    "2. **MaxPool** over $A$: pooled value $y = \\max(A)=0$.\n",
    "   (There’s a tie; suppose the pool **stores** top-left index, $(0,0)$, by convention.)\n",
    "3. **Loss**: $L = y$ ⇒ $\\frac{\\partial L}{\\partial y}=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* Through MaxPool: gradient goes to the stored index in $A$:\n",
    "  $\\frac{\\partial L}{\\partial A}=\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}$.\n",
    "* Through ReLU: $A=\\operatorname{ReLU}(Z)\\Rightarrow \\operatorname{ReLU}'(Z)=0$ element-wise (since all $Z\\le 0$).\n",
    "  $\\frac{\\partial L}{\\partial Z}=\\frac{\\partial L}{\\partial A}\\odot \\operatorname{ReLU}'(Z)=\\mathbf{0}$.\n",
    "\n",
    "**Result:** $\\frac{\\partial L}{\\partial Z}=0$ everywhere ⇒ **no weight updates** from this window.\n",
    "\n",
    "#### Order B: Conv → MaxPool → ReLU\n",
    "\n",
    "1. **MaxPool** over $Z$: pooled pre-ReLU value $y'=\\max(Z)=-1$ at index $(1,1)$ (bottom-right).\n",
    "2. **ReLU**: $y=\\operatorname{ReLU}(y')=\\operatorname{ReLU}(-1)=0$.\n",
    "3. **Loss**: $L=y$ ⇒ $\\frac{\\partial L}{\\partial y}=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* Through ReLU at $y'=-1$: $\\operatorname{ReLU}'(y')=0$ ⇒ $\\frac{\\partial L}{\\partial y'}=0$.\n",
    "* Through MaxPool: the gradient to $Z$ at the max index is $0$, others $0$ too.\n",
    "  $\\frac{\\partial L}{\\partial Z}=\\mathbf{0}$.\n",
    "\n",
    "**Result:** again **no weight updates**.\n",
    "\n",
    "**Conclusion (all-negative case):** Picking $-1$ (Order B) vs a top-left tie at 0 (Order A) **does not help** — ReLU’s derivative is 0 at negatives, so gradients die either way.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Mixed-sign window → gradients flow (and are the same)\n",
    "\n",
    "Now let\n",
    "\n",
    "$$\n",
    "Z=\\begin{bmatrix}\n",
    "-4 & \\mathbf{2}\\\\\n",
    "1 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Order A (ReLU first):** $A=\\begin{bmatrix}0&2\\\\1&0\\end{bmatrix}$, MaxPool picks $2$ at $(0,1)$.\n",
    "* **Order B (Pool first):** Max of $Z$ is $2$ at $(0,1)$; ReLU keeps it $2$.\n",
    "\n",
    "Both orders: $y=2$, $L=y\\Rightarrow \\partial L/\\partial y=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* The pool **stores the same index** $(0,1)$.\n",
    "* ReLU derivative at that location is **1** (since $Z_{0,1}=2>0$).\n",
    "* So $\\frac{\\partial L}{\\partial Z}$ is **1 at $(0,1)$** and **0 elsewhere** — **for both orders**.\n",
    "\n",
    "**Result:** **Same forward, same gradient flow** when there’s a strictly positive max.\n",
    "\n",
    "---\n",
    "\n",
    "#### Takeaways\n",
    "\n",
    "* **MaxPool has no learnable parameters.** It only stores **indices** of maxima.\n",
    "* With **all negatives**, both orders yield **zero gradient** due to $\\operatorname{ReLU}'=0$ at negatives.\n",
    "* With **positives present**, both orders pick the same (strict) max and route **the same gradient**.\n",
    "* Differences can occur in **stored indices** for the all-nonpositive case (tie vs least negative), but **gradients still end up zero**.\n",
    "* That’s why the community standardizes on **Conv → ReLU → MaxPool**: cleaner semantics (pool only over active features) without risking odd index choices that don’t help learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
