{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31525039-344f-4a4b-a192-d47bd6d55d5e",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "\n",
    "### Data Genertation\n",
    "```python\n",
    "    # Generate data\n",
    "    n_samples = 1000  # More data\n",
    "    x = torch.linspace(-20, 20, n_samples).reshape(-1, 1)\n",
    "    y = torch.sin(x) + 0.05 * torch.randn(n_samples, 1)\n",
    "\n",
    "    # Convert to numpy for plotting\n",
    "    x_np = x.numpy()\n",
    "    y_np = y.numpy()\n",
    "```\n",
    "\n",
    "\n",
    "### Normalization Step\n",
    "\n",
    "Why do we normalize?\n",
    "Neural networks learn more efficiently when the input features are in a \n",
    "consistent, small range (e.g. [-1, 1] or [0, 1]). Large raw values can \n",
    "cause unstable gradients and slow convergence, especially with activation \n",
    "functions like `ReLU`, `tanh`, or `sigmoid`.\n",
    "\n",
    "In this case:\n",
    "  - `x` originally ranges from `[-20, 20]`\n",
    "  - We scale it down by dividing by `20` → new range is `[-1, 1]`\n",
    "  - `y` is already bounded (sin(x) ∈ `[-1, 1])` and only has small added noise,\n",
    "    so we can safely leave y as-is.\n",
    "\n",
    "This makes the optimization problem numerically stable and easier to solve.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "```python\n",
    "\n",
    "    \n",
    "    x_normalized = x / 20.0  # Scale x to [-1, 1]\n",
    "    y_normalized = y         # y is already in a good range\n",
    "   \n",
    "    # Create dataset and dataloader with normalized data\n",
    "    function_dataset = FunctionDataset(x_normalized, y_normalized)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Normalization Is Needed\n",
    "\n",
    "* **Stability**: Large input values (e.g., -20 to 20) can cause very large outputs in linear layers (`wx + b`), making training unstable.\n",
    "* **Gradient flow**: With normalized inputs, activations stay in their \"useful\" range (e.g., tanh not saturated at ±1).\n",
    "* **Generalization**: Models trained on normalized data usually generalize better because the optimizer doesn’t have to fight scale differences.\n",
    "\n",
    "---\n",
    "\n",
    "###  Comparison with Image Input\n",
    "\n",
    "In **image tasks**, we almost always normalize inputs as well, but the method differs slightly:\n",
    "\n",
    "* **Raw pixel range**: Images are typically in `[0, 255]`.\n",
    "\n",
    "* **Normalization**:\n",
    "\n",
    "  1. First scaled to `[0, 1]` by dividing by 255.\n",
    "  2. Then standardized using mean and std (per channel):\n",
    "\n",
    "     ```python\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "     ```\n",
    "\n",
    "     These values come from ImageNet statistics and center the data around 0 with unit variance.\n",
    "\n",
    "* **Comparison to our function input**:\n",
    "\n",
    "  * Your `x` normalization is a simple **min-max scaling** to `[-1, 1]`.\n",
    "  * Image preprocessing is usually **standardization** (subtract mean, divide by std), because RGB channels have different distributions and need per-channel correction.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**:\n",
    "\n",
    "* For **1D function regression** → simple scaling to `[-1, 1]` is sufficient.\n",
    "* For **images** → standardization is more robust, because each channel has different brightness/contrast distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier/Glorot initialization\n",
    "\n",
    "\n",
    "```python\n",
    "    # Better initialization (optional, but recommended)\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Xavier/Glorot initialization:\n",
    "            # - Fills the weight matrix with values drawn from a uniform distribution\n",
    "            #   with bounds based on the number of input and output units.\n",
    "            # - Keeps the variance of activations roughly the same across layers.\n",
    "            # - Prevents vanishing/exploding activations and gradients.\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "            # Bias is set to 0:\n",
    "            # - Because bias can start from zero without symmetry-breaking issues.\n",
    "            # - Keeping it zero lets the network learn the required offset naturally.\n",
    "            nn.init.zeros_(module.bias)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* **Default initialization in PyTorch**\n",
    "  By default, PyTorch already initializes `nn.Linear` weights with a uniform distribution depending on the layer size. This works fine, but it’s *generic*.\n",
    "\n",
    "* **Xavier (Glorot) initialization**\n",
    "  Designed specifically for **tanh/ReLU-like activations**, it sets the weights so that:\n",
    "\n",
    "  $$\n",
    "  \\text{Var}(z_{in}) \\approx \\text{Var}(z_{out})\n",
    "  $$\n",
    "\n",
    "  That means signals neither blow up (explode) nor shrink to almost zero (vanish) as they pass through layers.\n",
    "\n",
    "  For a Linear layer with `fan_in` inputs and `fan_out` outputs, weights are sampled from:\n",
    "\n",
    "  $$\n",
    "  U\\Big(-\\sqrt{\\frac{6}{fan_{in} + fan_{out}}}, \\; \\sqrt{\\frac{6}{fan_{in} + fan_{out}}}\\Big)\n",
    "  $$\n",
    "\n",
    "* **Bias = 0**\n",
    "  Bias terms don’t suffer from the same scaling problem as weights, so initializing them to 0 is common and safe. The network quickly learns non-zero bias values if needed.\n",
    "\n",
    "---\n",
    "\n",
    "###  Effect in our case\n",
    "\n",
    "Since your network is fairly deep (4 linear layers with ReLU and Dropout), **better initialization**:\n",
    "\n",
    "* Speeds up convergence.\n",
    "* Reduces the risk of flat/unstable training curves.\n",
    "* Makes it easier for the optimizer (Adam here) to find a good minimum.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**:\n",
    "That block improves training stability because **Xavier initialization preserves variance across layers**, preventing vanishing/exploding activations, while **zero bias** is a safe default.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eee70f-af50-4e78-b02e-c5a8eec14470",
   "metadata": {},
   "source": [
    "###  Xavier (Glorot) vs. He (Kaiming) Initialization\n",
    "\n",
    "#### 1. **Xavier (Glorot) Initialization**\n",
    "\n",
    "* Formula (uniform):\n",
    "\n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\tfrac{6}{fan_{in} + fan_{out}}}, \\; \\sqrt{\\tfrac{6}{fan_{in} + fan_{out}}}\\Big)\n",
    "  $$\n",
    "* Goal: Keep the **variance of activations** roughly the same across layers.\n",
    "* Works best with **symmetric, saturating activations** like:\n",
    "\n",
    "  * `tanh`\n",
    "  * `sigmoid`\n",
    "* Why? Because tanh/sigmoid squash values into a small range (\\[-1, 1] or \\[0, 1]), so you need careful balancing of variance both forward (inputs) and backward (gradients).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **He (Kaiming) Initialization**\n",
    "\n",
    "* Formula (for uniform):\n",
    "\n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\tfrac{6}{fan_{in}}}, \\; \\sqrt{\\tfrac{6}{fan_{in}}}\\Big)\n",
    "  $$\n",
    "* Goal: Compensate for the fact that **ReLU zeroes out half of its inputs** on average.\n",
    "* Works best with **non-saturating, rectifier activations** like:\n",
    "\n",
    "  * `ReLU`\n",
    "  * `LeakyReLU`\n",
    "  * `ELU`\n",
    "* Why? ReLU discards negative values, so variance would shrink as you go deeper unless you give it a “boost” → He init scales by only `fan_in` (not both `fan_in + fan_out`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Rule of Thumb**\n",
    "\n",
    "* **Use Xavier** → if your network uses **tanh** or **sigmoid** activations.\n",
    "* **Use He/Kaiming** → if your network uses **ReLU-like** activations.\n",
    "* **If unsure** → and your architecture is modern (ReLU/LeakyReLU etc.), He is usually the safer choice.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Our case**\n",
    "\n",
    "In your `FunctionModel`, you’re using **ReLU**:\n",
    "\n",
    "```python\n",
    "x = torch.relu(self.fc1(x))\n",
    "```\n",
    "\n",
    "That means **He initialization is more appropriate** than Xavier, because ReLU cuts off negative activations and Xavier doesn’t account for that.\n",
    "\n",
    "You could implement it like this:\n",
    "\n",
    "```python\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(module.bias)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "* Xavier = good for `tanh` / `sigmoid`.\n",
    "* He = good for `ReLU` / `LeakyReLU`.\n",
    "* In practice, **modern deep networks almost always use ReLU variants → use He init**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7b27f-821f-4cdd-a29c-413ad8f47459",
   "metadata": {},
   "source": [
    "\n",
    "#### He (Kaiming) Initialization\n",
    "![](images/kaiming_uniform.svg)\n",
    "\n",
    "#### Xavier (Glorot)\n",
    "![](images/xavier.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e2f66-f5a9-4b57-9cfe-c8d2059cdd19",
   "metadata": {},
   "source": [
    "###  What happens if you **don’t** initialize manually?\n",
    "\n",
    "When you create a layer in PyTorch, like:\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(1, 64)\n",
    "```\n",
    "\n",
    "PyTorch automatically initializes the weights and biases for you.\n",
    "The defaults come from the **Kaiming uniform initialization** (also called **LeCun uniform** in some contexts), but with slightly different parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Default for `nn.Linear`\n",
    "\n",
    "* **Weights (`W`)**:\n",
    "\n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\tfrac{1}{fan_{in}}}, \\; \\sqrt{\\tfrac{1}{fan_{in}}}\\Big)\n",
    "  $$\n",
    "\n",
    "  where `fan_in` = number of input features.\n",
    "  → This is **Kaiming uniform** but with `a=√5` (to account for bias variance).\n",
    "\n",
    "* **Biases (`b`)**:\n",
    "\n",
    "  $$\n",
    "  b \\sim U\\Big(-\\tfrac{1}{\\sqrt{fan_{in}}}, \\; \\tfrac{1}{\\sqrt{fan_{in}}}\\Big)\n",
    "  $$\n",
    "\n",
    "So by default, you’re already getting something *reasonable*, not random chaos.\n",
    "\n",
    "---\n",
    "\n",
    "###  Comparing with Explicit Xavier/He\n",
    "\n",
    "* **Default (PyTorch)** → Kaiming uniform with a specific parameter choice. Works fairly well in most cases.\n",
    "* **Explicit Xavier** → Balanced for tanh/sigmoid, may not be ideal for ReLU (could slightly slow convergence).\n",
    "* **Explicit He/Kaiming** → Optimized for ReLU/LeakyReLU, often speeds up convergence and improves stability.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why results differ\n",
    "\n",
    "When you re-initialize with `nn.init.xavier_uniform_` or `nn.init.kaiming_uniform_`, you’re **changing the distribution of starting weights**.\n",
    "\n",
    "Since neural network training is highly sensitive to initialization:\n",
    "\n",
    "* Early layers may saturate or stay alive longer.\n",
    "* The optimizer may explore a different trajectory.\n",
    "* With poor initialization, training might get stuck in plateaus.\n",
    "\n",
    "That’s why you see different results when you add/remove that initialization block.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **If you don’t initialize** → PyTorch uses a built-in Kaiming-uniform-like scheme.\n",
    "* **If you do initialize** → You can *tailor* it (Xavier vs. He) to your activation function.\n",
    "* **Different results** happen because initialization controls how signals and gradients flow at the very start — changing the whole optimization trajectory.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
