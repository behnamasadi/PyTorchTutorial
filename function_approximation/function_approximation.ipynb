{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31525039-344f-4a4b-a192-d47bd6d55d5e",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "\n",
    "### Data Genertation\n",
    "```python\n",
    "    # ===== CONFIGURATION: Change these values to test different ranges =====\n",
    "    x_min, x_max = -20, 20  # Try: (-10, 10), (0, 50), (-100, 100), etc.\n",
    "    n_samples = 1000\n",
    "    # ========================================================================\n",
    "\n",
    "    # Generate data\n",
    "    x = torch.linspace(x_min, x_max, n_samples).reshape(-1, 1).to(device)\n",
    "    y = torch.sin(x) + 0.05 * torch.randn(n_samples, 1).to(device)\n",
    "\n",
    "    # Convert to numpy for plotting\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "```\n",
    "\n",
    "\n",
    "### Normalization Step\n",
    "\n",
    "Neural networks learn more efficiently when the input features are in a \n",
    "consistent, small range (e.g. [-1, 1] or [0, 1]). Large raw values can \n",
    "cause unstable gradients and slow convergence, especially with activation \n",
    "functions like `ReLU`, `tanh`, or `sigmoid`.\n",
    "\n",
    "In this case:\n",
    "  - `x` originally ranges from `[-20, 20]`\n",
    "  - We scale it down by dividing by `20` → new range is `[-1, 1]`\n",
    "  - `y` is already bounded (sin(x) ∈ `[-1, 1])` and only has small added noise,\n",
    "    so we can safely leave y as-is.\n",
    "\n",
    "This makes the optimization problem numerically stable and easier to solve.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "```python\n",
    "    # Normalize both x and y using mean/std standardization (consistent approach)\n",
    "    x_mean = x.mean()\n",
    "    x_std = x.std()\n",
    "    x_normalized = (x - x_mean) / x_std  # Standardize x to ~N(0,1)\n",
    "\n",
    "    y_mean = y.mean()\n",
    "    y_std = y.std()\n",
    "    y_normalized = (y - y_mean) / y_std  # Standardize y to ~N(0,1)```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ed487-d0bb-4803-9c3a-ca49015dea8c",
   "metadata": {},
   "source": [
    "###  Why Normalization Is Needed\n",
    "\n",
    "* **Stability**: Large input values (e.g., -20 to 20) can cause very large outputs in linear layers (`wx + b`), making training unstable.\n",
    "* **Gradient flow**: With normalized inputs, activations stay in their \"useful\" range (e.g., tanh not saturated at ±1).\n",
    "* **Generalization**: Models trained on normalized data usually generalize better because the optimizer doesn’t have to fight scale differences.\n",
    "\n",
    "---\n",
    "\n",
    "###  Comparison with Image Input\n",
    "\n",
    "In **image tasks**, we almost always normalize inputs as well, but the method differs slightly:\n",
    "\n",
    "* **Raw pixel range**: Images are typically in `[0, 255]`.\n",
    "\n",
    "* **Normalization**:\n",
    "\n",
    "  1. First scaled to `[0, 1]` by dividing by 255.\n",
    "  2. Then standardized using mean and std (per channel):\n",
    "\n",
    "     ```python\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "     ```\n",
    "\n",
    "     These values come from ImageNet statistics and center the data around 0 with unit variance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier/Glorot initialization\n",
    "\n",
    "\n",
    "```python\n",
    "    # Better initialization (optional, but recommended)\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Xavier/Glorot initialization:\n",
    "            # - Fills the weight matrix with values drawn from a uniform distribution\n",
    "            #   with bounds based on the number of input and output units.\n",
    "            # - Keeps the variance of activations roughly the same across layers.\n",
    "            # - Prevents vanishing/exploding activations and gradients.\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "            # Bias is set to 0:\n",
    "            # - Because bias can start from zero without symmetry-breaking issues.\n",
    "            # - Keeping it zero lets the network learn the required offset naturally.\n",
    "            nn.init.zeros_(module.bias)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7b27f-821f-4cdd-a29c-413ad8f47459",
   "metadata": {},
   "source": [
    "\n",
    "#### He (Kaiming) Initialization\n",
    "![](images/kaiming_uniform.svg)\n",
    "\n",
    "#### Xavier (Glorot)\n",
    "![](images/xavier.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e2f66-f5a9-4b57-9cfe-c8d2059cdd19",
   "metadata": {},
   "source": [
    "###  What happens if you **don’t** initialize manually?\n",
    "\n",
    "When you create a layer in PyTorch, like:\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(1, 64)\n",
    "```\n",
    "\n",
    "PyTorch automatically initializes the weights and biases for you.\n",
    "The defaults come from the **Kaiming uniform initialization** (also called **LeCun uniform** in some contexts), but with slightly different parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Default for `nn.Linear`\n",
    "\n",
    "* **Weights (`W`)**:\n",
    "\n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\tfrac{1}{fan_{in}}}, \\; \\sqrt{\\tfrac{1}{fan_{in}}}\\Big)\n",
    "  $$\n",
    "\n",
    "  where `fan_in` = number of input features.\n",
    "  → This is **Kaiming uniform** but with `a=√5` (to account for bias variance).\n",
    "\n",
    "* **Biases (`b`)**:\n",
    "\n",
    "  $$\n",
    "  b \\sim U\\Big(-\\tfrac{1}{\\sqrt{fan_{in}}}, \\; \\tfrac{1}{\\sqrt{fan_{in}}}\\Big)\n",
    "  $$\n",
    "\n",
    "So by default, you’re already getting something *reasonable*, not random chaos.\n",
    "\n",
    "---\n",
    "\n",
    "###  Comparing with Explicit Xavier/He\n",
    "\n",
    "* **Default (PyTorch)** → Kaiming uniform with a specific parameter choice. Works fairly well in most cases.\n",
    "* **Explicit Xavier** → Balanced for tanh/sigmoid, may not be ideal for ReLU (could slightly slow convergence).\n",
    "* **Explicit He/Kaiming** → Optimized for ReLU/LeakyReLU, often speeds up convergence and improves stability.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why results differ\n",
    "\n",
    "When you re-initialize with `nn.init.xavier_uniform_` or `nn.init.kaiming_uniform_`, you’re **changing the distribution of starting weights**.\n",
    "\n",
    "Since neural network training is highly sensitive to initialization:\n",
    "\n",
    "* Early layers may saturate or stay alive longer.\n",
    "* The optimizer may explore a different trajectory.\n",
    "* With poor initialization, training might get stuck in plateaus.\n",
    "\n",
    "That’s why you see different results when you add/remove that initialization block.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **If you don’t initialize** → PyTorch uses a built-in Kaiming-uniform-like scheme.\n",
    "* **If you do initialize** → You can *tailor* it (Xavier vs. He) to your activation function.\n",
    "* **Different results** happen because initialization controls how signals and gradients flow at the very start — changing the whole optimization trajectory.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
