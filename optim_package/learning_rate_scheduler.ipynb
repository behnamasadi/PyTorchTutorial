{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1c2f32-d34f-4d90-a8a8-9d08b60e0a57",
   "metadata": {},
   "source": [
    "## What is a Learning Rate Scheduler?\n",
    "\n",
    "The **learning rate (LR)** controls how big each update step is during optimization:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t , \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "where $ \\eta_t $ is the **learning rate at step $t$**.\n",
    "\n",
    "A **scheduler** automatically adjusts $ \\eta_t $ during training to:\n",
    "\n",
    "* Speed up convergence early on,\n",
    "* Avoid overshooting minima,\n",
    "* Fine-tune learning near convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup: Basic Training Loop (Before Scheduler)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy model, dataset, and loss\n",
    "model = nn.Linear(10, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "### Basic training loop without scheduler\n",
    "\n",
    "```python\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Add a Scheduler\n",
    "\n",
    "You create a scheduler **after** the optimizer.\n",
    "\n",
    "**Example — StepLR**\n",
    "\n",
    "This scheduler **decays the learning rate by a factor of `gamma` every `step_size` epochs**.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "```\n",
    "\n",
    "Here, every `5` epochs the $LR$ will be multiplied by `0.1`.\n",
    "\n",
    "---\n",
    "\n",
    "### Training loop (with StepLR)\n",
    "\n",
    "```python\n",
    "for epoch in range(20):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print current learning rate\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1}: LR = {lr:.5f}\")\n",
    "```\n",
    "\n",
    "**Order summary:**\n",
    "\n",
    "1. Forward\n",
    "2. Backward\n",
    "3. Optimizer step\n",
    "4. Scheduler step (usually once per epoch)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Popular Schedulers\n",
    "\n",
    "| Scheduler             | Description                                      | Common use                          |\n",
    "| --------------------- | ------------------------------------------------ | ----------------------------------- |\n",
    "| **StepLR**            | Decrease LR every fixed number of epochs         | Simple training with known LR drops |\n",
    "| **MultiStepLR**       | Drop LR at specified epochs                      | More flexible version of StepLR     |\n",
    "| **ExponentialLR**     | Multiply LR by `gamma` every epoch               | Smooth decay                        |\n",
    "| **CosineAnnealingLR** | Cosine-shaped LR curve                           | Vision Transformers, segmentation   |\n",
    "| **ReduceLROnPlateau** | Decrease LR when validation loss stops improving | Adaptive scheduling                 |\n",
    "| **OneCycleLR**        | Increase then decrease LR within one cycle       | Fast convergence (common for CNNs)  |\n",
    "| **LambdaLR**          | Define a custom lambda function for LR           | Full control                        |\n",
    "\n",
    "---\n",
    "\n",
    "**Example: `ReduceLROnPlateau`**\n",
    "\n",
    "This one is different — you call `.step(metric)` with the **validation loss** instead of every epoch.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_one_epoch(...)\n",
    "    val_loss = evaluate(...)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**`OneCycleLR`**\n",
    "\n",
    "Used for fast and stable convergence, especially in CNNs and Transformers.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.1, steps_per_epoch=len(dataloader), epochs=10\n",
    ")\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "Here, `.step()` is called **every batch**, not every epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### Checking the Learning Rate Schedule\n",
    "\n",
    "You can visualize the learning rate over time to verify it behaves as expected.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = []\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.plot(range(1, 21), lrs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('LR schedule')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ Calling `scheduler.step()` **before** the first optimizer step — always step **after** optimizer update.\n",
    "✅ Exception: when using **OneCycleLR**, step every iteration after `optimizer.step()`.\n",
    "\n",
    "❌ Forgetting to pass validation loss for `ReduceLROnPlateau`.\n",
    "\n",
    "✅ Use `scheduler.get_last_lr()` to log current LR to TensorBoard/W&B.\n",
    "\n",
    "---\n",
    "\n",
    "###  Quick Comparison Table\n",
    "\n",
    "| Scheduler         | Step When | Input Needed    | LR Behavior              |\n",
    "| ----------------- | --------- | --------------- | ------------------------ |\n",
    "| StepLR            | epoch     | none            | abrupt drops             |\n",
    "| MultiStepLR       | epoch     | none            | controlled drops         |\n",
    "| ExponentialLR     | epoch     | none            | smooth exponential decay |\n",
    "| ReduceLROnPlateau | epoch     | validation loss | adaptive decay           |\n",
    "| OneCycleLR        | batch     | none            | cyclic pattern           |\n",
    "| CosineAnnealingLR | epoch     | none            | smooth cosine decay      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d57ca1-709a-44a4-81b5-297120463b46",
   "metadata": {},
   "source": [
    "## **Best Practices**\n",
    "\n",
    "### 1. **Understand the goal of your scheduler**\n",
    "\n",
    "Schedulers aren’t magic — they’re a way to **control the learning dynamics**:\n",
    "\n",
    "| Goal                                                        | Scheduler Type                                       | Why                                |\n",
    "| ----------------------------------------------------------- | ---------------------------------------------------- | ---------------------------------- |\n",
    "| Fast convergence at start, small steps near minima          | **StepLR**, **ExponentialLR**, **CosineAnnealingLR** | Reduces overshooting               |\n",
    "| Avoid getting stuck in local minima                         | **CosineAnnealingLR**, **OneCycleLR**                | Uses cyclical or smooth schedules  |\n",
    "| Dynamically adapt to validation loss                        | **ReduceLROnPlateau**                                | Reduces LR when learning stagnates |\n",
    "| Highly tuned training for large models (Transformers, ViTs) | **CosineAnnealingWarmRestarts** or **OneCycleLR**    | Smooth control of LR dynamics      |\n",
    "\n",
    "---\n",
    "\n",
    "###  2. **Call order matters**\n",
    "\n",
    "Always follow this inside your loop:\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(model(inputs), targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "scheduler.step()  # after optimizer.step()\n",
    "```\n",
    "\n",
    "Exceptions:\n",
    "\n",
    "* **OneCycleLR** → step **every batch** (not every epoch)\n",
    "* **ReduceLROnPlateau** → step **after validation**, with `scheduler.step(val_loss)`\n",
    "\n",
    "---\n",
    "\n",
    "###  3. **Common default heuristics**\n",
    "\n",
    "| Scenario                      | Recommended Scheduler                         | Base LR                             |\n",
    "| ----------------------------- | --------------------------------------------- | ----------------------------------- |\n",
    "| Simple CNN, small dataset     | `StepLR(step_size=10, gamma=0.1)`             | 1e-2 or 1e-3                        |\n",
    "| ResNet / UNet training        | `CosineAnnealingLR(T_max=epochs)`             | 1e-3                                |\n",
    "| Transformer / ViT             | `CosineAnnealingWarmRestarts` or `OneCycleLR` | use warmup start: e.g., 1e-4 → 1e-3 |\n",
    "| Fine-tuning pretrained model  | `ReduceLROnPlateau`                           | start smaller (1e-4)                |\n",
    "| Very large batch size (>1024) | `OneCycleLR`                                  | can start higher, e.g., 1e-2        |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Warm-up phase**\n",
    "\n",
    "Large models (especially Transformers, Vision Transformers, or very deep CNNs) often fail to converge if the learning rate starts too high.\n",
    "\n",
    "Use a **warm-up** scheduler — start with a small LR and gradually increase:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{\\text{max}} \\times \\frac{t}{t_\\text{warmup}}\n",
    "$$\n",
    "\n",
    "Implementation:\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def warmup_linear_lr(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0  # keep LR constant after warmup\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup_linear_lr)\n",
    "```\n",
    "\n",
    "Or use `OneCycleLR` (it includes warm-up automatically).\n",
    "\n",
    "---\n",
    "\n",
    "###  5. **Log and visualize your learning rate**\n",
    "\n",
    "Track the LR every epoch or batch — for example, with W&B or TensorBoard:\n",
    "\n",
    "```python\n",
    "lr = scheduler.get_last_lr()[0]\n",
    "wandb.log({'learning_rate': lr})\n",
    "```\n",
    "\n",
    "Or plot it to ensure your schedule behaves as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Do not over-schedule**\n",
    "\n",
    "If you reduce the LR too early or too aggressively:\n",
    "\n",
    "* The model might get stuck in a suboptimal local minimum.\n",
    "* Training might become too slow.\n",
    "\n",
    "**Tip:** Watch the training and validation loss.\n",
    "If both flatten out together → fine.\n",
    "If training keeps improving but validation does not → use `ReduceLROnPlateau`.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Practical recipe**\n",
    "\n",
    "For most models (CNNs, UNets, ViTs):\n",
    "\n",
    "1. Start with a **moderate initial LR** (1e-3 or 1e-4).\n",
    "2. Use **CosineAnnealingLR** or **OneCycleLR** for smooth decay.\n",
    "3. Add **warm-up** if the model or dataset is large.\n",
    "4. If training stalls, switch to **ReduceLROnPlateau**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Inspect learning rate range (LR finder)**\n",
    "\n",
    "A powerful trick: use the **Learning Rate Finder** to automatically discover a good LR range.\n",
    "\n",
    "In fastai or manual implementation:\n",
    "\n",
    "* Gradually increase LR from 1e-7 → 1e-1 during a few mini-batches.\n",
    "* Plot loss vs LR.\n",
    "* Choose LR slightly before loss starts increasing.\n",
    "\n",
    "Example:\n",
    "If loss drops fastest around 1e-3 → that’s your sweet spot.\n",
    "\n",
    "---\n",
    "\n",
    "###  9. **Batch-based vs epoch-based schedulers**\n",
    "\n",
    "| Scheduler                     | Step Frequency            | Typical Use                     |\n",
    "| ----------------------------- | ------------------------- | ------------------------------- |\n",
    "| `StepLR`, `CosineAnnealingLR` | per **epoch**             | small to medium datasets        |\n",
    "| `OneCycleLR`, `CyclicLR`      | per **batch**             | large datasets, fine LR control |\n",
    "| `ReduceLROnPlateau`           | per **epoch (after val)** | validation-driven control       |\n",
    "\n",
    "---\n",
    "\n",
    "###  10. **Combine schedulers**\n",
    "\n",
    "You can chain schedulers for complex schedules, for example:\n",
    "\n",
    "* Warm-up for 5 epochs\n",
    "* Then cosine annealing\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        LinearLR(optimizer, start_factor=0.01, total_iters=5),\n",
    "        CosineAnnealingLR(optimizer, T_max=95)\n",
    "    ],\n",
    "    milestones=[5]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Rules of Thumb\n",
    "\n",
    "| Rule                                 | Explanation                                             |\n",
    "| ------------------------------------ | ------------------------------------------------------- |\n",
    "| **Always decay LR**                  | Don’t train with constant LR unless you monitor closely |\n",
    "| **Use warmup for Transformers/ViTs** | Prevents unstable gradients early on                    |\n",
    "| **Validate at every epoch**          | Needed if using `ReduceLROnPlateau`                     |\n",
    "| **Visualize LR curve**               | To debug scheduling problems                            |\n",
    "| **Match step frequency**             | `.step()` placement depends on scheduler type           |\n",
    "| **Avoid too many drops**             | Over-decay → underfitting                               |\n",
    "| **Try CosineAnnealingLR first**      | Works well in most cases                                |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9477d-4c73-4760-a8a6-edb8512960e3",
   "metadata": {},
   "source": [
    "## **Learning Rate Scheduling** and **Weight Decay**\n",
    "\n",
    "**learning rate scheduling** and **weight decay** are related in that both affect *how parameters are updated*, but they control **different mechanisms**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The optimization update equation\n",
    "\n",
    "In most optimizers (like SGD or Adam), each parameter update looks like this:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t , (\\nabla_\\theta \\mathcal{L}(\\theta_t) + \\lambda , \\theta_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ \\eta_t $ → learning rate (possibly scheduled over time)\n",
    "* $ \\lambda $ → weight decay ($L2$ regularization term)\n",
    "* $ \\nabla_\\theta \\mathcal{L} $ → gradient of the loss\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What each term does\n",
    "\n",
    "| Concept           | Symbol                 | Effect                                                       | Controlled by                 |\n",
    "| ----------------- | ---------------------- | ------------------------------------------------------------ | ----------------------------- |\n",
    "| **Learning rate** | $ \\eta_t $             | Scales the *step size* — how far parameters move each update | `optimizer.lr` + scheduler    |\n",
    "| **Weight decay**  | $ \\lambda , \\theta_t $ | Penalizes large weights (shrinks them towards zero)          | `optimizer(weight_decay=...)` |\n",
    "\n",
    "So:\n",
    "\n",
    "* **Learning rate:** controls *speed* of learning.\n",
    "* **Weight decay:** controls *magnitude* of parameters (regularization).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How they interact\n",
    "\n",
    "They **multiply each other** during the update step.\n",
    "A scheduler changes $\\eta_t $ over time, so the *effective strength* of weight decay also changes proportionally.\n",
    "\n",
    "For example, if you use a `StepLR` or `CosineAnnealingLR`:\n",
    "\n",
    "$$\n",
    "\\text{effective regularization} = \\eta_t , \\lambda\n",
    "$$\n",
    "\n",
    "When your LR decreases, the overall regularization effect becomes weaker — because both the gradient and weight decay terms are scaled by the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Practical intuition\n",
    "\n",
    "| Phase                    | Learning Rate | Effect on Weight Decay |\n",
    "| ------------------------ | ------------- | ---------------------- |\n",
    "| Early training (high LR) | Large steps   | Stronger shrinkage     |\n",
    "| Late training (low LR)   | Small steps   | Weaker shrinkage       |\n",
    "\n",
    "So scheduling the LR indirectly affects how strong weight decay feels through training — even if `weight_decay` itself stays constant.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Modern optimizers: decoupled weight decay (AdamW)\n",
    "\n",
    "Older Adam/SGD implementations *mix* weight decay with gradient scaling.\n",
    "**AdamW** (and similar optimizers) *decouple* them:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = (1 - \\eta_t \\lambda) , \\theta_t - \\eta_t , \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* Weight decay acts **independently** of gradient scale.\n",
    "* This gives more predictable regularization, especially with LR schedulers.\n",
    "\n",
    "That’s why **AdamW** is now standard for deep learning.\n",
    "\n",
    "**If you use a scheduler, always prefer AdamW (not Adam) when weight decay is important.**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Practical tuning tips\n",
    "\n",
    "| Tip                                  | Explanation                                                  |\n",
    "| ------------------------------------ | ------------------------------------------------------------ |\n",
    "| Keep `weight_decay` constant         | It’s usually fixed (e.g. 1e-4 or 1e-5) while LR changes      |\n",
    "| LR scheduling affects decay strength | Lower LR → smaller effective regularization                  |\n",
    "| For ViT / Transformers               | Common: `lr=3e-4`, `weight_decay=0.05`, `scheduler=cosine`   |\n",
    "| For CNNs                             | Common: `lr=1e-3`, `weight_decay=1e-4`, `scheduler=StepLR`   |\n",
    "| Never decay bias or norm weights     | Exclude `bias` and `BatchNorm` from weight decay (see below) |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Excluding certain parameters from weight decay\n",
    "\n",
    "Often you don’t want to regularize biases or normalization parameters:\n",
    "\n",
    "```python\n",
    "decay, no_decay = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name or 'norm' in name:\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay, 'weight_decay': 1e-4},\n",
    "    {'params': no_decay, 'weight_decay': 0.0}\n",
    "], lr=1e-3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "\n",
    "| Concept           | Controls            | Typical Value                   | Scheduler interaction     |\n",
    "| ----------------- | ------------------- | ------------------------------- | ------------------------- |\n",
    "| **Learning rate** | Step size           | 1e-4 – 1e-2                     | Changes dynamically       |\n",
    "| **Weight decay**  | Regularization (L2) | 1e-5 – 1e-3                     | Indirectly affected by LR |\n",
    "| **Optimizer**     | Update rule         | AdamW, SGD                      | Applies both terms        |\n",
    "| **Scheduler**     | LR evolution        | StepLR, CosineAnnealingLR, etc. | Changes only LR, not λ    |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7f5ac-6fc2-4767-9424-4fdfa6b1c51a",
   "metadata": {},
   "source": [
    "# 1. Most common LR schedulers (ranked)\n",
    "\n",
    "## 1. StepLR / MultiStepLR\n",
    "\n",
    "**Most widely used for classical CNNs (ResNet, VGG, EfficientNet).**\n",
    "\n",
    "### How it works\n",
    "\n",
    "Reduces learning rate by a factor ( \\gamma ) every fixed number of epochs.\n",
    "\n",
    "### Equation\n",
    "\n",
    "If the base learning rate is ( \\eta_0 ), after ( k = \\lfloor \\tfrac{t}{\\text{step_size}} \\rfloor ):\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\gamma^{k}\n",
    "$$\n",
    "\n",
    "MultiStepLR is the same but steps at a list of epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ExponentialLR\n",
    "\n",
    "### Equation\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\gamma^{,t}\n",
    "$$\n",
    "\n",
    "Decreases smoothly every epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cosine Annealing (CosineAnnealingLR)\n",
    "\n",
    "**Most common in Transformers, ViT, ConvNeXt, MAE, LLaMA-style training.**\n",
    "\n",
    "### Equation\n",
    "\n",
    "If total epochs are ( T ), and ( t ) is current epoch:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{\\min} + \\frac{1}{2} (\\eta_0 - \\eta_{\\min}) \\left(1 + \\cos\\frac{\\pi t}{T}\\right)\n",
    "$$\n",
    "\n",
    "Gives a smooth cosine curve. No abrupt jumps.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Cosine Annealing with Warm Restarts (SGDR)\n",
    "\n",
    "### Equation\n",
    "\n",
    "Resets periodically: for cycle ( i ) with length ( T_i ):\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{\\min} + \\frac{1}{2} (\\eta_0 - \\eta_{\\min}) \\left(1 + \\cos\\frac{\\pi (t - T_{\\text{start}})}{T_i}\\right)\n",
    "$$\n",
    "\n",
    "Common in large-scale training.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Linear Warmup + Something (most common in modern DL)\n",
    "\n",
    "Used in Transformer, ViT, GPT, diffusion models, CLIP training.\n",
    "\n",
    "### Warmup\n",
    "\n",
    "For warmup steps ( w ):\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\frac{t}{w}, \\quad t \\le w\n",
    "$$\n",
    "\n",
    "After warmup, you switch to a scheduler (cosine, polynomial, constant).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ReduceLROnPlateau\n",
    "\n",
    "**Most used in Keras and PyTorch training when validation loss is unstable.**\n",
    "\n",
    "### How it works\n",
    "\n",
    "Monitors a metric (val loss).\n",
    "Reduces LR by factor ( \\gamma ) if metric has not improved for ( p ) epochs.\n",
    "\n",
    "### Equation\n",
    "\n",
    "Not analytical; conceptually:\n",
    "\n",
    "$$\n",
    "\\eta_{t+1} =\n",
    "\\begin{cases}\n",
    "\\gamma,\\eta_t & \\text{if no improvement for } p \\text{ epochs} \\\n",
    "\\eta_t & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Polynomial decay (PolyLR)\n",
    "\n",
    "Very common in segmentation models (DeepLab, HRNet, etc).\n",
    "\n",
    "### Equation\n",
    "\n",
    "For power ( p ):\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0, \\left(1 - \\frac{t}{T}\\right)^p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. OneCycleLR\n",
    "\n",
    "Used in fastai, YOLO training.\n",
    "\n",
    "### Phases\n",
    "\n",
    "1. Linear increase\n",
    "2. Cosine/linear decrease\n",
    "3. Optional annealing at the end\n",
    "\n",
    "### Equation\n",
    "\n",
    "Piecewise, but core idea:\n",
    "\n",
    "$$\n",
    "\\eta_t \\uparrow \\text{then} \\downarrow\n",
    "$$\n",
    "\n",
    "Improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. CyclicLR\n",
    "\n",
    "Oscillates between minimum and maximum LR.\n",
    "\n",
    "### Equation\n",
    "\n",
    "Triangular wave or cosine wave between ( \\eta_{\\min} ) and ( \\eta_{\\max} ).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. LambdaLR\n",
    "\n",
    "User-defined LR: any function you define:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\lambda(t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 11. ConstantLR\n",
    "\n",
    "Keeps LR constant for a portion of training, then steps.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Inverse Square Root (Transformer LR)\n",
    "\n",
    "Used in original Transformer, T5, BART.\n",
    "\n",
    "### Equation\n",
    "\n",
    "Used after warmup:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\frac{1}{\\sqrt{t}}\n",
    "$$\n",
    "\n",
    "More precisely:\n",
    "\n",
    "$$\n",
    "\\eta_t = d^{-0.5} \\cdot \\min(t^{-0.5}, t \\cdot w^{-1.5})\n",
    "$$\n",
    "\n",
    "where ( d ) is the model dimension, ( w ) warmup steps.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Summary Table (ranked by real-world usage)\n",
    "\n",
    "| Rank | Scheduler          | Usage                    |\n",
    "| ---- | ------------------ | ------------------------ |\n",
    "| 1    | StepLR             | Classic vision models    |\n",
    "| 2    | Cosine Annealing   | Transformers, ViT        |\n",
    "| 3    | Warmup + Cosine    | GPT, ViT, diffusion      |\n",
    "| 4    | ReduceLROnPlateau  | Keras, unstable training |\n",
    "| 5    | ExponentialLR      | Classic ML, simple CNN   |\n",
    "| 6    | Polynomial         | Segmentation             |\n",
    "| 7    | OneCycleLR         | YOLO, fastai             |\n",
    "| 8    | CyclicLR           | General                  |\n",
    "| 9    | Linear Warmup only | Large models             |\n",
    "| 10   | LambdaLR           | Custom behaviors         |\n",
    "\n",
    "---\n",
    "\n",
    "# 3. How to plot any scheduler\n",
    "\n",
    "You can easily **plot LR vs epoch** or **LR vs training error** or **LR vs validation loss** by manually calling the scheduler each epoch.\n",
    "\n",
    "Example: **plot learning rate vs epoch** for any PyTorch scheduler:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb895f4d-cda7-443a-84d7-7ccf00b4a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behnam/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = torch.optim.Adam([torch.zeros(1)], lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6410188-30dc-4fdb-a07d-8fe2f280d9d2",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Plot LR vs validation loss (Plateau scheduler example)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(len(val_losses)))\n",
    "lrs = logged_learning_rates  # you log lr each epoch\n",
    "\n",
    "plt.plot(epochs, val_losses, label=\"val loss\")\n",
    "plt.plot(epochs, lrs, label=\"learning rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "You can also plot LR vs training loss in the same way.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Example: visualizing warmup + cosine\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eta0 = 1e-3\n",
    "eta_min = 1e-6\n",
    "T = 100\n",
    "warmup = 10\n",
    "\n",
    "lr_values = []\n",
    "for t in range(T):\n",
    "    if t < warmup:\n",
    "        lr = eta0 * t / warmup\n",
    "    else:\n",
    "        lr = eta_min + 0.5*(eta0 - eta_min)*(1 + np.cos(np.pi*(t-warmup)/(T-warmup)))\n",
    "    lr_values.append(lr)\n",
    "\n",
    "plt.plot(lr_values)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Which scheduler should you use?\n",
    "\n",
    "### For CNNs (classification)\n",
    "\n",
    "• StepLR\n",
    "• CosineAnnealingLR\n",
    "• OneCycleLR\n",
    "\n",
    "### For segmentation\n",
    "\n",
    "• Polynomial decay\n",
    "• Cosine\n",
    "• StepLR\n",
    "\n",
    "### For Transformer, ViT, GPT\n",
    "\n",
    "• Warmup + Cosine\n",
    "• Inverse Square Root\n",
    "• Linear warmup + constant for fine-tuning\n",
    "\n",
    "### For unstable datasets\n",
    "\n",
    "• ReduceLROnPlateau\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can generate **plots for all schedulers** or give you **a single notebook that visualizes all LR curves**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
