{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1c2f32-d34f-4d90-a8a8-9d08b60e0a57",
   "metadata": {},
   "source": [
    "## What is a Learning Rate Scheduler?\n",
    "\n",
    "The **learning rate (LR)** controls how big each update step is during optimization:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t , \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "where $ \\eta_t $ is the **learning rate at step $t$**.\n",
    "\n",
    "A **scheduler** automatically adjusts $ \\eta_t $ during training to:\n",
    "\n",
    "* Speed up convergence early on,\n",
    "* Avoid overshooting minima,\n",
    "* Fine-tune learning near convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup: Basic Training Loop (Before Scheduler)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy model, dataset, and loss\n",
    "model = nn.Linear(10, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "### Basic training loop without scheduler\n",
    "\n",
    "```python\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Add a Scheduler\n",
    "\n",
    "You create a scheduler **after** the optimizer.\n",
    "\n",
    "**Example — StepLR**\n",
    "\n",
    "This scheduler **decays the learning rate by a factor of `gamma` every `step_size` epochs**.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "```\n",
    "\n",
    "Here, every `5` epochs the $LR$ will be multiplied by `0.1`.\n",
    "\n",
    "---\n",
    "\n",
    "### Training loop (with StepLR)\n",
    "\n",
    "```python\n",
    "for epoch in range(20):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print current learning rate\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1}: LR = {lr:.5f}\")\n",
    "```\n",
    "\n",
    "**Order summary:**\n",
    "\n",
    "1. Forward\n",
    "2. Backward\n",
    "3. Optimizer step\n",
    "4. Scheduler step (usually once per epoch)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Popular Schedulers\n",
    "\n",
    "| Scheduler             | Description                                      | Common use                          |\n",
    "| --------------------- | ------------------------------------------------ | ----------------------------------- |\n",
    "| **StepLR**            | Decrease LR every fixed number of epochs         | Simple training with known LR drops |\n",
    "| **MultiStepLR**       | Drop LR at specified epochs                      | More flexible version of StepLR     |\n",
    "| **ExponentialLR**     | Multiply LR by `gamma` every epoch               | Smooth decay                        |\n",
    "| **CosineAnnealingLR** | Cosine-shaped LR curve                           | Vision Transformers, segmentation   |\n",
    "| **ReduceLROnPlateau** | Decrease LR when validation loss stops improving | Adaptive scheduling                 |\n",
    "| **OneCycleLR**        | Increase then decrease LR within one cycle       | Fast convergence (common for CNNs)  |\n",
    "| **LambdaLR**          | Define a custom lambda function for LR           | Full control                        |\n",
    "\n",
    "---\n",
    "\n",
    "**Example: `ReduceLROnPlateau`**\n",
    "\n",
    "This one is different — you call `.step(metric)` with the **validation loss** instead of every epoch.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_one_epoch(...)\n",
    "    val_loss = evaluate(...)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**`OneCycleLR`**\n",
    "\n",
    "Used for fast and stable convergence, especially in CNNs and Transformers.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.1, steps_per_epoch=len(dataloader), epochs=10\n",
    ")\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "Here, `.step()` is called **every batch**, not every epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### Checking the Learning Rate Schedule\n",
    "\n",
    "You can visualize the learning rate over time to verify it behaves as expected.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lrs = []\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.plot(range(1, 21), lrs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('LR schedule')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ Calling `scheduler.step()` **before** the first optimizer step — always step **after** optimizer update.\n",
    "✅ Exception: when using **OneCycleLR**, step every iteration after `optimizer.step()`.\n",
    "\n",
    "❌ Forgetting to pass validation loss for `ReduceLROnPlateau`.\n",
    "\n",
    "✅ Use `scheduler.get_last_lr()` to log current LR to TensorBoard/W&B.\n",
    "\n",
    "---\n",
    "\n",
    "###  Quick Comparison Table\n",
    "\n",
    "| Scheduler         | Step When | Input Needed    | LR Behavior              |\n",
    "| ----------------- | --------- | --------------- | ------------------------ |\n",
    "| StepLR            | epoch     | none            | abrupt drops             |\n",
    "| MultiStepLR       | epoch     | none            | controlled drops         |\n",
    "| ExponentialLR     | epoch     | none            | smooth exponential decay |\n",
    "| ReduceLROnPlateau | epoch     | validation loss | adaptive decay           |\n",
    "| OneCycleLR        | batch     | none            | cyclic pattern           |\n",
    "| CosineAnnealingLR | epoch     | none            | smooth cosine decay      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d57ca1-709a-44a4-81b5-297120463b46",
   "metadata": {},
   "source": [
    "## **Best Practices**\n",
    "\n",
    "### 1. **Understand the goal of your scheduler**\n",
    "\n",
    "Schedulers aren’t magic — they’re a way to **control the learning dynamics**:\n",
    "\n",
    "| Goal                                                        | Scheduler Type                                       | Why                                |\n",
    "| ----------------------------------------------------------- | ---------------------------------------------------- | ---------------------------------- |\n",
    "| Fast convergence at start, small steps near minima          | **StepLR**, **ExponentialLR**, **CosineAnnealingLR** | Reduces overshooting               |\n",
    "| Avoid getting stuck in local minima                         | **CosineAnnealingLR**, **OneCycleLR**                | Uses cyclical or smooth schedules  |\n",
    "| Dynamically adapt to validation loss                        | **ReduceLROnPlateau**                                | Reduces LR when learning stagnates |\n",
    "| Highly tuned training for large models (Transformers, ViTs) | **CosineAnnealingWarmRestarts** or **OneCycleLR**    | Smooth control of LR dynamics      |\n",
    "\n",
    "---\n",
    "\n",
    "###  2. **Call order matters**\n",
    "\n",
    "Always follow this inside your loop:\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(model(inputs), targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "scheduler.step()  # after optimizer.step()\n",
    "```\n",
    "\n",
    "Exceptions:\n",
    "\n",
    "* **OneCycleLR** → step **every batch** (not every epoch)\n",
    "* **ReduceLROnPlateau** → step **after validation**, with `scheduler.step(val_loss)`\n",
    "\n",
    "---\n",
    "\n",
    "###  3. **Common default heuristics**\n",
    "\n",
    "| Scenario                      | Recommended Scheduler                         | Base LR                             |\n",
    "| ----------------------------- | --------------------------------------------- | ----------------------------------- |\n",
    "| Simple CNN, small dataset     | `StepLR(step_size=10, gamma=0.1)`             | 1e-2 or 1e-3                        |\n",
    "| ResNet / UNet training        | `CosineAnnealingLR(T_max=epochs)`             | 1e-3                                |\n",
    "| Transformer / ViT             | `CosineAnnealingWarmRestarts` or `OneCycleLR` | use warmup start: e.g., 1e-4 → 1e-3 |\n",
    "| Fine-tuning pretrained model  | `ReduceLROnPlateau`                           | start smaller (1e-4)                |\n",
    "| Very large batch size (>1024) | `OneCycleLR`                                  | can start higher, e.g., 1e-2        |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Warm-up phase**\n",
    "\n",
    "Large models (especially Transformers, Vision Transformers, or very deep CNNs) often fail to converge if the learning rate starts too high.\n",
    "\n",
    "Use a **warm-up** scheduler — start with a small LR and gradually increase:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{\\text{max}} \\times \\frac{t}{t_\\text{warmup}}\n",
    "$$\n",
    "\n",
    "Implementation:\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def warmup_linear_lr(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0  # keep LR constant after warmup\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup_linear_lr)\n",
    "```\n",
    "\n",
    "Or use `OneCycleLR` (it includes warm-up automatically).\n",
    "\n",
    "---\n",
    "\n",
    "###  5. **Log and visualize your learning rate**\n",
    "\n",
    "Track the LR every epoch or batch — for example, with W&B or TensorBoard:\n",
    "\n",
    "```python\n",
    "lr = scheduler.get_last_lr()[0]\n",
    "wandb.log({'learning_rate': lr})\n",
    "```\n",
    "\n",
    "Or plot it to ensure your schedule behaves as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Do not over-schedule**\n",
    "\n",
    "If you reduce the LR too early or too aggressively:\n",
    "\n",
    "* The model might get stuck in a suboptimal local minimum.\n",
    "* Training might become too slow.\n",
    "\n",
    "**Tip:** Watch the training and validation loss.\n",
    "If both flatten out together → fine.\n",
    "If training keeps improving but validation does not → use `ReduceLROnPlateau`.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Practical recipe**\n",
    "\n",
    "For most models (CNNs, UNets, ViTs):\n",
    "\n",
    "1. Start with a **moderate initial LR** (1e-3 or 1e-4).\n",
    "2. Use **CosineAnnealingLR** or **OneCycleLR** for smooth decay.\n",
    "3. Add **warm-up** if the model or dataset is large.\n",
    "4. If training stalls, switch to **ReduceLROnPlateau**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Inspect learning rate range (LR finder)**\n",
    "\n",
    "A powerful trick: use the **Learning Rate Finder** to automatically discover a good LR range.\n",
    "\n",
    "In fastai or manual implementation:\n",
    "\n",
    "* Gradually increase LR from 1e-7 → 1e-1 during a few mini-batches.\n",
    "* Plot loss vs LR.\n",
    "* Choose LR slightly before loss starts increasing.\n",
    "\n",
    "Example:\n",
    "If loss drops fastest around 1e-3 → that’s your sweet spot.\n",
    "\n",
    "---\n",
    "\n",
    "###  9. **Batch-based vs epoch-based schedulers**\n",
    "\n",
    "| Scheduler                     | Step Frequency            | Typical Use                     |\n",
    "| ----------------------------- | ------------------------- | ------------------------------- |\n",
    "| `StepLR`, `CosineAnnealingLR` | per **epoch**             | small to medium datasets        |\n",
    "| `OneCycleLR`, `CyclicLR`      | per **batch**             | large datasets, fine LR control |\n",
    "| `ReduceLROnPlateau`           | per **epoch (after val)** | validation-driven control       |\n",
    "\n",
    "---\n",
    "\n",
    "###  10. **Combine schedulers**\n",
    "\n",
    "You can chain schedulers for complex schedules, for example:\n",
    "\n",
    "* Warm-up for 5 epochs\n",
    "* Then cosine annealing\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        LinearLR(optimizer, start_factor=0.01, total_iters=5),\n",
    "        CosineAnnealingLR(optimizer, T_max=95)\n",
    "    ],\n",
    "    milestones=[5]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Rules of Thumb\n",
    "\n",
    "| Rule                                 | Explanation                                             |\n",
    "| ------------------------------------ | ------------------------------------------------------- |\n",
    "| **Always decay LR**                  | Don’t train with constant LR unless you monitor closely |\n",
    "| **Use warmup for Transformers/ViTs** | Prevents unstable gradients early on                    |\n",
    "| **Validate at every epoch**          | Needed if using `ReduceLROnPlateau`                     |\n",
    "| **Visualize LR curve**               | To debug scheduling problems                            |\n",
    "| **Match step frequency**             | `.step()` placement depends on scheduler type           |\n",
    "| **Avoid too many drops**             | Over-decay → underfitting                               |\n",
    "| **Try CosineAnnealingLR first**      | Works well in most cases                                |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9477d-4c73-4760-a8a6-edb8512960e3",
   "metadata": {},
   "source": [
    "## **Learning Rate Scheduling** and **Weight Decay**\n",
    "\n",
    "**learning rate scheduling** and **weight decay** are related in that both affect *how parameters are updated*, but they control **different mechanisms**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The optimization update equation\n",
    "\n",
    "In most optimizers (like SGD or Adam), each parameter update looks like this:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t , (\\nabla_\\theta \\mathcal{L}(\\theta_t) + \\lambda , \\theta_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ \\eta_t $ → learning rate (possibly scheduled over time)\n",
    "* $ \\lambda $ → weight decay ($L2$ regularization term)\n",
    "* $ \\nabla_\\theta \\mathcal{L} $ → gradient of the loss\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What each term does\n",
    "\n",
    "| Concept           | Symbol                 | Effect                                                       | Controlled by                 |\n",
    "| ----------------- | ---------------------- | ------------------------------------------------------------ | ----------------------------- |\n",
    "| **Learning rate** | $ \\eta_t $             | Scales the *step size* — how far parameters move each update | `optimizer.lr` + scheduler    |\n",
    "| **Weight decay**  | $ \\lambda , \\theta_t $ | Penalizes large weights (shrinks them towards zero)          | `optimizer(weight_decay=...)` |\n",
    "\n",
    "So:\n",
    "\n",
    "* **Learning rate:** controls *speed* of learning.\n",
    "* **Weight decay:** controls *magnitude* of parameters (regularization).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How they interact\n",
    "\n",
    "They **multiply each other** during the update step.\n",
    "A scheduler changes $\\eta_t $ over time, so the *effective strength* of weight decay also changes proportionally.\n",
    "\n",
    "For example, if you use a `StepLR` or `CosineAnnealingLR`:\n",
    "\n",
    "$$\n",
    "\\text{effective regularization} = \\eta_t , \\lambda\n",
    "$$\n",
    "\n",
    "When your LR decreases, the overall regularization effect becomes weaker — because both the gradient and weight decay terms are scaled by the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Practical intuition\n",
    "\n",
    "| Phase                    | Learning Rate | Effect on Weight Decay |\n",
    "| ------------------------ | ------------- | ---------------------- |\n",
    "| Early training (high LR) | Large steps   | Stronger shrinkage     |\n",
    "| Late training (low LR)   | Small steps   | Weaker shrinkage       |\n",
    "\n",
    "So scheduling the LR indirectly affects how strong weight decay feels through training — even if `weight_decay` itself stays constant.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Modern optimizers: decoupled weight decay (AdamW)\n",
    "\n",
    "Older Adam/SGD implementations *mix* weight decay with gradient scaling.\n",
    "**AdamW** (and similar optimizers) *decouple* them:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = (1 - \\eta_t \\lambda) , \\theta_t - \\eta_t , \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* Weight decay acts **independently** of gradient scale.\n",
    "* This gives more predictable regularization, especially with LR schedulers.\n",
    "\n",
    "That’s why **AdamW** is now standard for deep learning.\n",
    "\n",
    "**If you use a scheduler, always prefer AdamW (not Adam) when weight decay is important.**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Practical tuning tips\n",
    "\n",
    "| Tip                                  | Explanation                                                  |\n",
    "| ------------------------------------ | ------------------------------------------------------------ |\n",
    "| Keep `weight_decay` constant         | It’s usually fixed (e.g. 1e-4 or 1e-5) while LR changes      |\n",
    "| LR scheduling affects decay strength | Lower LR → smaller effective regularization                  |\n",
    "| For ViT / Transformers               | Common: `lr=3e-4`, `weight_decay=0.05`, `scheduler=cosine`   |\n",
    "| For CNNs                             | Common: `lr=1e-3`, `weight_decay=1e-4`, `scheduler=StepLR`   |\n",
    "| Never decay bias or norm weights     | Exclude `bias` and `BatchNorm` from weight decay (see below) |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Excluding certain parameters from weight decay\n",
    "\n",
    "Often you don’t want to regularize biases or normalization parameters:\n",
    "\n",
    "```python\n",
    "decay, no_decay = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name or 'norm' in name:\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay, 'weight_decay': 1e-4},\n",
    "    {'params': no_decay, 'weight_decay': 0.0}\n",
    "], lr=1e-3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "\n",
    "| Concept           | Controls            | Typical Value                   | Scheduler interaction     |\n",
    "| ----------------- | ------------------- | ------------------------------- | ------------------------- |\n",
    "| **Learning rate** | Step size           | 1e-4 – 1e-2                     | Changes dynamically       |\n",
    "| **Weight decay**  | Regularization (L2) | 1e-5 – 1e-3                     | Indirectly affected by LR |\n",
    "| **Optimizer**     | Update rule         | AdamW, SGD                      | Applies both terms        |\n",
    "| **Scheduler**     | LR evolution        | StepLR, CosineAnnealingLR, etc. | Changes only LR, not λ    |\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "* **Learning rate scheduler:** adjusts *how fast* you learn.\n",
    "* **Weight decay:** keeps weights small, improving generalization.\n",
    "* **They interact**, since smaller LR means weaker decay effect.\n",
    "* Use **AdamW** with schedulers — it decouples the two for stability.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
