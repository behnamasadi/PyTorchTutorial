{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1c2f32-d34f-4d90-a8a8-9d08b60e0a57",
   "metadata": {},
   "source": [
    "## **1. What is a Learning Rate Scheduler?**\n",
    "\n",
    "The **learning rate (LR)** controls how big each update step is during optimization:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t , \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "where $ \\eta_t $ is the **learning rate at step $t$**.\n",
    "\n",
    "A **scheduler** automatically adjusts $ \\eta_t $ during training to:\n",
    "\n",
    "* Speed up convergence early on,\n",
    "* Avoid overshooting minima,\n",
    "* Fine-tune learning near convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Setup: Basic Training Loop (Before Scheduler)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f217185c-11fa-4a5a-97ba-4fdfb617f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return torch.sin(x)+x**2\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.arange(start=0, end=10, step=0.5).reshape(-1, 1)\n",
    "\n",
    "\n",
    "labels = func(x)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        # self.conv1=torch.nn.Conv2d()\n",
    "        self.fc1 = nn.Linear(in_features=1, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=10)\n",
    "        self.fc3 = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class NumberDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "input_dataset = NumberDataset(x, labels)\n",
    "\n",
    "batch_size = 2\n",
    "data_loader = torch.utils.data.DataLoader(dataset=input_dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9b01c-d38f-4e26-b81b-ba91730b9d97",
   "metadata": {},
   "source": [
    "\n",
    "#### Basic training loop without scheduler\n",
    "\n",
    "```python\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Add a Scheduler**\n",
    "\n",
    "You create a scheduler **after** the optimizer.\n",
    "\n",
    "#### **3.1 StepLR**\n",
    "\n",
    "This scheduler **decays the learning rate by a factor of `gamma` every `step_size` epochs**.\n",
    "\n",
    "```python\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "```\n",
    "\n",
    "Here, every `5` epochs the $LR$ will be multiplied by `0.1`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5bfc7d1-9d78-404f-845a-c0b5e75a93bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: LR = 0.00100\n",
      "Epoch 2: LR = 0.00100\n",
      "Epoch 3: LR = 0.00100\n",
      "Epoch 4: LR = 0.00100\n",
      "Epoch 5: LR = 0.00010\n",
      "Epoch 6: LR = 0.00010\n",
      "Epoch 7: LR = 0.00010\n",
      "Epoch 8: LR = 0.00010\n",
      "Epoch 9: LR = 0.00010\n",
      "Epoch 10: LR = 0.00001\n",
      "Epoch 11: LR = 0.00001\n",
      "Epoch 12: LR = 0.00001\n",
      "Epoch 13: LR = 0.00001\n",
      "Epoch 14: LR = 0.00001\n",
      "Epoch 15: LR = 0.00000\n",
      "Epoch 16: LR = 0.00000\n",
      "Epoch 17: LR = 0.00000\n",
      "Epoch 18: LR = 0.00000\n",
      "Epoch 19: LR = 0.00000\n",
      "Epoch 20: LR = 0.00000\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "opt = torch.optim.AdamW(lr=lr, params=model.parameters())\n",
    "lr_scheduler_StepLR = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=5, gamma=0.1)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x, label in data_loader:\n",
    "        opt.zero_grad()\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        label = label.to(device, non_blocking=True)\n",
    "        y = model(x)\n",
    "        loss = criterion(y, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    # Step the scheduler after each epoch        \n",
    "    lr_scheduler_StepLR.step()\n",
    "    lr = lr_scheduler_StepLR.get_last_lr()[0]\n",
    "    # Print current learning rate\n",
    "    print(f\"Epoch {epoch+1}: LR = {lr:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d57ca1-709a-44a4-81b5-297120463b46",
   "metadata": {},
   "source": [
    "Here is **exactly why StepLR gives you this LR schedule** and why it quickly becomes **0.0**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2. What StepLR does**\n",
    "\n",
    "The definition:\n",
    "\n",
    "```python\n",
    "torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=opt, \n",
    "    step_size=5, \n",
    "    gamma=0.1\n",
    ")\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "* Every **step_size** epochs\n",
    "* Multiply the learning rate by **gamma**\n",
    "\n",
    "Formally:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{lr}(t) = \\text{lr}_0 \\cdot \\gamma^{\\left\\lfloor \\frac{t}{\\text{step\\_size}} \\right\\rfloor}\n",
    "$$\n",
    "\n",
    "\n",
    "Where t = epoch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.3. Your schedule numerically**\n",
    "\n",
    "You start with:\n",
    "\n",
    "* initial LR = 0.001\n",
    "* step_size = 5\n",
    "* gamma = 0.1\n",
    "\n",
    "Thus:\n",
    "\n",
    "**First decay: at epoch 5**\n",
    "\n",
    "$$\n",
    "0.001 \\times 0.1 = 0.0001\n",
    "$$\n",
    "\n",
    "**Second decay: at epoch 10**\n",
    "\n",
    "$$\n",
    "0.0001 \\times 0.1 = 0.00001\n",
    "$$\n",
    "\n",
    "**Third decay: at epoch 15**\n",
    "\n",
    "$$\n",
    "0.00001 \\times 0.1 = 0.000001\n",
    "$$\n",
    "\n",
    "But PyTorch rounds this into **scientific precision** and depending on how you print it you get:\n",
    "\n",
    "* 0.000001 → printed as 0.00000 (rounded)\n",
    "* Then next decay:\n",
    "  $$\n",
    "  0.000001 \\times 0.1 = 0.0000001\n",
    "  $$\n",
    "* Then:\n",
    "  $$\n",
    "  0.0000001 \\times 0.1 = 0.00000001\n",
    "  $$\n",
    "\n",
    "Python formatting:\n",
    "\n",
    "```python\n",
    "print(f\"{lr:.5f}\")\n",
    "```\n",
    "\n",
    "will show:\n",
    "\n",
    "```\n",
    "0.00000\n",
    "0.00000\n",
    "0.00000\n",
    "...\n",
    "```\n",
    "\n",
    "But the LR is not literally zero; it is just **very small and rounded**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.4 The key reason your LR “becomes zero”**\n",
    "\n",
    "Because:\n",
    "\n",
    "**StepLR decays too aggressively.**\n",
    "\n",
    "With gamma=0.1 every 5 epochs, the LR becomes:\n",
    "\n",
    "| Epoch | LR   |\n",
    "| ----- | ---- |\n",
    "| 0–4   | 1e-3 |\n",
    "| 5–9   | 1e-4 |\n",
    "| 10–14 | 1e-5 |\n",
    "| 15–19 | 1e-6 |\n",
    "| 20–24 | 1e-7 |\n",
    "| 25–29 | 1e-8 |\n",
    "| 30–34 | 1e-9 |\n",
    "| ...   | ...  |\n",
    "\n",
    "After ~25–30 epochs LR is:\n",
    "\n",
    "$$\n",
    "10^{-8}, 10^{-9}, 10^{-10}, \\dots\n",
    "$$\n",
    "\n",
    "In **formatted printing**, any value < 0.000005 will print as:\n",
    "\n",
    "```\n",
    "0.00000\n",
    "```\n",
    "\n",
    "So the LR is not mathematically zero, but it is effectively too small to be useful.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.5. Why StepLR is rarely used today**\n",
    "\n",
    "StepLR is considered **too abrupt**, especially for deep networks.\n",
    "\n",
    "It behaves like:\n",
    "\n",
    "```\n",
    "LR suddenly drops → sudden jump in loss → unstable training\n",
    "```\n",
    "\n",
    "Modern schedules avoid hard jumps:\n",
    "\n",
    "* CosineAnnealingLR\n",
    "* CosineAnnealingWarmRestarts\n",
    "* OneCycleLR\n",
    "* LambdaLR (custom smooth schedule)\n",
    "* ReduceLROnPlateau (validation-based)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c594e-a24f-4320-83ca-bb5d292b88ca",
   "metadata": {},
   "source": [
    "## **4. ConstantLR Schedules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311bc56-b01e-41f1-9a85-d1e93eddaddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor: 0.3333333333333333\n",
      "total_iters: 5\n",
      "last_epoch: 0\n",
      "Epoch 1: LR = 0.00033\n",
      "Epoch 2: LR = 0.00033\n",
      "Epoch 3: LR = 0.00033\n",
      "Epoch 4: LR = 0.00033\n",
      "Epoch 5: LR = 0.00100\n",
      "Epoch 6: LR = 0.00100\n",
      "Epoch 7: LR = 0.00100\n",
      "Epoch 8: LR = 0.00100\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "opt = torch.optim.AdamW(lr=lr, params=model.parameters())\n",
    "lr_scheduler_ConstantLR = torch.optim.lr_scheduler.ConstantLR(optimizer=opt)\n",
    "\n",
    "print(\"factor:\", lr_scheduler_ConstantLR.factor)\n",
    "print(\"total_iters:\", lr_scheduler_ConstantLR.total_iters)\n",
    "print(\"last_epoch:\", lr_scheduler_ConstantLR.last_epoch)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x, label in data_loader:\n",
    "        opt.zero_grad()\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        label = label.to(device, non_blocking=True)\n",
    "        y = model(x)\n",
    "        loss = criterion(y, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    lr_scheduler_ConstantLR.step()\n",
    "    lr = lr_scheduler_ConstantLR.get_last_lr()[0]\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: LR = {lr:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca1985-038e-4d92-9938-dd78a7a76598",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4.1. Why your LR starts at 0.00033**\n",
    "\n",
    "You used:\n",
    "\n",
    "```python\n",
    "lr_scheduler_ConstantLR = torch.optim.lr_scheduler.ConstantLR(optimizer=opt)\n",
    "```\n",
    "\n",
    "Default values are:\n",
    "\n",
    "```python\n",
    "factor = 1/3       # = 0.333333...\n",
    "total_iters = 5\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "**During the first 5 scheduler.step() calls:**\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{lr}_t = \\text{base\\_lr} \\cdot \\text{factor} = 0.001 \\times 0.33333 = 0.00033333\n",
    "$$\n",
    "\n",
    "After **5 iterations**, the LR snaps back to:\n",
    "\n",
    "$$\n",
    "\\text{lr}_t = \\text{base\\_lr} = 0.001\n",
    "$$\n",
    "\n",
    "This matches your output:\n",
    "\n",
    "* Epoch 1–4: approx **0.00033**\n",
    "* Epoch 5: **0.001** again\n",
    "* Rest of training: **0.001**\n",
    "\n",
    "So the scheduler is doing exactly what it is designed for.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2. Why ConstantLR is *not* actually constant**\n",
    "\n",
    "Its name is misleading.\n",
    "\n",
    "It means:\n",
    "\n",
    "**Hold the LR at a *constant fraction* of initial LR for a few steps**,\n",
    "then automatically restore the original LR.\n",
    "\n",
    "It is usually used for **warmup**:\n",
    "\n",
    "* Keep LR low for first *k* steps\n",
    "* Then return to normal LR and continue with a main schedule\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.3. Your code applies ConstantLR per **epoch**, not per batch**\n",
    "\n",
    "You do:\n",
    "\n",
    "```python\n",
    "lr_scheduler_ConstantLR.step()\n",
    "```\n",
    "\n",
    "**once per epoch**.\n",
    "\n",
    "Thus:\n",
    "\n",
    "* It warms up for 5 epochs\n",
    "* Then normal LR for the remaining 95 epochs\n",
    "\n",
    "If you wanted warmup per **batch**, you should call `.step()` inside the batch loop.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a1c4b-b577-4ee6-8ed9-a914248dcae5",
   "metadata": {},
   "source": [
    "## **5. ReduceLROnPlateau Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34443e8a-6e90-4f9a-a783-3d6369bf644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "data_loader = [(torch.randn(4, 10), torch.randn(4, 2))]\n",
    "\n",
    "\n",
    "# Dummy model\n",
    "model = nn.Linear(10, 2).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-3\n",
    "opt = torch.optim.AdamW(lr=lr, params=model.parameters())\n",
    "\n",
    "# ReduceLROnPlateau scheduler\n",
    "lr_scheduler_ReduceLROnPlateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=opt,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=5,     # reduce LR after 5 bad epochs\n",
    ")\n",
    "\n",
    "print(\"eps: \", lr_scheduler_ReduceLROnPlateau.eps)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Manually craft validation loss that plateaus\n",
    "# ---------------------------------------------\n",
    "# First improves, then stops improving after epoch 12\n",
    "val_losses = [\n",
    "    1.0, 0.9, 0.8, 0.7, 0.6,\n",
    "    0.55, 0.52, 0.50, 0.49, 0.485,\n",
    "    0.480, 0.479,       # still improving\n",
    "    0.479, 0.479, 0.479, 0.480, 0.481,   # plateau begins\n",
    "    0.482, 0.481, 0.483, 0.484, 0.485,\n",
    "    0.486, 0.487, 0.488, 0.487,\n",
    "    0.489, 0.490, 0.491, 0.492\n",
    "]\n",
    "# length = 30\n",
    "\n",
    "\n",
    "# Dummy dataloader (not relevant for LR scheduler demo)\n",
    "data_loader = [(torch.randn(4, 10), torch.randn(4, 2))]\n",
    "\n",
    "\n",
    "best = float(\"inf\")\n",
    "\n",
    "eps = lr_scheduler_ReduceLROnPlateau.eps\n",
    "\n",
    "print(\"\\nDEBUG INFO\\n----------\")\n",
    "\n",
    "for epoch in range(len(val_losses)):\n",
    "\n",
    "    # fake training step\n",
    "    model.train()\n",
    "    for x, label in data_loader:\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        label = label.to(device, non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # --------------------------------\n",
    "    # Validation loss for this epoch\n",
    "    # --------------------------------\n",
    "    current = val_losses[epoch]\n",
    "\n",
    "    # --------------------------------\n",
    "    # Condition for improvement\n",
    "    # --------------------------------\n",
    "    improved = current < best - eps\n",
    "\n",
    "    # --------------------------------\n",
    "    # Debug printing\n",
    "    # --------------------------------\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d}: \"\n",
    "        f\"current={current:.6f}, best={best:.6f}, \"\n",
    "        f\"\\ncurrent < best - eps=\\n{current:.6f} < {best:.6f}-{eps}= {best-eps}\\n\"\n",
    "        f\"condition(current < best - eps) = {improved}\"\n",
    "    )\n",
    "\n",
    "    # Update best manually\n",
    "    if improved:\n",
    "        best = current\n",
    "    else:\n",
    "        pass  # counted as bad epoch (scheduler tracks internally)\n",
    "\n",
    "    # --------------------------------\n",
    "    # Call scheduler\n",
    "    # --------------------------------\n",
    "    lr_scheduler_ReduceLROnPlateau.step(current)\n",
    "\n",
    "    lr = opt.param_groups[0][\"lr\"]\n",
    "\n",
    "    print(f\"          LR = {lr:.6f}\\n\")\n",
    "    print(f\"Epoch {epoch+1}: LR = {lr:.5f}\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02e481-66cc-4d6e-b3e4-74b11e5308a7",
   "metadata": {},
   "source": [
    "#### **5.1. How ReduceLROnPlateau**\n",
    "\n",
    "The scheduler monitors a metric (usually validation loss) and **reduces the learning rate when the metric has stopped improving**.\n",
    "\n",
    "You call it like:\n",
    "\n",
    "```python\n",
    "scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2. What triggers the LR reduction?**\n",
    "\n",
    "The scheduler tracks the **best value seen so far**.\n",
    "\n",
    "For `mode=\"min\"`:\n",
    "\n",
    "* Improvement means:\n",
    "  $$\\text{current} < \\text{best} - \\epsilon$$\n",
    "* If this happens → **reset patience counter**.\n",
    "\n",
    "For `mode=\"max\"`:\n",
    "\n",
    "* Improvement means:\n",
    "  $$\\text{current} > \\text{best} + \\epsilon$$\n",
    "\n",
    "Here\n",
    "\n",
    "* **patience** = number of \"bad epochs\" allowed before reducing LR\n",
    "* **epsilon** = `eps` parameter (default `eps=1e-8`)\n",
    "  A very tiny threshold. Not a true tolerance — just a safety margin.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.3. What counts as “no improvement”?**\n",
    "\n",
    "An epoch without improvement is a **bad epoch**.\n",
    "Bad epoch happens when:\n",
    "\n",
    "$$\\text{current loss} \\ge \\text{best loss} - \\text{eps}$$\n",
    "\n",
    "So even a very small improvement still counts as improvement (unless < eps).\n",
    "\n",
    "Example:\n",
    "\n",
    "* best loss: 0.250000\n",
    "* current loss: 0.2499998\n",
    "* eps: 1e-8\n",
    "\n",
    "This **is considered an improvement**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.4. After *patience* bad epochs → LR decreases**\n",
    "\n",
    "If you set:\n",
    "\n",
    "```python\n",
    "patience=5\n",
    "factor=0.1\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* If 10 consecutive epochs show no improvement →\n",
    "  new learning rate becomes:\n",
    "\n",
    "$$\\text{lr}*{new} = \\text{lr}*{old} \\times \\text{factor}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "Initial LR = 1e-3\n",
    "factor = 0.1\n",
    "\n",
    "After plateau:\n",
    "\n",
    "New LR =\n",
    "$$10^{-3} \\times 0.1 = 10^{-4}$$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8908e3e-e067-42d4-88e8-158b3c6c9fdf",
   "metadata": {},
   "source": [
    "## **6. Linear Warmup**\n",
    "\n",
    "\n",
    "#### 1. Warmup\n",
    "\n",
    "Warmup is a **temporary increase** of the learning rate from a very small value up to the target learning rate.\n",
    "\n",
    "Why?\n",
    "Early in training, gradients are unstable. If LR is too large, updates can explode or overshoot.\n",
    "Warmup prevents this by starting small, then ramping up to the desired value.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.1. General Warmup Concept\n",
    "\n",
    "During warmup steps $ t \\in [0, T_{\\text{warmup}}] $:\n",
    "\n",
    "The LR increases from:\n",
    "\n",
    "* starting LR: $ \\eta_{\\text{start}} $\n",
    "* target LR:   $ \\eta_{\\text{max}} $\n",
    "\n",
    "Warmup is usually **linear**, but can be exponential or cosine.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2. Linear Warmup Equation\n",
    "\n",
    "For step $ t $ during warmup:\n",
    "\n",
    "$$ \\eta(t) = \\eta_{\\text{start}} +\n",
    " \\frac{t}{T_{\\text{warmup}}} \\left(\\eta_{\\text{max}} - \\eta_{\\text{start}}\\right)  $$\n",
    "\n",
    "At end of warmup:\n",
    "\n",
    "* $ t = T_{\\text{warmup}} $\n",
    "* $ \\eta(T_{\\text{warmup}}) = \\eta_{\\text{max}} $\n",
    "\n",
    "After this point, a different scheduler takes over (cosine, exponential, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.3. Why Warmup Works\n",
    "\n",
    "During the first batches:\n",
    "\n",
    "* gradients are noisy\n",
    "* batch statistics are unstable\n",
    "* weights are unscaled\n",
    "* Adam/AdamW bias corrections are still adapting\n",
    "\n",
    "Warmup prevents catastrophic updates.\n",
    "\n",
    "Transformers and ViTs require warmup almost always.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0650c-7e9d-46e3-a9a8-fd874247a513",
   "metadata": {},
   "source": [
    "## **7. Cosine Annealing**\n",
    "\n",
    "\n",
    "#### 7.1. Cosine Annealing (without warmup)\n",
    "\n",
    "Cosine annealing reduces LR following half a cosine wave.\n",
    "\n",
    "Idea: **large LR at beginning, very slow LR at the end**, smooth + no abrupt changes.\n",
    "\n",
    "Time variable: $ t = 0, 1, 2, \\ldots, T_{\\text{max}}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.2.Equation\n",
    "\n",
    "$$\\eta(t) = \\eta_{\\text{min}}+ \\frac{\\eta_{\\text{max}} - \\eta_{\\text{min}}}{2}\n",
    "  \\left(1 + \\cos\\left(\\pi \\frac{t}{T_{\\text{max}}}\\right)\\right)  $$\n",
    "\n",
    "This is one half-period of cosine.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.3. Why decreasing?\n",
    "\n",
    "Cosine goes from:\n",
    "\n",
    "* $ \\cos(0) = 1 $\n",
    "* $ \\cos(\\pi) = -1 $\n",
    "\n",
    "So the LR decreases smoothly:\n",
    "\n",
    "$$\n",
    "\\eta(0) = \\eta_{\\text{max}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta(T_{\\text{max}}) = \\eta_{\\text{min}}\n",
    "$$\n",
    "\n",
    "No oscillation because only **half** cosine is used (not periodic).\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.4. Intuition\n",
    "\n",
    "Cosine annealing decreases fast at start, then extremely slow near the end.\n",
    "This helps the model settle gently into a good minimum.\n",
    "\n",
    "Used in transformers, ViT, GPT, diffusion, Stable Diffusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a38419-ae87-4fd3-94e6-174f88ebf104",
   "metadata": {},
   "source": [
    "## **8. Exponential Decay**\n",
    "\n",
    "LR decays by a constant ratio every epoch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.1. Equation**\n",
    "\n",
    "Let decay rate = $ \\gamma $ (e.g., $ \\gamma = 0.95 $).\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\eta(t) = \\eta_{0} \\cdot \\gamma^{t}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "* $ \\gamma = 0.95 $\n",
    "* after 20 epochs:\n",
    "\n",
    "$$\n",
    "\\eta(20) = \\eta_0 \\cdot 0.95^{20}\n",
    "$$\n",
    "\n",
    "Exponential decay drops quickly early, then flattens.\n",
    "\n",
    "Used in older CNN models, RNNs, classical ML.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Intuition**\n",
    "\n",
    "Simple, predictable, but not smooth near the end (keeps decreasing forever).\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Linear Warmup + Cosine Decay**\n",
    "\n",
    "Most common combined schedule:\n",
    "\n",
    "1. **Warmup phase**: LR increases linearly\n",
    "2. **Cosine schedule**: LR decreases smoothly\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.1. Equation (piecewise)**\n",
    "\n",
    "Let warmup last $ T_{\\text{warmup}} $ steps.\n",
    "\n",
    "Warmup:\n",
    "\n",
    "$$\\eta(t) = \\eta_{\\text{start}} + \\frac{t}{T_{\\text{warmup}}}(\\eta_{\\text{max}} - \\eta_{\\text{start}}),\n",
    "  \\quad t < T_{\\text{warmup}}\n",
    "  $$\n",
    "\n",
    "Cosine (after warmup):\n",
    "\n",
    "Let $ s = t - T_{\\text{warmup}} $\n",
    "\n",
    "$$\\eta(t) = \\eta_{\\text{min}} + \\frac{\\eta_{\\text{max}} - \\eta_{\\text{min}}}{2}\n",
    "  \\left( 1 + \\cos\\left(\\pi \\frac{s}{T_{\\text{total}} - T_{\\text{warmup}}} \\right)\\right),\n",
    "  \\quad t \\ge T_{\\text{warmup}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.2. Intuition**\n",
    "\n",
    "Warmup stabilizes the start.\n",
    "Cosine provides a smooth landing.\n",
    "\n",
    "This is the standard schedule for:\n",
    "\n",
    "* ViT\n",
    "* GPT\n",
    "* BERT\n",
    "* diffusion models\n",
    "* Stable Diffusion\n",
    "* large CNNs\n",
    "* all modern LLMs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01380e5-82e5-40fe-beec-ac66ae44a3e6",
   "metadata": {},
   "source": [
    "## **10. When should you choose which?**\n",
    "\n",
    "#### Use ReduceLROnPlateau when:\n",
    "\n",
    "* training small/medium models\n",
    "* val loss is reliable\n",
    "* dataset is small\n",
    "* want fully automatic LR reduction\n",
    "\n",
    "Typical in:\n",
    "\n",
    "* medical imaging\n",
    "* segmentation tasks\n",
    "* small CNNs\n",
    "* Kaggle tasks\n",
    "* classic PyTorch tutorials\n",
    "\n",
    "---\n",
    "\n",
    "#### Use Cosine Annealing when:\n",
    "\n",
    "* training modern architectures\n",
    "* training transformers, ViT, CLIP, GPT\n",
    "* using AdamW\n",
    "* training large models on large datasets\n",
    "* want smooth LR decay\n",
    "* want reproducibility and stability\n",
    "\n",
    "This includes:\n",
    "\n",
    "* ImageNet training\n",
    "* ViT fine-tuning\n",
    "* LLM training\n",
    "* diffusion models\n",
    "* all JAX/DeepMind/Google setups\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12945a-bb22-476a-8534-c5f6b954fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "steps = 200                     # number of epochs or iterations to simulate\n",
    "base_lr = 1e-3                  # starting LR\n",
    "\n",
    "\n",
    "def new_opt(lr=base_lr):\n",
    "    return torch.optim.Adam([torch.zeros(1)], lr=lr)\n",
    "\n",
    "\n",
    "def record_scheduler(name, scheduler, optimizer, mode=\"epoch\"):\n",
    "    \"\"\"\n",
    "    mode = 'epoch' or 'iter'\n",
    "    \"\"\"\n",
    "    lrs = []\n",
    "\n",
    "    for t in range(steps):\n",
    "        # required to avoid warning: optimizer.step() must run before scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        if name == \"ReduceLROnPlateau\":\n",
    "            # create fake validation loss curve\n",
    "            # first improvements then plateau, then slight increase\n",
    "            val_loss = 1 + np.sin(t / 15) * 0.1 + (t / steps) * 0.3\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return name, lrs\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) StepLR\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=40, gamma=0.5)\n",
    "results.append(record_scheduler(\"StepLR\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Cosine Annealing\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=steps, eta_min=1e-6)\n",
    "results.append(record_scheduler(\"CosineAnnealing\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Warmup + Cosine (Linear warmup first 20 steps)\n",
    "# ------------------------------------------------------------\n",
    "total_warmup = 20\n",
    "opt = new_opt()\n",
    "\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    opt, start_factor=0.1, total_iters=total_warmup\n",
    ")\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    opt, T_max=steps - total_warmup, eta_min=1e-6\n",
    ")\n",
    "\n",
    "lrs = []\n",
    "for t in range(steps):\n",
    "    opt.step()\n",
    "\n",
    "    if t < total_warmup:\n",
    "        scheduler_warmup.step()\n",
    "    else:\n",
    "        scheduler_cosine.step()\n",
    "\n",
    "    lrs.append(opt.param_groups[0][\"lr\"])\n",
    "\n",
    "results.append((\"Warmup+Cosine\", lrs))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) ReduceLROnPlateau\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, mode=\"min\", factor=0.5, patience=10, threshold=1e-4\n",
    ")\n",
    "results.append(record_scheduler(\"ReduceLROnPlateau\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) ExponentialLR\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.97)\n",
    "results.append(record_scheduler(\"ExponentialLR\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) PolynomialLR\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "sched = torch.optim.lr_scheduler.PolynomialLR(opt, total_iters=steps, power=2)\n",
    "results.append(record_scheduler(\"PolynomialLR\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) OneCycleLR (per iteration)\n",
    "# ------------------------------------------------------------\n",
    "max_lr = 1e-2\n",
    "opt = new_opt(lr=1e-4)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    opt, max_lr=max_lr, total_steps=steps\n",
    ")\n",
    "results.append(record_scheduler(\"OneCycleLR\", sched, opt, mode=\"iter\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) CyclicLR (per iteration)\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt(lr=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CyclicLR(\n",
    "    opt,\n",
    "    base_lr=1e-4,\n",
    "    max_lr=1e-2,\n",
    "    step_size_up=steps // 4,\n",
    "    cycle_momentum=False,\n",
    ")\n",
    "results.append(record_scheduler(\"CyclicLR\", sched, opt, mode=\"iter\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) Linear Warmup only\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt(lr=1e-6)\n",
    "sched = torch.optim.lr_scheduler.LinearLR(\n",
    "    opt, start_factor=1e-3, total_iters=steps\n",
    ")\n",
    "results.append(record_scheduler(\"LinearWarmupOnly\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10) LambdaLR\n",
    "# ------------------------------------------------------------\n",
    "opt = new_opt()\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    # simple example: inverse sqrt decay\n",
    "    return 1.0 / np.sqrt(epoch + 1)\n",
    "\n",
    "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "results.append(record_scheduler(\"LambdaLR\", sched, opt))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot all curves\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for name, lrs in results:\n",
    "    plt.plot(lrs, label=name)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd13a7e-027f-4ad5-8da2-8b368265e4bd",
   "metadata": {},
   "source": [
    "## **11. The Intuition of the Most Common LR Schedules**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **11.1. StepLR**\n",
    "\n",
    "Decreases LR with sudden drops.\n",
    "\n",
    "```\n",
    "High  ————\n",
    "           |\n",
    "           ————\n",
    "               |\n",
    "               ———— low\n",
    "```\n",
    "\n",
    "Used in **classic CNNs** where step-decay empirically worked well.\n",
    "\n",
    "Interpretation\n",
    "Model improves → reach plateau → drop LR → improve again.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.2. Cosine Annealing**\n",
    "\n",
    "LR follows half of a cosine wave:\n",
    "\n",
    "$$\n",
    "LR(t) = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})(1 + \\cos(\\pi t / T_{\\max}))\n",
    "$$\n",
    "\n",
    "Intuition\n",
    "Starts high → decreases smoothly → tiny values at the end.\n",
    "\n",
    "Used in **ViT / Transformers** and modern architectures.\n",
    "\n",
    "Why popular?\n",
    "Because cosine gives a **very smooth**, **stable**, non-jumpy decay.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.3. Warmup + Cosine**\n",
    "\n",
    "Two phases:\n",
    "\n",
    "**Phase A: Warmup**\n",
    "\n",
    "LR increases from very small → target LR.\n",
    "\n",
    "Why warmup?\n",
    "During first batches, gradients are unstable.\n",
    "Warmup prevents “explosions”.\n",
    "\n",
    "**Phase B: Cosine decay**\n",
    "\n",
    "Same as above.\n",
    "\n",
    "Used in **GPT, ViT, diffusion models**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.4. ReduceLROnPlateau**\n",
    "\n",
    "Monitors validation loss:\n",
    "\n",
    "If val_loss stops improving → reduce LR.\n",
    "\n",
    "Very intuitive:\n",
    "Model stuck? → make smaller steps.\n",
    "\n",
    "Perfect for unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.5. Exponential decay**\n",
    "\n",
    "Decays LR every epoch by a factor:\n",
    "\n",
    "$$LR = LR_0 \\cdot \\gamma^t$$\n",
    "\n",
    "Simple, steady shrink.\n",
    "Used in simple CNNs or classic ML.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.6. Polynomial decay**\n",
    "\n",
    "Used in segmentation:\n",
    "\n",
    "$$LR = LR_0 (1 - t/T)^{p}$$\n",
    "\n",
    "Sharper decay near the end.\n",
    "Helps fine-grained pixel-level tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.7. OneCycleLR**\n",
    "\n",
    "Starts low → goes high → drops very low.\n",
    "\n",
    "Intuition:\n",
    "Large LR in the middle helps escape poor minima.\n",
    "Final very low LR helps convergence.\n",
    "\n",
    "Used in YOLO & fastai.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.8. CyclicLR**\n",
    "\n",
    "LR oscillates between low and high:\n",
    "\n",
    "Low → High → Low → High …\n",
    "\n",
    "Good when training is noisy and non-stationary.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.9. Linear warmup**\n",
    "\n",
    "Just the warmup phase:\n",
    "\n",
    "Low → gradually → target LR.\n",
    "\n",
    "Used in **huge models** where cosine is added later.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.10. LambdaLR**\n",
    "\n",
    "Fully custom schedule.\n",
    "\n",
    "You define a function:\n",
    "\n",
    "```python\n",
    "lr = lr0 * f(epoch)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a046c-8a03-4552-b7c9-23628b3b6437",
   "metadata": {},
   "source": [
    "## **12. Weight Decay**\n",
    "\n",
    "Weight decay (L2 regularization) **always pushes weights toward zero**, and it does so in a very precise mathematical way.\n",
    "Let’s make it extremely simple, intuitive, and fully correct.\n",
    "\n",
    "Why shrink weights?\n",
    "\n",
    "Because large weights mean the model **relies too heavily on specific features**, leading to:\n",
    "\n",
    "* overfitting\n",
    "* sharp minima\n",
    "* unstable training\n",
    "* poor generalization\n",
    "\n",
    "Weight decay forces smoother, simpler models.\n",
    "\n",
    "---\n",
    "\n",
    "**Weight decay ≈ push weights to stay small**\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "w = w - \\eta (\\nabla w + \\lambda w)\n",
    "$$\n",
    "\n",
    "The term\n",
    "$$\\lambda w$$\n",
    "is what pulls weights toward zero.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95951d58-4482-4078-9d63-1bae832464d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **12.1. Core idea**\n",
    "\n",
    "Weight decay adds a penalty term\n",
    "$$\n",
    "\\lambda \\lVert w \\rVert^2\n",
    "$$\n",
    "to the loss.\n",
    "\n",
    "The gradient of this penalty is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} \\lambda \\lVert w \\rVert^2 = 2\\lambda w\n",
    "$$\n",
    "\n",
    "This is **always in the same direction as the weight itself**.\n",
    "\n",
    "Then the optimizer update becomes:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta ( \\nabla L + 2\\lambda w )\n",
    "$$\n",
    "\n",
    "Rewrite:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\nabla L - 2\\lambda \\eta w\n",
    "$$\n",
    "\n",
    "Group the weight-dependent term:\n",
    "\n",
    "$$\n",
    "w \\leftarrow (1 - 2\\lambda \\eta), w - \\eta \\nabla L\n",
    "$$\n",
    "\n",
    "This is the key.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.2. Why this always shrinks the weight**\n",
    "\n",
    "Look at the factor\n",
    "\n",
    "$$\n",
    "1 - 2\\lambda \\eta\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "* learning rate $\\eta > 0$\n",
    "* weight decay $\\lambda > 0$\n",
    "\n",
    "you have:\n",
    "\n",
    "$$\n",
    "1 - 2\\lambda \\eta < 1.\n",
    "$$\n",
    "\n",
    "Therefore every update multiplies the weight by some number slightly less than 1.\n",
    "\n",
    "Example:\n",
    "If\n",
    "\n",
    "* $\\eta = 0.001$\n",
    "* $\\lambda = 0.01$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "1 - 2\\lambda\\eta = 1 - 0.0002 = 0.9998.\n",
    "$$\n",
    "\n",
    "So each step:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = 0.9998 \\cdot w - \\eta \\nabla L.\n",
    "$$\n",
    "\n",
    "This means that **even if the gradient is zero**, the weight shrinks:\n",
    "\n",
    "$$\n",
    "w_{t+1} = 0.9998 w_t.\n",
    "$$\n",
    "\n",
    "After many steps, it approaches zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.3. Addressing your question directly**\n",
    "\n",
    "> *\"How does λw pull weights toward zero? I mean it is adding something to it, it could be plus or minus.\"*\n",
    "\n",
    "Good confusion — here is the clean resolution:\n",
    "\n",
    "#### **12.3 The added gradient is always aligned with w**\n",
    "\n",
    "The penalty contributes a gradient:\n",
    "\n",
    "$$\n",
    "2\\lambda w\n",
    "$$\n",
    "\n",
    "If $w$ is positive → gradient is positive\n",
    "If $w$ is negative → gradient is negative\n",
    "\n",
    "But the optimizer **subtracts** gradients:\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta (2\\lambda w)\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "* If $w > 0$:\n",
    "  $$w \\leftarrow w - \\eta (2\\lambda w) = w(1 - 2\\lambda\\eta)$$\n",
    "  → weight decreases\n",
    "\n",
    "* If $w < 0$:\n",
    "  $$w \\leftarrow w - \\eta (2\\lambda w) = w(1 - 2\\lambda\\eta)$$\n",
    "  → weight increases (toward 0)\n",
    "\n",
    "So no matter the sign:\n",
    "\n",
    "**Why it works:**\n",
    "You are subtracting a term proportional to the weight itself.\n",
    "\n",
    "This is like “friction” in physics — it slows motion and reduces magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.4. Pure intuition (best explanation)**\n",
    "\n",
    "Think of weight decay as applying a **brake** to parameter magnitude.\n",
    "\n",
    "The bigger a weight is, the stronger the “pull-to-zero” is.\n",
    "\n",
    "The penalty gradient is of the form:\n",
    "\n",
    "* positive if weight is positive\n",
    "* negative if weight is negative\n",
    "\n",
    "But because we **subtract** the gradient during update, that pushes the weight toward zero in both cases.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.5. Why weight decay is essential**\n",
    "\n",
    "- **5.1 It prevents exploding weights**\n",
    "\n",
    "Without decay, weights may drift outward during noisy updates.\n",
    "\n",
    "- **5.2 It smooths the loss landscape**\n",
    "\n",
    "Large weights → sharp minima → poor generalization\n",
    "Shrinked weights → flatter minima → good generalization\n",
    "\n",
    "- **5.3 Why AdamW is preferred**\n",
    "\n",
    "Adam incorrectly mixes weight decay with gradient updates.\n",
    "AdamW decouples the term, correctly applying:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w (1 - \\eta \\lambda)\n",
    "$$\n",
    "\n",
    "as a separate operation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.6. Minimal numeric example**\n",
    "\n",
    "Let:\n",
    "\n",
    "* $w = 10$\n",
    "* $\\eta = 0.1$\n",
    "* $\\lambda = 0.01$\n",
    "\n",
    "Decay factor:\n",
    "\n",
    "$$\n",
    "1 - \\eta \\lambda = 0.999\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = 10 \\cdot 0.999 = 9.99.\n",
    "$$\n",
    "\n",
    "If $w = -10$:\n",
    "\n",
    "$$\n",
    "-10 \\cdot 0.999 = -9.99.\n",
    "$$\n",
    "\n",
    "Both cases → magnitude shrinks.\n",
    "\n",
    "---\n",
    "\n",
    "### **12.One-sentence summary**\n",
    "\n",
    "> **Weight decay works because we subtract a gradient term proportional to the weight itself, which multiplies the weight by a factor slightly less than 1 every step, shrinking it toward zero regardless of its sign.**\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can also:\n",
    "\n",
    "* Show the difference between L2 vs weight decay in Adam\n",
    "* Plot weight decay dynamics\n",
    "* Explain why biases and LayerNorm weights should not have weight decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db70d83-6a02-465e-84f1-7ed41eb0b1df",
   "metadata": {},
   "source": [
    "## **13. The relationship between LR and Weight Decay?**\n",
    "\n",
    "They interact **directly** in the update rule.\n",
    "\n",
    "High LR makes weight decay *act stronger*.\n",
    "Low LR makes weight decay *act weaker*.\n",
    "\n",
    "So:\n",
    "\n",
    "### Rule of thumb\n",
    "\n",
    "* When you use **large LR**, weight decay is **very influential**.\n",
    "* When you use **very small LR**, weight decay barely changes anything.\n",
    "\n",
    "This is why modern optimizers (AdamW) separate them cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "# Best practical settings\n",
    "\n",
    "### Vision models (ConvNeXt, ViT)\n",
    "\n",
    "* Warmup + Cosine\n",
    "* Weight decay: 0.02–0.1\n",
    "\n",
    "### Small CNNs\n",
    "\n",
    "* StepLR\n",
    "* Weight decay: 1e-4\n",
    "\n",
    "### Transformers\n",
    "\n",
    "* Warmup (5–10% of training)\n",
    "* Cosine decay\n",
    "* Weight decay: 0.05\n",
    "\n",
    "### YOLO / Object detection\n",
    "\n",
    "* OneCycleLR\n",
    "* Weight decay: 5e-4\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
