{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de086ec-d129-40b4-8c88-e4c3db19a11a",
   "metadata": {},
   "source": [
    "##  1. **SGD (Stochastic Gradient Descent)**\n",
    "```python\n",
    "torch.optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "####  Equation:\n",
    "$\n",
    "x_{t+1} = x_t - \\eta \\cdot \\nabla f(x_t)\n",
    "$\n",
    "\n",
    "With momentum:\n",
    "\n",
    "$\n",
    "v_{t+1} = \\mu v_t + \\eta \\cdot \\nabla f(x_t) \n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "x_{t+1} = x_t - v_{t+1}\n",
    "$\n",
    "\n",
    "\n",
    "####  Properties:\n",
    "- Simple and widely used\n",
    "- Supports **momentum** and **weight decay (L2 regularization)**\n",
    "- Can escape local minima with momentum\n",
    "\n",
    "####  When to Use:\n",
    "- Large datasets\n",
    "- Classical models like linear/logistic regression\n",
    "- When you want full control over optimization (e.g., with custom learning rate schedules)\n",
    "\n",
    "#### Be Careful:\n",
    "- Sensitive to learning rate\n",
    "- Requires good manual tuning\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9e868-8883-41bb-a60b-980160e78218",
   "metadata": {},
   "source": [
    "##  2. **Adam (Adaptive Moment Estimation)**\n",
    "```python\n",
    "torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999))\n",
    "```\n",
    "\n",
    "\n",
    "####  Equation:\n",
    "$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla f(x_t) \n",
    "$\n",
    "\n",
    "$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla f(x_t))^2 \n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \n",
    "$\n",
    "$\n",
    "x_{t+1} = x_t - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$\n",
    "\n",
    "\n",
    "####  Properties:\n",
    "- Combines **momentum** and **adaptive learning rate**\n",
    "- Keeps track of **moving averages of gradient and squared gradient**\n",
    "- Usually requires **less tuning** than SGD\n",
    "\n",
    "####  When to Use:\n",
    "- Deep learning models (CNNs, RNNs, Transformers)\n",
    "- Works well out of the box\n",
    "- Most popular choice for **research and prototyping**\n",
    "\n",
    "####  Be Careful:\n",
    "- May generalize worse than SGD in some cases\n",
    "- Can converge to sharp minima\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55316948-7d03-44ef-b885-e0193551f445",
   "metadata": {},
   "source": [
    "##  3. **AdamW (Adam with decoupled weight decay)**\n",
    "```python\n",
    "torch.optim.AdamW(params, lr=0.001, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "####  Equation:\n",
    "\n",
    "$\n",
    "x_{t+1} = x_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda x_t \\right)\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "####  Properties:\n",
    "- Like Adam but fixes how weight decay is applied\n",
    "- **Recommended for transformers and NLP models**\n",
    "\n",
    "####  When to Use:\n",
    "- Training large models like BERT, Vision Transformers\n",
    "- When using learning rate schedulers like `cosine annealing`\n",
    "\n",
    "####  Popular in:\n",
    "- HuggingFace Transformers\n",
    "- Vision transformer training\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421036c-4090-42c9-a22e-3931fe8a47b3",
   "metadata": {},
   "source": [
    "<img src=\"images/step_size_momentum.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baee105-c280-4f0f-905c-776c3d98c68f",
   "metadata": {},
   "source": [
    "[Full reference](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b307eeb-4254-4ca9-8e6b-19ef21724f38",
   "metadata": {},
   "source": [
    "[MOMENTUM Gradient Descent](https://www.youtube.com/watch?v=iudXf5n_3ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3699d0-74a5-4d80-bb82-e00fc7724178",
   "metadata": {},
   "source": [
    "[AdamW - L2 Regularization vs Weight Decay](https://www.youtube.com/watch?v=oWZbcq_figk&t=134s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc725572-2b6f-45e5-a8ed-6745a2c1ea34",
   "metadata": {},
   "source": [
    "[Optimizers - EXPLAINED](https://www.youtube.com/watch?v=mdKjMPmcWjY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b026e-c16a-49e2-b076-1ab950459042",
   "metadata": {},
   "source": [
    "[https://www.youtube.com/watch?v=MD2fYip6QsQ](https://www.youtube.com/watch?v=MD2fYip6QsQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a28cf-f8f7-40b5-8094-0510a667dab5",
   "metadata": {},
   "source": [
    "[Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)](https://www.youtube.com/watch?v=NE88eqLngkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5070b5a-b13e-416a-acdc-f5fb5d789349",
   "metadata": {},
   "source": [
    "[Optimization in Deep Learning](https://www.youtube.com/watch?v=M2xkmc2oHUc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a3c56-8d64-4d1f-9144-de2d44db125b",
   "metadata": {},
   "source": [
    "`AdamW` is a variant of the Adam optimizer that **decouples weight decay from the optimization step**, which leads to better regularization and generalization compared to the standard `Adam` with L2 regularization.\n",
    "\n",
    "\n",
    "In `Adam`, weight decay was incorrectly implemented as L2 regularization—adding it directly to the gradient. But this interferes with Adam’s adaptive moment estimates.\n",
    "\n",
    "**AdamW corrects this** by applying weight decay **directly to the weights** after the gradient update:\n",
    "\n",
    "\n",
    "\n",
    "**Adam with Incorrect L2 Regularization**\n",
    "\n",
    "This is the **incorrect** way (used in classic Adam):\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\left( \\nabla_{\\theta} \\mathcal{L}(\\theta_t) + \\lambda \\theta_t \\right)\n",
    "$\n",
    "\n",
    "- $ \\eta $: learning rate  \n",
    "- $ \\lambda $: weight decay coefficient  \n",
    "- $ \\nabla_{\\theta} \\mathcal{L}(\\theta_t) $: gradient of the loss\n",
    "\n",
    "---\n",
    "\n",
    "**AdamW (Correct Decoupled Weight Decay)**\n",
    "\n",
    "This is the **correct** AdamW approach:\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\left( \\theta_t - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta_t) \\right) \\cdot (1 - \\eta \\lambda)\n",
    "$\n",
    "\n",
    "Alternatively, split into two steps for clarity:\n",
    "\n",
    "1. Gradient step:\n",
    "$\n",
    "\\theta_t' = \\theta_t - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n",
    "$\n",
    "\n",
    "2. Weight decay step:\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t' \\cdot (1 - \\eta \\lambda)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  What is `weight_decay`?\n",
    "- It’s equivalent to L2 regularization but applied the **correct** way (as a multiplicative decay on weights).\n",
    "- Helps **prevent overfitting** by penalizing large weights.\n",
    "\n",
    "####  When you set:\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "```\n",
    "You're telling it to:\n",
    "- Use Adam-style updates\n",
    "- **Decay weights by 1% per step** (adjusted by the learning rate)\n",
    "\n",
    "Tip: Usually, `weight_decay=1e-2` or `1e-4` is a good starting point. Don’t apply it to **biases or normalization layers** (they don’t benefit from it and can hurt performance).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfbc60-43e2-4c23-a486-0a9b7511afcc",
   "metadata": {},
   "source": [
    "\n",
    "###  **Learning Rate Schedulers (Schedulers)**\n",
    " Learning Rate Schedulers (LRS) are **super important** for deep learning optimization, especially when training deep networks like transformers, CNNs, or anything using `AdamW`. They help your model **converge faster, avoid overfitting, and escape local minima**.\n",
    "\n",
    "---\n",
    "\n",
    "##  What is a Learning Rate Scheduler?\n",
    "\n",
    "The **learning rate (LR)** determines how big a step the optimizer takes when updating weights. A **scheduler** dynamically changes the learning rate during training to:\n",
    "\n",
    "- **Start with a high LR** to explore faster\n",
    "- **Gradually lower it** to fine-tune the weights\n",
    "\n",
    "\n",
    "Schedulers help reduce the learning rate during training, especially when the model hits a plateau.\n",
    "\n",
    "Some common PyTorch schedulers:\n",
    "\n",
    "#### 1. **StepLR**\n",
    "```python\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "```\n",
    "Every 10 epochs, LR = LR × 0.1\n",
    "\n",
    "#### 2. **ReduceLROnPlateau**\n",
    "```python\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "```\n",
    "Reduces LR when validation loss stops improving for 5 epochs.\n",
    "\n",
    "#### 3. **CosineAnnealingLR**\n",
    "```python\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "```\n",
    "Cosine decay over 50 epochs — good for smooth convergence.\n",
    "\n",
    "#### 4. **OneCycleLR**\n",
    "Very effective for training vision transformers and large models.\n",
    "\n",
    "```python\n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=10)\n",
    "```\n",
    "\n",
    "[PyTorch LR Scheduler](https://www.youtube.com/watch?v=81NJgoR5RfY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e976ce1-abd4-407f-9113-93abadc3491d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Putting It All Together: AdamW + Scheduler + Regularization\n",
    "\n",
    "```python\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-2  # regularization\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        loss = compute_loss(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    scheduler.step()  # update LR after each epoch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "- ✅ Use `AdamW` instead of `Adam + L2`\n",
    "- ✅ Set `weight_decay` (but not for biases/BatchNorm layers)\n",
    "- ✅ Use a scheduler like `CosineAnnealingLR` or `OneCycleLR` for smoother training\n",
    "- ✅ Tune `lr`, `weight_decay`, and scheduler parameters on a validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c3bbf-0e4a-41a6-89c3-007fda900a36",
   "metadata": {},
   "source": [
    "##  4. **RMSprop**\n",
    "```python\n",
    "torch.optim.RMSprop(params, lr=0.01, alpha=0.99)\n",
    "```\n",
    "\n",
    "#### Equation:\n",
    "\n",
    "RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to deal with the problem of diminishing or exploding learning rates.\n",
    "\n",
    "Let:\n",
    "- $ \\theta $ be the parameters (weights) of your model,\n",
    "- $ g_t = \\nabla_\\theta J(\\theta_t) $ be the gradient of the loss function at time step $ t $,\n",
    "- $ E[g^2]_t $ be the exponentially weighted moving average of the squared gradients,\n",
    "- $ \\eta $ be the learning rate,\n",
    "- $ \\gamma $ be the decay rate (typically around 0.9),\n",
    "- $ \\epsilon $ be a small value to avoid division by zero (e.g., $10^{-8}$).\n",
    "\n",
    "Then the RMSprop update rule is:\n",
    "\n",
    "1. **Running average of squared gradients:**\n",
    "   $\n",
    "   E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "   $\n",
    "\n",
    "2. **Parameter update:**\n",
    "   $\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "- RMSprop maintains a running average of the squared gradients and divides the learning rate by the root of this average.\n",
    "- It adapts the learning rate for each parameter individually, allowing faster convergence and better handling of noisy gradients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####  Properties:\n",
    "- Adaptive learning rate\n",
    "- Suitable for non-stationary objectives (e.g., reinforcement learning)\n",
    "\n",
    "####  When to Use:\n",
    "- RNNs\n",
    "- Reinforcement learning\n",
    "- If Adam performs poorly in your case\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef8f6e-531d-4b52-8b40-5e4ed99b9d59",
   "metadata": {},
   "source": [
    "\n",
    "##  5. **Adagrad**\n",
    "```python\n",
    "torch.optim.Adagrad(params, lr=0.01)\n",
    "```\n",
    "\n",
    "#### Equations: \n",
    "\n",
    "Let:\n",
    "- $ \\theta $ be the parameters of the model,\n",
    "- $ g_t = \\nabla_\\theta J(\\theta_t) $ be the gradient at time step $ t $,\n",
    "- $ G_t $ be the **accumulated sum of squared gradients** up to time $ t $,\n",
    "- $ \\eta $ be the initial learning rate,\n",
    "- $ \\epsilon $ be a small constant to prevent division by zero (e.g., $10^{-8}$).\n",
    "\n",
    "Then:\n",
    "\n",
    "1. **Accumulated squared gradients:**\n",
    "   $\n",
    "   G_t = G_{t-1} + g_t^2\n",
    "   $\n",
    "\n",
    "2. **Parameter update:**\n",
    "   $\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "**Key Idea**\n",
    "\n",
    "- Adagrad adapts the learning rate for each parameter based on how frequently it’s updated.\n",
    "- Parameters with **larger past gradients** get **smaller updates**, and parameters with **smaller gradients** get **relatively larger updates**.\n",
    "- It works well for **sparse data** (e.g., NLP or text data).\n",
    "\n",
    "---\n",
    "\n",
    "####  Properties:\n",
    "- Increases stability by adapting learning rate based on past gradients\n",
    "- Learning rate decreases over time (which can be limiting)\n",
    "\n",
    "###  Limitation\n",
    "\n",
    "- Because $ G_t $ keeps accumulating, it can **grow very large** over time, causing the learning rate to **shrink too much** — this can **stop learning prematurely**.\n",
    "- This is the main reason why RMSprop and Adam were developed to improve on it.\n",
    "\n",
    "\n",
    "####  When to Use:\n",
    "- Sparse data (e.g., NLP tasks with sparse inputs)\n",
    "- Models with infrequent updates (e.g., embeddings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5490db-2802-42f5-a0a0-b7537ab3311d",
   "metadata": {},
   "source": [
    "\n",
    "##  6. **Adadelta**\n",
    "```python\n",
    "torch.optim.Adadelta(params)\n",
    "```\n",
    "\n",
    "####  Properties:\n",
    "- Improves Adagrad by limiting the decrease in learning rate\n",
    "- No need to manually set a learning rate\n",
    "\n",
    "\n",
    "\n",
    "####  When to Use:\n",
    "- Similar cases as Adagrad, but more robust\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19a16f-4f4f-4dc7-8b87-a60bd6d79040",
   "metadata": {},
   "source": [
    "##  7. **NAdam (Nesterov-accelerated Adam)**\n",
    "```python\n",
    "torch.optim.NAdam(params, lr=0.001)\n",
    "```\n",
    "\n",
    "####  Properties:\n",
    "- Combines Adam with Nesterov momentum\n",
    "- Slightly faster convergence in some settings\n",
    "\n",
    "####  When to Use:\n",
    "- Similar tasks as Adam, but try if Adam is not converging fast enough\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4171c-70d6-4b6e-9088-a4358fb73173",
   "metadata": {},
   "source": [
    "##  8. **LBFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)**\n",
    "```python\n",
    "torch.optim.LBFGS(params)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What is L-BFGS?**\n",
    "\n",
    "- L-BFGS is an approximation of the **BFGS** optimization algorithm, designed to be **memory-efficient** (hence \"limited-memory\").\n",
    "- It approximates the **inverse Hessian matrix** (second derivatives) using gradients and parameter updates from previous steps.\n",
    "- Unlike SGD, RMSprop, or Adam, L-BFGS is typically **used for smaller datasets** (e.g., in classical ML), not minibatch training.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Idea**\n",
    "\n",
    "Instead of computing and storing the full Hessian $ H_t \\in \\mathbb{R}^{n \\times n} $, L-BFGS maintains a **limited history** (say $ m $ past updates) of:\n",
    "\n",
    "- $ s_k = \\theta_{k+1} - \\theta_k $ (parameter differences)\n",
    "- $ y_k = \\nabla f_{k+1} - \\nabla f_k $ (gradient differences)\n",
    "\n",
    "These vectors are used to update the inverse Hessian approximation $ H_k $ via a **recursive formula**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Equations: \n",
    "\n",
    "At each iteration $ t $:\n",
    "\n",
    "1. **Compute gradient**: $ g_t = \\nabla_\\theta J(\\theta_t) $\n",
    "\n",
    "2. **Compute direction** (approximate Newton step):\n",
    "   $\n",
    "   p_t = -H_t g_t\n",
    "   $\n",
    "\n",
    "3. **Line search** (optional but common): Find optimal step size $ \\alpha_t $\n",
    "\n",
    "4. **Parameter update**:\n",
    "   $\n",
    "   \\theta_{t+1} = \\theta_t + \\alpha_t p_t\n",
    "   $\n",
    "\n",
    "5. **Update history** with $ s_t = \\theta_{t+1} - \\theta_t $, $ y_t = g_{t+1} - g_t $\n",
    "\n",
    "---\n",
    "\n",
    "**Highlights**\n",
    "\n",
    "- **No need to store the full Hessian**, just the past $ m $ pairs of $ (s_k, y_k) $\n",
    "- Typically works **best for convex, smooth problems**\n",
    "- Can **converge faster** than gradient descent when the curvature of the loss is important\n",
    "\n",
    "---\n",
    "\n",
    "###  Limitations\n",
    "\n",
    "- Not ideal for **large-scale deep learning**, where stochastic gradient methods (SGD, Adam) are better.\n",
    "- More **sensitive to noise** and **not naturally mini-batch friendly**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  Properties:\n",
    "- Second-order optimizer (quasi-Newton method)\n",
    "- Requires full batch computation\n",
    "- Slower but more accurate\n",
    "\n",
    "####  When to Use:\n",
    "- Small models with a few parameters\n",
    "- When you want precise convergence (e.g., curve fitting, inverse problems)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494714e-66e8-44e5-baea-33aa01172295",
   "metadata": {},
   "source": [
    "**Summary: Which Optimizer is Most Common?**\n",
    "\n",
    "| Use Case                         | Recommended Optimizer |\n",
    "|----------------------------------|------------------------|\n",
    "| Most DL models (CNNs, Transformers) | **Adam** / **AdamW**     |\n",
    "| Fine-tuning Transformers         | **AdamW**              |\n",
    "| Classical models (Linear, Logistic) | **SGD**                |\n",
    "| Sparse data (e.g., NLP)          | **Adagrad**            |\n",
    "| Reinforcement Learning           | **RMSprop**            |\n",
    "| Small precise models             | **LBFGS**              |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
