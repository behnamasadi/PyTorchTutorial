digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	132057621345488 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	132054465089392 [label=AddmmBackward0]
	132054465095824 -> 132054465089392
	132054418968080 [label="fc.bias
 (1000)" fillcolor=lightblue]
	132054418968080 -> 132054465095824
	132054465095824 [label=AccumulateGrad]
	132054465089296 -> 132054465089392
	132054465089296 [label=ViewBackward0]
	132054465093952 -> 132054465089296
	132054465093952 [label=MeanBackward1]
	132054465087856 -> 132054465093952
	132054465087856 [label=ReluBackward0]
	132054465101632 -> 132054465087856
	132054465101632 [label=AddBackward0]
	132058501843120 -> 132054465101632
	132058501843120 [label=NativeBatchNormBackward0]
	132058501842832 -> 132058501843120
	132058501842832 [label=ConvolutionBackward0]
	132058501841440 -> 132058501842832
	132058501841440 [label=ReluBackward0]
	132058501844416 -> 132058501841440
	132058501844416 [label=NativeBatchNormBackward0]
	132058501842112 -> 132058501844416
	132058501842112 [label=ConvolutionBackward0]
	132058501843264 -> 132058501842112
	132058501843264 [label=ReluBackward0]
	132054418758368 -> 132058501843264
	132054418758368 [label=AddBackward0]
	132054418757888 -> 132054418758368
	132054418757888 [label=NativeBatchNormBackward0]
	132054418759040 -> 132054418757888
	132054418759040 [label=ConvolutionBackward0]
	132054418759136 -> 132054418759040
	132054418759136 [label=ReluBackward0]
	132054418758752 -> 132054418759136
	132054418758752 [label=NativeBatchNormBackward0]
	132054418753136 -> 132054418758752
	132054418753136 [label=ConvolutionBackward0]
	132054418754672 -> 132054418753136
	132054418754672 [label=ReluBackward0]
	132054418754864 -> 132054418754672
	132054418754864 [label=AddBackward0]
	132054418756784 -> 132054418754864
	132054418756784 [label=NativeBatchNormBackward0]
	132054418756208 -> 132054418756784
	132054418756208 [label=ConvolutionBackward0]
	132054418753040 -> 132054418756208
	132054418753040 [label=ReluBackward0]
	132054418751552 -> 132054418753040
	132054418751552 [label=NativeBatchNormBackward0]
	132054418751600 -> 132054418751552
	132054418751600 [label=ConvolutionBackward0]
	132054418754000 -> 132054418751600
	132054418754000 [label=ReluBackward0]
	132054418751840 -> 132054418754000
	132054418751840 [label=AddBackward0]
	132054418760144 -> 132054418751840
	132054418760144 [label=NativeBatchNormBackward0]
	132054418760240 -> 132054418760144
	132054418760240 [label=ConvolutionBackward0]
	132054418760384 -> 132054418760240
	132054418760384 [label=ReluBackward0]
	132054418760576 -> 132054418760384
	132054418760576 [label=NativeBatchNormBackward0]
	132054418760624 -> 132054418760576
	132054418760624 [label=ConvolutionBackward0]
	132054418752560 -> 132054418760624
	132054418752560 [label=ReluBackward0]
	132054418752848 -> 132054418752560
	132054418752848 [label=AddBackward0]
	132054418752896 -> 132054418752848
	132054418752896 [label=NativeBatchNormBackward0]
	132054418761248 -> 132054418752896
	132054418761248 [label=ConvolutionBackward0]
	132054418761392 -> 132054418761248
	132054418761392 [label=ReluBackward0]
	132054418753424 -> 132054418761392
	132054418753424 [label=NativeBatchNormBackward0]
	132054418761728 -> 132054418753424
	132054418761728 [label=ConvolutionBackward0]
	132054418761056 -> 132054418761728
	132054418761056 [label=ReluBackward0]
	132054418762016 -> 132054418761056
	132054418762016 [label=AddBackward0]
	132054418762208 -> 132054418762016
	132054418762208 [label=NativeBatchNormBackward0]
	132054418754096 -> 132054418762208
	132054418754096 [label=ConvolutionBackward0]
	132054418754480 -> 132054418754096
	132054418754480 [label=ReluBackward0]
	132054418755152 -> 132054418754480
	132054418755152 [label=NativeBatchNormBackward0]
	132054418755488 -> 132054418755152
	132054418755488 [label=ConvolutionBackward0]
	132054418756352 -> 132054418755488
	132054418756352 [label=ReluBackward0]
	132054418757360 -> 132054418756352
	132054418757360 [label=AddBackward0]
	132054418758080 -> 132054418757360
	132054418758080 [label=NativeBatchNormBackward0]
	132054418757648 -> 132054418758080
	132054418757648 [label=ConvolutionBackward0]
	132054418761440 -> 132054418757648
	132054418761440 [label=ReluBackward0]
	132054418761680 -> 132054418761440
	132054418761680 [label=NativeBatchNormBackward0]
	132054418761488 -> 132054418761680
	132054418761488 [label=ConvolutionBackward0]
	132054418757984 -> 132054418761488
	132054418757984 [label=ReluBackward0]
	132054418762400 -> 132054418757984
	132054418762400 [label=AddBackward0]
	132054418762544 -> 132054418762400
	132054418762544 [label=NativeBatchNormBackward0]
	132054418762736 -> 132054418762544
	132054418762736 [label=ConvolutionBackward0]
	132054418762928 -> 132054418762736
	132054418762928 [label=ReluBackward0]
	132054418763072 -> 132054418762928
	132054418763072 [label=NativeBatchNormBackward0]
	132054418763168 -> 132054418763072
	132054418763168 [label=ConvolutionBackward0]
	132054418762448 -> 132054418763168
	132054418762448 [label=MaxPool2DWithIndicesBackward0]
	132054418763456 -> 132054418762448
	132054418763456 [label=ReluBackward0]
	132054418763552 -> 132054418763456
	132054418763552 [label=NativeBatchNormBackward0]
	132054418763648 -> 132054418763552
	132054418763648 [label=ConvolutionBackward0]
	132054418763840 -> 132054418763648
	132058526083632 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	132058526083632 -> 132054418763840
	132054418763840 [label=AccumulateGrad]
	132054418763600 -> 132054418763552
	132058526082112 [label="bn1.weight
 (64)" fillcolor=lightblue]
	132058526082112 -> 132054418763600
	132054418763600 [label=AccumulateGrad]
	132054418763264 -> 132054418763552
	132058526085472 [label="bn1.bias
 (64)" fillcolor=lightblue]
	132058526085472 -> 132054418763264
	132054418763264 [label=AccumulateGrad]
	132054418763360 -> 132054418763168
	132058526084912 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132058526084912 -> 132054418763360
	132054418763360 [label=AccumulateGrad]
	132054418763120 -> 132054418763072
	132058526084112 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	132058526084112 -> 132054418763120
	132054418763120 [label=AccumulateGrad]
	132054418762976 -> 132054418763072
	132054418780112 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	132054418780112 -> 132054418762976
	132054418762976 [label=AccumulateGrad]
	132054418762880 -> 132054418762736
	132054418780592 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132054418780592 -> 132054418762880
	132054418762880 [label=AccumulateGrad]
	132054418762688 -> 132054418762544
	132054465372608 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	132054465372608 -> 132054418762688
	132054418762688 [label=AccumulateGrad]
	132054418762640 -> 132054418762544
	132054465372768 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	132054465372768 -> 132054418762640
	132054418762640 [label=AccumulateGrad]
	132054418762448 -> 132054418762400
	132054418762160 -> 132054418761488
	132054465373648 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132054465373648 -> 132054418762160
	132054418762160 [label=AccumulateGrad]
	132054418762064 -> 132054418761680
	132054465373568 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	132054465373568 -> 132054418762064
	132054418762064 [label=AccumulateGrad]
	132054418761008 -> 132054418761680
	132054465380208 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	132054465380208 -> 132054418761008
	132054418761008 [label=AccumulateGrad]
	132054418760912 -> 132054418757648
	132054465378848 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132054465378848 -> 132054418760912
	132054418760912 [label=AccumulateGrad]
	132054418757696 -> 132054418758080
	132054465379088 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	132054465379088 -> 132054418757696
	132054418757696 [label=AccumulateGrad]
	132054418758176 -> 132054418758080
	132054465378768 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	132054465378768 -> 132054418758176
	132054418758176 [label=AccumulateGrad]
	132054418757984 -> 132054418757360
	132054418755968 -> 132054418755488
	132054465376528 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	132054465376528 -> 132054418755968
	132054418755968 [label=AccumulateGrad]
	132054418755392 -> 132054418755152
	132054465376448 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	132054465376448 -> 132054418755392
	132054418755392 [label=AccumulateGrad]
	132054418754960 -> 132054418755152
	132054465376128 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	132054465376128 -> 132054418754960
	132054418754960 [label=AccumulateGrad]
	132054418762496 -> 132054418754096
	132054465374928 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132054465374928 -> 132054418762496
	132054418762496 [label=AccumulateGrad]
	132054418754048 -> 132054418762208
	132054465375328 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	132054465375328 -> 132054418754048
	132054418754048 [label=AccumulateGrad]
	132054418762256 -> 132054418762208
	132054465375248 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	132054465375248 -> 132054418762256
	132054418762256 [label=AccumulateGrad]
	132054418762112 -> 132054418762016
	132054418762112 [label=NativeBatchNormBackward0]
	132054418757072 -> 132054418762112
	132054418757072 [label=ConvolutionBackward0]
	132054418756352 -> 132054418757072
	132054418755776 -> 132054418757072
	132054465377648 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	132054465377648 -> 132054418755776
	132054418755776 [label=AccumulateGrad]
	132054418755008 -> 132054418762112
	132054465377888 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	132054465377888 -> 132054418755008
	132054418755008 [label=AccumulateGrad]
	132054418762304 -> 132054418762112
	132054465377408 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	132054465377408 -> 132054418762304
	132054418762304 [label=AccumulateGrad]
	132054418761920 -> 132054418761728
	132054465374448 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132054465374448 -> 132054418761920
	132054418761920 [label=AccumulateGrad]
	132054418761632 -> 132054418753424
	132054465374688 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	132054465374688 -> 132054418761632
	132054418761632 [label=AccumulateGrad]
	132054418761536 -> 132054418753424
	132054418780192 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	132054418780192 -> 132054418761536
	132054418761536 [label=AccumulateGrad]
	132054418753184 -> 132054418761248
	132054418781072 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132054418781072 -> 132054418753184
	132054418753184 [label=AccumulateGrad]
	132054418761200 -> 132054418752896
	132054418780992 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	132054418780992 -> 132054418761200
	132054418761200 [label=AccumulateGrad]
	132054418761152 -> 132054418752896
	132054418781152 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	132054418781152 -> 132054418761152
	132054418761152 [label=AccumulateGrad]
	132054418761056 -> 132054418752848
	132054418760768 -> 132054418760624
	132054418782352 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	132054418782352 -> 132054418760768
	132054418760768 [label=AccumulateGrad]
	132054418752416 -> 132054418760576
	132054418782272 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	132054418782272 -> 132054418752416
	132054418752416 [label=AccumulateGrad]
	132054418752224 -> 132054418760576
	132054418782432 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	132054418782432 -> 132054418752224
	132054418752224 [label=AccumulateGrad]
	132054418760336 -> 132054418760240
	132054418782992 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132054418782992 -> 132054418760336
	132054418760336 [label=AccumulateGrad]
	132054418752032 -> 132054418760144
	132054418782912 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	132054418782912 -> 132054418752032
	132054418752032 [label=AccumulateGrad]
	132054418760192 -> 132054418760144
	132054418782752 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	132054418782752 -> 132054418760192
	132054418760192 [label=AccumulateGrad]
	132054418760048 -> 132054418751840
	132054418760048 [label=NativeBatchNormBackward0]
	132054418760864 -> 132054418760048
	132054418760864 [label=ConvolutionBackward0]
	132054418752560 -> 132054418760864
	132054418752512 -> 132054418760864
	132054418781552 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	132054418781552 -> 132054418752512
	132054418752512 [label=AccumulateGrad]
	132054418760432 -> 132054418760048
	132054418781632 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	132054418781632 -> 132054418760432
	132054418760432 [label=AccumulateGrad]
	132054418752080 -> 132054418760048
	132054418781712 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	132054418781712 -> 132054418752080
	132054418752080 [label=AccumulateGrad]
	132054418759952 -> 132054418751600
	132054418783472 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132054418783472 -> 132054418759952
	132054418759952 [label=AccumulateGrad]
	132054418759760 -> 132054418751552
	132054418783392 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	132054418783392 -> 132054418759760
	132054418759760 [label=AccumulateGrad]
	132054418754912 -> 132054418751552
	132054418783552 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	132054418783552 -> 132054418754912
	132054418754912 [label=AccumulateGrad]
	132054418751696 -> 132054418756208
	132054418784112 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132054418784112 -> 132054418751696
	132054418751696 [label=AccumulateGrad]
	132054418753472 -> 132054418756784
	132054418783952 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	132054418783952 -> 132054418753472
	132054418753472 [label=AccumulateGrad]
	132054418753856 -> 132054418756784
	132054418784192 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	132054418784192 -> 132054418753856
	132054418753856 [label=AccumulateGrad]
	132054418754000 -> 132054418754864
	132054418753232 -> 132054418753136
	132054418965760 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	132054418965760 -> 132054418753232
	132054418753232 [label=AccumulateGrad]
	132054418757936 -> 132054418758752
	132054418965680 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	132054418965680 -> 132054418757936
	132054418757936 [label=AccumulateGrad]
	132054418758128 -> 132054418758752
	132054418965840 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	132054418965840 -> 132054418758128
	132054418758128 [label=AccumulateGrad]
	132054418758560 -> 132054418759040
	132054418966400 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132054418966400 -> 132054418758560
	132054418758560 [label=AccumulateGrad]
	132054418759904 -> 132054418757888
	132054418966320 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	132054418966320 -> 132054418759904
	132054418759904 [label=AccumulateGrad]
	132054418760096 -> 132054418757888
	132054418966480 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	132054418966480 -> 132054418760096
	132054418760096 [label=AccumulateGrad]
	132054418758992 -> 132054418758368
	132054418758992 [label=NativeBatchNormBackward0]
	132054418756160 -> 132054418758992
	132054418756160 [label=ConvolutionBackward0]
	132054418754672 -> 132054418756160
	132054418757552 -> 132054418756160
	132054418964960 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	132054418964960 -> 132054418757552
	132054418757552 [label=AccumulateGrad]
	132054418758512 -> 132054418758992
	132054418965040 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	132054418965040 -> 132054418758512
	132054418758512 [label=AccumulateGrad]
	132054418759520 -> 132054418758992
	132054418965120 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	132054418965120 -> 132054418759520
	132054418759520 [label=AccumulateGrad]
	132054418757840 -> 132058501842112
	132054418966960 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132054418966960 -> 132054418757840
	132054418757840 [label=AccumulateGrad]
	132058501842928 -> 132058501844416
	132054418966880 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	132054418966880 -> 132058501842928
	132058501842928 [label=AccumulateGrad]
	132054418757312 -> 132058501844416
	132054418967040 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	132054418967040 -> 132054418757312
	132054418757312 [label=AccumulateGrad]
	132058501843072 -> 132058501842832
	132054418967600 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132054418967600 -> 132058501843072
	132058501843072 [label=AccumulateGrad]
	132058501841152 -> 132058501843120
	132054418967520 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	132054418967520 -> 132058501841152
	132058501841152 [label=AccumulateGrad]
	132058501841344 -> 132058501843120
	132054418967680 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	132054418967680 -> 132058501841344
	132058501841344 [label=AccumulateGrad]
	132058501843264 -> 132054465101632
	132054465090016 -> 132054465089392
	132054465090016 [label=TBackward0]
	132054465089536 -> 132054465090016
	132054418968000 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	132054418968000 -> 132054465089536
	132054465089536 [label=AccumulateGrad]
	132054465089392 -> 132057621345488
}
