digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	135252074786048 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	135251784274640 [label=AddmmBackward0]
	135251784282704 -> 135251784274640
	135251782734688 [label="fc.bias
 (1000)" fillcolor=lightblue]
	135251782734688 -> 135251784282704
	135251784282704 [label=AccumulateGrad]
	135251784276080 -> 135251784274640
	135251784276080 [label=ViewBackward0]
	135251784273536 -> 135251784276080
	135251784273536 [label=MeanBackward1]
	135255769583328 -> 135251784273536
	135255769583328 [label=ReluBackward0]
	135251784282176 -> 135255769583328
	135251784282176 [label=AddBackward0]
	135251782512368 -> 135251784282176
	135251782512368 [label=NativeBatchNormBackward0]
	135251782507376 -> 135251782512368
	135251782507376 [label=ConvolutionBackward0]
	135251782512992 -> 135251782507376
	135251782512992 [label=ReluBackward0]
	135251782500560 -> 135251782512992
	135251782500560 [label=NativeBatchNormBackward0]
	135251782502960 -> 135251782500560
	135251782502960 [label=ConvolutionBackward0]
	135251782505504 -> 135251782502960
	135251782505504 [label=ReluBackward0]
	135251782515152 -> 135251782505504
	135251782515152 [label=AddBackward0]
	135251782504064 -> 135251782515152
	135251782504064 [label=NativeBatchNormBackward0]
	135251782516064 -> 135251782504064
	135251782516064 [label=ConvolutionBackward0]
	135251782514720 -> 135251782516064
	135251782514720 [label=ReluBackward0]
	135251782514672 -> 135251782514720
	135251782514672 [label=NativeBatchNormBackward0]
	135251782501952 -> 135251782514672
	135251782501952 [label=ConvolutionBackward0]
	135251782505312 -> 135251782501952
	135251782505312 [label=ReluBackward0]
	135251782510784 -> 135251782505312
	135251782510784 [label=AddBackward0]
	135251782504640 -> 135251782510784
	135251782504640 [label=NativeBatchNormBackward0]
	135251782512320 -> 135251782504640
	135251782512320 [label=ConvolutionBackward0]
	135251782505264 -> 135251782512320
	135251782505264 [label=ReluBackward0]
	135251782510400 -> 135251782505264
	135251782510400 [label=NativeBatchNormBackward0]
	135251782512176 -> 135251782510400
	135251782512176 [label=ConvolutionBackward0]
	135251782512800 -> 135251782512176
	135251782512800 [label=ReluBackward0]
	135251782509152 -> 135251782512800
	135251782509152 [label=AddBackward0]
	135251782501088 -> 135251782509152
	135251782501088 [label=NativeBatchNormBackward0]
	135251782509584 -> 135251782501088
	135251782509584 [label=ConvolutionBackward0]
	135251782509968 -> 135251782509584
	135251782509968 [label=ReluBackward0]
	135251782510160 -> 135251782509968
	135251782510160 [label=NativeBatchNormBackward0]
	135251782510448 -> 135251782510160
	135251782510448 [label=ConvolutionBackward0]
	135251782510832 -> 135251782510448
	135251782510832 [label=ReluBackward0]
	135251782511024 -> 135251782510832
	135251782511024 [label=AddBackward0]
	135251782512560 -> 135251782511024
	135251782512560 [label=NativeBatchNormBackward0]
	135251782503584 -> 135251782512560
	135251782503584 [label=ConvolutionBackward0]
	135251782503200 -> 135251782503584
	135251782503200 [label=ReluBackward0]
	135251782860960 -> 135251782503200
	135251782860960 [label=NativeBatchNormBackward0]
	135251782863456 -> 135251782860960
	135251782863456 [label=ConvolutionBackward0]
	135251782503680 -> 135251782863456
	135251782503680 [label=ReluBackward0]
	135251782863168 -> 135251782503680
	135251782863168 [label=AddBackward0]
	135251782863072 -> 135251782863168
	135251782863072 [label=NativeBatchNormBackward0]
	135251782862928 -> 135251782863072
	135251782862928 [label=ConvolutionBackward0]
	135251782862736 -> 135251782862928
	135251782862736 [label=ReluBackward0]
	135251782862592 -> 135251782862736
	135251782862592 [label=NativeBatchNormBackward0]
	135251782862496 -> 135251782862592
	135251782862496 [label=ConvolutionBackward0]
	135251782862304 -> 135251782862496
	135251782862304 [label=ReluBackward0]
	135251782862160 -> 135251782862304
	135251782862160 [label=AddBackward0]
	135251782861968 -> 135251782862160
	135251782861968 [label=NativeBatchNormBackward0]
	135251782861632 -> 135251782861968
	135251782861632 [label=ConvolutionBackward0]
	135251782861200 -> 135251782861632
	135251782861200 [label=ReluBackward0]
	135251782863648 -> 135251782861200
	135251782863648 [label=NativeBatchNormBackward0]
	135251782863744 -> 135251782863648
	135251782863744 [label=ConvolutionBackward0]
	135251782862112 -> 135251782863744
	135251782862112 [label=ReluBackward0]
	135251782864032 -> 135251782862112
	135251782864032 [label=AddBackward0]
	135251782864128 -> 135251782864032
	135251782864128 [label=NativeBatchNormBackward0]
	135251782864272 -> 135251782864128
	135251782864272 [label=ConvolutionBackward0]
	135251782864464 -> 135251782864272
	135251782864464 [label=ReluBackward0]
	135251782864608 -> 135251782864464
	135251782864608 [label=NativeBatchNormBackward0]
	135251782864704 -> 135251782864608
	135251782864704 [label=ConvolutionBackward0]
	135251782864080 -> 135251782864704
	135251782864080 [label=MaxPool2DWithIndicesBackward0]
	135251782864992 -> 135251782864080
	135251782864992 [label=ReluBackward0]
	135251782865088 -> 135251782864992
	135251782865088 [label=NativeBatchNormBackward0]
	135251782865184 -> 135251782865088
	135251782865184 [label=ConvolutionBackward0]
	135251782865376 -> 135251782865184
	135251784334176 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	135251784334176 -> 135251782865376
	135251782865376 [label=AccumulateGrad]
	135251782865136 -> 135251782865088
	135251784326256 [label="bn1.weight
 (64)" fillcolor=lightblue]
	135251784326256 -> 135251782865136
	135251782865136 [label=AccumulateGrad]
	135251782864800 -> 135251782865088
	135255769539248 [label="bn1.bias
 (64)" fillcolor=lightblue]
	135255769539248 -> 135251782864800
	135251782864800 [label=AccumulateGrad]
	135251782864896 -> 135251782864704
	135251782579488 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	135251782579488 -> 135251782864896
	135251782864896 [label=AccumulateGrad]
	135251782864656 -> 135251782864608
	135251782578768 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	135251782578768 -> 135251782864656
	135251782864656 [label=AccumulateGrad]
	135251782864512 -> 135251782864608
	135251782578688 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	135251782578688 -> 135251782864512
	135251782864512 [label=AccumulateGrad]
	135251782864416 -> 135251782864272
	135251782579888 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	135251782579888 -> 135251782864416
	135251782864416 [label=AccumulateGrad]
	135251782864224 -> 135251782864128
	135251782579808 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	135251782579808 -> 135251782864224
	135251782864224 [label=AccumulateGrad]
	135251782864176 -> 135251782864128
	135251782579968 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	135251782579968 -> 135251782864176
	135251782864176 [label=AccumulateGrad]
	135251782864080 -> 135251782864032
	135251782863936 -> 135251782863744
	135251782543120 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	135251782543120 -> 135251782863936
	135251782863936 [label=AccumulateGrad]
	135251782863696 -> 135251782863648
	135251782542880 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	135251782542880 -> 135251782863696
	135251782863696 [label=AccumulateGrad]
	135251782861056 -> 135251782863648
	135251782543040 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	135251782543040 -> 135251782861056
	135251782861056 [label=AccumulateGrad]
	135251782861344 -> 135251782861632
	135251782549280 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	135251782549280 -> 135251782861344
	135251782861344 [label=AccumulateGrad]
	135251782861680 -> 135251782861968
	135251782544080 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	135251782544080 -> 135251782861680
	135251782861680 [label=AccumulateGrad]
	135251782861824 -> 135251782861968
	135251782549200 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	135251782549200 -> 135251782861824
	135251782861824 [label=AccumulateGrad]
	135251782862112 -> 135251782862160
	135251782862352 -> 135251782862496
	135251782546880 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	135251782546880 -> 135251782862352
	135251782862352 [label=AccumulateGrad]
	135251782862544 -> 135251782862592
	135251782546800 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	135251782546800 -> 135251782862544
	135251782862544 [label=AccumulateGrad]
	135251782862688 -> 135251782862592
	135251782546560 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	135251782546560 -> 135251782862688
	135251782862688 [label=AccumulateGrad]
	135251782862784 -> 135251782862928
	135251782545680 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	135251782545680 -> 135251782862784
	135251782862784 [label=AccumulateGrad]
	135251782862976 -> 135251782863072
	135251782545760 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	135251782545760 -> 135251782862976
	135251782862976 [label=AccumulateGrad]
	135251782863024 -> 135251782863072
	135251782545440 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	135251782545440 -> 135251782863024
	135251782863024 [label=AccumulateGrad]
	135251782863120 -> 135251782863168
	135251782863120 [label=NativeBatchNormBackward0]
	135251782862208 -> 135251782863120
	135251782862208 [label=ConvolutionBackward0]
	135251782862304 -> 135251782862208
	135251782862400 -> 135251782862208
	135251782548240 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	135251782548240 -> 135251782862400
	135251782862400 [label=AccumulateGrad]
	135251782862640 -> 135251782863120
	135251782548320 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	135251782548320 -> 135251782862640
	135251782862640 [label=AccumulateGrad]
	135251782862880 -> 135251782863120
	135251782547760 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	135251782547760 -> 135251782862880
	135251782862880 [label=AccumulateGrad]
	135251782863264 -> 135251782863456
	135251783770960 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	135251783770960 -> 135251782863264
	135251782863264 [label=AccumulateGrad]
	135251782863552 -> 135251782860960
	135251782544960 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	135251782544960 -> 135251782863552
	135251782863552 [label=AccumulateGrad]
	135251782861872 -> 135251782860960
	135251784331856 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	135251784331856 -> 135251782861872
	135251782861872 [label=AccumulateGrad]
	135251782861392 -> 135251782503584
	135251782580368 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	135251782580368 -> 135251782861392
	135251782861392 [label=AccumulateGrad]
	135251782504496 -> 135251782512560
	135251782580288 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	135251782580288 -> 135251782504496
	135251782504496 [label=AccumulateGrad]
	135251782512656 -> 135251782512560
	135251782580448 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	135251782580448 -> 135251782512656
	135251782512656 [label=AccumulateGrad]
	135251782503680 -> 135251782511024
	135251782502528 -> 135251782510448
	135251782581648 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	135251782581648 -> 135251782502528
	135251782502528 [label=AccumulateGrad]
	135251782502144 -> 135251782510160
	135251782581568 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	135251782581568 -> 135251782502144
	135251782502144 [label=AccumulateGrad]
	135251782501856 -> 135251782510160
	135251782581728 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	135251782581728 -> 135251782501856
	135251782501856 [label=AccumulateGrad]
	135251782501664 -> 135251782509584
	135251782729808 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	135251782729808 -> 135251782501664
	135251782501664 [label=AccumulateGrad]
	135251782509488 -> 135251782501088
	135251782582208 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	135251782582208 -> 135251782509488
	135251782509488 [label=AccumulateGrad]
	135251782509296 -> 135251782501088
	135251782729888 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	135251782729888 -> 135251782509296
	135251782509296 [label=AccumulateGrad]
	135251782500992 -> 135251782509152
	135251782500992 [label=NativeBatchNormBackward0]
	135251782502816 -> 135251782500992
	135251782502816 [label=ConvolutionBackward0]
	135251782510832 -> 135251782502816
	135251782510640 -> 135251782502816
	135251782580848 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	135251782580848 -> 135251782510640
	135251782510640 [label=AccumulateGrad]
	135251782510112 -> 135251782500992
	135251782580928 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	135251782580928 -> 135251782510112
	135251782510112 [label=AccumulateGrad]
	135251782501472 -> 135251782500992
	135251782581008 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	135251782581008 -> 135251782501472
	135251782501472 [label=AccumulateGrad]
	135251782500608 -> 135251782512176
	135251782730368 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	135251782730368 -> 135251782500608
	135251782500608 [label=AccumulateGrad]
	135251782504976 -> 135251782510400
	135251782730288 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	135251782730288 -> 135251782504976
	135251782504976 [label=AccumulateGrad]
	135251782506944 -> 135251782510400
	135251782730448 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	135251782730448 -> 135251782506944
	135251782506944 [label=AccumulateGrad]
	135251782508912 -> 135251782512320
	135251782731008 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	135251782731008 -> 135251782508912
	135251782508912 [label=AccumulateGrad]
	135251782508288 -> 135251782504640
	135251782730928 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	135251782730928 -> 135251782508288
	135251782508288 [label=AccumulateGrad]
	135251782511696 -> 135251782504640
	135251782731088 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	135251782731088 -> 135251782511696
	135251782511696 [label=AccumulateGrad]
	135251782512800 -> 135251782510784
	135251782503392 -> 135251782501952
	135251782732368 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	135251782732368 -> 135251782503392
	135251782503392 [label=AccumulateGrad]
	135251782514912 -> 135251782514672
	135251782732288 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	135251782732288 -> 135251782514912
	135251782514912 [label=AccumulateGrad]
	135251782515296 -> 135251782514672
	135251782732448 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	135251782732448 -> 135251782515296
	135251782515296 [label=AccumulateGrad]
	135251782515104 -> 135251782516064
	135251782733008 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	135251782733008 -> 135251782515104
	135251782515104 [label=AccumulateGrad]
	135251782516256 -> 135251782504064
	135251782732928 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	135251782732928 -> 135251782516256
	135251782516256 [label=AccumulateGrad]
	135251782510304 -> 135251782504064
	135251782733088 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	135251782733088 -> 135251782510304
	135251782510304 [label=AccumulateGrad]
	135251782504208 -> 135251782515152
	135251782504208 [label=NativeBatchNormBackward0]
	135251782504688 -> 135251782504208
	135251782504688 [label=ConvolutionBackward0]
	135251782505312 -> 135251782504688
	135251782505024 -> 135251782504688
	135251782731568 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	135251782731568 -> 135251782505024
	135251782505024 [label=AccumulateGrad]
	135251782501280 -> 135251782504208
	135251782731648 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	135251782731648 -> 135251782501280
	135251782501280 [label=AccumulateGrad]
	135251782515200 -> 135251782504208
	135251782731728 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	135251782731728 -> 135251782515200
	135251782515200 [label=AccumulateGrad]
	135251782513712 -> 135251782502960
	135251782733488 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	135251782733488 -> 135251782513712
	135251782513712 [label=AccumulateGrad]
	135251782503824 -> 135251782500560
	135251782733408 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	135251782733408 -> 135251782503824
	135251782503824 [label=AccumulateGrad]
	135251782511168 -> 135251782500560
	135251782733568 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	135251782733568 -> 135251782511168
	135251782511168 [label=AccumulateGrad]
	135251782507760 -> 135251782507376
	135251782734128 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	135251782734128 -> 135251782507760
	135251782507760 [label=AccumulateGrad]
	135251782512128 -> 135251782512368
	135251782734048 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	135251782734048 -> 135251782512128
	135251782512128 [label=AccumulateGrad]
	135251782509440 -> 135251782512368
	135251782734208 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	135251782734208 -> 135251782509440
	135251782509440 [label=AccumulateGrad]
	135251782505504 -> 135251784282176
	135251784284672 -> 135251784274640
	135251784284672 [label=TBackward0]
	135251784273584 -> 135251784284672
	135251782734608 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	135251782734608 -> 135251784273584
	135251784273584 [label=AccumulateGrad]
	135251784274640 -> 135252074786048
}
