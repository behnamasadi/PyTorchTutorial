digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	130135078458688 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	130135126321200 [label=AddmmBackward0]
	130135126316976 -> 130135126321200
	130135078365424 [label="fc.bias
 (1000)" fillcolor=lightblue]
	130135078365424 -> 130135126316976
	130135126316976 [label=AccumulateGrad]
	130135126315920 -> 130135126321200
	130135126315920 [label=ViewBackward0]
	130135126326288 -> 130135126315920
	130135126326288 [label=MeanBackward1]
	130135126315968 -> 130135126326288
	130135126315968 [label=ReluBackward0]
	130135126313760 -> 130135126315968
	130135126313760 [label=AddBackward0]
	130135126324224 -> 130135126313760
	130135126324224 [label=NativeBatchNormBackward0]
	130135126318848 -> 130135126324224
	130135126318848 [label=ConvolutionBackward0]
	130135126619952 -> 130135126318848
	130135126619952 [label=ReluBackward0]
	130135126617696 -> 130135126619952
	130135126617696 [label=NativeBatchNormBackward0]
	130135126619616 -> 130135126617696
	130135126619616 [label=ConvolutionBackward0]
	130135126316208 -> 130135126619616
	130135126316208 [label=ReluBackward0]
	130135126617840 -> 130135126316208
	130135126617840 [label=AddBackward0]
	130135126613232 -> 130135126617840
	130135126613232 [label=NativeBatchNormBackward0]
	130135126612080 -> 130135126613232
	130135126612080 [label=ConvolutionBackward0]
	130135126618176 -> 130135126612080
	130135126618176 [label=ReluBackward0]
	130135126618848 -> 130135126618176
	130135126618848 [label=NativeBatchNormBackward0]
	130135126620336 -> 130135126618848
	130135126620336 [label=ConvolutionBackward0]
	130135126609584 -> 130135126620336
	130135126609584 [label=ReluBackward0]
	130135126608912 -> 130135126609584
	130135126608912 [label=AddBackward0]
	130135126616736 -> 130135126608912
	130135126616736 [label=NativeBatchNormBackward0]
	130135126616544 -> 130135126616736
	130135126616544 [label=ConvolutionBackward0]
	130135126615632 -> 130135126616544
	130135126615632 [label=ReluBackward0]
	130135126614528 -> 130135126615632
	130135126614528 [label=NativeBatchNormBackward0]
	130135126615440 -> 130135126614528
	130135126615440 [label=ConvolutionBackward0]
	130135126619712 -> 130135126615440
	130135126619712 [label=ReluBackward0]
	130135126613760 -> 130135126619712
	130135126613760 [label=AddBackward0]
	130135126606944 -> 130135126613760
	130135126606944 [label=NativeBatchNormBackward0]
	130135126615392 -> 130135126606944
	130135126615392 [label=ConvolutionBackward0]
	130135126605888 -> 130135126615392
	130135126605888 [label=ReluBackward0]
	130135126605984 -> 130135126605888
	130135126605984 [label=NativeBatchNormBackward0]
	130135126606032 -> 130135126605984
	130135126606032 [label=ConvolutionBackward0]
	130135126606320 -> 130135126606032
	130135126606320 [label=ReluBackward0]
	130135126606464 -> 130135126606320
	130135126606464 [label=AddBackward0]
	130135126610112 -> 130135126606464
	130135126610112 [label=NativeBatchNormBackward0]
	130135126614864 -> 130135126610112
	130135126614864 [label=ConvolutionBackward0]
	130135126615104 -> 130135126614864
	130135126615104 [label=ReluBackward0]
	130135126615248 -> 130135126615104
	130135126615248 [label=NativeBatchNormBackward0]
	130135126615296 -> 130135126615248
	130135126615296 [label=ConvolutionBackward0]
	130135126608192 -> 130135126615296
	130135126608192 [label=ReluBackward0]
	130135126615776 -> 130135126608192
	130135126615776 [label=AddBackward0]
	130135126615872 -> 130135126615776
	130135126615872 [label=NativeBatchNormBackward0]
	130135126607808 -> 130135126615872
	130135126607808 [label=ConvolutionBackward0]
	130135126616208 -> 130135126607808
	130135126616208 [label=ReluBackward0]
	130135126616304 -> 130135126616208
	130135126616304 [label=NativeBatchNormBackward0]
	130135126616352 -> 130135126616304
	130135126616352 [label=ConvolutionBackward0]
	130135126616496 -> 130135126616352
	130135126616496 [label=ReluBackward0]
	130135126616688 -> 130135126616496
	130135126616688 [label=AddBackward0]
	130135126616784 -> 130135126616688
	130135126616784 [label=NativeBatchNormBackward0]
	130135126616880 -> 130135126616784
	130135126616880 [label=ConvolutionBackward0]
	130135126617360 -> 130135126616880
	130135126617360 [label=ReluBackward0]
	130135126618320 -> 130135126617360
	130135126618320 [label=NativeBatchNormBackward0]
	130135126618944 -> 130135126618320
	130135126618944 [label=ConvolutionBackward0]
	130135126608528 -> 130135126618944
	130135126608528 [label=ReluBackward0]
	130135126620816 -> 130135126608528
	130135126620816 [label=AddBackward0]
	130135126620624 -> 130135126620816
	130135126620624 [label=NativeBatchNormBackward0]
	130135126620912 -> 130135126620624
	130135126620912 [label=ConvolutionBackward0]
	130135126621200 -> 130135126620912
	130135126621200 [label=ReluBackward0]
	130135126621344 -> 130135126621200
	130135126621344 [label=NativeBatchNormBackward0]
	130135126621440 -> 130135126621344
	130135126621440 [label=ConvolutionBackward0]
	130135126620432 -> 130135126621440
	130135126620432 [label=MaxPool2DWithIndicesBackward0]
	130135126621728 -> 130135126620432
	130135126621728 [label=ReluBackward0]
	130135126621824 -> 130135126621728
	130135126621824 [label=NativeBatchNormBackward0]
	130135126621920 -> 130135126621824
	130135126621920 [label=ConvolutionBackward0]
	130135126622112 -> 130135126621920
	130135328770400 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	130135328770400 -> 130135126622112
	130135126622112 [label=AccumulateGrad]
	130135126621872 -> 130135126621824
	130135328770720 [label="bn1.weight
 (64)" fillcolor=lightblue]
	130135328770720 -> 130135126621872
	130135126621872 [label=AccumulateGrad]
	130135126621536 -> 130135126621824
	130135328771120 [label="bn1.bias
 (64)" fillcolor=lightblue]
	130135328771120 -> 130135126621536
	130135126621536 [label=AccumulateGrad]
	130135126621632 -> 130135126621440
	130135328769920 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130135328769920 -> 130135126621632
	130135126621632 [label=AccumulateGrad]
	130135126621392 -> 130135126621344
	130135328771600 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	130135328771600 -> 130135126621392
	130135126621392 [label=AccumulateGrad]
	130135126621248 -> 130135126621344
	130135328770240 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	130135328770240 -> 130135126621248
	130135126621248 [label=AccumulateGrad]
	130135126621152 -> 130135126620912
	130135328772800 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130135328772800 -> 130135126621152
	130135126621152 [label=AccumulateGrad]
	130135126620864 -> 130135126620624
	130135328772480 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	130135328772480 -> 130135126620864
	130135126620864 [label=AccumulateGrad]
	130135126621056 -> 130135126620624
	130135328772560 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	130135328772560 -> 130135126621056
	130135126621056 [label=AccumulateGrad]
	130135126620432 -> 130135126620816
	130135126618608 -> 130135126618944
	130135126652752 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130135126652752 -> 130135126618608
	130135126618608 [label=AccumulateGrad]
	130135126618368 -> 130135126618320
	130135126650032 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	130135126650032 -> 130135126618368
	130135126618368 [label=AccumulateGrad]
	130135126617600 -> 130135126618320
	130135126650672 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	130135126650672 -> 130135126617600
	130135126617600 [label=AccumulateGrad]
	130135126616976 -> 130135126616880
	130135078356144 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130135078356144 -> 130135126616976
	130135126616976 [label=AccumulateGrad]
	130135126608672 -> 130135126616784
	130135078356304 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	130135078356304 -> 130135126608672
	130135126608672 [label=AccumulateGrad]
	130135126616832 -> 130135126616784
	130135078358464 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	130135078358464 -> 130135126616832
	130135126616832 [label=AccumulateGrad]
	130135126608528 -> 130135126616688
	130135126608288 -> 130135126616352
	130135078357184 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	130135078357184 -> 130135126608288
	130135126608288 [label=AccumulateGrad]
	130135126608144 -> 130135126616304
	130135078355664 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	130135078355664 -> 130135126608144
	130135126608144 [label=AccumulateGrad]
	130135126616256 -> 130135126616304
	130135078356064 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	130135078356064 -> 130135126616256
	130135126616256 [label=AccumulateGrad]
	130135126616112 -> 130135126607808
	130135126654432 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130135126654432 -> 130135126616112
	130135126616112 [label=AccumulateGrad]
	130135126615968 -> 130135126615872
	130135126654272 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	130135126654272 -> 130135126615968
	130135126615968 [label=AccumulateGrad]
	130135126615920 -> 130135126615872
	130135126652912 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	130135126652912 -> 130135126615920
	130135126615920 [label=AccumulateGrad]
	130135126607664 -> 130135126615776
	130135126607664 [label=NativeBatchNormBackward0]
	130135126608480 -> 130135126607664
	130135126608480 [label=ConvolutionBackward0]
	130135126616496 -> 130135126608480
	130135126616448 -> 130135126608480
	130135078357664 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	130135078357664 -> 130135126616448
	130135126616448 [label=AccumulateGrad]
	130135126608096 -> 130135126607664
	130135078358224 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	130135078358224 -> 130135126608096
	130135126608096 [label=AccumulateGrad]
	130135126616016 -> 130135126607664
	130135078357904 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	130135078357904 -> 130135126616016
	130135126616016 [label=AccumulateGrad]
	130135126615584 -> 130135126615296
	130135126596240 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130135126596240 -> 130135126615584
	130135126615584 [label=AccumulateGrad]
	130135126607088 -> 130135126615248
	130135126596080 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	130135126596080 -> 130135126607088
	130135126607088 [label=AccumulateGrad]
	130135126609152 -> 130135126615248
	130135126601920 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	130135126601920 -> 130135126609152
	130135126609152 [label=AccumulateGrad]
	130135126606800 -> 130135126614864
	130135126602960 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130135126602960 -> 130135126606800
	130135126606800 [label=AccumulateGrad]
	130135126606560 -> 130135126610112
	130135126602480 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	130135126602480 -> 130135126606560
	130135126606560 [label=AccumulateGrad]
	130135126614720 -> 130135126610112
	130135126604720 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	130135126604720 -> 130135126614720
	130135126614720 [label=AccumulateGrad]
	130135126608192 -> 130135126606464
	130135126606272 -> 130135126606032
	130135126603920 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	130135126603920 -> 130135126606272
	130135126606272 [label=AccumulateGrad]
	130135126614192 -> 130135126605984
	130135126603440 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	130135126603440 -> 130135126614192
	130135126614192 [label=AccumulateGrad]
	130135126605936 -> 130135126605984
	130135126603680 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	130135126603680 -> 130135126605936
	130135126605936 [label=AccumulateGrad]
	130135126614048 -> 130135126615392
	130135126598720 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	130135126598720 -> 130135126614048
	130135126614048 [label=AccumulateGrad]
	130135126610400 -> 130135126606944
	130135126598160 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	130135126598160 -> 130135126610400
	130135126610400 [label=AccumulateGrad]
	130135126617312 -> 130135126606944
	130135126598000 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	130135126598000 -> 130135126617312
	130135126617312 [label=AccumulateGrad]
	130135126619040 -> 130135126613760
	130135126619040 [label=NativeBatchNormBackward0]
	130135126614624 -> 130135126619040
	130135126614624 [label=ConvolutionBackward0]
	130135126606320 -> 130135126614624
	130135126614384 -> 130135126614624
	130135126599920 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	130135126599920 -> 130135126614384
	130135126614384 [label=AccumulateGrad]
	130135126609344 -> 130135126619040
	130135126601040 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	130135126601040 -> 130135126609344
	130135126609344 [label=AccumulateGrad]
	130135126618464 -> 130135126619040
	130135126601680 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	130135126601680 -> 130135126618464
	130135126618464 [label=AccumulateGrad]
	130135126616592 -> 130135126615440
	130135078371024 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	130135078371024 -> 130135126616592
	130135126616592 [label=AccumulateGrad]
	130135126615152 -> 130135126614528
	130135078370944 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	130135078370944 -> 130135126615152
	130135126615152 [label=AccumulateGrad]
	130135126607040 -> 130135126614528
	130135078370784 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	130135078370784 -> 130135126607040
	130135126607040 [label=AccumulateGrad]
	130135126616160 -> 130135126616544
	130135078370144 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	130135078370144 -> 130135126616160
	130135126616160 [label=AccumulateGrad]
	130135126617648 -> 130135126616736
	130135078370304 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	130135078370304 -> 130135126617648
	130135126617648 [label=AccumulateGrad]
	130135126617024 -> 130135126616736
	130135078370064 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	130135078370064 -> 130135126617024
	130135126617024 [label=AccumulateGrad]
	130135126619712 -> 130135126608912
	130135126620144 -> 130135126620336
	130135078368384 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	130135078368384 -> 130135126620144
	130135126620144 [label=AccumulateGrad]
	130135126610448 -> 130135126618848
	130135078368544 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	130135078368544 -> 130135126610448
	130135126610448 [label=AccumulateGrad]
	130135126618512 -> 130135126618848
	130135078368304 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	130135078368304 -> 130135126618512
	130135126618512 [label=AccumulateGrad]
	130135126617936 -> 130135126612080
	130135078367744 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	130135078367744 -> 130135126617936
	130135126617936 [label=AccumulateGrad]
	130135126612032 -> 130135126613232
	130135078367584 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	130135078367584 -> 130135126612032
	130135126612032 [label=AccumulateGrad]
	130135126611888 -> 130135126613232
	130135078367664 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	130135078367664 -> 130135126611888
	130135126611888 [label=AccumulateGrad]
	130135126612944 -> 130135126617840
	130135126612944 [label=NativeBatchNormBackward0]
	130135126617792 -> 130135126612944
	130135126617792 [label=ConvolutionBackward0]
	130135126609584 -> 130135126617792
	130135126610688 -> 130135126617792
	130135078369264 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	130135078369264 -> 130135126610688
	130135126610688 [label=AccumulateGrad]
	130135126611504 -> 130135126612944
	130135078369344 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	130135078369344 -> 130135126611504
	130135126611504 [label=AccumulateGrad]
	130135126612128 -> 130135126612944
	130135078369184 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	130135078369184 -> 130135126612128
	130135126612128 [label=AccumulateGrad]
	130135126611216 -> 130135126619616
	130135078366944 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	130135078366944 -> 130135126611216
	130135126611216 [label=AccumulateGrad]
	130135126610928 -> 130135126617696
	130135078367104 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	130135078367104 -> 130135126610928
	130135126610928 [label=AccumulateGrad]
	130135126619280 -> 130135126617696
	130135078366864 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	130135078366864 -> 130135126619280
	130135126619280 [label=AccumulateGrad]
	130135126609536 -> 130135126318848
	130135078366224 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	130135078366224 -> 130135126609536
	130135126609536 [label=AccumulateGrad]
	130135126315488 -> 130135126324224
	130135078366304 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	130135078366304 -> 130135126315488
	130135126315488 [label=AccumulateGrad]
	130135126317936 -> 130135126324224
	130135078365984 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	130135078365984 -> 130135126317936
	130135126317936 [label=AccumulateGrad]
	130135126316208 -> 130135126313760
	130135126316448 -> 130135126321200
	130135126316448 [label=TBackward0]
	130135126321632 -> 130135126316448
	130135078365584 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	130135078365584 -> 130135126321632
	130135126321632 [label=AccumulateGrad]
	130135126321200 -> 130135078458688
}
