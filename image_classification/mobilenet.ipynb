{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca857924-8137-451b-b56f-a510b04f69a2",
   "metadata": {},
   "source": [
    "# **MobileNet**\n",
    "**MobileNet** is a family of lightweight convolutional neural networks designed for **mobile and embedded vision applications**, where **computational efficiency** and **model size** are crucial.\n",
    "\n",
    "Its key innovation is the **depthwise separable convolution**, which drastically reduces computation and parameters compared to standard convolutions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "In a standard convolutional layer, for an input feature map of size\n",
    "$$\n",
    "H \\times W \\times M\n",
    "$$\n",
    "(where $ M $ is the number of input channels), and $ N $ output channels, a kernel of size $ K \\times K $ requires:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}} = H \\times W \\times M \\times N \\times K^2\n",
    "$$\n",
    "\n",
    "This is computationally expensive, especially for large ( M, N, K ).\n",
    "\n",
    "MobileNet replaces this with **depthwise separable convolution**, which breaks the convolution into two simpler steps:\n",
    "\n",
    "1. **Depthwise Convolution** — apply a single ( K \\times K ) filter per input channel (no mixing between channels).\n",
    "2. **Pointwise Convolution** — a ( 1 \\times 1 ) convolution to combine the output of depthwise convolution across channels.\n",
    "\n",
    "This decomposition reduces computation by roughly:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} + \\frac{1}{K^2}\n",
    "$$\n",
    "\n",
    "For typical values (e.g., ( K = 3, N \\gg 1 )), this is about **8–9× less computation**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 2. MobileNetV1 — Full Architecture\n",
    "\n",
    "MobileNetV1 (2017) is a simple **stack of depthwise-separable convolutions** with gradually increasing channel width and downsampling at certain stages.\n",
    "\n",
    "#### Structure\n",
    "\n",
    "|     # | Type                       | Kernel / Stride | Output Channels | Input Size (for 224×224 input) |\n",
    "| ----: | -------------------------- | --------------- | --------------: | ------------------------------ |\n",
    "|     1 | Conv2D                     | 3×3 / 2         |              32 | 112×112×32                     |\n",
    "|     2 | Depthwise Conv             | 3×3 / 1         |              32 | 112×112×32                     |\n",
    "|     3 | Pointwise Conv             | 1×1 / 1         |              64 | 112×112×64                     |\n",
    "|     4 | Depthwise Conv             | 3×3 / 2         |              64 | 56×56×64                       |\n",
    "|     5 | Pointwise Conv             | 1×1 / 1         |             128 | 56×56×128                      |\n",
    "|     6 | Depthwise Conv             | 3×3 / 1         |             128 | 56×56×128                      |\n",
    "|     7 | Pointwise Conv             | 1×1 / 1         |             128 | 56×56×128                      |\n",
    "|     8 | Depthwise Conv             | 3×3 / 2         |             128 | 28×28×128                      |\n",
    "|     9 | Pointwise Conv             | 1×1 / 1         |             256 | 28×28×256                      |\n",
    "|    10 | Depthwise Conv             | 3×3 / 1         |             256 | 28×28×256                      |\n",
    "|    11 | Pointwise Conv             | 1×1 / 1         |             256 | 28×28×256                      |\n",
    "|    12 | Depthwise Conv             | 3×3 / 2         |             256 | 14×14×256                      |\n",
    "|    13 | Pointwise Conv             | 1×1 / 1         |             512 | 14×14×512                      |\n",
    "| 14–18 | [Depthwise + Pointwise] ×5 | 3×3 / 1         |             512 | 14×14×512                      |\n",
    "|    19 | Depthwise Conv             | 3×3 / 2         |             512 | 7×7×512                        |\n",
    "|    20 | Pointwise Conv             | 1×1 / 1         |            1024 | 7×7×1024                       |\n",
    "|    21 | Depthwise Conv             | 3×3 / 1         |            1024 | 7×7×1024                       |\n",
    "|    22 | Pointwise Conv             | 1×1 / 1         |            1024 | 7×7×1024                       |\n",
    "|    23 | AvgPool                    | 7×7             |            1024 | 1×1×1024                       |\n",
    "|    24 | Fully Connected            | —               |            1000 | 1000 classes                   |\n",
    "\n",
    "**Total parameters:** ~4.2 million\n",
    "**FLOPs:** ~569 million\n",
    "\n",
    "So: it’s essentially **a deep stack of depthwise-separable convs**, with downsampling every few layers and a final global average pooling before classification.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Building Block\n",
    "\n",
    "Each **MobileNet block** (except the first layer) follows:\n",
    "$$\n",
    "\\text{Conv}*{3\\times3}^{dw} \\rightarrow \\text{BN} \\rightarrow \\text{ReLU6} \\rightarrow \\text{Conv}*{1\\times1}^{pw} \\rightarrow \\text{BN} \\rightarrow \\text{ReLU6}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* **ReLU6** is used instead of ReLU to improve robustness on low-precision devices.\n",
    "* **BN** stands for batch normalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Width and Resolution Multipliers\n",
    "\n",
    "MobileNet introduces two hyperparameters to trade off accuracy vs. efficiency:\n",
    "\n",
    "1. **Width Multiplier (α)** — scales the number of channels:\n",
    "   $$\n",
    "   M' = \\alpha M, \\quad N' = \\alpha N\n",
    "   $$\n",
    "   Smaller α → fewer parameters and computations.\n",
    "\n",
    "2. **Resolution Multiplier (ρ)** — scales the input image size:\n",
    "   $$\n",
    "   H' = \\rho H, \\quad W' = \\rho W\n",
    "   $$\n",
    "   Lower resolution → faster inference.\n",
    "\n",
    "Example:\n",
    "MobileNet-V1 with α=0.75 and ρ=0.5 runs much faster than the full model but with slightly lower accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.  MobileNetV2 —  Inverted Residuals and Linear Bottlenecks\n",
    "\n",
    "**MobileNetV2 (2018)** improves over V1 using two new ideas:\n",
    "\n",
    "#### (a) Bottleneck Residual Block\n",
    "\n",
    "Instead of a simple depthwise separable block, V2 uses:\n",
    "$$\n",
    "\\text{1×1 expansion} \\rightarrow \\text{3×3 depthwise} \\rightarrow \\text{1×1 projection}\n",
    "$$\n",
    "\n",
    "This expands channels by a factor ( t ) (typically 6), applies depthwise convolution, then projects back to a low-dimensional space.\n",
    "\n",
    "#### (b) Linear Bottleneck\n",
    "\n",
    "After projection, **no ReLU** is applied at the end — this preserves information that would otherwise be lost due to non-linearity in a narrow bottleneck space.\n",
    "\n",
    "---\n",
    "\n",
    "MobileNetV2 (2018) introduced **inverted residual bottlenecks** with expansion and projection.\n",
    "\n",
    "Each block has parameters:\n",
    "\n",
    "* **t**: expansion factor\n",
    "* **c**: output channels\n",
    "* **n**: number of repeats\n",
    "* **s**: stride for the first block\n",
    "\n",
    "### Structure\n",
    "\n",
    "| Stage | Input      | Operator       |  t  |   c  |  n  |  s  |\n",
    "| :---- | :--------- | :------------- | :-: | :--: | :-: | :-: |\n",
    "| 0     | 224×224×3  | Conv2D 3×3     |  —  |  32  |  1  |  2  |\n",
    "| 1     | 112×112×32 | Bottleneck     |  1  |  16  |  1  |  1  |\n",
    "| 2     | 112×112×16 | Bottleneck     |  6  |  24  |  2  |  2  |\n",
    "| 3     | 56×56×24   | Bottleneck     |  6  |  32  |  3  |  2  |\n",
    "| 4     | 28×28×32   | Bottleneck     |  6  |  64  |  4  |  2  |\n",
    "| 5     | 14×14×64   | Bottleneck     |  6  |  96  |  3  |  1  |\n",
    "| 6     | 14×14×96   | Bottleneck     |  6  |  160 |  3  |  2  |\n",
    "| 7     | 7×7×160    | Bottleneck     |  6  |  320 |  1  |  1  |\n",
    "| 8     | 7×7×320    | Conv2D 1×1     |  —  | 1280 |  1  |  1  |\n",
    "| 9     | 7×7×1280   | Global AvgPool |  —  | 1280 |  —  |  —  |\n",
    "| 10    | 1×1×1280   | FC + Softmax   |  —  | 1000 |  —  |  —  |\n",
    "\n",
    "Each *Bottleneck* consists of:\n",
    "$$\n",
    "1\\times1\\ \\text{Conv (expand)} \\rightarrow 3\\times3\\ \\text{Depthwise} \\rightarrow 1\\times1\\ \\text{Conv (project)}\n",
    "$$\n",
    "and uses a **residual connection** if stride = 1 and input/output channels match.\n",
    "\n",
    "**Total parameters:** ~3.4 million\n",
    "**FLOPs:** ~300 million\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. MobileNetV3 – Efficient Search and Squeeze-Excitation\n",
    "\n",
    "**MobileNetV3 (2019)** uses **Neural Architecture Search (NAS)** and **Squeeze-and-Excitation (SE)** blocks for improved accuracy/efficiency.\n",
    "\n",
    "Key components:\n",
    "\n",
    "* **SE Block**: channel attention mechanism to reweight features.\n",
    "* **Hard-Swish (h-swish)** activation: a computationally efficient approximation of Swish:\n",
    "  $$\n",
    "  \\text{h-swish}(x) = x \\cdot \\frac{\\text{ReLU6}(x + 3)}{6}\n",
    "  $$\n",
    "* Mix of **bottleneck residuals (from V2)** and **efficient NAS-designed layers**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "MobileNetV3 (2019) is the result of **Neural Architecture Search** plus **SE blocks** and **h-swish activation**. It has two main variants: **Large** and **Small**.\n",
    "\n",
    "Below is **MobileNetV3-Large** (for 224×224 input):\n",
    "\n",
    "| Stage | Operator         |  k  | exp |   c  |  SE |    NL   |  s  |\n",
    "| :---- | :--------------- | :-: | :-: | :--: | :-: | :-----: | :-: |\n",
    "| 0     | Conv2D           | 3×3 |  —  |  16  |  —  | h-swish |  2  |\n",
    "| 1     | Bottleneck       | 3×3 |  16 |  16  |  No |   ReLU  |  1  |\n",
    "| 2     | Bottleneck       | 3×3 |  64 |  24  |  No |   ReLU  |  2  |\n",
    "| 3     | Bottleneck       | 3×3 |  72 |  24  |  No |   ReLU  |  1  |\n",
    "| 4     | Bottleneck       | 5×5 |  72 |  40  | Yes |   ReLU  |  2  |\n",
    "| 5     | Bottleneck       | 5×5 | 120 |  40  | Yes |   ReLU  |  1  |\n",
    "| 6     | Bottleneck       | 5×5 | 120 |  40  | Yes |   ReLU  |  1  |\n",
    "| 7     | Bottleneck       | 3×3 | 240 |  80  |  No | h-swish |  2  |\n",
    "| 8     | Bottleneck       | 3×3 | 200 |  80  |  No | h-swish |  1  |\n",
    "| 9     | Bottleneck       | 3×3 | 184 |  80  |  No | h-swish |  1  |\n",
    "| 10    | Bottleneck       | 3×3 | 184 |  80  |  No | h-swish |  1  |\n",
    "| 11    | Bottleneck       | 3×3 | 480 |  112 | Yes | h-swish |  1  |\n",
    "| 12    | Bottleneck       | 3×3 | 672 |  160 | Yes | h-swish |  2  |\n",
    "| 13    | Bottleneck       | 3×3 | 960 |  160 | Yes | h-swish |  1  |\n",
    "| 14    | Conv2D           | 1×1 |  —  |  960 |  —  | h-swish |  1  |\n",
    "| 15    | Pool + SE + Conv |  —  |  —  | 1280 |  —  | h-swish |  —  |\n",
    "| 16    | FC + Softmax     |  —  |  —  | 1000 |  —  |    —    |  —  |\n",
    "\n",
    "**MobileNetV3-Small** is similar but optimized for lower latency and smaller memory footprint.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a1bf5-1296-498d-8526-a5adeedfe190",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5. Comparison Summary\n",
    "\n",
    "| Version     | Key Innovation                        | Approx. Params | Typical Use                     |\n",
    "| ----------- | ------------------------------------- | -------------- | ------------------------------- |\n",
    "| MobileNetV1 | Depthwise separable conv              | ~4.2M          | Simple, fast models             |\n",
    "| MobileNetV2 | Inverted residual + linear bottleneck | ~3.4M          | Balance accuracy & speed        |\n",
    "| MobileNetV3 | NAS + SE + h-swish                    | ~5.4M          | Most accurate, mobile-optimized |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. When to Use MobileNet\n",
    "\n",
    "* **Real-time inference on edge devices** (phones, drones, embedded systems).\n",
    "* **Feature extractor in lightweight pipelines** (e.g., object detection with SSD or segmentation with DeepLab).\n",
    "* **Transfer learning** for small datasets where training from scratch is impractical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e2443-4308-4564-8153-057552020bdf",
   "metadata": {},
   "source": [
    "## **7. MobileNetV3 Variants From timm**\n",
    "\n",
    "These are all **MobileNetV3** variants from the [timm](https://rwightman.github.io/pytorch-image-models/) library, but each suffix encodes how and on what dataset the model was trained or fine-tuned.\n",
    "Let’s decode each part systematically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373b672b-dd7d-4b72-95f9-8330c2b649e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenetv3_large_100.miil_in21k\n",
      "mobilenetv3_large_100.miil_in21k_ft_in1k\n",
      "mobilenetv3_large_100.ra4_e3600_r224_in1k\n",
      "mobilenetv3_large_100.ra_in1k\n",
      "mobilenetv3_large_150d.ra4_e3600_r256_in1k\n",
      "mobilenetv3_rw.rmsp_in1k\n",
      "mobilenetv3_small_050.lamb_in1k\n",
      "mobilenetv3_small_075.lamb_in1k\n",
      "mobilenetv3_small_100.lamb_in1k\n",
      "tf_mobilenetv3_large_075.in1k\n",
      "tf_mobilenetv3_large_100.in1k\n",
      "tf_mobilenetv3_large_minimal_100.in1k\n",
      "tf_mobilenetv3_small_075.in1k\n",
      "tf_mobilenetv3_small_100.in1k\n",
      "tf_mobilenetv3_small_minimal_100.in1k\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "all_mobilenetV3 = timm.list_models(\"*mobilenetv3*\", pretrained=True)\n",
    "for m in all_mobilenetV3:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a032bf-23b8-45cb-b016-22e8668bc618",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Models and Name Explanation\n",
    "#### **8.1. Base model name**\n",
    "\n",
    "Example: `mobilenetv3_large_100`\n",
    "\n",
    "* **mobilenetv3** → Architecture version (MobileNetV3, introduced by Howard et al. 2019).\n",
    "* **large / small** → Two design variants:\n",
    "\n",
    "  * *large*: higher accuracy, heavier.\n",
    "  * *small*: lighter, for mobile CPUs.\n",
    "* **100 / 075 / 050 / 150d** → Width multiplier:\n",
    "\n",
    "  * `100` = 1.0× (default width)\n",
    "  * `075` = 0.75× narrower\n",
    "  * `050` = 0.5× narrower\n",
    "  * `150d` = 1.5× wider (from “d” = **dilated/expanded** version in timm).\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Prefixes**\n",
    "\n",
    "Example: `tf_mobilenetv3_large_075`\n",
    "\n",
    "* **tf_** → TensorFlow-converted weights\n",
    "  These models come from the **TensorFlow** (TF-Slim / TF-Hub) implementation of MobileNetV3.\n",
    "  They match TF preprocessing (e.g., normalization to [0, 1], not ImageNet mean/std).\n",
    "\n",
    "* **mobilenetv3_rw** → “rw” = Ross Wightman, the timm author’s own re-implementation\n",
    "  Usually trained from scratch or fine-tuned using timm’s standard recipe.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.3. Training recipe / dataset tags**\n",
    "\n",
    "| Suffix                                  | Meaning                                                                                                                                              |\n",
    "| :-------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **in1k**                                | Trained on ImageNet-1k (1.28 M images / 1000 classes).                                                                                               |\n",
    "| **in21k**                               | Pretrained on ImageNet-21k (14 M images / 21 000 classes).                                                                                           |\n",
    "| **ft_in1k**                             | “Fine-tuned on ImageNet-1k” after pretraining on a larger dataset (e.g., IN-21k).                                                                    |\n",
    "| **miil**                                | Trained by **MIIL** (Mean Images Lab, from the paper “Training ImageNet-21k Models for ImageNet-1k Classification”), known for high-quality recipes. |\n",
    "| **ra_in1k**                             | “RA” = **RandAugment** recipe from timm (`train_config=’ra’`).                                                                                       |\n",
    "| **ra4_e3600_r224_in1k**                 | Detailed training recipe:                                                                                                                            |\n",
    "| – **ra4** → RandAugment magnitude = 4.  |                                                                                                                                                      |\n",
    "| – **e3600** → Trained for 3600 epochs.  |                                                                                                                                                      |\n",
    "| – **r224** → Input resolution = 224 px. |                                                                                                                                                      |\n",
    "| – **in1k** → Dataset = ImageNet-1k.     |                                                                                                                                                      |\n",
    "| **rmsp**                                | Optimizer = **RMSProp** (original TensorFlow MobileNet recipe).                                                                                      |\n",
    "| **lamb**                                | Optimizer = **LAMB** (Layer-wise Adaptive Moments optimizer) used for small-model training stability.                                                |\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.4. Minimal variants**\n",
    "\n",
    "Example: `tf_mobilenetv3_large_minimal_100.in1k`\n",
    "\n",
    "* **minimal** → Uses **“minimalistic” MobileNetV3** (no squeeze-and-excitation, no h-swish).\n",
    "  This reduces complexity and latency, sacrificing some accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.5. Putting it all together**\n",
    "\n",
    "| Model                                          | Meaning                                                                       |\n",
    "| :--------------------------------------------- | :---------------------------------------------------------------------------- |\n",
    "| **mobilenetv3_large_100.miil_in21k**           | MobileNetV3-Large 1.0× pretrained on ImageNet-21k by MIIL team.               |\n",
    "| **mobilenetv3_large_100.miil_in21k_ft_in1k**   | Same as above, then fine-tuned on ImageNet-1k.                                |\n",
    "| **mobilenetv3_large_100.ra4_e3600_r224_in1k**  | Large 1.0× trained on IN-1k using RandAugment m4, 3600 epochs, 224 px inputs. |\n",
    "| **mobilenetv3_large_100.ra_in1k**              | Large 1.0× trained on IN-1k with standard RandAugment recipe.                 |\n",
    "| **mobilenetv3_large_150d.ra4_e3600_r256_in1k** | 1.5× wider variant, trained 3600 epochs on 256 px inputs.                     |\n",
    "| **mobilenetv3_rw.rmsp_in1k**                   | Ross Wightman’s re-implementation trained with RMSProp on IN-1k.              |\n",
    "| **mobilenetv3_small_050.lamb_in1k**            | Small 0.5× trained with LAMB optimizer on IN-1k.                              |\n",
    "| **mobilenetv3_small_075.lamb_in1k**            | Small 0.75× trained with LAMB optimizer.                                      |\n",
    "| **mobilenetv3_small_100.lamb_in1k**            | Small 1.0× trained with LAMB optimizer.                                       |\n",
    "| **tf_mobilenetv3_large_075.in1k**              | TF-converted Large 0.75× trained on IN-1k.                                    |\n",
    "| **tf_mobilenetv3_large_100.in1k**              | TF-converted Large 1.0× on IN-1k.                                             |\n",
    "| **tf_mobilenetv3_large_minimal_100.in1k**      | TF-converted minimalistic Large 1.0× on IN-1k.                                |\n",
    "| **tf_mobilenetv3_small_075.in1k**              | TF-converted Small 0.75× on IN-1k.                                            |\n",
    "| **tf_mobilenetv3_small_100.in1k**              | TF-converted Small 1.0× on IN-1k.                                             |\n",
    "| **tf_mobilenetv3_small_minimal_100.in1k**      | TF-converted minimalistic Small 1.0× on IN-1k.                                |\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.6. Which to choose**\n",
    "\n",
    "| Goal                                      | Recommended variant                        |\n",
    "| :---------------------------------------- | :----------------------------------------- |\n",
    "| Best accuracy overall                     | `mobilenetv3_large_100.miil_in21k_ft_in1k` |\n",
    "| Lightweight for mobile apps               | `tf_mobilenetv3_small_075.in1k`            |\n",
    "| Balanced speed / accuracy                 | `mobilenetv3_large_100.ra_in1k`            |\n",
    "| Training from scratch                     | `mobilenetv3_rw.rmsp_in1k`                 |\n",
    "| Low-latency inference (no SE, no h-swish) | `tf_mobilenetv3_large_minimal_100.in1k`    |\n",
    "| fine-tune on your own dataset | `miil_in21k_ft_in1k (better pretrained features)`    |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2ff6c-f6e2-4b5a-a44a-60e69bc1c88b",
   "metadata": {},
   "source": [
    "## **9. Comparison Table of accuracy (top-1) vs Model size (MB)** \n",
    "Here’s a comparison table with approximate parameters & accuracy figures for several of the timm MobileNetV3 variants you listed. Note that for some variants full metrics (e.g., size in MB, or top-1 accuracy) are not publicly listed, so the table uses available metadata and best­-known numbers.\n",
    "\n",
    "| Model                                        | Params (M)                                         | Top-1 accuracy (ImageNet-1k)                           | Notes                                                       |\n",
    "| -------------------------------------------- | -------------------------------------------------- | ------------------------------------------------------ | ----------------------------------------------------------- |\n",
    "| `mobilenetv3_large_100.ra_in1k`              | ~5.5 M ([huggingface.co][1])                       | ~75.8% ([huggingface.co][2])                           | Large 1.0×, “RA” recipe                                     |\n",
    "| `mobilenetv3_large_100.ra4_e3600_r224_in1k`  | ~5.5 M ([huggingface.co][3])                       | ~76.31% @224px, ~77.16% @256px ([GitHub][4])           | “RA4”, 3600 epochs, improved recipe                         |\n",
    "| `mobilenetv3_large_100.miil_in21k_ft_in1k`   | ~5.5 M ([huggingface.co][5])                       | (Not clearly published)                                | Pretrained on IN-21k by MIIL, then fine-tuned on IN-1k      |\n",
    "| `mobilenetv3_large_100.miil_in21k`           | ~5.5 M ([Dataloop][6])                             | (Not clearly published)                                | Pretrained on IN-21k (no fine-tune details)                 |\n",
    "| `mobilenetv3_large_150d.ra4_e3600_r256_in1k` | (Params not clearly listed)                        | ~80.94% @256px, ~81.81% @320px ([GitHub][4])           | Wider 1.5× variant, stronger accuracy                       |\n",
    "| `mobilenetv3_rw.rmsp_in1k`                   | (Params unclear)                                   | —                                                      | “rw” = Ross Wightman’s re-implementation, RMSProp optimizer |\n",
    "| `mobilenetv3_small_100.lamb_in1k`            | (Params not clearly listed)                        | —                                                      | Small 1.0× variant trained with LAMB optimizer              |\n",
    "| `tf_mobilenetv3_large_100.in1k`              | ~5.5 M (from original paper) ([huggingface.co][2]) | ~75.5% (original Google weights) ([huggingface.co][2]) | TensorFlow-style weights, baseline for MNV3 Large           |\n",
    "\n",
    "### Some observations\n",
    "\n",
    "* Many of the models cluster around **5.5 M parameters** for the “Large” 1.0× versions, regardless of training recipe.\n",
    "* Improved training recipes (e.g., RA4, more epochs, better augmentation) yield noticeable gains (≈1% top-1 improvement in some cases).\n",
    "* The wider 1.5× variant (150d) shows a much stronger jump in accuracy (≈80%+) compared to ~76–77% for 1.0× versions.\n",
    "* Models pretrained on larger datasets (IN-21k) then fine-tuned tend to have the strongest potential, though exact public metrics can be harder to find.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "* “Size in MB” is not consistently reported in the model cards; I haven’t listed MB values because they weren’t reliably published.\n",
    "* Some of the “small”, “minimal”, or “tf_” variants you listed don’t have full public metric details available in the sources I found.\n",
    "* Accuracy values may depend on evaluation resolution (224 vs 256 px) and specific test cropping/resizing protocols.\n",
    "\n",
    "---\n",
    "\n",
    "If you like, I can **scrape the full timm model listing CSV** to extract all available accuracy, param & size details for *all* your listed variants (and any missing ones) into a more complete table.\n",
    "\n",
    "[1]: https://huggingface.co/timm/mobilenetv3_large_100.ra_in1k?utm_source=chatgpt.com \"timm/mobilenetv3_large_100.ra_in1k - Hugging Face\"\n",
    "[2]: https://huggingface.co/blog/rwightman/mobilenet-baselines?utm_source=chatgpt.com \"MobileNet Baselines - Hugging Face\"\n",
    "[3]: https://huggingface.co/timm/mobilenetv3_large_100.ra4_e3600_r224_in1k?utm_source=chatgpt.com \"timm/mobilenetv3_large_100.ra4_e3600_r224_in1k - Hugging Face\"\n",
    "[4]: https://github.com/huggingface/pytorch-image-models?utm_source=chatgpt.com \"huggingface/pytorch-image-models - GitHub\"\n",
    "[5]: https://huggingface.co/timm/mobilenetv3_large_100.miil_in21k_ft_in1k?utm_source=chatgpt.com \"timm/mobilenetv3_large_100.miil_in21k_ft_in1k\"\n",
    "[6]: https://dataloop.ai/library/model/timm_mobilenetv3_large_100miil_in21k_ft_in1k/?utm_source=chatgpt.com \"Mobilenetv3 large 100.miil in21k ft in1k · Models\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
