{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdfa87b-fc82-4197-ac93-54d61122a422",
   "metadata": {},
   "source": [
    "# **MobileNet**\n",
    "\n",
    "* Standard CNNs like VGG or ResNet are accurate but computationally heavy (many parameters, FLOPs).\n",
    "* MobileNet was designed (by Google, 2017) to run efficiently on **mobile and embedded devices** with limited compute/memory while keeping good accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "##  **1. Key Idea – Depthwise Separable Convolution**\n",
    "\n",
    "MobileNet replaces standard convolutions with a cheaper two-step operation:\n",
    "\n",
    "####  **1.1. Standard convolution**\n",
    "\n",
    "For an input of size $H \\times W \\times M$ (height × width × input channels) and $N$ filters of size $k \\times k$:\n",
    "\n",
    "* **Cost (MACs)**: $H \\cdot W \\cdot M \\cdot N \\cdot k^2$\n",
    "\n",
    "####  **1.2. Depthwise Separable convolution**\n",
    "\n",
    "Split into two layers:\n",
    "\n",
    "1. **Depthwise convolution**:\n",
    "\n",
    "   * Each input channel has its own $k \\times k$ filter (no cross-channel mixing).\n",
    "   * Cost: $H \\cdot W \\cdot M \\cdot k^2$\n",
    "\n",
    "\n",
    "\n",
    "2. **Pointwise convolution** (a $1 \\times 1$ conv):\n",
    "\n",
    "   * Combines the outputs of the depthwise step across channels.\n",
    "   * Cost: $H \\cdot W \\cdot M \\cdot N$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Total cost:**\n",
    "$\n",
    "H W (M k^2 + M N)\n",
    "$\n",
    "which is much less than the standard convolution cost for typical (k=3).\n",
    "\n",
    "#### **1.3. Efficiency gain**\n",
    "\n",
    "Reduction factor ≈\n",
    "$\n",
    "\\frac{k^2 M N}{k^2 M + M N} \\quad \\text{(usually ~8–9× fewer computations)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005f0a0-9e33-43f9-aade-752d8b68d7f4",
   "metadata": {},
   "source": [
    "#### **1.4. Standard Convolution Weight Tensor shape**\n",
    "\n",
    "If the input feature map has shape **(H, W, M)** and you want **N** output channels with a kernel of size **k×k**, then:\n",
    "\n",
    "* Each **output channel** has its own filter of shape **(M, k, k)** (one k×k kernel per input channel).\n",
    "* Collectively, the weight tensor is:\n",
    "\n",
    "$$ W_\\text{shape} = (N,M,k,k) $$\n",
    "\n",
    "So yes — you need **N** such filters, each spanning all **M** input channels.\n",
    "That’s why the cost is:\n",
    "\n",
    "$$H ,W, M , N , k^2$$\n",
    "\n",
    "operations (MACs).\n",
    "\n",
    "\n",
    "\n",
    "<img src='../conv/images/06_03.png' height=\"50%\" width=\"50%\"/>\n",
    "<img src='../conv/images/06_09.png' height=\"50%\" width=\"50%\" />\n",
    "<img src='../conv/images/3_channel_conv.gif' height=\"50%\" width=\"50%\" />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.5. Depthwise separable convolution weight shapes**\n",
    "\n",
    "MobileNet breaks that heavy convolution into two:\n",
    "\n",
    "**(a) Depthwise convolution**\n",
    "\n",
    "* One filter per **input channel**, *not* per output channel.\n",
    "* Weight shape:\n",
    "\n",
    "$$ W_\\text{depthwise} = (M,1,k,k)$$\n",
    "\n",
    "so only **M** filters total, each applied to a single channel.\n",
    "\n",
    "**(b) Pointwise (1×1) convolution**\n",
    "\n",
    "* After the depthwise stage, you have still **M** channels.\n",
    "* You then mix them to get **N** channels using a 1×1 conv.\n",
    "* Weight shape:\n",
    "\n",
    "$$ W_\\text{pointwise} = (N,M,1,1)$$\n",
    "\n",
    "This is much cheaper than having $N,M,k,k$ directly.\n",
    "\n",
    "\n",
    "**Depthwise-separable (8 Groups followed by pointwise)**\n",
    "![](../conv/images/depthwise-separable-convolution-animation-3x3-kernel.gif)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.6. Visual comparison**\n",
    "\n",
    "| Stage              | Input shape | Weight shape | Output channels |\n",
    "| ------------------ | ----------- | ------------ | --------------- |\n",
    "| **Standard Conv**  | (H,W,M)     | (N,M,k,k)    | N               |\n",
    "| **Depthwise Conv** | (H,W,M)     | (M,1,k,k)    | M               |\n",
    "| **Pointwise Conv** | (H,W,M)     | (N,M,1,1)    | N               |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.7. Why cost drops**\n",
    "\n",
    "Standard conv cost:\n",
    "$H W M N k^2$\n",
    "\n",
    "Depthwise + pointwise cost:\n",
    "$H W (M k^2 + M N)$\n",
    "\n",
    "For typical numbers (k=3, M=N), the second expression is about **1/k^2 + 1/N** of the original → ~8–9× smaller.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* **Standard conv:** N filters of shape M×k×k.\n",
    "* **MobileNet (depthwise separable):** M filters of shape 1×k×k **plus** N filters of shape M×1×1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807d4c3-9033-4390-9c8b-45ff11615ed1",
   "metadata": {},
   "source": [
    "## **Numerical Example**\n",
    "\n",
    "Let’s gets at the heart of **EfficientNet** and **MobileNetV2/V3** architectures.\n",
    "\n",
    "#### 1. What is MBConv?\n",
    "\n",
    "**MBConv (Mobile Inverted Bottleneck Convolution)** was introduced in **MobileNetV2** and reused in **EfficientNet**.\n",
    "\n",
    "It’s called *inverted* because:\n",
    "\n",
    "* A normal bottleneck first **reduces** channels, then applies convolution.\n",
    "* MBConv first **expands** channels, does computation, and then **projects back** to fewer channels.\n",
    "\n",
    "---\n",
    "\n",
    "#### MBConv block structure\n",
    "\n",
    "1. **Expansion (1×1 convolution)**\n",
    "   Expands from input channels $C_{in}$ to $t \\times C_{in}$, where $t$ is the **expansion factor** (usually 6).\n",
    "\n",
    "   $$\n",
    "   X_{expand} = \\text{ReLU6}(\\text{BN}(\\text{Conv}_{1\\times1}(X_{in})))\n",
    "   $$\n",
    "\n",
    "2. **Depthwise convolution (3×3 convolution per channel)**\n",
    "   Applies spatial convolution **independently** for each channel.\n",
    "\n",
    "   $$\n",
    "   X_{depth} = \\text{ReLU6}(\\text{BN}(\\text{ConvDepthwise}_{3\\times3}(X_{expand})))\n",
    "   $$\n",
    "\n",
    "3. **Projection (1×1 convolution)**\n",
    "   Reduces back to $C_{out}$ channels.\n",
    "\n",
    "   $$\n",
    "   X_{out} = \\text{BN}(\\text{Conv}*{1\\times1}(X*{depth}))\n",
    "   $$\n",
    "\n",
    "4. **Skip connection (optional)**\n",
    "   If stride = 1 and $C_{in} = C_{out}$:\n",
    "   $$\n",
    "   Y = X_{in} + X_{out}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Is MBConv the same as Depthwise-Separable Conv?\n",
    "\n",
    "❌ **Not exactly.**\n",
    "✅ It *includes* a depthwise convolution, but **adds expansion and projection** around it.\n",
    "\n",
    "**Depthwise Separable Conv** (used in **MobileNetV1**) only has:\n",
    "\n",
    "1. Depthwise 3×3 conv\n",
    "2. Pointwise (1×1) conv\n",
    "\n",
    "**MBConv = Expansion (1×1) → Depthwise (3×3) → Projection (1×1)**\n",
    "**Depthwise Separable = Depthwise (3×3) → Pointwise (1×1)**\n",
    "\n",
    "So MBConv is a **generalized and more expressive** version of Depthwise Separable Conv.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Numerical Example\n",
    "\n",
    "Let’s take a **tiny example** to see the shapes.\n",
    "\n",
    "| Parameter        | Symbol       | Value |\n",
    "| ---------------- | ------------ | ----- |\n",
    "| Input size       | $H \\times W$ | 8 × 8 |\n",
    "| Input channels   | $C_{in}$     | 4     |\n",
    "| Output channels  | $C_{out}$    | 4     |\n",
    "| Expansion factor | $t$          | 6     |\n",
    "| Kernel size      | $k$          | 3     |\n",
    "| Stride           | 1            |       |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Expansion (1×1 conv)\n",
    "\n",
    "$$\n",
    "C_{expand} = t \\times C_{in} = 6 \\times 4 = 24\n",
    "$$\n",
    "\n",
    "Output tensor shape:\n",
    "$$\n",
    "[8, 8, 24]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Depthwise 3×3 conv\n",
    "\n",
    "Each of the 24 channels gets its own 3×3 filter → no channel mixing.\n",
    "\n",
    "Output tensor shape:\n",
    "$$\n",
    "[8, 8, 24]\n",
    "$$\n",
    "\n",
    "(assuming padding = 1, stride = 1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Projection (1×1 conv)\n",
    "\n",
    "Reduces back to 4 channels:\n",
    "\n",
    "$$\n",
    "[8, 8, 24] \\xrightarrow{\\text{Conv1×1}} [8, 8, 4]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Skip connection\n",
    "\n",
    "Since stride = 1 and input/output channels are the same (4), we add:\n",
    "\n",
    "$$\n",
    "Y = X_{in} + X_{out}\n",
    "$$\n",
    "\n",
    "Final output shape:\n",
    "$$\n",
    "[8, 8, 4]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Parameter Comparison\n",
    "\n",
    "Let’s roughly compare MBConv vs Depthwise-Separable Conv for the same example.\n",
    "\n",
    "| Layer type                         | Parameters                       |\n",
    "| ---------------------------------- | -------------------------------- |\n",
    "| Expansion 1×1 conv                 | $1×1×4×24 = 96$                  |\n",
    "| Depthwise 3×3 conv                 | $3×3×24 = 216$                   |\n",
    "| Projection 1×1 conv                | $1×1×24×4 = 96$                  |\n",
    "| **Total MBConv**                   | **408**                          |\n",
    "| Depthwise-separable (no expansion) | $3×3×4 + 1×1×4×4 = 36 + 16 = 52$ |\n",
    "\n",
    "So MBConv has **more parameters**, but also **more expressive capacity** and nonlinearities (two ReLUs, one linear projection), enabling better accuracy at similar compute.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Key Differences Summary\n",
    "\n",
    "| Feature         | Depthwise-Separable | MBConv                                                 |\n",
    "| --------------- | ------------------- | ------------------------------------------------------ |\n",
    "| Expansion       | No                  | Yes (1×1 expand to $t \\times C_{in}$)                  |\n",
    "| Nonlinearity    | After both convs    | ReLU6 after expansion and depthwise, linear projection |\n",
    "| Skip connection | Optional            | Yes (if stride=1 and $C_{in}=C_{out}$)                 |\n",
    "| Used in         | MobileNetV1         | MobileNetV2, EfficientNet                              |\n",
    "| Expressiveness  | Moderate            | High                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show this same example in **PyTorch code**, so you can visualize the tensor shapes and operations for MBConv?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906451c-4bb3-4eaa-a444-798f64ef299b",
   "metadata": {},
   "source": [
    "##  **2. MobileNet Architecture**\n",
    "\n",
    "* Built mainly from **(Depthwise conv + Pointwise conv) + BatchNorm + ReLU** blocks.\n",
    "* Ends with a fully-connected layer for classification.\n",
    "* Lightweight, fewer parameters, and faster inference.\n",
    "\n",
    "---\n",
    "##  **3. Hyperparameters for Trade-offs**\n",
    "\n",
    "MobileNet introduces two knobs to scale size vs. accuracy:\n",
    "\n",
    "1. **Width Multiplier $ \\alpha $**\n",
    "\n",
    "   * Scales the number of channels in each layer:\n",
    "     $M' = \\alpha M$, $N' = \\alpha N$.\n",
    "   * $\\alpha \\in (0,1]$ makes the network thinner.\n",
    "\n",
    "2. **Resolution Multiplier $ \\rho $**\n",
    "\n",
    "   * Scales the input image resolution:\n",
    "     $H' = \\rho H$, $W' = \\rho W$.\n",
    "   * Reduces compute quadratically.\n",
    "\n",
    "By adjusting $\\alpha$ and $\\rho$, you can deploy MobileNet variants for different devices.\n",
    "\n",
    "---\n",
    "\n",
    "##  **4. Evolution of MobileNet Versions**\n",
    "\n",
    "| Version         | Year | Key Improvements                                                                               |\n",
    "| --------------- | ---- | ---------------------------------------------------------------------------------------------- |\n",
    "| **MobileNetV1** | 2017 | Depthwise separable conv + width/resolution multipliers                                        |\n",
    "| **MobileNetV2** | 2018 | Introduced **Inverted Residuals** + **Linear Bottlenecks** (similar to ResNet but lightweight) |\n",
    "| **MobileNetV3** | 2019 | Added **SE blocks** (Squeeze-and-Excitation) + **hard-swish** activation + NAS-based search    |\n",
    "\n",
    "---\n",
    "\n",
    "##  **5. When to Use**\n",
    "\n",
    "* On-device classification or detection (phones, IoT devices).\n",
    "* As a backbone for mobile-friendly models (SSD-MobileNet, DeepLabV3-MobileNet).\n",
    "* Great for applications where latency, power, or memory are limited.\n",
    "\n",
    "---\n",
    "\n",
    "##  **6. Quick Visual of a Block**\n",
    "\n",
    "```\n",
    "Input\n",
    "  │\n",
    "Depthwise Conv (3x3, per-channel)\n",
    "  │\n",
    "BatchNorm + ReLU\n",
    "  │\n",
    "Pointwise Conv (1x1, across channels)\n",
    "  │\n",
    "BatchNorm + ReLU\n",
    "  │\n",
    "Output\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261325db-133a-4ac4-b5cc-de927e705da6",
   "metadata": {},
   "source": [
    "## **7. PyTorch grouped and depthwise Convolution**\n",
    "PyTorch’s `groups` argument in `nn.Conv2d` is exactly what lets you move from a **full convolution** to **grouped** or even **depthwise** convolutions (like MobileNet uses).\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.1. Default (groups = 1)**\n",
    "\n",
    "* **Shape:** weights = `(out_channels, in_channels, kH, kW)`\n",
    "* Every output channel sees **all** input channels.\n",
    "* This is the standard convolution we described first.\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.2 Grouped convolution (1 < groups < in_channels))**\n",
    "\n",
    "* Split the **input channels** and **output channels** into `groups` parts.\n",
    "* Each group of output channels only looks at a subset of the input channels.\n",
    "* The weight shape becomes:\n",
    "\n",
    "$$\n",
    "\\text{weight shape} = \\left( \\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}}, kH, kW \\right)\n",
    "$$\n",
    "\n",
    "* Cost drops roughly by a factor of `groups` compared to a full conv.\n",
    "\n",
    "Example:\n",
    "`nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, groups=4)`\n",
    "→ Input split into 4 groups of 16 channels, output split into 4 groups of 32 channels. Each group processes only its slice.\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.3 Depthwise convolution (groups = in_channels)**\n",
    "\n",
    "* Special case of grouped convolution.\n",
    "* You set:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels=M, out_channels=M, kernel_size=3, groups=M)\n",
    "```\n",
    "\n",
    "* Each input channel has its own filter and produces its own output channel (one-to-one).\n",
    "* This is exactly the **depthwise** step in MobileNet.\n",
    "\n",
    "Weight shape:\n",
    "\n",
    "$$\n",
    "(M,1,kH,kW)\n",
    "$$\n",
    "\n",
    "— which matches what we wrote earlier.\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.4 Depthwise separable convolution in PyTorch**\n",
    "\n",
    "You implement it as two layers:\n",
    "\n",
    "```python\n",
    "# depthwise\n",
    "self.depthwise = nn.Conv2d(in_channels=M,\n",
    "                           out_channels=M,\n",
    "                           kernel_size=3,\n",
    "                           groups=M,\n",
    "                           padding=1)\n",
    "\n",
    "# pointwise\n",
    "self.pointwise = nn.Conv2d(in_channels=M,\n",
    "                           out_channels=N,\n",
    "                           kernel_size=1)\n",
    "```\n",
    "\n",
    "This is literally a MobileNet block.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| groups value    | Name           | Effect                                           |\n",
    "| --------------- | -------------- | ------------------------------------------------ |\n",
    "| 1               | Standard conv  | Each output channel sees all input channels      |\n",
    "| g (1<g<in_ch)   | Grouped conv   | Each group sees a fraction of input channels     |\n",
    "| in_ch (=out_ch) | Depthwise conv | Each channel has its own filter (MobileNet step) |\n",
    "\n",
    "---\n",
    "\n",
    "So : `groups` in `nn.Conv2d` is PyTorch’s general mechanism for this; **depthwise convolution = groups = in_channels**.\n",
    "\n",
    "\n",
    "#### **7.5 Comparing the output shapes for `groups=1`, `groups=2`, and `groups=in_channels`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58db8757-8728-44a0-be18-67ea4377194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5]) torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(1, 4, 5, 5)  # batch=1, 4 input channels\n",
    "\n",
    "conv1 = nn.Conv2d(4, 8, kernel_size=3, groups=1, padding=1)\n",
    "conv2 = nn.Conv2d(4, 8, kernel_size=3, groups=2, padding=1)\n",
    "\n",
    "y1 = conv1(x)\n",
    "y2 = conv2(x)\n",
    "\n",
    "print(y1.shape, y2.shape)  # both (1,8,5,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bd03a-f274-4656-a292-2f352efcb388",
   "metadata": {},
   "source": [
    "`groups=2` will **not** give the same result as `groups=1` (unless you very specially choose the weights so it behaves identically).\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "---\n",
    "\n",
    "####  **7.6 What happens when `groups=1`**\n",
    "\n",
    "* All `in_channels` are connected to all `out_channels`.\n",
    "* Each filter can combine information from **every input channel**.\n",
    "\n",
    "####  **7.7 What happens when `groups=2`**\n",
    "\n",
    "* PyTorch splits the input channels into 2 equal groups.\n",
    "\n",
    "  * Group 1: channels `0..in_channels/2-1`\n",
    "  * Group 2: channels `in_channels/2..end`\n",
    "* The output channels are also split into 2 groups.\n",
    "* Each group’s filters only “see” its **own half** of the input channels; there is **no cross-talk** between groups.\n",
    "\n",
    "So effectively you’re doing two separate convolutions in parallel and then concatenating their outputs along the channel dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9210df12-27dc-46e3-af50-ed1ea175c75c",
   "metadata": {},
   "source": [
    "## **8.What “SE” Stands For**\n",
    "\n",
    "**SE block** = **Squeeze-and-Excitation block**.\n",
    "It’s a lightweight attention mechanism for CNNs introduced in the paper:\n",
    "\n",
    "> *“Squeeze-and-Excitation Networks” (Hu et al., CVPR 2018)*\n",
    "\n",
    "It improves a network’s ability to model the **importance of each channel** in feature maps.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Why We Need It**\n",
    "\n",
    "Convolutions learn spatial filters but treat all channels equally.\n",
    "Some channels might carry more relevant information for the current task.\n",
    "An SE block lets the network **recalibrate channel-wise feature responses** adaptively.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.3. How an SE Block Works**\n",
    "\n",
    "Let’s say the input feature map is $X\\in \\mathbb{R}^{H\\times W\\times C}$ (height, width, channels).\n",
    "\n",
    "#### Step A: **Squeeze** (Global information)\n",
    "\n",
    "* Perform **global average pooling** over spatial dimensions $H \\times W$ to get one value per channel.\n",
    "* This yields a vector $z\\in \\mathbb{R}^{C}$ summarizing each channel’s global response.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "z_c = \\frac{1}{H , W}\\sum_{i=1}^{H}\\sum_{j=1}^{W} X_{i,j,c}\n",
    "$$\n",
    "\n",
    "#### Step B: **Excitation** (Learn channel attention)\n",
    "\n",
    "* Pass (z) through a small **two-layer MLP**:\n",
    "\n",
    "  * First layer reduces dimension to $C/r$ (bottleneck; $r$ is reduction ratio like 16)\n",
    "  * Apply ReLU.\n",
    "  * Second layer expands back to $C$.\n",
    "  * Apply sigmoid to get weights $s\\in[0,1]^C$.\n",
    "\n",
    "$$\n",
    "s = \\sigma(W_2 , \\delta(W_1 z))\n",
    "$$\n",
    "\n",
    "where $\\delta$ is ReLU, $\\sigma$ is sigmoid.\n",
    "\n",
    "#### Step C: **Scale** (Recalibration)\n",
    "\n",
    "* Multiply the original feature map channels by the learned weights:\n",
    "\n",
    "$$\n",
    "\\tilde{X}{i,j,c} = s_c \\cdot X{i,j,c}\n",
    "$$\n",
    "\n",
    "So channels the block deems “important” get boosted; less important channels get suppressed.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.4. Visual Diagram**\n",
    "\n",
    "```\n",
    "Input Feature Map (H×W×C)\n",
    "       │\n",
    " Global Average Pooling (Squeeze)\n",
    "       ↓\n",
    " Channel Descriptor (C)\n",
    "       │\n",
    " Fully Connected (reduce C→C/r), ReLU\n",
    "       │\n",
    " Fully Connected (C/r→C), Sigmoid (Excitation)\n",
    "       ↓\n",
    " Channel Weights (C)\n",
    "       │\n",
    " Scale original feature map channel-wise\n",
    "       ↓\n",
    "Output Feature Map (H×W×C)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.5. Where It’s Used**\n",
    "\n",
    "* Originally introduced in **SENet** (ImageNet winner 2017/2018).\n",
    "* Incorporated into **MobileNetV3**, **EfficientNet** (in MBConv blocks), ResNeXt, etc.\n",
    "* Adds only a tiny computational overhead but often improves accuracy significantly.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.6. PyTorch Implementation (simple)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)        # Squeeze\n",
    "        y = self.fc(y).view(b, c, 1, 1)        # Excitation\n",
    "        return x * y                          # Scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.7. Key Takeaways**\n",
    "\n",
    "* **SE block** = channel-wise attention module.\n",
    "* Steps: **Squeeze → Excite → Scale**.\n",
    "* Helps the network emphasize informative features dynamically.\n",
    "* Tiny overhead, noticeable accuracy gain.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also explain **how SE differs from spatial attention** (e.g., CBAM)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d1ce9-0094-4030-a381-528d6744a7d5",
   "metadata": {},
   "source": [
    "## **9. Compute cost (MACs)** for a standard conv vs a **depthwise-separable** (depthwise + pointwise) conv.\n",
    "\n",
    "* **Standard conv** (in_channels = $M$, out_channels = $N$, kernel $k\\times k$, output size $H\\times W)$\n",
    "  $$\n",
    "  \\text{MACs}_{\\text{std}} = H,W,M,N,k^2\n",
    "  $$\n",
    "\n",
    "* **Depthwise-separable conv**\n",
    "  Depthwise: $H,W,M,k^2$\n",
    "  Pointwise $1×1$: $H,W,M,N$\n",
    "  $$\n",
    "  \\text{MACs}_{\\text{dw+pw}} = H,W,(M k^2 + M N) = H,W,M,(k^2 + N)\n",
    "  $$\n",
    "\n",
    "* **Ratio (how much of the standard cost remains)**\n",
    "  $$\n",
    "  \\frac{\\text{MACs}*{\\text{dw+pw}}}{\\text{MACs}*{\\text{std}}}\n",
    "  = \\frac{M(k^2+N)}{MNk^2}\n",
    "  = \\frac{k^2+N}{N k^2}\n",
    "  = \\frac{1}{k^2} + \\frac{1}{N}\n",
    "  $$\n",
    "  So the **speed-up** (std / dw+pw) is:\n",
    "  $$\n",
    "  \\text{Speed-up} = \\frac{N k^2}{k^2 + N}\n",
    "  $$\n",
    "\n",
    "Your RGB→5-channel example (M=3, N=5, k=3)\n",
    "\n",
    "* **Per-pixel MACs**\n",
    "\n",
    "  * Standard: $M N k^2 = 3 \\cdot 5 \\cdot 9 = 135$\n",
    "  * Depthwise+Pointwise: $M k^2 + M N = 3\\cdot 9 + 3 \\cdot 5 = 27 + 15 = 42$\n",
    "\n",
    "  **Ratio:** $42/135 = 14/45 \\approx 0.311$ → ~**69% fewer MACs**\n",
    "  **Speed-up:** $135/42 = 45/14 \\approx 3.21\\times$\n",
    "\n",
    "* **If output is 224×224** (same spatial size):\n",
    "\n",
    "  * Standard: $50{,}176 \\times 135 = 6{,}773{,}760$ MACs\n",
    "  * Depthwise+Pointwise: $50{,}176 \\times 42 = 2{,}107{,}392$ MACs\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* Standard conv mixes **spatial** and **cross-channel** interactions in one heavy op.\n",
    "* Depthwise handles **spatial** per-channel cheaply; pointwise (1×1) then mixes **channels**.\n",
    "* With typical (k=3) and larger $N$, the ratio $\\frac{1}{k^2}+\\frac{1}{N}$ gets close to $1/9$, i.e., ~**9×** less compute. For small $N$ (like 5), the saving is smaller $~3.2×$, as the 1×1 mixing dominates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ddefa0-b4d6-46b2-8da0-ec572f0ca72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced shape: torch.Size([1, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we want to reduce 64 channels -> 16 channels\n",
    "reduce_channels = nn.Conv2d(64, 16, kernel_size=1)\n",
    "\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "y = reduce_channels(x)\n",
    "\n",
    "print(\"Reduced shape:\", y.shape)  # (1, 16, 32, 32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
