{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4232c1-8230-4ddf-b137-891cfe4729da",
   "metadata": {},
   "source": [
    "## Squeeze-and-Excitation Networks (SENet)\n",
    "\n",
    "\n",
    "## **What ‚ÄúSE‚Äù Stands For**\n",
    "\n",
    "**SE block** = **Squeeze-and-Excitation block**.\n",
    "It‚Äôs a lightweight attention mechanism for CNNs introduced in the paper:\n",
    "\n",
    "> *‚ÄúSqueeze-and-Excitation Networks‚Äù (Hu et al., CVPR 2018)*\n",
    "\n",
    "It improves a network‚Äôs ability to model the **importance of each channel** in feature maps.\n",
    "\n",
    "\n",
    "<img src=\"images/squeeze_and_excitation_networks_SENet.png\" />\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why We Need It**\n",
    "\n",
    "Convolutions learn spatial filters but treat all channels equally.\n",
    "Some channels might carry more relevant information for the current task.\n",
    "An SE block lets the network **recalibrate channel-wise feature responses** adaptively.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How an SE Block Works**\n",
    "\n",
    "Let‚Äôs say the input feature map is $X\\in \\mathbb{R}^{H\\times W\\times C}$ (height, width, channels).\n",
    "\n",
    "#### Step A: **Squeeze** (Global information)\n",
    "\n",
    "* Perform **global average pooling** over spatial dimensions $H \\times W$ to get one value per channel.\n",
    "* This yields a vector $z\\in \\mathbb{R}^{C}$ summarizing each channel‚Äôs global response.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "z_c = \\frac{1}{H , W}\\sum_{i=1}^{H}\\sum_{j=1}^{W} X_{i,j,c}\n",
    "$$\n",
    "\n",
    "#### Step B: **Excitation** (Learn channel attention)\n",
    "\n",
    "* Pass (z) through a small **two-layer MLP**:\n",
    "\n",
    "  * First layer reduces dimension to $C/r$ (bottleneck; $r$ is reduction ratio like 16)\n",
    "  * Apply ReLU.\n",
    "  * Second layer expands back to $C$.\n",
    "  * Apply sigmoid to get weights $s\\in[0,1]^C$.\n",
    "\n",
    "$$\n",
    "s = \\sigma(W_2 , \\delta(W_1 z))\n",
    "$$\n",
    "\n",
    "where $\\delta$ is ReLU, $\\sigma$ is sigmoid.\n",
    "\n",
    "#### Step C: **Scale** (Recalibration)\n",
    "\n",
    "* Multiply the original feature map channels by the learned weights:\n",
    "\n",
    "$$\n",
    "\\tilde{X}{i,j,c} = s_c \\cdot X{i,j,c}\n",
    "$$\n",
    "\n",
    "So channels the block deems ‚Äúimportant‚Äù get boosted; less important channels get suppressed.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Visual Diagram**\n",
    "\n",
    "```\n",
    "Input Feature Map (H√óW√óC)\n",
    "       ‚îÇ\n",
    " Global Average Pooling (Squeeze)\n",
    "       ‚Üì\n",
    " Channel Descriptor (C)\n",
    "       ‚îÇ\n",
    " Fully Connected (reduce C‚ÜíC/r), ReLU\n",
    "       ‚îÇ\n",
    " Fully Connected (C/r‚ÜíC), Sigmoid (Excitation)\n",
    "       ‚Üì\n",
    " Channel Weights (C)\n",
    "       ‚îÇ\n",
    " Scale original feature map channel-wise\n",
    "       ‚Üì\n",
    "Output Feature Map (H√óW√óC)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Where It‚Äôs Used**\n",
    "\n",
    "* Originally introduced in **SENet** (ImageNet winner 2017/2018).\n",
    "* Incorporated into **MobileNetV3**, **EfficientNet** (in MBConv blocks), ResNeXt, etc.\n",
    "* Adds only a tiny computational overhead but often improves accuracy significantly.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a8bd1-14ee-4c3f-b12a-cabdfb03c8ec",
   "metadata": {},
   "source": [
    "## **Numerical Eexample**\n",
    "\n",
    "\n",
    "Assume we have a small **feature map** from some CNN layer:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{4\\times4\\times3}\n",
    "$$\n",
    "So:\n",
    "\n",
    "* Height = 4\n",
    "* Width = 4\n",
    "* Channels = 3\n",
    "\n",
    "We‚Äôll pick a **reduction ratio** $r = 3$ ‚Üí bottleneck dimension = $C / r = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Squeeze** (Global Average Pooling)\n",
    "\n",
    "Compute one average per channel:\n",
    "\n",
    "$$\n",
    "z_c = \\frac{1}{H \\times W} \\sum_{i,j} X_{i,j,c}\n",
    "$$\n",
    "\n",
    "Let‚Äôs assume the averages come out as:\n",
    "$$\n",
    "z = [2.0, 0.5, 1.0]\n",
    "$$\n",
    "\n",
    "So now we have a vector of size (3).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Excitation** (Two small fully-connected layers)\n",
    "\n",
    "**First FC layer (reduce channels)**\n",
    "\n",
    "$$\n",
    "W_1: \\mathbb{R}^{3 \\rightarrow 1}\n",
    "$$\n",
    "\n",
    "We‚Äôll use these weights for illustration:\n",
    "$$\n",
    "W_1 = [0.2, 0.4, 0.1]\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "h = W_1 \\cdot z = 0.2(2.0) + 0.4(0.5) + 0.1(1.0) = 0.4 + 0.2 + 0.1 = 0.7\n",
    "$$\n",
    "\n",
    "Apply ReLU:\n",
    "$$\n",
    "h = \\max(0, 0.7) = 0.7\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Second FC layer (expand back)**\n",
    "\n",
    "$$\n",
    "W_2: \\mathbb{R}^{1 \\rightarrow 3}\n",
    "$$\n",
    "\n",
    "Say $W_2 = [0.5, 1.0, -0.5]^T$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "s = W_2 \\cdot h = [0.5√ó0.7, 1.0√ó0.7, -0.5√ó0.7] = [0.35, 0.7, -0.35]\n",
    "$$\n",
    "\n",
    "Apply **sigmoid** to get weights between 0 and 1:\n",
    "$$\n",
    "\\sigma(s) = [0.586, 0.668, 0.413]\n",
    "$$\n",
    "\n",
    "These are our **channel attention weights**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Scale** (Channel Recalibration)\n",
    "\n",
    "Now, multiply each channel of the original feature map by its corresponding weight:\n",
    "\n",
    "| Channel | Scale weight | Example original mean | Scaled mean        |\n",
    "| ------- | ------------ | --------------------- | ------------------ |\n",
    "| 1       | 0.586        | 2.0                   | 2.0 √ó 0.586 = 1.17 |\n",
    "| 2       | 0.668        | 0.5                   | 0.5 √ó 0.668 = 0.33 |\n",
    "| 3       | 0.413        | 1.0                   | 1.0 √ó 0.413 = 0.41 |\n",
    "\n",
    "‚Üí The first channel gets boosted moderately,\n",
    "‚Üí The second a bit,\n",
    "‚Üí The third is suppressed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "* The SE block **‚Äúsqueezed‚Äù** spatial info into 3 global averages.\n",
    "* Then it **‚Äúexcited‚Äù** them with a small MLP to learn *which channels matter*.\n",
    "* Finally, it **scaled** the original channels by those learned importance weights.\n",
    "\n",
    "So the network learns to emphasize informative channels and dampen unhelpful ones ‚Äî dynamically, per input image.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08508f6a-c78c-4220-a0aa-66a33dafb666",
   "metadata": {},
   "source": [
    "#### **PyTorch Implementation (simple)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)        # Squeeze\n",
    "        y = self.fc(y).view(b, c, 1, 1)        # Excitation\n",
    "        return x * y                          # Scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4af6fa-d39b-44b9-bdc5-aeee8ea25ac9",
   "metadata": {},
   "source": [
    "## **Spatial Attention vs Spatial Attention**\n",
    "####  1. The Core Difference\n",
    "\n",
    "| Type                            | Focus                 | What it learns                         | Output shape  |\n",
    "| ------------------------------- | --------------------- | -------------------------------------- | ------------- |\n",
    "| **Squeeze-and-Excitation (SE)** | **Channel attention** | How important each *channel* is        | ( (1, 1, C) ) |\n",
    "| **Spatial Attention**           | **Spatial attention** | How important each *pixel/location* is | ( (H, W, 1) ) |\n",
    "\n",
    "In short:\n",
    "\n",
    "* **SE ‚Üí ‚ÄúWhich feature maps (channels) should I amplify?‚Äù**\n",
    "* **Spatial ‚Üí ‚ÄúWhich spatial regions (pixels) should I focus on?‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "####  **2. Mechanism Breakdown**\n",
    "\n",
    "A. **Squeeze-and-Excitation (Channel Attention)**\n",
    "\n",
    "1. **Squeeze:** Global average pooling ‚Üí summarize each channel ‚Üí vector of length (C).\n",
    "2. **Excite:** MLP ‚Üí outputs one weight per channel (values 0‚Äì1).\n",
    "3. **Scale:** Multiply each channel globally by its weight.\n",
    "\n",
    "  **Effect:** Emphasizes useful *feature types* (e.g., ‚Äúedges,‚Äù ‚Äútextures,‚Äù ‚Äúobject color‚Äù channels).\n",
    "Every pixel within a channel gets scaled equally.\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "Input:  H√óW√óC\n",
    " ‚Üì (Global Average Pool)\n",
    "Vector (1√ó1√óC)\n",
    " ‚Üì (2 FC layers + sigmoid)\n",
    "Weights (1√ó1√óC)\n",
    " ‚Üì (channel-wise multiply)\n",
    "Output: H√óW√óC\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "B. **Spatial Attention**\n",
    "\n",
    "1. **Squeeze channels:** Compute an importance map for *each spatial location* instead of each channel.\n",
    "   Typically:\n",
    "\n",
    "   * Apply average pooling and max pooling along channels ‚Üí two 2D maps (H√óW√ó1 each).\n",
    "2. **Concatenate them** and run a small 2D convolution (e.g., 7√ó7).\n",
    "3. **Sigmoid** ‚Üí get spatial attention map (H√óW√ó1).\n",
    "4. **Multiply** original feature map spatially.\n",
    "\n",
    "   **Effect:** Emphasizes *where* in the image to focus (e.g., object regions vs. background).\n",
    "Each pixel gets a unique weight.\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "Input:  H√óW√óC\n",
    " ‚Üì (Channel AvgPool + MaxPool ‚Üí concat)\n",
    "Feature (H√óW√ó2)\n",
    " ‚Üì (Conv 7√ó7 + sigmoid)\n",
    "Attention Map (H√óW√ó1)\n",
    " ‚Üì (spatial multiply)\n",
    "Output: H√óW√óC\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####  3. Comparison Table\n",
    "\n",
    "| Aspect             | Squeeze-and-Excitation (SE)           | Spatial Attention                      |\n",
    "| ------------------ | ------------------------------------- | -------------------------------------- |\n",
    "| Focus              | Channels                              | Spatial positions                      |\n",
    "| Key question       | *‚ÄúWhich feature maps are important?‚Äù* | *‚ÄúWhich image regions are important?‚Äù* |\n",
    "| Pooling used       | Global average pooling (over H, W)    | Pool over channels (avg & max)         |\n",
    "| Weight shape       | (1√ó1√óC)                               | (H√óW√ó1)                                |\n",
    "| Computational cost | Very low                              | Slightly higher (7√ó7 conv)             |\n",
    "| Used in            | EfficientNet, SENet, MobileNetV3      | CBAM, BAM, attention-based CNNs        |\n",
    "| Effect             | Reweight feature *types*              | Reweight feature *locations*           |\n",
    "\n",
    "---\n",
    "\n",
    "####  4. Combining Both (CBAM)\n",
    "\n",
    "Many modern networks (e.g., **CBAM ‚Äî Convolutional Block Attention Module**) use *both*:\n",
    "\n",
    "1. Apply SE (channel attention).\n",
    "2. Then apply spatial attention.\n",
    "\n",
    "This way the network learns **what** and **where** to focus.\n",
    "\n",
    "---\n",
    "\n",
    "####  5. Intuitive Analogy\n",
    "\n",
    "| Analogy           | Meaning                                                         |\n",
    "| ----------------- | --------------------------------------------------------------- |\n",
    "| SE block          | ‚ÄúTurn up the volume for the *important instruments* (channels)‚Äù |\n",
    "| Spatial attention | ‚ÄúFocus your *eyes* on where the action happens in the image‚Äù    |\n",
    "\n",
    "Together ‚Üí you both **listen carefully** and **look carefully** üëÄüéß\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **small visual example (with a heatmap)** comparing how SE and spatial attention would reweight an example 3√ó3√ó3 feature map?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
