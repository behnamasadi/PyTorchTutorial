{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb0f020-20d8-4090-93ed-9b0f500bba23",
   "metadata": {},
   "source": [
    "## 1. Residual Neural\n",
    "\n",
    "\n",
    "When we stack more and more layers in a deep neural network, training becomes harder:\n",
    "\n",
    "* **Vanishing/exploding gradients**: gradients shrink or grow as they backpropagate, making early layers learn very slowly or unstably.\n",
    "* **Degradation problem**: simply adding more layers sometimes *reduces* training accuracy (not just test accuracy).\n",
    "\n",
    "> The issue wasn’t overfitting — it was optimization difficulty.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.1 Key idea: “Residual” learning\n",
    "\n",
    "Instead of making a stack of layers learn a direct mapping $H(x)$ from input $x$ to output, ResNet makes the stack learn a **residual mapping**:\n",
    "\n",
    "$\n",
    "F(x) = H(x) - x \\quad \\Rightarrow \\quad H(x) = F(x) + x\n",
    "$\n",
    "\n",
    "So the block learns only the *change* to apply to the input. The original input is added back at the end via a **skip connection**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2 A residual block\n",
    "\n",
    "**Standard block:**\n",
    "\n",
    "```text\n",
    "x ──> [Conv → BN → ReLU → Conv → BN] ──> + ──> ReLU ──> output\n",
    "       ^                                   │\n",
    "       └───────────── skip connection ─────┘\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "\\text{output} = \\text{ReLU}(F(x; W) + x)\n",
    "$\n",
    "\n",
    "* $F(x; W)$: the output of the two convolutions (the “residual”)\n",
    "* $x:$ the identity/skip connection input\n",
    "\n",
    "This allows gradients to flow directly through the skip connection during backpropagation, which stabilizes training even with very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf42c5c-f5d9-4b88-85c0-282d49cabe5c",
   "metadata": {},
   "source": [
    "#### 1.3 What does it mean \"Skip Connection\"\n",
    "\n",
    "A **skip connection** (also called a **shortcut connection**) is literally what it sounds like:\n",
    "a pathway that *skips over* one or more layers and feeds the input directly to a later point in the network.\n",
    "\n",
    "---\n",
    "\n",
    "**In a normal feed-forward block**\n",
    "\n",
    "```\n",
    "x ──> [Layer(s)] ──> output\n",
    "```\n",
    "\n",
    "All information flows through the layers.\n",
    "\n",
    "---\n",
    "\n",
    "**With a skip (shortcut) connection**\n",
    "\n",
    "```\n",
    "        ┌───────────────┐\n",
    "x ──> [Layer(s)] ──> + ──> output\n",
    "   └───────────────┘ ↑\n",
    "        (skip x)─────┘\n",
    "```\n",
    "\n",
    "You still process (x) through some layers to get $F(x)$, **but at the same time you also send (x) forward unchanged** and add it back in at the end.\n",
    "\n",
    "Mathematically:\n",
    "$\n",
    "\\text{output} = F(x) + x\n",
    "$\n",
    "\n",
    "* $F(x)$: what the block learned (residual)\n",
    "* $x$: original input passed along the shortcut path\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4 Two main skip-connection types\n",
    "\n",
    "* **Identity skip** (when input/output have same shape): just add $x$ to $F(x)$.\n",
    "* **Projection skip** (when shapes differ): apply a $1 \\times 1$ convolution (and maybe stride) to $x$ before adding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0e96b-bacb-42b6-91bb-f959e7112959",
   "metadata": {},
   "source": [
    "#### 1.5 The network want to learn F(x) or H(x)?\n",
    "\n",
    "\n",
    "A plain stack of layers takes an input (x) and tries to directly learn\n",
    "$\n",
    "H(x) \\quad \\text{(desired mapping)}\n",
    "$\n",
    "\n",
    "What ResNet does instead\n",
    "\n",
    "ResNet rewrites the problem so that the stacked layers learn the **residual function**\n",
    "\n",
    "$\n",
    "F(x) = H(x) - x\n",
    "$\n",
    "\n",
    "and then adds the input back:\n",
    "\n",
    "$\n",
    "\\text{Output} = F(x) + x = H(x)\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee834d1-61d5-4af0-b5f8-20c65489e968",
   "metadata": {},
   "source": [
    "**Residual block forward pass**\n",
    "\n",
    "$\n",
    "y = x + F(x;,W)\n",
    "$\n",
    "\n",
    "where $F(x;W)$ are the layers with parameters $W$ (two convs, etc.), and $x$ is the input to the block.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.6 Gradient through a normal (plain) block\n",
    "\n",
    "If you had\n",
    "\n",
    "$\n",
    "y = F(x;W)\n",
    "$\n",
    "\n",
    "then\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}=\n",
    "\\frac{\\partial L}{\\partial y}\n",
    "\\frac{\\partial F(x;W)}{\\partial x}\n",
    "$\n",
    "\n",
    "\n",
    "So all the gradient information must flow through the derivative of (F). If $\\partial F/\\partial x$ is very small (vanishing gradient), the gradient almost disappears before it reaches earlier layers.\n",
    "\n",
    "---\n",
    "\n",
    "Gradient with skip connection:\n",
    "\n",
    "With\n",
    "$\n",
    "y = x + F(x;W)\n",
    "$\n",
    "\n",
    "we have\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\n",
    "\\frac{\\partial (x + F(x;W))}{\\partial x}\n",
    "=\\frac{\\partial L}{\\partial y},(I + \\frac{\\partial F}{\\partial x})\n",
    "$\n",
    "\n",
    "Notice the **identity term (I)** coming from $\\partial x/\\partial x = 1$.\n",
    "\n",
    "This means that even if $\\partial F/\\partial x$ is tiny, there is still a direct gradient path:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x} \\approx \\frac{\\partial L}{\\partial y}\n",
    "$\n",
    "\n",
    "So the gradient flows directly back to the input (x) (and thus to layers before the block) without being multiplied by small numbers. That’s the “highway” for gradients people talk about.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Yes, the gradient can go **directly to the layer that produced (x)** via the identity/skip branch.\n",
    "* It doesn’t have to be squeezed entirely through the complicated (F(x)) path.\n",
    "* This keeps earlier layers trainable even in very deep networks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3a8a7-3e32-4751-b26f-4253b88002d8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad027f9-727e-406a-bd30-83964178cb59",
   "metadata": {},
   "source": [
    "## 2. ResNet Implementation\n",
    "\n",
    "* **`block_cls`** → a **class** (`BasicBlock` or `BottleNeck`)\n",
    "* **`block_mod`** → an **instance** of that class (a module you add to the network)\n",
    "\n",
    "#### 2.1 Basic residual block \n",
    "\n",
    "```python \n",
    "# ---------- Base interface for residual blocks ----------\n",
    "class ResidualBlockBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class to type residual blocks.\n",
    "    Child classes must define a class attribute `expansion` (int).\n",
    "    \"\"\"\n",
    "    expansion: int = 1  # how many times `planes` the block outputs (Basic=1, Bottleneck=4)\n",
    "\n",
    "\n",
    "# ---------- Basic residual block (ResNet-18/34 style) ----------\n",
    "class BasicBlock(ResidualBlockBase):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, in_channels: int, planes: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        out_channels = planes * self.expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Identity or 1x1 projection on the skip path to match shape\n",
    "        needs_projection = (stride != 1) or (in_channels != out_channels)\n",
    "        if needs_projection:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)  # <-- \"skip connection\" path\n",
    "\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "\n",
    "        y = y + identity          # residual addition: F(x) + x\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ae8e6-50f4-4808-ac6c-e732837d5803",
   "metadata": {},
   "source": [
    "What exactly is `expansion: int = 1`?\n",
    "\n",
    "It’s just a **class attribute** that tells the ResNet “builder” how many channels come **out** of a block relative to the `planes` you passed in.\n",
    "\n",
    "* For a **BasicBlock**:\n",
    "\n",
    "  * You pass `planes=64`.\n",
    "  * The last conv also outputs 64 channels.\n",
    "  * `expansion = 1`.\n",
    "  * So the block’s output is `planes * expansion = 64 * 1 = 64`.\n",
    "\n",
    "* For a **Bottleneck block** (ResNet-50/101/152):\n",
    "\n",
    "  * You pass `planes=64`.\n",
    "  * Inside the block:\n",
    "    `1×1 conv (in→64)` → `3×3 conv (64→64)` → `1×1 conv (64→256)`.\n",
    "  * The final conv outputs **256** channels.\n",
    "  * `expansion = 4`.\n",
    "  * So the block’s output is `planes * expansion = 64 * 4 = 256`.\n",
    "\n",
    "The ResNet class can therefore always do:\n",
    "\n",
    "```python\n",
    "self.current_channels = planes * block_cls.expansion\n",
    "```\n",
    "\n",
    "…without hard-coding different rules for different block types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de579b-a1b9-4243-ab3b-15de973de1e1",
   "metadata": {},
   "source": [
    "```python\n",
    "class ResidualBlockBase(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "class BasicBlock(ResidualBlockBase): ...\n",
    "class Bottleneck(ResidualBlockBase): ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579d938-ab22-4218-88fd-51c6279e0d96",
   "metadata": {},
   "source": [
    "Inside the block, we have:\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(in_channels, planes, kernel_size=3, stride=stride, padding=1, bias=False)`\n",
    "```\n",
    "\n",
    "so we follow the input for **stride** however in the second cobe we have **stride=1** so the input shape doesn't change\n",
    "\n",
    "```python\n",
    "self.conv2 = nn.Conv2d(planes, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e1ddc-ab32-49f5-8cd3-53b250fb4d70",
   "metadata": {},
   "source": [
    "### Skip Connection Shape Change\n",
    "In a residual block, we compute:\n",
    "\n",
    "$$\n",
    "y = F(x) + x\n",
    "$$\n",
    "\n",
    "This only makes sense if:\n",
    "\n",
    "$$\n",
    "\\text{shape}(F(x)) = \\text{shape}(x)\n",
    "$$\n",
    "\n",
    "That means:\n",
    "\n",
    "* same **number of channels**,\n",
    "* same **height**,\n",
    "* same **width**.\n",
    "\n",
    "Otherwise, the addition operation will fail.\n",
    "\n",
    "---\n",
    "\n",
    "#### When do shapes mismatch?\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(in_channels, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "self.conv2 = nn.Conv2d(planes, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "```\n",
    "\n",
    "So after the two convolutions:\n",
    "\n",
    "* If `stride = 1`, the **spatial resolution** (H, W) stays the same.\n",
    "* If `stride = 2`, the **spatial resolution halves**:\n",
    "  $$\n",
    "  H_{out} = \\frac{H_{in}}{2}, \\quad W_{out} = \\frac{W_{in}}{2}\n",
    "  $$\n",
    "* The **number of channels** changes from `in_channels` → `out_channels = planes * expansion`.\n",
    "\n",
    "Hence, when:\n",
    "\n",
    "* `stride != 1`  → size mismatch in H, W.\n",
    "* `in_channels != out_channels` → mismatch in C.\n",
    "\n",
    "We can’t directly add `x` and `F(x)`.\n",
    "\n",
    "---\n",
    "\n",
    "#### The solution: the skip (projection) path\n",
    "\n",
    "This part of your code detects when a mismatch occurs:\n",
    "\n",
    "```python\n",
    "needs_projection = (stride != 1) or (in_channels != out_channels)\n",
    "```\n",
    "\n",
    "If true, it applies a **1×1 convolution**:\n",
    "\n",
    "```python\n",
    "self.skip = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "              stride=stride, bias=False),\n",
    "    nn.BatchNorm2d(out_channels),\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why it works (mechanically)\n",
    "\n",
    "**Matching channels**\n",
    "\n",
    "* The 1×1 convolution maps the number of **input channels** → **output channels**:\n",
    "\n",
    "  $$\n",
    "  (B, C_{in}, H, W) \\xrightarrow[\\text{1×1 conv}]{C_{out}} (B, C_{out}, H, W)\n",
    "  $$\n",
    "\n",
    "This changes only the **number of channels**, not the spatial size.\n",
    "\n",
    "**Matching height and width**\n",
    "\n",
    "* When `stride = 2`, the convolution also halves the **height** and **width**:\n",
    "\n",
    "  $$\n",
    "  (B, C_{in}, H, W) \\xrightarrow[\\text{stride}=2]{\\text{1×1 conv}} (B, C_{out}, H/2, W/2)\n",
    "  $$\n",
    "\n",
    "Thus, after the projection:\n",
    "\n",
    "* the skip path produces a tensor of shape **(B, out_channels, H_out, W_out)**,\n",
    "* which now matches the shape of **F(x)** from the main branch.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b5c29-79c2-4df3-af34-8c9c24db88cf",
   "metadata": {},
   "source": [
    "#### 2.2 ResNet\n",
    "\n",
    "```python\n",
    "# ---------- ResNet backbone using a block CLASS (not an instance) ----------\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic ResNet that can be built with different block classes (e.g., BasicBlock, Bottleneck).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    block_cls : Type[ResidualBlockBase]\n",
    "        The CLASS of the residual block to instantiate (e.g., `BasicBlock`), not an instance.\n",
    "    layers_per_stage : Sequence[int]\n",
    "        Number of residual blocks in each of the 4 stages, e.g. [2,2,2,2] or [3,4,6,3].\n",
    "    num_classes : int\n",
    "        Output classes for the final classifier.\n",
    "    in_channels : int\n",
    "        Input image channels (3 for RGB).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_cls: Type[ResidualBlockBase],\n",
    "        layers_per_stage: Sequence[int],\n",
    "        num_classes: int = 1000,\n",
    "        in_channels: int = 3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(layers_per_stage) == 4, \"Expected 4 stages\"\n",
    "        self.block_cls: Type[ResidualBlockBase] = block_cls\n",
    "\n",
    "        # ----- Stem -----\n",
    "        self.stem_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.stem_bn   = nn.BatchNorm2d(64)\n",
    "        self.stem_relu = nn.ReLU(inplace=True)\n",
    "        self.stem_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Track current #channels flowing into the next stage\n",
    "        self.current_channels: int = 64\n",
    "\n",
    "        # ----- Stages (conv2_x .. conv5_x) -----\n",
    "        self.layer1 = self._build_stage(planes=64,  num_blocks=layers_per_stage[0], first_stride=1)\n",
    "        self.layer2 = self._build_stage(planes=128, num_blocks=layers_per_stage[1], first_stride=2)\n",
    "        self.layer3 = self._build_stage(planes=256, num_blocks=layers_per_stage[2], first_stride=2)\n",
    "        self.layer4 = self._build_stage(planes=512, num_blocks=layers_per_stage[3], first_stride=2)\n",
    "\n",
    "        # ----- Head -----\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc      = nn.Linear(512 * self.block_cls.expansion, num_classes)\n",
    "\n",
    "        # He init for convs; BN to ones/zeros (standard for ResNet)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def _build_stage(self, planes: int, num_blocks: int, first_stride: int) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Build one ResNet stage (e.g., conv3_x), returning a Sequential of residual block INSTANCES.\n",
    "\n",
    "        - The first block may downsample via `first_stride`.\n",
    "        - Subsequent blocks keep stride=1.\n",
    "        \"\"\"\n",
    "        blocks: List[nn.Module] = []\n",
    "\n",
    "        # 1) First block in the stage: may change spatial size & width\n",
    "        block_mod = self.block_cls(  # <-- instantiate the CLASS\n",
    "            in_channels=self.current_channels,\n",
    "            planes=planes,\n",
    "            stride=first_stride,\n",
    "        )\n",
    "        blocks.append(block_mod)\n",
    "\n",
    "        # After the first block, the stage's channel width is planes * expansion\n",
    "        self.current_channels = planes * self.block_cls.expansion\n",
    "\n",
    "        # 2) Remaining blocks: keep same width and stride=1\n",
    "        for _ in range(1, num_blocks):\n",
    "            block_mod = self.block_cls(\n",
    "                in_channels=self.current_channels,\n",
    "                planes=planes,\n",
    "                stride=1,\n",
    "            )\n",
    "            blocks.append(block_mod)\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Stem\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.stem_bn(x)\n",
    "        x = self.stem_relu(x)\n",
    "        x = self.stem_pool(x)\n",
    "\n",
    "        # Stages\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Head\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a5f97-f0ce-4233-ba0c-a4cf8a080f1b",
   "metadata": {},
   "source": [
    "Here in the stages, only in `layer1` we have `first_stride=1` and for the rest we have `first_stride=2`, this will set the height and width into `height/2` and `width/2` for the first convolution in the first block:\n",
    "\n",
    "\n",
    "```python\n",
    "self.layer1 = self._build_stage(planes=64,  num_blocks=layers_per_stage[0], first_stride=1)\n",
    "self.layer2 = self._build_stage(planes=128, num_blocks=layers_per_stage[1], first_stride=2)\n",
    "```\n",
    "\n",
    "\n",
    "This will cause no changes in `H`,`W` in the first block: \n",
    "\n",
    "\n",
    "![](images/ResNet-18-First-Block.png)\n",
    "\n",
    "but in the second, third, in the first convolution we have `height/2` and `width/2`:\n",
    "\n",
    "\n",
    "![](images/ResNet-18-Second-Block.png)\n",
    "\n",
    "\n",
    "For the second block, we set `stride=1` explicitly, so have no changes in `H`,`W`:\n",
    "\n",
    "```python\n",
    "for i in range(1, num_blocks):\n",
    "            block_mod = self.block_cls(\n",
    "                in_channels=self.current_channels,\n",
    "                planes=planes,\n",
    "                stride=1)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8666a-47d3-4338-ba31-d4ad6adccb36",
   "metadata": {},
   "source": [
    "#### 2.3  ResNet18\n",
    "\n",
    "```python\n",
    "# ---------- Factory helpers ----------\n",
    "def resnet18(num_classes: int = 1000, in_channels: int = 3) -> ResNet:\n",
    "    \"\"\"ResNet-18 uses BasicBlock with layers_per_stage = [2, 2, 2, 2].\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079264b7-04b3-4b32-94b8-ba2f881b43fa",
   "metadata": {},
   "source": [
    "![](images/ResNet-18-Architecture.png)\n",
    "\n",
    "\n",
    "\n",
    "![](images/ResNet-18-architecture-diagram-conv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436aeb9f-10ee-4589-997b-abae1acbfc4d",
   "metadata": {},
   "source": [
    "#### ResNet34\n",
    "```python\n",
    "def resnet34(num_classes: int = 1000, in_channels: int = 3) -> ResNet:\n",
    "    \"\"\"ResNet-34 uses BasicBlock with layers_per_stage = [3, 4, 6, 3].\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd2e8e-4a38-490e-be29-0180def4e109",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    model = resnet18(num_classes=10)\n",
    "    x = torch.randn(2, 3, 224, 224)\n",
    "    logits = model(x)\n",
    "    print(type(BasicBlock))         # <class 'type'>  (a CLASS)\n",
    "    print(isinstance(model.layer1[0], BasicBlock))  # True (an INSTANCE inside the stage)\n",
    "    print(logits.shape)             # torch.Size([2, 10])\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d9b84-83a5-48fa-9a9c-7011f05eba95",
   "metadata": {},
   "source": [
    "#### 2.4 Bottleneck Class\n",
    "\n",
    "It’s the *other* residual block type used in ResNets 50/101/152.\n",
    "Where `BasicBlock` does **two 3×3 convolutions**, the **Bottleneck** block does\n",
    "**1×1 → 3×3 → 1×1** convolutions and sets `expansion = 4`.\n",
    "\n",
    "\n",
    "\n",
    "* First 1×1 conv **reduces channels** (the “bottleneck”).\n",
    "* Middle 3×3 conv does the heavy lifting at a reduced width (cheaper).\n",
    "* Last 1×1 conv **expands channels back** to `planes * 4`.\n",
    "\n",
    "So each block still outputs the full width, but most of the compute happens at a narrower “bottleneck” width.\n",
    "\n",
    "\n",
    "This lets you build much deeper networks (50+ layers) without exploding compute.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867d38e-f352-468c-96c9-741f0a65daa4",
   "metadata": {},
   "source": [
    "#### 2.5 Typical Bottleneck class (PyTorch)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # output channels = planes * 4\n",
    "\n",
    "    def __init__(self, in_channels: int, planes: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        out_channels = planes * self.expansion\n",
    "\n",
    "        # 1x1 conv: reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 3x3 conv: main processing\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 1x1 conv: expand channels back up\n",
    "        self.conv3 = nn.Conv2d(planes, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # skip connection projection if needed\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        y = self.conv3(y)\n",
    "        y = self.bn3(y)\n",
    "\n",
    "        y = y + identity\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "class ResidualBlockBase(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "class BasicBlock(ResidualBlockBase):\n",
    "    expansion = 1\n",
    "    # ... two 3x3 convs ...\n",
    "\n",
    "class Bottleneck(ResidualBlockBase):\n",
    "    expansion = 4\n",
    "    # ... 1x1, 3x3, 1x1 convs ...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* `planes` = the reduced “bottleneck” width.\n",
    "* `out_channels = planes * expansion = planes * 4`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90694426-1947-4432-a38f-a27b04920f64",
   "metadata": {},
   "source": [
    "#### 2.6 resnet50\n",
    "\n",
    "```python\n",
    "# ResNet-50 uses Bottleneck blocks\n",
    "resnet50 = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ed939-380e-4e90-9791-4b33a426995a",
   "metadata": {},
   "source": [
    "#### 2.7 What exactly do **18 / 34 / 50 / 101 / 152** mean” in **ResNet-X**.\n",
    "\n",
    "\n",
    "They’re the **total number of weight-bearing layers** (convolutions + the final fully-connected layer) in the network.\n",
    "\n",
    "* **ResNet-18** has 18 such layers.\n",
    "* **ResNet-34** has 34 layers.\n",
    "* **ResNet-50** has 50 layers.\n",
    "* **ResNet-101** has 101 layers.\n",
    "* **ResNet-152** has 152 layers.\n",
    "\n",
    "Pooling layers and ReLU/BatchNorm don’t count toward the number.\n",
    "\n",
    "---\n",
    "\n",
    "| Model          | Block type           | Blocks per stage         | Convs per block | 1st conv | Total conv layers | +FC = total layers |\n",
    "| -------------- | -------------------- | ------------------------ | --------------- | -------- | ----------------- | ------------------ |\n",
    "| **ResNet-18**  | BasicBlock (2 convs) | [2, 2, 2, 2] = 8 blocks  | 2               | 1        | 1 + 8×2 = 17      | +1 FC = **18**     |\n",
    "| **ResNet-34**  | BasicBlock (2 convs) | [3, 4, 6, 3] = 16 blocks | 2               | 1        | 1 + 16×2 = 33     | +1 FC = **34**     |\n",
    "| **ResNet-50**  | Bottleneck (3 convs) | [3, 4, 6, 3] = 16 blocks | 3               | 1        | 1 + 16×3 = 49     | +1 FC = **50**     |\n",
    "| **ResNet-101** | Bottleneck (3 convs) | [3, 4, 23,3]=33 blocks   | 3               | 1        | 1 + 33×3 =100     | +1 FC = **101**    |\n",
    "| **ResNet-152** | Bottleneck (3 convs) | [3,8,36,3]=50 blocks     | 3               | 1        | 1 + 50×3 =151     | +1 FC = **152**    |\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "* **First conv**: the 7×7 conv at the “stem” = 1 layer.\n",
    "* **Blocks per stage**: how many residual blocks in conv2_x, conv3_x, conv4_x, conv5_x.\n",
    "* **Convs per block**: 2 for BasicBlock, 3 for Bottleneck.\n",
    "* Add them up + 1 for the final fully-connected = the ResNet number.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* ResNet-18/34 use BasicBlock (2 convs each).\n",
    "* ResNet-50/101/152 use Bottleneck (3 convs each, but with 1×1–3×3–1×1 pattern).\n",
    "\n",
    "So you can make the network deeper either by:\n",
    "\n",
    "* Adding more blocks per stage ([3,4,6,3] vs [2,2,2,2]),\n",
    "* Or switching to Bottleneck blocks with 3 convs each.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a11579-d670-45cf-9db1-642ac3aab149",
   "metadata": {},
   "source": [
    "## 3. ResNet-X Input Size\n",
    "\n",
    "The **“ResNet-18/34/50/101/152” names say nothing about input size**.\n",
    "They only describe the *depth* (number of weight-bearing layers).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1  The “default” input size in the paper / torchvision\n",
    "\n",
    "All the ImageNet-trained ResNets assume **224×224 RGB images**:\n",
    "\n",
    "```\n",
    "Input: (N, 3, 224, 224)\n",
    "Conv7×7 stride2 → (N, 64, 112, 112)\n",
    "MaxPool stride2 → (N, 64, 56, 56)\n",
    "layer1 (conv2_x) → (N, 64, 56, 56)\n",
    "layer2 (conv3_x) → (N, 128, 28, 28)\n",
    "layer3 (conv4_x) → (N, 256, 14, 14)\n",
    "layer4 (conv5_x) → (N, 512, 7, 7)  or (planes*expansion)\n",
    "GlobalAvgPool → (N, 512*expansion, 1, 1)\n",
    "FC → (N, num_classes)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2  But ResNet is **fully convolutional** until the FC layer\n",
    "\n",
    "All layers before the final average pool are convolution + BN + ReLU.\n",
    "So you can feed **any spatial size** (H×W) as long as it’s large enough for the downsamplings.\n",
    "\n",
    "* Each stride-2 halves H and W.\n",
    "* By the end of layer4 you’ve done stride (2×2×2×2=16) (or 32 if you count the first conv and maxpool), so your spatial size is roughly (\\frac{H}{32} × \\frac{W}{32}).\n",
    "\n",
    "If the input is too small, you’ll hit zero or negative dimensions; otherwise it works.\n",
    "\n",
    "---\n",
    "\n",
    "**Typical requirements:**\n",
    "\n",
    "* For ImageNet: 224×224 → final 7×7 feature map → global avg pool to 1×1.\n",
    "* For CIFAR-10: inputs are 32×32. People use a **modified stem** (3×3 conv stride1 instead of 7×7 stride2 + maxpool) so they don’t downsample away everything.\n",
    "* For segmentation: people remove the FC and keep the convolutional part as a “backbone” to get feature maps of arbitrary size.\n",
    "\n",
    "---\n",
    "\n",
    "**Rule of thumb for output spatial size**\n",
    "\n",
    "If your input is ((N, 3, H, W)), after the four stages you’ll have:\n",
    "\n",
    "$\n",
    "H_{\\text{out}} = \\left\\lfloor\\frac{H}{32}\\right\\rfloor,\n",
    "\\quad\n",
    "W_{\\text{out}} = \\left\\lfloor\\frac{W}{32}\\right\\rfloor\n",
    "$\n",
    "\n",
    "for the standard ResNet stem.\n",
    "The channel dimension at the end = $512×\\text{expansion}$.\n",
    "\n",
    "Then global average pool reduces $H_\\text{out}×W_\\text{out}$ to 1×1 regardless of input.\n",
    "\n",
    "---\n",
    "\n",
    "* The “18/34/50/…” numbers do **not** encode input size.\n",
    "* Default pretrained ResNets use 224×224 inputs.\n",
    "* Because ResNet is convolutional + global pooling, you can use larger or smaller images (e.g. 256×256, 512×512) — you just get correspondingly larger or smaller feature maps before pooling.\n",
    "* For very small inputs (like CIFAR-10), you typically adjust the stem to avoid over-downsampling.\n",
    "\n",
    "\n",
    "#### 3.3 Pretrained ResNet Weight Input Size\n",
    "\n",
    "If you download a pretrained ResNet weight from PyTorch, do you have to feed it exactly 224×224 images?\n",
    "\n",
    "All the conv, BN, ReLU layers in ResNet have kernels and weights that don’t depend on the spatial size of the input.\n",
    "They just slide across whatever height × width you give them.\n",
    "\n",
    "So the pretrained weights from `torchvision.models.resnet50(pretrained=True)` will happily process, say, $(3,256,256)$ or $(3,512,512)$ images.\n",
    "You don’t need to change anything for the conv layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4 The **final fully-connected layer** *does* expect a fixed input width\n",
    "\n",
    "But ResNet’s last step before the FC layer is a **global average pooling**:\n",
    "\n",
    "```python\n",
    "x = self.avgpool(x)   # (N, C, H_out, W_out) → (N, C, 1, 1)\n",
    "x = torch.flatten(x, 1)  # (N, C)\n",
    "x = self.fc(x)        # (N, num_classes)\n",
    "```\n",
    "\n",
    "That `AdaptiveAvgPool2d((1,1))` always gives you a (C,1,1) tensor no matter what `H_out,W_out` were.\n",
    "Flatten → (N,C) → FC layer.\n",
    "So the FC sees exactly `C = 512*expansion` features no matter what the input size was.\n",
    "\n",
    "That’s why the pretrained FC layer still works.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 3.5 If you change the number of input channels\n",
    "\n",
    "If you go from RGB (3 channels) to grayscale (1 channel) or multispectral (5 channels), the **first conv**’s weights won’t match.\n",
    "You either:\n",
    "\n",
    "* Replace the first conv and randomly init it, or\n",
    "* Load pretrained weights partially and adapt the first conv.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e306c50-b6c7-4a50-9671-accba7f1d739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d135272-611a-48da-a374-662dcb1ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchviz\n",
    "\n",
    "\n",
    "#ResNet18_Weights.DEFAULT is equivalent to ResNet18_Weights.IMAGENET1K_V1. \n",
    "resnet18=torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT', progress= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238ac29f-8892-4a00-941a-b381ecbbdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 input size:  512\n",
      "resnet18 output size:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \",resnet18.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d08aff-8ef0-4759-8d2c-0cf648e32383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/resnet18_graph.svg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  resnet18 has an averagepool layer at the end.\n",
    "#  So the input size does not matter much provided the feature map size is greater than kernel size.\n",
    "\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8b3b3-77e3-491f-b6da-eac9acb6df50",
   "metadata": {},
   "source": [
    "![](images/resnet18_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef909944-f0a0-41b8-95a8-c73d9ec95b33",
   "metadata": {},
   "source": [
    "## 4. Finetune the model on a new dataset with 10 labels\n",
    "\n",
    "\n",
    "Let’s say we want to finetune the model on a new dataset with `10` labels. In resnet, the classifier is the last linear layer `model.fc.` We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.\n",
    "\n",
    "\n",
    "```python\n",
    "for params in resnet18.parameters():\n",
    "    params.requiers_gard=False\n",
    "\n",
    "resnet18.fc=torch.nn.Linear(512,10)\n",
    "\n",
    "```\n",
    "\n",
    "Now all parameters in the model, except the parameters of `model.fc`, are frozen. The only parameters that compute gradients are the `weights` and `bias` of `model.fc.`\n",
    "\n",
    "```python\n",
    "optimizer=torch.optim.SGD(resnet18.fc.parameters(),lr=1e-2,momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711034c0-1cbc-4168-b40a-4afcd5ee36aa",
   "metadata": {},
   "source": [
    "## Black and white Image Input\n",
    "\n",
    "The pretrained ResNet-18 expects 3-channel RGB at `conv1`. With a single-channel (monochrome) input you have a few good options:\n",
    "\n",
    "### 1) Duplicate the channel (fastest, no weight surgery)\n",
    "\n",
    "Preprocess your 1-channel image to 3 channels by repeating it. `transforms.Grayscale`:\n",
    "- If num_output_channels == 1 : returned image is single channel\n",
    "- If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
    "\n",
    "```python\n",
    "# during transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # 1→3 by duplication\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),  # ImageNet stats\n",
    "])\n",
    "```\n",
    "\n",
    "Pros: zero code change to the model; you keep pretrained weights intact.\n",
    "Cons: a tiny bit redundant, but works very well in practice.\n",
    "\n",
    "### 2) Replace `conv1` with 1 input channel and **port** pretrained weights\n",
    "\n",
    "Average the RGB kernels into a single channel:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "m = models.resnet18(weights='models.ResNet18_Weights.IMAGENET1K_V1')\n",
    "w = m.conv1.weight  # [64, 3, 7, 7]\n",
    "\n",
    "m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    m.conv1.weight[:] = w.mean(dim=1, keepdim=True)  # [64,1,7,7]\n",
    "```\n",
    "\n",
    "(You can also use a weighted sum like `0.2989 R + 0.5870 G + 0.1140 B` instead of `.mean`.)\n",
    "\n",
    "Pros: no wasted computation; uses pretrained filters sensibly.\n",
    "Cons: a tiny bit of code; but this is the cleanest if you’re truly single-channel end-to-end.\n",
    "\n",
    "### 3) Add a learnable 1→3 adapter in front\n",
    "\n",
    "Keep the pretrained model intact and learn a shallow mapping:\n",
    "\n",
    "```python\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)  # start as “repeat”\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.map(x)\n",
    "\n",
    "adapter = GrayToRGB()\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "full = nn.Sequential(adapter, model)\n",
    "```\n",
    "\n",
    "Pros: lets the network learn the best 1→3 projection.\n",
    "Cons: a few extra params; slightly more moving parts.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization notes\n",
    "\n",
    "* If you **duplicate to 3-ch**, you can keep ImageNet mean/std as above, or compute dataset-specific stats and use those for all 3 channels (same numbers repeated).\n",
    "* If you **switch to 1-ch conv1**, use single mean/std (e.g., `(mean,)` and `(std,)`) matching your grayscale dataset.\n",
    "\n",
    "### Which should you pick?\n",
    "\n",
    "* **Quick wins / transfer learning**: Option **1** (duplicate) is perfectly fine and very common.\n",
    "* **Purist, minimal compute**: Option **2** (port weights) is elegant and usually performs best.\n",
    "* **Data shift concerns** (e.g., MRI/CT with unusual intensity): Option **3** gives flexibility; also consider dataset-specific normalization and fine-tuning early layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb833e9a-a91b-4563-b85d-ecc62b0724c6",
   "metadata": {},
   "source": [
    "### How to train the option 3 Network (learnable 1→3 adapter in front)\n",
    "\n",
    "With option 3 (a learnable 1→3 “adapter” in front of a pretrained ResNet-18), you’ve got three common training strategies. Pick one based on data size and how different your grayscale data is from ImageNet.\n",
    "\n",
    "### A. Freeze backbone first, train adapter + head (safe start)\n",
    "\n",
    "1–3 epochs:\n",
    "\n",
    "* Freeze **all** ResNet18 params.\n",
    "* Train only the `GrayToRGB` adapter and the final `fc`.\n",
    "\n",
    "Then unfreeze the backbone (optionally with a lower LR) and fine-tune.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)\n",
    "\n",
    "    def forward(self, x): return self.map(x)\n",
    "\n",
    "backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(backbone.fc.in_features, n_classes)  # replace head\n",
    "\n",
    "model = nn.Sequential(GrayToRGB(), backbone)\n",
    "\n",
    "# --- Phase 1: freeze backbone ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# optimize only adapter + fc\n",
    "params = list(model[0].parameters()) + list(backbone.fc.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "After a few epochs:\n",
    "\n",
    "```python\n",
    "# --- Phase 2: unfreeze backbone with smaller LR ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model[0].parameters(), \"lr\": 1e-3},          # adapter\n",
    "    {\"params\": backbone.layer1.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(), \"lr\": 2.5e-4},\n",
    "    {\"params\": backbone.layer3.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.layer4.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.fc.parameters(),     \"lr\": 1e-3},\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### B. Train everything, but with **discriminative learning rates** (faster)\n",
    "\n",
    "Good when you have a moderate dataset and want quick convergence without a freezing phase.\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model[0].parameters(),           \"lr\": 1e-3},  # adapter highest\n",
    "    {\"params\": backbone.layer1.parameters(),    \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(),    \"lr\": 3e-4},\n",
    "    {\"params\": backbone.layer3.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.layer4.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.fc.parameters(),        \"lr\": 1e-3},  # head high\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### C. Unfreeze progressively (“gradual unfreezing”)\n",
    "\n",
    "Start with only adapter+fc, then unfreeze layers one block at a time every few epochs (layer4 → layer3 → …). This is handy with small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Do we freeze the adapter conv?\n",
    "\n",
    "* **Usually not.** Let it learn a smart projection beyond simple channel copy.\n",
    "* If the dataset is tiny and unstable, you *can* freeze it for the first few hundred steps.\n",
    "\n",
    "### BatchNorm tips\n",
    "\n",
    "* If batch size is small (≤16), consider putting the backbone’s BN layers in **eval** mode during early training:\n",
    "\n",
    "  ```python\n",
    "  def set_bn_eval(m):\n",
    "      if isinstance(m, nn.BatchNorm2d):\n",
    "          m.eval()\n",
    "  backbone.apply(set_bn_eval)\n",
    "  ```\n",
    "\n",
    "  (Params can still be trainable; this just freezes running stats.)\n",
    "\n",
    "### Weight decay hygiene (optional but nice)\n",
    "\n",
    "Avoid weight decay on BN and bias:\n",
    "\n",
    "```python\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    if n.endswith('bias') or 'bn' in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": decay, \"weight_decay\": 1e-4, \"lr\": 3e-4},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0, \"lr\": 3e-4},\n",
    "])\n",
    "```\n",
    "\n",
    "### Schedulers (keep it simple)\n",
    "\n",
    "* **Cosine with warmup** or **OneCycleLR** both work well:\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "# or:\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(loader), epochs=E)\n",
    "```\n",
    "\n",
    "### Quick guidance\n",
    "\n",
    "* **Small dataset / big domain shift (e.g., MRI):** A or C.\n",
    "* **Moderate dataset / some domain shift:** B with discriminative LRs.\n",
    "* **Plenty of data:** Train all, normal LRs, standard fine-tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08897a7e-5b26-46f3-9850-709e9247eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
