{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb0f020-20d8-4090-93ed-9b0f500bba23",
   "metadata": {},
   "source": [
    "## Residual Neural\n",
    "\n",
    "\n",
    "When we stack more and more layers in a deep neural network, training becomes harder:\n",
    "\n",
    "* **Vanishing/exploding gradients**: gradients shrink or grow as they backpropagate, making early layers learn very slowly or unstably.\n",
    "* **Degradation problem**: simply adding more layers sometimes *reduces* training accuracy (not just test accuracy).\n",
    "\n",
    "> The issue wasn’t overfitting — it was optimization difficulty.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key idea: “Residual” learning\n",
    "\n",
    "Instead of making a stack of layers learn a direct mapping $H(x)$ from input $x$ to output, ResNet makes the stack learn a **residual mapping**:\n",
    "\n",
    "$\n",
    "F(x) = H(x) - x \\quad \\Rightarrow \\quad H(x) = F(x) + x\n",
    "$\n",
    "\n",
    "So the block learns only the *change* to apply to the input. The original input is added back at the end via a **skip connection**.\n",
    "\n",
    "---\n",
    "\n",
    "#### A residual block\n",
    "\n",
    "**Standard block:**\n",
    "\n",
    "```text\n",
    "x ──> [Conv → BN → ReLU → Conv → BN] ──> + ──> ReLU ──> output\n",
    "       ^                                   │\n",
    "       └───────────── skip connection ─────┘\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "\\text{output} = \\text{ReLU}(F(x; W) + x)\n",
    "$\n",
    "\n",
    "* $F(x; W)$: the output of the two convolutions (the “residual”)\n",
    "* $x:$ the identity/skip connection input\n",
    "\n",
    "This allows gradients to flow directly through the skip connection during backpropagation, which stabilizes training even with very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf42c5c-f5d9-4b88-85c0-282d49cabe5c",
   "metadata": {},
   "source": [
    "#### What does it mean \"Skip Connection\"\n",
    "\n",
    "A **skip connection** (also called a **shortcut connection**) is literally what it sounds like:\n",
    "a pathway that *skips over* one or more layers and feeds the input directly to a later point in the network.\n",
    "\n",
    "---\n",
    "\n",
    "**In a normal feed-forward block**\n",
    "\n",
    "```\n",
    "x ──> [Layer(s)] ──> output\n",
    "```\n",
    "\n",
    "All information flows through the layers.\n",
    "\n",
    "---\n",
    "\n",
    "**With a skip (shortcut) connection**\n",
    "\n",
    "```\n",
    "        ┌───────────────┐\n",
    "x ──> [Layer(s)] ──> + ──> output\n",
    "   └───────────────┘ ↑\n",
    "        (skip x)─────┘\n",
    "```\n",
    "\n",
    "You still process (x) through some layers to get $F(x)$, **but at the same time you also send (x) forward unchanged** and add it back in at the end.\n",
    "\n",
    "Mathematically:\n",
    "$\n",
    "\\text{output} = F(x) + x\n",
    "$\n",
    "\n",
    "* $F(x)$: what the block learned (residual)\n",
    "* $x$: original input passed along the shortcut path\n",
    "\n",
    "---\n",
    "\n",
    "#### Two main skip-connection types\n",
    "\n",
    "* **Identity skip** (when input/output have same shape): just add $x$ to $F(x)$.\n",
    "* **Projection skip** (when shapes differ): apply a $1 \\times 1$ convolution (and maybe stride) to $x$ before adding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0e96b-bacb-42b6-91bb-f959e7112959",
   "metadata": {},
   "source": [
    "#### The network want to learn F(x) or H(x)?\n",
    "\n",
    "\n",
    "A plain stack of layers takes an input (x) and tries to directly learn\n",
    "$\n",
    "H(x) \\quad \\text{(desired mapping)}\n",
    "$\n",
    "\n",
    "What ResNet does instead\n",
    "\n",
    "ResNet rewrites the problem so that the stacked layers learn the **residual function**\n",
    "\n",
    "$\n",
    "F(x) = H(x) - x\n",
    "$\n",
    "\n",
    "and then adds the input back:\n",
    "\n",
    "$\n",
    "\\text{Output} = F(x) + x = H(x)\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee834d1-61d5-4af0-b5f8-20c65489e968",
   "metadata": {},
   "source": [
    "**Residual block forward pass**\n",
    "\n",
    "$\n",
    "y = x + F(x;,W)\n",
    "$\n",
    "\n",
    "where $F(x;W)$ are the layers with parameters $W$ (two convs, etc.), and $x$ is the input to the block.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradient through a normal (plain) block\n",
    "\n",
    "If you had\n",
    "\n",
    "$\n",
    "y = F(x;W)\n",
    "$\n",
    "\n",
    "then\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}=\n",
    "\\frac{\\partial L}{\\partial y}\n",
    "\\frac{\\partial F(x;W)}{\\partial x}\n",
    "$\n",
    "\n",
    "\n",
    "So all the gradient information must flow through the derivative of (F). If $\\partial F/\\partial x$ is very small (vanishing gradient), the gradient almost disappears before it reaches earlier layers.\n",
    "\n",
    "---\n",
    "\n",
    "Gradient with skip connection:\n",
    "\n",
    "With\n",
    "$\n",
    "y = x + F(x;W)\n",
    "$\n",
    "\n",
    "we have\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}\n",
    "\\frac{\\partial (x + F(x;W))}{\\partial x}\n",
    "=\\frac{\\partial L}{\\partial y},(I + \\frac{\\partial F}{\\partial x})\n",
    "$\n",
    "\n",
    "Notice the **identity term (I)** coming from $\\partial x/\\partial x = 1$.\n",
    "\n",
    "This means that even if $\\partial F/\\partial x$ is tiny, there is still a direct gradient path:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial x} \\approx \\frac{\\partial L}{\\partial y}\n",
    "$\n",
    "\n",
    "So the gradient flows directly back to the input (x) (and thus to layers before the block) without being multiplied by small numbers. That’s the “highway” for gradients people talk about.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Yes, the gradient can go **directly to the layer that produced (x)** via the identity/skip branch.\n",
    "* It doesn’t have to be squeezed entirely through the complicated (F(x)) path.\n",
    "* This keeps earlier layers trainable even in very deep networks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784f16f-dfe1-473c-b899-88965b26c1ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Simple PyTorch example of a residual block\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # projection if dimensions change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity    # skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c7422-bc72-4200-a92e-5e36f7d8ddb4",
   "metadata": {},
   "source": [
    "```python\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, in_channels=3):\n",
    "        \"\"\"\n",
    "        block   : block class (BasicBlock)\n",
    "        layers  : list with number of blocks in each stage, e.g. [2,2,2,2] for ResNet-18\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        # Stem\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Stages\n",
    "        self.layer1 = self._make_layer(block,  64, layers[0], stride=1)  # /4 spatial\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)  # /8\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)  # /16\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)  # /32\n",
    "\n",
    "        # Head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # (N, C, 1, 1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # Kaiming init (common for ResNets)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride):\n",
    "        \"\"\"\n",
    "        planes : out_channels of this stage\n",
    "        blocks : how many blocks in this stage\n",
    "        stride : stride of the first block (downsample if 2)\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride=stride))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        # Stages\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # Head\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------- Factory helpers (ImageNet-style) --------\n",
    "def resnet18(num_classes=1000, in_channels=3):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels)\n",
    "\n",
    "def resnet34(num_classes=1000, in_channels=3):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels)\n",
    "\n",
    "\n",
    "# -------- Quick smoke test --------\n",
    "if __name__ == \"__main__\":\n",
    "    model = resnet18(num_classes=10)  # e.g., 10 classes\n",
    "    x = torch.randn(2, 3, 224, 224)   # batch of 2 images\n",
    "    logits = model(x)\n",
    "    print(logits.shape)  # -> torch.Size([2, 10])\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ada6dd-3cc1-41f9-92a6-fe3196306d63",
   "metadata": {},
   "source": [
    "\n",
    "#### Architecture overview\n",
    "\n",
    "* **ResNet-18/34**: Basic blocks (two 3×3 conv layers per block).\n",
    "* **ResNet-50/101/152**: Bottleneck blocks (1×1 → 3×3 → 1×1 conv) to reduce computation.\n",
    "\n",
    "A typical ResNet-50 looks like:\n",
    "\n",
    "```\n",
    "Conv7x7 → MaxPool → [3 residual blocks] →\n",
    "[4 residual blocks] → [6 residual blocks] →\n",
    "[3 residual blocks] → GlobalAvgPool → FC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d135272-611a-48da-a374-662dcb1ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchviz\n",
    "\n",
    "\n",
    "#ResNet18_Weights.DEFAULT is equivalent to ResNet18_Weights.IMAGENET1K_V1. \n",
    "resnet18=torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT', progress= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a11579-d670-45cf-9db1-642ac3aab149",
   "metadata": {},
   "source": [
    "## How to find model input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238ac29f-8892-4a00-941a-b381ecbbdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 input size:  512\n",
      "resnet18 output size:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \",resnet18.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d08aff-8ef0-4759-8d2c-0cf648e32383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/resnet18_graph.svg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  resnet18 has an averagepool layer at the end.\n",
    "#  So the input size does not matter much provided the feature map size is greater than kernel size.\n",
    "\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8b3b3-77e3-491f-b6da-eac9acb6df50",
   "metadata": {},
   "source": [
    "![](images/resnet18_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef909944-f0a0-41b8-95a8-c73d9ec95b33",
   "metadata": {},
   "source": [
    "## Finetune the model on a new dataset with 10 labels\n",
    "\n",
    "\n",
    "Let’s say we want to finetune the model on a new dataset with `10` labels. In resnet, the classifier is the last linear layer `model.fc.` We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.\n",
    "\n",
    "\n",
    "```python\n",
    "for params in resnet18.parameters():\n",
    "    params.requiers_gard=False\n",
    "\n",
    "resnet18.fc=torch.nn.Linear(512,10)\n",
    "\n",
    "```\n",
    "\n",
    "Now all parameters in the model, except the parameters of `model.fc`, are frozen. The only parameters that compute gradients are the `weights` and `bias` of `model.fc.`\n",
    "\n",
    "```python\n",
    "optimizer=torch.optim.SGD(resnet18.fc.parameters(),lr=1e-2,momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711034c0-1cbc-4168-b40a-4afcd5ee36aa",
   "metadata": {},
   "source": [
    "## Black and white Image Input\n",
    "\n",
    "The pretrained ResNet-18 expects 3-channel RGB at `conv1`. With a single-channel (monochrome) input you have a few good options:\n",
    "\n",
    "### 1) Duplicate the channel (fastest, no weight surgery)\n",
    "\n",
    "Preprocess your 1-channel image to 3 channels by repeating it. `transforms.Grayscale`:\n",
    "- If num_output_channels == 1 : returned image is single channel\n",
    "- If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
    "\n",
    "```python\n",
    "# during transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # 1→3 by duplication\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),  # ImageNet stats\n",
    "])\n",
    "```\n",
    "\n",
    "Pros: zero code change to the model; you keep pretrained weights intact.\n",
    "Cons: a tiny bit redundant, but works very well in practice.\n",
    "\n",
    "### 2) Replace `conv1` with 1 input channel and **port** pretrained weights\n",
    "\n",
    "Average the RGB kernels into a single channel:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "m = models.resnet18(weights='models.ResNet18_Weights.IMAGENET1K_V1')\n",
    "w = m.conv1.weight  # [64, 3, 7, 7]\n",
    "\n",
    "m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    m.conv1.weight[:] = w.mean(dim=1, keepdim=True)  # [64,1,7,7]\n",
    "```\n",
    "\n",
    "(You can also use a weighted sum like `0.2989 R + 0.5870 G + 0.1140 B` instead of `.mean`.)\n",
    "\n",
    "Pros: no wasted computation; uses pretrained filters sensibly.\n",
    "Cons: a tiny bit of code; but this is the cleanest if you’re truly single-channel end-to-end.\n",
    "\n",
    "### 3) Add a learnable 1→3 adapter in front\n",
    "\n",
    "Keep the pretrained model intact and learn a shallow mapping:\n",
    "\n",
    "```python\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)  # start as “repeat”\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.map(x)\n",
    "\n",
    "adapter = GrayToRGB()\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "full = nn.Sequential(adapter, model)\n",
    "```\n",
    "\n",
    "Pros: lets the network learn the best 1→3 projection.\n",
    "Cons: a few extra params; slightly more moving parts.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization notes\n",
    "\n",
    "* If you **duplicate to 3-ch**, you can keep ImageNet mean/std as above, or compute dataset-specific stats and use those for all 3 channels (same numbers repeated).\n",
    "* If you **switch to 1-ch conv1**, use single mean/std (e.g., `(mean,)` and `(std,)`) matching your grayscale dataset.\n",
    "\n",
    "### Which should you pick?\n",
    "\n",
    "* **Quick wins / transfer learning**: Option **1** (duplicate) is perfectly fine and very common.\n",
    "* **Purist, minimal compute**: Option **2** (port weights) is elegant and usually performs best.\n",
    "* **Data shift concerns** (e.g., MRI/CT with unusual intensity): Option **3** gives flexibility; also consider dataset-specific normalization and fine-tuning early layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb833e9a-a91b-4563-b85d-ecc62b0724c6",
   "metadata": {},
   "source": [
    "### How to train the option 3 Network (learnable 1→3 adapter in front)\n",
    "\n",
    "With option 3 (a learnable 1→3 “adapter” in front of a pretrained ResNet-18), you’ve got three common training strategies. Pick one based on data size and how different your grayscale data is from ImageNet.\n",
    "\n",
    "### A. Freeze backbone first, train adapter + head (safe start)\n",
    "\n",
    "1–3 epochs:\n",
    "\n",
    "* Freeze **all** ResNet18 params.\n",
    "* Train only the `GrayToRGB` adapter and the final `fc`.\n",
    "\n",
    "Then unfreeze the backbone (optionally with a lower LR) and fine-tune.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)\n",
    "\n",
    "    def forward(self, x): return self.map(x)\n",
    "\n",
    "backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(backbone.fc.in_features, n_classes)  # replace head\n",
    "\n",
    "model = nn.Sequential(GrayToRGB(), backbone)\n",
    "\n",
    "# --- Phase 1: freeze backbone ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# optimize only adapter + fc\n",
    "params = list(model[0].parameters()) + list(backbone.fc.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "After a few epochs:\n",
    "\n",
    "```python\n",
    "# --- Phase 2: unfreeze backbone with smaller LR ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model[0].parameters(), \"lr\": 1e-3},          # adapter\n",
    "    {\"params\": backbone.layer1.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(), \"lr\": 2.5e-4},\n",
    "    {\"params\": backbone.layer3.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.layer4.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.fc.parameters(),     \"lr\": 1e-3},\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### B. Train everything, but with **discriminative learning rates** (faster)\n",
    "\n",
    "Good when you have a moderate dataset and want quick convergence without a freezing phase.\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model[0].parameters(),           \"lr\": 1e-3},  # adapter highest\n",
    "    {\"params\": backbone.layer1.parameters(),    \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(),    \"lr\": 3e-4},\n",
    "    {\"params\": backbone.layer3.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.layer4.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.fc.parameters(),        \"lr\": 1e-3},  # head high\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### C. Unfreeze progressively (“gradual unfreezing”)\n",
    "\n",
    "Start with only adapter+fc, then unfreeze layers one block at a time every few epochs (layer4 → layer3 → …). This is handy with small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Do we freeze the adapter conv?\n",
    "\n",
    "* **Usually not.** Let it learn a smart projection beyond simple channel copy.\n",
    "* If the dataset is tiny and unstable, you *can* freeze it for the first few hundred steps.\n",
    "\n",
    "### BatchNorm tips\n",
    "\n",
    "* If batch size is small (≤16), consider putting the backbone’s BN layers in **eval** mode during early training:\n",
    "\n",
    "  ```python\n",
    "  def set_bn_eval(m):\n",
    "      if isinstance(m, nn.BatchNorm2d):\n",
    "          m.eval()\n",
    "  backbone.apply(set_bn_eval)\n",
    "  ```\n",
    "\n",
    "  (Params can still be trainable; this just freezes running stats.)\n",
    "\n",
    "### Weight decay hygiene (optional but nice)\n",
    "\n",
    "Avoid weight decay on BN and bias:\n",
    "\n",
    "```python\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    if n.endswith('bias') or 'bn' in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": decay, \"weight_decay\": 1e-4, \"lr\": 3e-4},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0, \"lr\": 3e-4},\n",
    "])\n",
    "```\n",
    "\n",
    "### Schedulers (keep it simple)\n",
    "\n",
    "* **Cosine with warmup** or **OneCycleLR** both work well:\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "# or:\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(loader), epochs=E)\n",
    "```\n",
    "\n",
    "### Quick guidance\n",
    "\n",
    "* **Small dataset / big domain shift (e.g., MRI):** A or C.\n",
    "* **Moderate dataset / some domain shift:** B with discriminative LRs.\n",
    "* **Plenty of data:** Train all, normal LRs, standard fine-tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08897a7e-5b26-46f3-9850-709e9247eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
