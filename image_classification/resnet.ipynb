{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02e9318-2c83-4e46-b549-de6d6f7c2b5a",
   "metadata": {},
   "source": [
    "## ResNet \n",
    "\n",
    "\n",
    "ResNet-18 is a **lightweight convolutional neural network** of the family **ResNet-34, ResNet-50, and ResNet-152**. \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Core Strengths**\n",
    "\n",
    "* **Feature extraction from images**\n",
    "  ResNet-18 learns rich low-to-mid level visual features — edges, textures, shapes — and can generalize well to many computer vision tasks.\n",
    "* **Transfer learning**\n",
    "  Because of its pretrained availability on ImageNet, it’s often used as a backbone for other tasks, fine-tuning only the last few layers.\n",
    "* **Efficiency**\n",
    "  Fewer layers and parameters mean **faster inference** and **lower memory usage**, making it ideal for edge devices and embedded systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Tasks ResNet-18 is Well-Suited For**\n",
    "\n",
    "| Category                             | Examples                                            | Why ResNet-18 Works Well                                                                           |\n",
    "| ------------------------------------ | --------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **Image Classification**             | CIFAR-10, ImageNet subset, medical images           | Pretrained weights + residual connections allow robust classification even with moderate datasets. |\n",
    "| **Object Detection (as a backbone)** | Faster R-CNN, YOLO variants (lightweight configs)   | Good trade-off between speed and accuracy; lighter than ResNet-50.                                 |\n",
    "| **Semantic Segmentation**            | U-Net / DeepLabV3 with ResNet-18 encoder            | Captures spatial features well without massive computation.                                        |\n",
    "| **Face Recognition & Verification**  | FaceNet/ArcFace style embeddings                    | Can be trained to produce discriminative embeddings.                                               |\n",
    "| **Medical Imaging**                  | X-ray, MRI, histopathology classification           | Handles 2D image modalities well; pretrained models transfer nicely.                               |\n",
    "| **Embedded & Mobile Vision**         | On drones, robots, Raspberry Pi                     | Low latency and small memory footprint.                                                            |\n",
    "| **Representation Learning**          | Self-supervised tasks (SimCLR, MoCo with ResNet-18) | Simpler backbone speeds up experimentation.                                                        |\n",
    "| **Anomaly / Defect Detection**       | Industrial inspection                               | Good for feature extraction, combined with anomaly detection models.                               |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When NOT to Choose ResNet-18**\n",
    "\n",
    "* If you need **state-of-the-art accuracy on very complex datasets** (e.g., full ImageNet competition, COCO detection) → deeper ResNets or vision transformers might do better.\n",
    "* If the input has **extreme fine-grained details** (e.g., high-res satellite imagery), a deeper backbone often performs better.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d135272-611a-48da-a374-662dcb1ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchviz\n",
    "\n",
    "\n",
    "#ResNet18_Weights.DEFAULT is equivalent to ResNet18_Weights.IMAGENET1K_V1. \n",
    "resnet18=torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT', progress= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a11579-d670-45cf-9db1-642ac3aab149",
   "metadata": {},
   "source": [
    "## How to find model input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238ac29f-8892-4a00-941a-b381ecbbdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18 input size:  512\n",
      "resnet18 output size:  1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \",resnet18.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d08aff-8ef0-4759-8d2c-0cf648e32383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/resnet18_graph.svg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  resnet18 has an averagepool layer at the end.\n",
    "#  So the input size does not matter much provided the feature map size is greater than kernel size.\n",
    "\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8b3b3-77e3-491f-b6da-eac9acb6df50",
   "metadata": {},
   "source": [
    "![](images/resnet18_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef909944-f0a0-41b8-95a8-c73d9ec95b33",
   "metadata": {},
   "source": [
    "## Finetune the model on a new dataset with 10 labels\n",
    "\n",
    "\n",
    "Let’s say we want to finetune the model on a new dataset with `10` labels. In resnet, the classifier is the last linear layer `model.fc.` We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.\n",
    "\n",
    "\n",
    "```python\n",
    "for params in resnet18.parameters():\n",
    "    params.requiers_gard=False\n",
    "\n",
    "resnet18.fc=torch.nn.Linear(512,10)\n",
    "\n",
    "```\n",
    "\n",
    "Now all parameters in the model, except the parameters of `model.fc`, are frozen. The only parameters that compute gradients are the `weights` and `bias` of `model.fc.`\n",
    "\n",
    "```python\n",
    "optimizer=torch.optim.SGD(resnet18.fc.parameters(),lr=1e-2,momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711034c0-1cbc-4168-b40a-4afcd5ee36aa",
   "metadata": {},
   "source": [
    "## Black and white Image Input\n",
    "\n",
    "The pretrained ResNet-18 expects 3-channel RGB at `conv1`. With a single-channel (monochrome) input you have a few good options:\n",
    "\n",
    "### 1) Duplicate the channel (fastest, no weight surgery)\n",
    "\n",
    "Preprocess your 1-channel image to 3 channels by repeating it. `transforms.Grayscale`:\n",
    "- If num_output_channels == 1 : returned image is single channel\n",
    "- If num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
    "\n",
    "```python\n",
    "# during transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # 1→3 by duplication\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),  # ImageNet stats\n",
    "])\n",
    "```\n",
    "\n",
    "Pros: zero code change to the model; you keep pretrained weights intact.\n",
    "Cons: a tiny bit redundant, but works very well in practice.\n",
    "\n",
    "### 2) Replace `conv1` with 1 input channel and **port** pretrained weights\n",
    "\n",
    "Average the RGB kernels into a single channel:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "m = models.resnet18(weights='models.ResNet18_Weights.IMAGENET1K_V1')\n",
    "w = m.conv1.weight  # [64, 3, 7, 7]\n",
    "\n",
    "m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    m.conv1.weight[:] = w.mean(dim=1, keepdim=True)  # [64,1,7,7]\n",
    "```\n",
    "\n",
    "(You can also use a weighted sum like `0.2989 R + 0.5870 G + 0.1140 B` instead of `.mean`.)\n",
    "\n",
    "Pros: no wasted computation; uses pretrained filters sensibly.\n",
    "Cons: a tiny bit of code; but this is the cleanest if you’re truly single-channel end-to-end.\n",
    "\n",
    "### 3) Add a learnable 1→3 adapter in front\n",
    "\n",
    "Keep the pretrained model intact and learn a shallow mapping:\n",
    "\n",
    "```python\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)  # start as “repeat”\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.map(x)\n",
    "\n",
    "adapter = GrayToRGB()\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "full = nn.Sequential(adapter, model)\n",
    "```\n",
    "\n",
    "Pros: lets the network learn the best 1→3 projection.\n",
    "Cons: a few extra params; slightly more moving parts.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization notes\n",
    "\n",
    "* If you **duplicate to 3-ch**, you can keep ImageNet mean/std as above, or compute dataset-specific stats and use those for all 3 channels (same numbers repeated).\n",
    "* If you **switch to 1-ch conv1**, use single mean/std (e.g., `(mean,)` and `(std,)`) matching your grayscale dataset.\n",
    "\n",
    "### Which should you pick?\n",
    "\n",
    "* **Quick wins / transfer learning**: Option **1** (duplicate) is perfectly fine and very common.\n",
    "* **Purist, minimal compute**: Option **2** (port weights) is elegant and usually performs best.\n",
    "* **Data shift concerns** (e.g., MRI/CT with unusual intensity): Option **3** gives flexibility; also consider dataset-specific normalization and fine-tuning early layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb833e9a-a91b-4563-b85d-ecc62b0724c6",
   "metadata": {},
   "source": [
    "### How to train the option 3 Network (learnable 1→3 adapter in front)\n",
    "\n",
    "With option 3 (a learnable 1→3 “adapter” in front of a pretrained ResNet-18), you’ve got three common training strategies. Pick one based on data size and how different your grayscale data is from ImageNet.\n",
    "\n",
    "### A. Freeze backbone first, train adapter + head (safe start)\n",
    "\n",
    "1–3 epochs:\n",
    "\n",
    "* Freeze **all** ResNet18 params.\n",
    "* Train only the `GrayToRGB` adapter and the final `fc`.\n",
    "\n",
    "Then unfreeze the backbone (optionally with a lower LR) and fine-tune.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class GrayToRGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.map = nn.Conv2d(1, 3, kernel_size=1, bias=False)\n",
    "        nn.init.constant_(self.map.weight, 1/3)\n",
    "\n",
    "    def forward(self, x): return self.map(x)\n",
    "\n",
    "backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(backbone.fc.in_features, n_classes)  # replace head\n",
    "\n",
    "model = nn.Sequential(GrayToRGB(), backbone)\n",
    "\n",
    "# --- Phase 1: freeze backbone ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# optimize only adapter + fc\n",
    "params = list(model[0].parameters()) + list(backbone.fc.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "After a few epochs:\n",
    "\n",
    "```python\n",
    "# --- Phase 2: unfreeze backbone with smaller LR ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model[0].parameters(), \"lr\": 1e-3},          # adapter\n",
    "    {\"params\": backbone.layer1.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(), \"lr\": 2.5e-4},\n",
    "    {\"params\": backbone.layer3.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.layer4.parameters(), \"lr\": 1.25e-4},\n",
    "    {\"params\": backbone.fc.parameters(),     \"lr\": 1e-3},\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### B. Train everything, but with **discriminative learning rates** (faster)\n",
    "\n",
    "Good when you have a moderate dataset and want quick convergence without a freezing phase.\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model[0].parameters(),           \"lr\": 1e-3},  # adapter highest\n",
    "    {\"params\": backbone.layer1.parameters(),    \"lr\": 5e-4},\n",
    "    {\"params\": backbone.layer2.parameters(),    \"lr\": 3e-4},\n",
    "    {\"params\": backbone.layer3.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.layer4.parameters(),    \"lr\": 2e-4},\n",
    "    {\"params\": backbone.fc.parameters(),        \"lr\": 1e-3},  # head high\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### C. Unfreeze progressively (“gradual unfreezing”)\n",
    "\n",
    "Start with only adapter+fc, then unfreeze layers one block at a time every few epochs (layer4 → layer3 → …). This is handy with small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Do we freeze the adapter conv?\n",
    "\n",
    "* **Usually not.** Let it learn a smart projection beyond simple channel copy.\n",
    "* If the dataset is tiny and unstable, you *can* freeze it for the first few hundred steps.\n",
    "\n",
    "### BatchNorm tips\n",
    "\n",
    "* If batch size is small (≤16), consider putting the backbone’s BN layers in **eval** mode during early training:\n",
    "\n",
    "  ```python\n",
    "  def set_bn_eval(m):\n",
    "      if isinstance(m, nn.BatchNorm2d):\n",
    "          m.eval()\n",
    "  backbone.apply(set_bn_eval)\n",
    "  ```\n",
    "\n",
    "  (Params can still be trainable; this just freezes running stats.)\n",
    "\n",
    "### Weight decay hygiene (optional but nice)\n",
    "\n",
    "Avoid weight decay on BN and bias:\n",
    "\n",
    "```python\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    if n.endswith('bias') or 'bn' in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": decay, \"weight_decay\": 1e-4, \"lr\": 3e-4},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0, \"lr\": 3e-4},\n",
    "])\n",
    "```\n",
    "\n",
    "### Schedulers (keep it simple)\n",
    "\n",
    "* **Cosine with warmup** or **OneCycleLR** both work well:\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "# or:\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(loader), epochs=E)\n",
    "```\n",
    "\n",
    "### Quick guidance\n",
    "\n",
    "* **Small dataset / big domain shift (e.g., MRI):** A or C.\n",
    "* **Moderate dataset / some domain shift:** B with discriminative LRs.\n",
    "* **Plenty of data:** Train all, normal LRs, standard fine-tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08897a7e-5b26-46f3-9850-709e9247eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
