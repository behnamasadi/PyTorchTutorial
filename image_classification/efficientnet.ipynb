{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae77b05-a43f-4891-856e-098b12a04afa",
   "metadata": {},
   "source": [
    "## **1. What EfficientNet Is**\n",
    "\n",
    "EfficientNet is a family of convolutional neural networks (CNNs) introduced by Google (2019) designed to achieve **high accuracy with much fewer parameters and FLOPs** compared to previous models like ResNet, Inception, or DenseNet.\n",
    "It’s basically a *scalable* architecture that balances **depth**, **width**, and **resolution**.\n",
    "\n",
    "\n",
    "\n",
    "#### **1.1. Depth, Width, and Resolution: Core Definitions**\n",
    "\n",
    "| Term           | Meaning                                                 | In Neural Networks                                                                             |\n",
    "| -------------- | ------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Depth**      | Number of layers (how *deep* the network is)            | How many convolutional or block layers are stacked sequentially (e.g. MBConv repeats).         |\n",
    "| **Width**      | Number of channels per layer (how *wide* each layer is) | The number of feature maps (filters) in each convolution — controls representational capacity. |\n",
    "| **Resolution** | Spatial size of input/output feature maps               | Height × Width of the image or intermediate feature maps.                                      |\n",
    "\n",
    "So yes:\n",
    "\n",
    "* **Depth →** number of layers.\n",
    "* **Width →** number of channels (feature maps).\n",
    "* **Resolution →** spatial size (H × W).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why EfficientNet Was Created**\n",
    "\n",
    "Traditional model-scaling methods (just making the network deeper or wider or feeding larger images) improve accuracy but quickly lead to inefficiency.\n",
    "EfficientNet uses a **principled scaling method** to get the most accuracy per computation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Two Key Ideas**\n",
    "\n",
    "#### A. **EfficientNet-B0 (the baseline)**\n",
    "\n",
    "* They searched for a small but powerful baseline network using **neural architecture search (NAS)**.\n",
    "* This gave a mobile-friendly architecture with compound building blocks (MBConv, similar to MobileNetV2).\n",
    "\n",
    "#### B. **Compound Scaling**\n",
    "\n",
    "* Instead of arbitrarily scaling depth, width, or input resolution, EfficientNet scales all three together using fixed **scaling coefficients**. EfficientNet’s **compound scaling** says: grow **depth**, **width**, and **input resolution** together by fixed multipliers so compute grows predictably.\n",
    "\n",
    "Formally, if you want to scale up EfficientNet:\n",
    "\n",
    "* Depth → $d = \\alpha^\\phi$\n",
    "* Width → $w = \\beta^\\phi$\n",
    "* Resolution → $r = \\gamma^\\phi$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2\n",
    "$$\n",
    "\n",
    "(where $\\phi$ is a user-chosen scaling factor indicating how much more compute you want to spend).\n",
    "This yields EfficientNet-B1 … B7 (each larger/more accurate than the last).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b256b1d-ea74-4120-a042-9f6516172591",
   "metadata": {},
   "source": [
    "## **4. The Rule**\n",
    "\n",
    "Choose constants $\\alpha,\\beta,\\gamma>1$ and a user knob $\\phi \\in {0,1,2,\\dots}$.\n",
    "\n",
    "* Depth: $d=\\alpha^{\\phi}$\n",
    "* Width $channels$: $w=\\beta^{\\phi}$\n",
    "* Resolution $image size$: $r=\\gamma^{\\phi}$\n",
    "\n",
    "Conv cost scales roughly as $ \\text{FLOPs} \\propto d \\cdot w^2 \\cdot r^2$.\n",
    "So if we enforce\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2,\n",
    "$$\n",
    "\n",
    "then each time you increase $\\phi$ by 1, **FLOPs ≈ double**.\n",
    "\n",
    "A commonly cited set (close to the original paper):\n",
    "$\\alpha=1.2,; \\beta=1.1,; \\gamma=1.15$ → $\\alpha\\beta^2\\gamma^2 \\approx 1.92 \\approx 2$.\n",
    "\n",
    "Below are concrete numbers using these.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2 Numerical examples (starting from a baseline B0)**\n",
    "\n",
    "Assume baseline depth/width/resolution are all “1×” (e.g., 224×224 input).\n",
    "\n",
    "| $\\phi$ | $ d=\\alpha^\\phi$ | $w=\\beta^\\phi$ | $r=\\gamma^\\phi$ |  New input (≈ $224\\cdot r$) | FLOPs scale $d,w^2,r^2$ |\n",
    "| -----: | --------------: | -------------: | --------------: | --------------------------: | ----------------------: |\n",
    "|      0 |           1.000 |          1.000 |           1.000 |                         224 |                   1.00× |\n",
    "|      1 |           1.200 |          1.100 |           1.150 |     **≈258** (round to 256) |              **≈1.92×** |\n",
    "|      2 |           1.440 |          1.210 |           1.322 | **≈296** (round to 296/288) |              **≈3.69×** |\n",
    "|      3 |           1.728 |          1.331 |           1.521 |     **≈341** (round to 336) |              **≈7.08×** |\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* At $\\phi=1$: make the net ~20% deeper, ~10% wider, feed ~15% larger images → ~1.9× compute.\n",
    "* At $\\phi=2$: apply those multipliers again → ~3.7× compute vs. baseline.\n",
    "* At $\\phi=3$: ~7.1× compute vs. baseline.\n",
    "\n",
    "*(In practice you round image sizes to multiples of 8/16 and channels to hardware-friendly sizes.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### Mini “what-if” examples\n",
    "\n",
    "1. **Channels and layers**\n",
    "   Baseline has 32 channels and 10 layers. With $\\phi=2$:\n",
    "\n",
    "* Width: $32 \\cdot \\beta^2 \\approx 32 \\cdot 1.21 \\approx 39$ → round to 40.\n",
    "* Depth: $10 \\cdot \\alpha^2 = 10 \\cdot 1.44 = 14.4$ → ~14–15 layers.\n",
    "* Resolution: $224 \\cdot \\gamma^2 \\approx 224 \\cdot 1.322 \\approx 296$ → round to 288/296.\n",
    "\n",
    "2. **Compute sanity check**\n",
    "   Jumping from $\\phi=1$ to $\\phi=3$ multiplies FLOPs by $\\approx 7.08/1.92 \\approx 3.7$.\n",
    "   That’s consistent because each +1 in $\\phi$ nearly doubles compute.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "* The constraint $\\alpha \\beta^2 \\gamma^2 \\approx 2$ ensures **predictable ~2× compute per step**.\n",
    "* Scaling **all three** (depth, width, resolution) is more *accuracy-efficient* than scaling any single dimension alone.\n",
    "* Real models round/tune sizes, but the compound law is the guiding principle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5842f-c327-4e9d-be8c-7891ebf77519",
   "metadata": {},
   "source": [
    "## **5. EfficientNet Architecture**\n",
    "Let’s go through the **EfficientNet** family (B0–B7) step by step, including:\n",
    "\n",
    "1. Core architectural **principles**\n",
    "2. **Building blocks** (MBConv, SE, etc.)\n",
    "3. **Differences across B0–B7**\n",
    "4. Discussion of **CBAM** (and where it can be integrated)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **5.1. Building Blocks**\n",
    "\n",
    "#### **(a) MBConv (Mobile Inverted Bottleneck Convolution)**\n",
    "\n",
    "The **MBConv** block comes from **MobileNetV2**.\n",
    "It uses a **depthwise separable convolution** + **inverted residuals**:\n",
    "\n",
    "```\n",
    "Input → 1x1 Expansion Conv → 3x3 Depthwise Conv → SE → 1x1 Projection Conv → Output\n",
    "```\n",
    "\n",
    "* **Expansion ratio (t):** how much to expand channels before depthwise conv\n",
    "* **Kernel size:** 3×3 or 5×5 depending on stage\n",
    "* **Skip connection:** only if stride=1 and input/output have same channels\n",
    "\n",
    "#### **(b) SE (Squeeze-and-Excitation)**\n",
    "\n",
    "Integrated inside each MBConv.\n",
    "It recalibrates channel importance:\n",
    "\n",
    "1. Global average pool (squeeze)\n",
    "2. Two FC layers (reduce → expand)\n",
    "3. Multiply sigmoid output with feature map\n",
    "\n",
    "Formally:\n",
    "$$\n",
    "\\mathbf{z}_c = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c,i,j}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{s} = \\sigma(W_2 \\cdot \\delta(W_1 \\cdot \\mathbf{z}))\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_c = \\mathbf{s}_c \\cdot X_c\n",
    "$$\n",
    "\n",
    "The SE ratio is typically **0.25** (i.e., reduction by 4×).\n",
    "\n",
    "#### **(c) CBAM (Convolutional Block Attention Module)**\n",
    "\n",
    "CBAM is **not** part of original EfficientNet, but sometimes researchers add it to enhance attention.\n",
    "It has:\n",
    "\n",
    "1. **Channel Attention (CA)** → similar to SE\n",
    "2. **Spatial Attention (SA)** → a 7×7 convolution over channel-aggregated map\n",
    "\n",
    "So CBAM = Channel Attention → Spatial Attention.\n",
    "If inserted, it’s usually placed after the SE block (or replacing SE).\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2. EfficientNet-B0 Architecture\n",
    "\n",
    "EfficientNet-B0 is the base model; others (B1–B7) scale it up.\n",
    "Below is the canonical configuration:\n",
    "\n",
    "| Stage | Operator | Resolution | Channels | Layers | Expansion | Kernel | SE | Stride |\n",
    "|:------|:----------|:------------|:----------|:--------|:-----------|:--------|:------|:-------|\n",
    "| Stem | Conv3x3 | 224x224 | 32 | 1 | – | 3x3 | – | 2 |\n",
    "| 1 | MBConv1 | 112x112 | 16 | 1 | 1 | 3x3 | ✓ | 1 |\n",
    "| 2 | MBConv6 | 112x112 | 24 | 2 | 6 | 3x3 | ✓ | 2 |\n",
    "| 3 | MBConv6 | 56x56 | 40 | 2 | 6 | 5x5 | ✓ | 2 |\n",
    "| 4 | MBConv6 | 28x28 | 80 | 3 | 6 | 3x3 | ✓ | 2 |\n",
    "| 5 | MBConv6 | 14x14 | 112 | 3 | 6 | 5x5 | ✓ | 1 |\n",
    "| 6 | MBConv6 | 14x14 | 192 | 4 | 6 | 5x5 | ✓ | 2 |\n",
    "| 7 | MBConv6 | 7x7 | 320 | 1 | 6 | 3x3 | ✓ | 1 |\n",
    "| Head | Conv1x1 + Pool + FC | 7x7 | 1280 | 1 | – | 1x1 | – | – |\n",
    "\n",
    "Total: **8 MBConv stages** (some repeated) + 1 stem + 1 head.\n",
    "All MBConv blocks have **SE** inside.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3. Scaling to B1–B7\n",
    "\n",
    "Scaling increases:\n",
    "\n",
    "* **Resolution** (input size)\n",
    "* **Depth** (more MBConv repeats)\n",
    "* **Width** (more channels)\n",
    "\n",
    "| Model | Input (px) | Depth Mult | Width Mult | #Params (M) | #MBConv Blocks |\n",
    "| :---- | :--------- | :--------- | :--------- | :---------- | :------------- |\n",
    "| B0    | 224        | 1.0        | 1.0        | 5.3         | 16             |\n",
    "| B1    | 240        | 1.1        | 1.0        | 7.8         | 16             |\n",
    "| B2    | 260        | 1.2        | 1.1        | 9.2         | 20             |\n",
    "| B3    | 300        | 1.4        | 1.2        | 12.0        | 24             |\n",
    "| B4    | 380        | 1.8        | 1.4        | 19.0        | 28             |\n",
    "| B5    | 456        | 2.2        | 1.6        | 30.0        | 32             |\n",
    "| B6    | 528        | 2.6        | 1.8        | 43.0        | 38             |\n",
    "| B7    | 600        | 3.1        | 2.0        | 66.0        | 44             |\n",
    "\n",
    "Every model uses the same **MBConv + SE** layout; only the scaling changes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4. EfficientNet + CBAM (modified variants)\n",
    "\n",
    "When **CBAM** is added:\n",
    "\n",
    "* Replace SE with CBAM, or\n",
    "* Add CBAM after SE (Channel Attention → Spatial Attention).\n",
    "\n",
    "Empirical studies show CBAM can improve accuracy by refining spatial focus, but at the cost of slightly more computation.\n",
    "\n",
    "Example (pseudo-layout for one block):\n",
    "\n",
    "```\n",
    "x = MBConv(...)\n",
    "x = SE(x)\n",
    "x = CBAM(x)\n",
    "```\n",
    "\n",
    "This yields “EfficientNet-CBAM” (seen in some research papers or GitHub repos).\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5. Summary of the Pipeline\n",
    "\n",
    "For any EfficientNet-Bx:\n",
    "\n",
    "```\n",
    "Input\n",
    "↓\n",
    "Conv3x3 (Stem)\n",
    "↓\n",
    "[MBConv1 + SE] × n1\n",
    "↓\n",
    "[MBConv6 + SE] × n2\n",
    "↓\n",
    "[MBConv6 + SE] × n3\n",
    "↓\n",
    "[MBConv6 + SE] × n4\n",
    "↓\n",
    "[MBConv6 + SE] × n5\n",
    "↓\n",
    "[MBConv6 + SE] × n6\n",
    "↓\n",
    "[MBConv6 + SE] × n7\n",
    "↓\n",
    "Conv1x1 + Pool + FC (Head)\n",
    "```\n",
    "\n",
    "Optionally add:\n",
    "\n",
    "```\n",
    "+ CBAM attention (optional enhancement)\n",
    "```\n",
    "\n",
    "and:\n",
    "$$\n",
    "[n_1, n_2, n_3, n_4, n_5, n_6, n_7] = [1, 2, 2, 3, 3, 4, 1]\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5228e0-306c-43da-9062-fbe708e87373",
   "metadata": {},
   "source": [
    "## Explanation of Notation\n",
    "**[MBConv1 + SE] × n1** or **[MBConv6 + SE] × n3**\n",
    "is standard in papers and tables describing **EfficientNet**, **MobileNetV2**, and related models.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 1. What the number (1 or 6) means\n",
    "\n",
    "The number after MBConv — e.g. **MBConv1** or **MBConv6** —\n",
    "is the **expansion ratio**:\n",
    "\n",
    "$$\n",
    "t = 1, 6\n",
    "$$\n",
    "\n",
    "It controls how much the channel dimension is expanded inside the block:\n",
    "\n",
    "$$\n",
    "C_{expanded} = t \\times C_{input}\n",
    "$$\n",
    "\n",
    "* **MBConv1** → expansion ratio ( t = 1 ): no expansion (input channels unchanged)\n",
    "* **MBConv6** → expansion ratio ( t = 6 ): 6× more channels in the middle layers\n",
    "\n",
    "Example:\n",
    "If input has 32 channels:\n",
    "\n",
    "* MBConv1 → expanded to 32\n",
    "* MBConv6 → expanded to 192\n",
    "\n",
    "✅ Larger expansion ratio → higher capacity but more compute.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. What SE means\n",
    "\n",
    "**SE** = *Squeeze-and-Excitation* block, which adds **channel attention**.\n",
    "\n",
    "It computes:\n",
    "$$\n",
    "\\text{scale} = \\sigma(W_2 , \\delta(W_1 , \\text{GAP}(x)))\n",
    "$$\n",
    "\n",
    "and multiplies this scale back to the feature map.\n",
    "(SE squeezes spatial info via global average pooling and excites important channels.)\n",
    "\n",
    "So, **MBConv6 + SE** = an MBConv block with expansion 6 and an SE attention inside.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. What × n means\n",
    "\n",
    "The **× n** tells you **how many times** that block is repeated in sequence (same stage).\n",
    "\n",
    "So:\n",
    "\n",
    "* `[MBConv1 + SE] × n1` → repeat that structure **n1 times**\n",
    "* `[MBConv6 + SE] × n3` → repeat that structure **n3 times**\n",
    "\n",
    "Each stage typically keeps the same input/output channel dimensions and stride (except first block of a stage, which may downsample).\n",
    "\n",
    "Example (EfficientNet-B0 simplified):\n",
    "\n",
    "| Stage | Operator         | #Repeats | Output Channels | Stride | Resolution |\n",
    "| ----- | ---------------- | -------- | --------------- | ------ | ---------- |\n",
    "| 1     | Conv3×3          | 1        | 32              | 2      | 112×112    |\n",
    "| 2     | **MBConv1 + SE** | **1**    | 16              | 1      | 112×112    |\n",
    "| 3     | **MBConv6 + SE** | **2**    | 24              | 2      | 56×56      |\n",
    "| 4     | **MBConv6 + SE** | **2**    | 40              | 2      | 28×28      |\n",
    "| 5     | **MBConv6 + SE** | **3**    | 80              | 2      | 14×14      |\n",
    "| 6     | **MBConv6 + SE** | **3**    | 112             | 1      | 14×14      |\n",
    "| 7     | **MBConv6 + SE** | **4**    | 192             | 2      | 7×7        |\n",
    "| 8     | **MBConv6 + SE** | **1**    | 320             | 1      | 7×7        |\n",
    "\n",
    "So for example:\n",
    "\n",
    "* `[MBConv6 + SE] × 3` means 3 consecutive MBConv6 blocks (with SE) in that stage.\n",
    "* The first of them might downsample (stride 2), the rest keep stride 1.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary of notation\n",
    "\n",
    "| Notation             | Meaning                                                                     |\n",
    "| -------------------- | --------------------------------------------------------------------------- |\n",
    "| MBConv               | Mobile Inverted Bottleneck Conv block                                       |\n",
    "| MBConv1              | Expansion ratio = 1                                                         |\n",
    "| MBConv6              | Expansion ratio = 6                                                         |\n",
    "| SE                   | Squeeze-and-Excitation block (channel attention)                            |\n",
    "| × n                  | Repeat n times in that stage                                                |\n",
    "| `[MBConv6 + SE] × 3` | Three repeated inverted-bottleneck blocks with expansion 6 and SE attention |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Quick interpretation example:**\n",
    "\n",
    "`[MBConv6 + SE] × 3`\n",
    "→ Each block:\n",
    "\n",
    "* expands channels by ×6,\n",
    "* applies depthwise conv,\n",
    "* applies SE attention,\n",
    "* projects back and maybe adds residual,\n",
    "  and there are **3 such blocks** in that stage.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e478a-5089-4165-9d23-1da7e01f79f9",
   "metadata": {},
   "source": [
    "## **6. EfficientNet Variants From timm**\n",
    "This touches the **core architectural evolution** from **EfficientNet (V1)** to **EfficientNetV2 (V2)**.\n",
    "Let’s go step by step and clarify **why EfficientNetV2 uses names like `s`, `m`, `l`, `xl`** instead of `b0–b8`, and what the difference really means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9079750a-bb17-4a49-9bde-49062043a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet_b0.ra4_e3600_r224_in1k\n",
      "efficientnet_b0.ra_in1k\n",
      "efficientnet_b1.ft_in1k\n",
      "efficientnet_b1.ra4_e3600_r240_in1k\n",
      "efficientnet_b1_pruned.in1k\n",
      "efficientnet_b2.ra_in1k\n",
      "efficientnet_b2_pruned.in1k\n",
      "efficientnet_b3.ra2_in1k\n",
      "efficientnet_b3_pruned.in1k\n",
      "efficientnet_b4.ra2_in1k\n",
      "efficientnet_b5.sw_in12k\n",
      "efficientnet_b5.sw_in12k_ft_in1k\n",
      "efficientnet_el.ra_in1k\n",
      "efficientnet_el_pruned.in1k\n",
      "efficientnet_em.ra2_in1k\n",
      "efficientnet_es.ra_in1k\n",
      "efficientnet_es_pruned.in1k\n",
      "efficientnet_lite0.ra_in1k\n",
      "efficientnetv2_rw_m.agc_in1k\n",
      "efficientnetv2_rw_s.ra2_in1k\n",
      "efficientnetv2_rw_t.ra2_in1k\n",
      "gc_efficientnetv2_rw_t.agc_in1k\n",
      "test_efficientnet.r160_in1k\n",
      "test_efficientnet_evos.r160_in1k\n",
      "test_efficientnet_gn.r160_in1k\n",
      "test_efficientnet_ln.r160_in1k\n",
      "tf_efficientnet_b0.aa_in1k\n",
      "tf_efficientnet_b0.ap_in1k\n",
      "tf_efficientnet_b0.in1k\n",
      "tf_efficientnet_b0.ns_jft_in1k\n",
      "tf_efficientnet_b1.aa_in1k\n",
      "tf_efficientnet_b1.ap_in1k\n",
      "tf_efficientnet_b1.in1k\n",
      "tf_efficientnet_b1.ns_jft_in1k\n",
      "tf_efficientnet_b2.aa_in1k\n",
      "tf_efficientnet_b2.ap_in1k\n",
      "tf_efficientnet_b2.in1k\n",
      "tf_efficientnet_b2.ns_jft_in1k\n",
      "tf_efficientnet_b3.aa_in1k\n",
      "tf_efficientnet_b3.ap_in1k\n",
      "tf_efficientnet_b3.in1k\n",
      "tf_efficientnet_b3.ns_jft_in1k\n",
      "tf_efficientnet_b4.aa_in1k\n",
      "tf_efficientnet_b4.ap_in1k\n",
      "tf_efficientnet_b4.in1k\n",
      "tf_efficientnet_b4.ns_jft_in1k\n",
      "tf_efficientnet_b5.aa_in1k\n",
      "tf_efficientnet_b5.ap_in1k\n",
      "tf_efficientnet_b5.in1k\n",
      "tf_efficientnet_b5.ns_jft_in1k\n",
      "tf_efficientnet_b5.ra_in1k\n",
      "tf_efficientnet_b6.aa_in1k\n",
      "tf_efficientnet_b6.ap_in1k\n",
      "tf_efficientnet_b6.ns_jft_in1k\n",
      "tf_efficientnet_b7.aa_in1k\n",
      "tf_efficientnet_b7.ap_in1k\n",
      "tf_efficientnet_b7.ns_jft_in1k\n",
      "tf_efficientnet_b7.ra_in1k\n",
      "tf_efficientnet_b8.ap_in1k\n",
      "tf_efficientnet_b8.ra_in1k\n",
      "tf_efficientnet_cc_b0_4e.in1k\n",
      "tf_efficientnet_cc_b0_8e.in1k\n",
      "tf_efficientnet_cc_b1_8e.in1k\n",
      "tf_efficientnet_el.in1k\n",
      "tf_efficientnet_em.in1k\n",
      "tf_efficientnet_es.in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k_475\n",
      "tf_efficientnet_lite0.in1k\n",
      "tf_efficientnet_lite1.in1k\n",
      "tf_efficientnet_lite2.in1k\n",
      "tf_efficientnet_lite3.in1k\n",
      "tf_efficientnet_lite4.in1k\n",
      "tf_efficientnetv2_b0.in1k\n",
      "tf_efficientnetv2_b1.in1k\n",
      "tf_efficientnetv2_b2.in1k\n",
      "tf_efficientnetv2_b3.in1k\n",
      "tf_efficientnetv2_b3.in21k\n",
      "tf_efficientnetv2_b3.in21k_ft_in1k\n",
      "tf_efficientnetv2_l.in1k\n",
      "tf_efficientnetv2_l.in21k\n",
      "tf_efficientnetv2_l.in21k_ft_in1k\n",
      "tf_efficientnetv2_m.in1k\n",
      "tf_efficientnetv2_m.in21k\n",
      "tf_efficientnetv2_m.in21k_ft_in1k\n",
      "tf_efficientnetv2_s.in1k\n",
      "tf_efficientnetv2_s.in21k\n",
      "tf_efficientnetv2_s.in21k_ft_in1k\n",
      "tf_efficientnetv2_xl.in21k\n",
      "tf_efficientnetv2_xl.in21k_ft_in1k\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "all_efficientnet = timm.list_models(\"*efficientnet*\", pretrained=True)\n",
    "for m in all_efficientnet:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f281aa-fe8b-4715-8ca6-24718e469eb0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##  **7. EfficientNet (V1): the original scaling idea (B0–B8)**\n",
    "\n",
    "The original **EfficientNet (2019)** paper introduced the **compound scaling rule**:\n",
    "$$\n",
    "\\text{depth} \\propto \\alpha^\\phi, \\quad\n",
    "\\text{width} \\propto \\beta^\\phi, \\quad\n",
    "\\text{resolution} \\propto \\gamma^\\phi\n",
    "$$\n",
    "\n",
    "with constraint:\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* **ϕ** is a compound coefficient controlling how “big” the model is.\n",
    "* **B0** is the baseline network.\n",
    "* **B1–B8** are scaled versions by increasing ϕ.\n",
    "\n",
    "| Model | Resolution | Parameters | Top-1 Acc. (ImageNet) |\n",
    "| ----- | ---------- | ---------- | --------------------- |\n",
    "| B0    | 224×224    | 5.3 M      | ~77%                  |\n",
    "| B1    | 240×240    | 7.8 M      | ~79%                  |\n",
    "| B2    | 260×260    | 9.2 M      | ~80%                  |\n",
    "| B3    | 300×300    | 12 M       | ~81.5%                |\n",
    "| B4    | 380×380    | 19 M       | ~83%                  |\n",
    "| B5    | 456×456    | 30 M       | ~84%                  |\n",
    "| B6    | 528×528    | 43 M       | ~84.5%                |\n",
    "| B7    | 600×600    | 66 M       | ~85%                  |\n",
    "| B8    | 672×672    | 87 M       | ~85.7%                |\n",
    "\n",
    "✅ **Interpretation:**\n",
    "EfficientNet-B0→B8 are *scaled-up* versions of the same base architecture using a mathematical rule.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822d6d7-82af-414d-8918-4bffd2888360",
   "metadata": {},
   "source": [
    "## **8. EfficientNetV2 Overview**\n",
    "\n",
    "EfficientNetV2 (Tan & Le, 2021, *Google Research*) is a redesigned, faster, and more efficient version of EfficientNetV1.\n",
    "Its goals:\n",
    "\n",
    "1. **Reduce training time** (up to 5× faster).\n",
    "2. **Reduce memory and FLOPs** while maintaining or improving accuracy.\n",
    "3. **Improve performance on small images and dense tasks** (like segmentation).\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.1. High-Level Pipeline**\n",
    "\n",
    "```\n",
    "Input (e.g. 224×224)\n",
    "↓\n",
    "Conv3×3 (Stem)\n",
    "↓\n",
    "[Fused-MBConv + SE] × n₁\n",
    "↓\n",
    "[Fused-MBConv + SE] × n₂\n",
    "↓\n",
    "[MBConv + SE] × n₃\n",
    "↓\n",
    "[MBConv + SE] × n₄\n",
    "↓\n",
    "[MBConv + SE] × n₅\n",
    "↓\n",
    "Conv1×1 (Head)\n",
    "↓\n",
    "Global Average Pool\n",
    "↓\n",
    "Fully Connected (Classifier)\n",
    "↓\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "✅ Early stages → **Fused-MBConv** (faster on GPU/TPU, small resolution).\n",
    "✅ Later stages → **MBConv (with SE)** (better representation capacity).\n",
    "\n",
    "---\n",
    "\n",
    "### **8.2. EfficientNetV2 Building Blocks**\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2.1 MBConv Block (same as V1)**\n",
    "\n",
    "The standard **Mobile Inverted Bottleneck (MBConv)** block used in EfficientNetV1 and still in later stages of V2:\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2.2 Fused-MBConv Block (new in V2)**\n",
    "\n",
    "To make early layers faster, V2 *fuses* the first two steps of MBConv:\n",
    "the **1×1 expansion** and **3×3 depthwise conv**\n",
    "→ replaced by **a single 3×3 regular convolution**.\n",
    "\n",
    "### **8.3 Structure:**\n",
    "\n",
    "```\n",
    "Input\n",
    "↓\n",
    "3×3 Conv (expands and convolves together)\n",
    "↓\n",
    "BN + Swish\n",
    "↓\n",
    "(optional) SE\n",
    "↓\n",
    "1×1 Conv (projection)\n",
    "↓\n",
    "BN\n",
    "↓\n",
    "Residual (if stride=1 and same channels)\n",
    "```\n",
    "\n",
    "So, instead of:\n",
    "\n",
    "```\n",
    "1×1 expand → 3×3 depthwise → 1×1 project\n",
    "```\n",
    "\n",
    "it becomes:\n",
    "\n",
    "```\n",
    "3×3 normal conv → 1×1 project\n",
    "```\n",
    "\n",
    "✅ **Why:** Depthwise conv is memory-bound and inefficient on GPU for small feature maps.\n",
    "✅ **Benefit:** Fused-MBConv improves speed and training efficiency for early high-resolution stages.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.4. EfficientNetV2 Stage Configuration**\n",
    "\n",
    "For **EfficientNetV2-S**, the canonical structure is:\n",
    "\n",
    "| Stage | Operator            | Expansion | Repeats (n) | Output Channels | Stride |    SE   | Input Res (example 224×224) |\n",
    "| :---: | :------------------ | :-------: | :---------: | :-------------: | :----: | :-----: | :-------------------------: |\n",
    "|   1   | **Fused-MBConv1**   |     1     |      2      |        24       |    1   |    No   |           224×224           |\n",
    "|   2   | **Fused-MBConv4**   |     4     |      4      |        48       |    2   |    No   |           112×112           |\n",
    "|   3   | **Fused-MBConv4**   |     4     |      4      |        64       |    2   |    No   |            56×56            |\n",
    "|   4   | **MBConv4**         |     4     |      6      |       128       |    2   | **Yes** |            28×28            |\n",
    "|   5   | **MBConv6**         |     6     |      9      |       160       |    1   | **Yes** |            14×14            |\n",
    "|   6   | **MBConv6**         |     6     |      15     |       256       |    2   | **Yes** |             7×7             |\n",
    "|   7   | Conv1×1 + Pool + FC |     –     |      –      |       1280      |    –   |    –    |             7×7             |\n",
    "\n",
    "✅ `n` here corresponds to n₁ … n₆ (number of repeats per stage).\n",
    "✅ The **first MBConv** in each stage may downsample (stride = 2).\n",
    "✅ Later repeats in the same stage use stride = 1 and residuals.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.5. Key Differences from V1**\n",
    "\n",
    "| Aspect            | EfficientNet-V1        | EfficientNet-V2                                                       |\n",
    "| :---------------- | :--------------------- | :-------------------------------------------------------------------- |\n",
    "| Early layers      | MBConv                 | **Fused-MBConv (standard conv)**                                      |\n",
    "| Block types       | MBConv only            | **Hybrid: Fused-MBConv + MBConv**                                     |\n",
    "| SE usage          | All blocks             | Only in MBConv (later stages)                                         |\n",
    "| Expansion ratios  | 1, 6                   | Variable (1, 4, 6)                                                    |\n",
    "| Training strategy | AutoAugment, fixed res | **Progressive learning:** start with small images, gradually increase |\n",
    "| Accuracy          | up to ~85.7%           | up to ~86.3%                                                          |\n",
    "| Training speed    | slower                 | up to 5× faster                                                       |\n",
    "| Scaling           | Compound rule (ϕ)      | Manual scaling for S/M/L/XL                                           |\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.6. EfficientNetV2 Family Summary**\n",
    "\n",
    "| Model   | Params | Resolution | Top-1 Accuracy (ImageNet) | Notable Use          |\n",
    "| :------ | :----: | :--------: | :-----------------------: | :------------------- |\n",
    "| `v2_b0` |  ~7 M  |     224    |            ~79%           | Mobile-friendly      |\n",
    "| `v2_b1` | ~8.5 M |     240    |            ~80%           | Small GPU            |\n",
    "| `v2_b2` |  ~10 M |     260    |            ~81%           | Lightweight training |\n",
    "| `v2_b3` |  ~14 M |     300    |           ~82.9%          | Balanced             |\n",
    "| `v2_s`  |  ~22 M |     384    |           ~83.9%          | Standard choice      |\n",
    "| `v2_m`  |  ~55 M |     480    |           ~85.1%          | High accuracy        |\n",
    "| `v2_l`  | ~120 M |     480    |           ~85.7%          | Large GPU            |\n",
    "| `v2_xl` | ~208 M |     512    |           ~86.3%          | Maximum accuracy     |\n",
    "\n",
    "- ✅ V2-B0→B3 = small versions, similar scale to V1 (but hybrid blocks).\n",
    "- ✅ V2-S/M/L/XL = large, high-accuracy architectures.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.7. Summary of Key Ideas**\n",
    "\n",
    "| Concept                       | Description                                                     |\n",
    "| :---------------------------- | :-------------------------------------------------------------- |\n",
    "| **Fused-MBConv**              | Combines expansion + depthwise into one 3×3 conv for efficiency |\n",
    "| **MBConv + SE**               | Used in deeper layers for accuracy                              |\n",
    "| **Variable expansion ratios** | t = 1, 4, 6 instead of fixed 6                                  |\n",
    "| **Progressive training**      | Start small, increase resolution/regularization gradually       |\n",
    "| **Manual scaling**            | Replace B0–B8 with Small/Medium/Large/XL tailored designs       |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Intuitive summary:**\n",
    "EfficientNetV1 = mathematically scaled MBConv tower.\n",
    "EfficientNetV2 = hand-optimized hybrid of Fused-MBConv (fast early) + MBConv+SE (strong late), trained progressively for better speed–accuracy trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **8.8. When to use which**\n",
    "\n",
    "| GPU budget  | Recommended model                    | Reason                       |\n",
    "| ----------- | ------------------------------------ | ---------------------------- |\n",
    "| ≤ 4 GB VRAM | `tf_efficientnetv2_b0.in1k`          | Small, accurate, lightweight |\n",
    "| 4–8 GB      | `tf_efficientnetv2_s.in21k_ft_in1k`  | Balanced, fast training      |\n",
    "| 8–16 GB     | `tf_efficientnetv2_m.in21k_ft_in1k`  | Higher accuracy              |\n",
    "| ≥ 16 GB     | `tf_efficientnetv2_l.in21k_ft_in1k`  | SOTA accuracy                |\n",
    "| > 24 GB     | `tf_efficientnetv2_xl.in21k_ft_in1k` | Maximum accuracy, expensive  |\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.9. Summary**\n",
    "\n",
    "| Family                | Naming          | Scaling Rule       | Block Type            | Accuracy | Training Speed |\n",
    "| --------------------- | --------------- | ------------------ | --------------------- | -------- | -------------- |\n",
    "| **EfficientNet (V1)** | B0–B8           | Compound (α,β,γ)   | MBConv                | 77–85.7% | Moderate       |\n",
    "| **EfficientNetV2**    | B0–B3, S/M/L/XL | Manual progressive | MBConv + Fused-MBConv | 79–86.3% | Much faster    |\n",
    "\n",
    "✅ **In short:**\n",
    "EfficientNetV2 replaced the rigid “B-scaling” formula with new hand-optimized variants (S, M, L, XL) designed for speed and accuracy trade-offs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
