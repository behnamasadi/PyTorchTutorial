{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae77b05-a43f-4891-856e-098b12a04afa",
   "metadata": {},
   "source": [
    "## **1. What EfficientNet Is**\n",
    "\n",
    "EfficientNet is a family of convolutional neural networks (CNNs) introduced by Google (2019) designed to achieve **high accuracy with much fewer parameters and FLOPs** compared to previous models like ResNet, Inception, or DenseNet.\n",
    "It’s basically a *scalable* architecture that balances **depth**, **width**, and **resolution**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why EfficientNet Was Created**\n",
    "\n",
    "Traditional model-scaling methods (just making the network deeper or wider or feeding larger images) improve accuracy but quickly lead to inefficiency.\n",
    "EfficientNet uses a **principled scaling method** to get the most accuracy per computation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Two Key Ideas**\n",
    "\n",
    "#### A. **EfficientNet-B0 (the baseline)**\n",
    "\n",
    "* They searched for a small but powerful baseline network using **neural architecture search (NAS)**.\n",
    "* This gave a mobile-friendly architecture with compound building blocks (MBConv, similar to MobileNetV2).\n",
    "\n",
    "#### B. **Compound Scaling**\n",
    "\n",
    "* Instead of arbitrarily scaling depth, width, or input resolution, EfficientNet scales all three together using fixed **scaling coefficients**. EfficientNet’s **compound scaling** says: grow **depth**, **width**, and **input resolution** together by fixed multipliers so compute grows predictably.\n",
    "\n",
    "Formally, if you want to scale up EfficientNet:\n",
    "\n",
    "* Depth → $d = \\alpha^\\phi$\n",
    "* Width → $w = \\beta^\\phi$\n",
    "* Resolution → $r = \\gamma^\\phi$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2\n",
    "$$\n",
    "\n",
    "(where $\\phi$ is a user-chosen scaling factor indicating how much more compute you want to spend).\n",
    "This yields EfficientNet-B1 … B7 (each larger/more accurate than the last).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b256b1d-ea74-4120-a042-9f6516172591",
   "metadata": {},
   "source": [
    "## **4. The Rule**\n",
    "\n",
    "Choose constants $\\alpha,\\beta,\\gamma>1$ and a user knob $\\phi \\in {0,1,2,\\dots}$.\n",
    "\n",
    "* Depth: $d=\\alpha^{\\phi}$\n",
    "* Width $channels$: $w=\\beta^{\\phi}$\n",
    "* Resolution $image size$: $r=\\gamma^{\\phi}$\n",
    "\n",
    "Conv cost scales roughly as $ \\text{FLOPs} \\propto d \\cdot w^2 \\cdot r^2$.\n",
    "So if we enforce\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2,\n",
    "$$\n",
    "\n",
    "then each time you increase $\\phi$ by 1, **FLOPs ≈ double**.\n",
    "\n",
    "A commonly cited set (close to the original paper):\n",
    "$\\alpha=1.2,; \\beta=1.1,; \\gamma=1.15$ → $\\alpha\\beta^2\\gamma^2 \\approx 1.92 \\approx 2$.\n",
    "\n",
    "Below are concrete numbers using these.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2 Numerical examples (starting from a baseline B0)**\n",
    "\n",
    "Assume baseline depth/width/resolution are all “1×” (e.g., 224×224 input).\n",
    "\n",
    "| $\\phi$ | $ d=\\alpha^\\phi$ | $w=\\beta^\\phi$ | $r=\\gamma^\\phi$ |  New input (≈ $224\\cdot r$) | FLOPs scale $d,w^2,r^2$ |\n",
    "| -----: | --------------: | -------------: | --------------: | --------------------------: | ----------------------: |\n",
    "|      0 |           1.000 |          1.000 |           1.000 |                         224 |                   1.00× |\n",
    "|      1 |           1.200 |          1.100 |           1.150 |     **≈258** (round to 256) |              **≈1.92×** |\n",
    "|      2 |           1.440 |          1.210 |           1.322 | **≈296** (round to 296/288) |              **≈3.69×** |\n",
    "|      3 |           1.728 |          1.331 |           1.521 |     **≈341** (round to 336) |              **≈7.08×** |\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* At $\\phi=1$: make the net ~20% deeper, ~10% wider, feed ~15% larger images → ~1.9× compute.\n",
    "* At $\\phi=2$: apply those multipliers again → ~3.7× compute vs. baseline.\n",
    "* At $\\phi=3$: ~7.1× compute vs. baseline.\n",
    "\n",
    "*(In practice you round image sizes to multiples of 8/16 and channels to hardware-friendly sizes.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.3 Mapping to real models**\n",
    "\n",
    "EfficientNet uses this idea to define B0…B7. The **actual input sizes** are chosen pragmatically (rounded/tuned), e.g.:\n",
    "\n",
    "* B0: 224\n",
    "* B1: 240\n",
    "* B2: 260\n",
    "* B3: 300\n",
    "* B4: 380\n",
    "* B5: 456\n",
    "* B6: 528\n",
    "* B7: 600\n",
    "\n",
    "These aren’t exactly $224\\cdot \\gamma^\\phi$ because of rounding and practical considerations, but they **follow the same trend**: as $\\phi$ increases, depth/width/resolution all grow together.\n",
    "\n",
    "---\n",
    "\n",
    "#### Mini “what-if” examples\n",
    "\n",
    "1. **Channels and layers**\n",
    "   Baseline has 32 channels and 10 layers. With $\\phi=2$:\n",
    "\n",
    "* Width: $32 \\cdot \\beta^2 \\approx 32 \\cdot 1.21 \\approx 39$ → round to 40.\n",
    "* Depth: $10 \\cdot \\alpha^2 = 10 \\cdot 1.44 = 14.4$ → ~14–15 layers.\n",
    "* Resolution: $224 \\cdot \\gamma^2 \\approx 224 \\cdot 1.322 \\approx 296$ → round to 288/296.\n",
    "\n",
    "2. **Compute sanity check**\n",
    "   Jumping from $\\phi=1$ to $\\phi=3$ multiplies FLOPs by $\\approx 7.08/1.92 \\approx 3.7$.\n",
    "   That’s consistent because each +1 in $\\phi$ nearly doubles compute.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "* The constraint $\\alpha \\beta^2 \\gamma^2 \\approx 2$ ensures **predictable ~2× compute per step**.\n",
    "* Scaling **all three** (depth, width, resolution) is more *accuracy-efficient* than scaling any single dimension alone.\n",
    "* Real models round/tune sizes, but the compound law is the guiding principle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f618f-fe36-482c-b7f4-6e4c6a63100d",
   "metadata": {},
   "source": [
    "\n",
    "## **5. MBConv Blocks**\n",
    "\n",
    "EfficientNet is built from **MBConv (Mobile Inverted Bottleneck Convolution)** blocks (same as MobileNetV2):\n",
    "\n",
    "* A 1×1 expansion convolution\n",
    "* A depthwise 3×3 convolution\n",
    "* A 1×1 projection convolution\n",
    "* With **squeeze-and-excitation** (SE) modules for channel attention\n",
    "\n",
    "This gives high efficiency with low computation.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097aa15-6f36-45e2-b33d-9aca6a364fb6",
   "metadata": {},
   "source": [
    "#### **5.1. What MBConv Stands For**\n",
    "\n",
    "**MBConv** = **M**obile **B**ottleneck **Conv**olution.\n",
    "\n",
    "It’s an *“inverted residual”* building block that makes CNNs much more efficient, especially on mobile devices.\n",
    "EfficientNet is essentially a big stack of MBConv blocks with squeeze-and-excitation modules.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.2. Why MBConv Exists**\n",
    "\n",
    "Regular convolution layers are expensive.\n",
    "Depthwise separable convolutions (used in MobileNetV1) are cheaper but sometimes lose accuracy.\n",
    "MBConv combines:\n",
    "\n",
    "* Depthwise separable convolution (low computation)\n",
    "* “Inverted” bottleneck structure (improves expressiveness)\n",
    "* Optional Squeeze-and-Excitation (channel attention)\n",
    "\n",
    "This gives high accuracy **per FLOP**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.3. Structure of an MBConv Block**\n",
    "\n",
    "**A. Expansion phase**\n",
    "\n",
    "* A **1×1 convolution** expands the number of channels by a factor (e.g., 6×).\n",
    "* Applies batch norm + nonlinearity (Swish/ReLU6).\n",
    "\n",
    "**B. Depthwise convolution**\n",
    "\n",
    "* A **3×3 (or 5×5) depthwise convolution** operates separately on each channel.\n",
    "* Much cheaper than full convolution.\n",
    "\n",
    "**C. Squeeze-and-Excitation (in EfficientNet)**\n",
    "\n",
    "* A small attention module: global average pool → two small FC layers → scale channels.\n",
    "\n",
    "**D. Projection phase**\n",
    "\n",
    "* A **1×1 convolution** projects channels back down to the desired output size.\n",
    "* Usually no activation here.\n",
    "\n",
    "**E. Skip connection (if input/output shapes match)**\n",
    "\n",
    "* Adds the input to the output (like a residual block).\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.4. Diagrammatically:**\n",
    "\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ├─ 1x1 Conv (expand channels)\n",
    "  │\n",
    "  ├─ Depthwise Conv (3x3)\n",
    "  │\n",
    "  ├─ Squeeze-and-Excitation (optional)\n",
    "  │\n",
    "  ├─ 1x1 Conv (project channels back)\n",
    "  │\n",
    "  └─ + Input (skip connection, if same shape)\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Why “Inverted Bottleneck”?**\n",
    "\n",
    "* In a classic bottleneck (ResNet), you go **down → process → up** in channels:\n",
    "\n",
    "  * 1×1 reduce channels → 3×3 conv → 1×1 expand channels.\n",
    "* In MBConv you go **up → process → down**:\n",
    "\n",
    "  * 1×1 expand channels → depthwise conv → 1×1 project down.\n",
    "* This inversion allows the cheap depthwise conv to act on a larger feature space, improving expressiveness.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Parameters**\n",
    "\n",
    "Typical MBConv parameters:\n",
    "\n",
    "* **Expansion ratio**: 6× (common)\n",
    "* **Kernel size**: 3 or 5 (sometimes 7)\n",
    "* **Stride**: 1 or 2 (for downsampling)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. In PyTorch (simplified MBConv):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, expand_ratio=6, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        hidden_dim = in_ch * expand_ratio\n",
    "\n",
    "        self.expand = nn.Conv2d(in_ch, hidden_dim, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act = nn.SiLU()  # or nn.ReLU6()\n",
    "\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            hidden_dim, hidden_dim, kernel_size, stride,\n",
    "            padding=kernel_size//2, groups=hidden_dim, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "\n",
    "        self.project = nn.Conv2d(hidden_dim, out_ch, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.use_residual = (stride == 1 and in_ch == out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.bn1(self.expand(x)))\n",
    "        out = self.act(self.bn2(self.depthwise(out)))\n",
    "        out = self.bn3(self.project(out))\n",
    "        if self.use_residual:\n",
    "            out = out + x\n",
    "        return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "* **MBConv** = efficient building block (expand → depthwise → project).\n",
    "* **Inverted bottleneck** = expand first, compress later.\n",
    "* **Squeeze-and-Excitation** (in EfficientNet) = channel attention on top of MBConv.\n",
    "* Foundation for MobileNetV2, MnasNet, EfficientNet.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **draw a small block diagram** of MBConv showing the data flow (expand → depthwise → project)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23547e-8d9c-42da-b6f7-21c49af2b6c2",
   "metadata": {},
   "source": [
    "## 5. Model Family\n",
    "\n",
    "| Model | Input Resolution | Parameters (M) | Top-1 Accuracy (ImageNet) |\n",
    "| ----- | ---------------- | -------------- | ------------------------- |\n",
    "| B0    | 224×224          | 5.3M           | ~77%                      |\n",
    "| B1    | 240×240          | 7.8M           | ~79%                      |\n",
    "| B2    | 260×260          | 9.2M           | ~80%                      |\n",
    "| B3    | 300×300          | 12M            | ~81%                      |\n",
    "| B4    | 380×380          | 19M            | ~83%                      |\n",
    "| B5    | 456×456          | 30M            | ~84%                      |\n",
    "| B6    | 528×528          | 43M            | ~84.5%                    |\n",
    "| B7    | 600×600          | 66M            | ~85%                      |\n",
    "\n",
    "You can see how input size, depth, and width grow together.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why It’s “Efficient”\n",
    "\n",
    "* **Better accuracy per parameter** than ResNet or DenseNet at all scales.\n",
    "* Works well as a feature extractor or backbone for transfer learning.\n",
    "* Because of compound scaling, no wasted parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Practical Usage\n",
    "\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load EfficientNet-B0 pretrained on ImageNet\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Replace the classifier for your own number of classes\n",
    "num_classes = 10\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Intuition:\n",
    "\n",
    "* **B0**: small but powerful baseline.\n",
    "* **B1-B7**: systematically scaled versions.\n",
    "* **Compound scaling**: balanced depth/width/resolution growth → high accuracy with low cost.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df8d93-63ca-44b0-ba02-77e86f0be04b",
   "metadata": {},
   "source": [
    "Here’s a clear, structured explanation of **SE blocks**:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What “SE” Stands For\n",
    "\n",
    "**SE block** = **Squeeze-and-Excitation block**.\n",
    "It’s a lightweight attention mechanism for CNNs introduced in the paper:\n",
    "\n",
    "> *“Squeeze-and-Excitation Networks” (Hu et al., CVPR 2018)*\n",
    "\n",
    "It improves a network’s ability to model the **importance of each channel** in feature maps.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why We Need It\n",
    "\n",
    "Convolutions learn spatial filters but treat all channels equally.\n",
    "Some channels might carry more relevant information for the current task.\n",
    "An SE block lets the network **recalibrate channel-wise feature responses** adaptively.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How an SE Block Works\n",
    "\n",
    "Let’s say the input feature map is (X\\in \\mathbb{R}^{H\\times W\\times C}) (height, width, channels).\n",
    "\n",
    "### Step A: **Squeeze** (Global information)\n",
    "\n",
    "* Perform **global average pooling** over spatial dimensions (H \\times W) to get one value per channel.\n",
    "* This yields a vector (z\\in \\mathbb{R}^{C}) summarizing each channel’s global response.\n",
    "\n",
    "Mathematically:\n",
    "[\n",
    "z_c = \\frac{1}{H , W}\\sum_{i=1}^{H}\\sum_{j=1}^{W} X_{i,j,c}\n",
    "]\n",
    "\n",
    "### Step B: **Excitation** (Learn channel attention)\n",
    "\n",
    "* Pass (z) through a small **two-layer MLP**:\n",
    "\n",
    "  * First layer reduces dimension to (C/r) (bottleneck; (r) is reduction ratio like 16)\n",
    "  * Apply ReLU.\n",
    "  * Second layer expands back to (C).\n",
    "  * Apply sigmoid to get weights (s\\in[0,1]^C).\n",
    "\n",
    "[\n",
    "s = \\sigma(W_2 , \\delta(W_1 z))\n",
    "]\n",
    "where (\\delta) is ReLU, (\\sigma) is sigmoid.\n",
    "\n",
    "### Step C: **Scale** (Recalibration)\n",
    "\n",
    "* Multiply the original feature map channels by the learned weights:\n",
    "\n",
    "[\n",
    "\\tilde{X}*{i,j,c} = s_c \\cdot X*{i,j,c}\n",
    "]\n",
    "\n",
    "So channels the block deems “important” get boosted; less important channels get suppressed.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Visual Diagram\n",
    "\n",
    "```\n",
    "Input Feature Map (H×W×C)\n",
    "       │\n",
    " Global Average Pooling (Squeeze)\n",
    "       ↓\n",
    " Channel Descriptor (C)\n",
    "       │\n",
    " Fully Connected (reduce C→C/r), ReLU\n",
    "       │\n",
    " Fully Connected (C/r→C), Sigmoid (Excitation)\n",
    "       ↓\n",
    " Channel Weights (C)\n",
    "       │\n",
    " Scale original feature map channel-wise\n",
    "       ↓\n",
    "Output Feature Map (H×W×C)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Where It’s Used\n",
    "\n",
    "* Originally introduced in **SENet** (ImageNet winner 2017/2018).\n",
    "* Incorporated into **MobileNetV3**, **EfficientNet** (in MBConv blocks), ResNeXt, etc.\n",
    "* Adds only a tiny computational overhead but often improves accuracy significantly.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. PyTorch Implementation (simple)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)        # Squeeze\n",
    "        y = self.fc(y).view(b, c, 1, 1)        # Excitation\n",
    "        return x * y                          # Scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "* **SE block** = channel-wise attention module.\n",
    "* Steps: **Squeeze → Excite → Scale**.\n",
    "* Helps the network emphasize informative features dynamically.\n",
    "* Tiny overhead, noticeable accuracy gain.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also explain **how SE differs from spatial attention** (e.g., CBAM)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a0d88-69e9-4f50-9eae-a5260e2a7cf6",
   "metadata": {},
   "source": [
    "Sure — here’s a concise **reference table** that captures the main specs of **EfficientNet B0–B7**: number of parameters, input size, and the actual MBConv layout (the “architecture” part).\n",
    "\n",
    "---\n",
    "\n",
    "## EfficientNet-B0 to B7 Overview\n",
    "\n",
    "| Model  | Input Size (px) | Parameters (M) | MBConv / Architecture Stages *(kernel × expansion, repeats, output channels, stride)*                                                                                                                                                                                                                                    |\n",
    "| ------ | --------------: | -------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **B0** |         224×224 |          5.3 M | Stem: 3×3 conv, 32 ch, s2 <br> MBConv1 k3×3, e1, r1, 16ch, s1 <br> MBConv6 k3×3, e6, r2, 24ch, s2 <br> MBConv6 k5×5, e6, r2, 40ch, s2 <br> MBConv6 k3×3, e6, r3, 80ch, s2 <br> MBConv6 k5×5, e6, r3, 112ch, s1 <br> MBConv6 k5×5, e6, r4, 192ch, s2 <br> MBConv6 k3×3, e6, r1, 320ch, s1 <br> Head: 1×1 conv 1280ch + FC |\n",
    "| **B1** |         240×240 |          7.8 M | Same pattern as B0 but scaled: deeper (some stages +1 repeat), slightly wider channels, input 240                                                                                                                                                                                                                        |\n",
    "| **B2** |         260×260 |          9.2 M | Same pattern, scaled: more repeats/channels, input 260                                                                                                                                                                                                                                                                   |\n",
    "| **B3** |         300×300 |           12 M | Same pattern, scaled: input 300                                                                                                                                                                                                                                                                                          |\n",
    "| **B4** |         380×380 |           19 M | Same pattern, scaled: input 380                                                                                                                                                                                                                                                                                          |\n",
    "| **B5** |         456×456 |           30 M | Same pattern, scaled: input 456                                                                                                                                                                                                                                                                                          |\n",
    "| **B6** |         528×528 |           43 M | Same pattern, scaled: input 528                                                                                                                                                                                                                                                                                          |\n",
    "| **B7** |         600×600 |           66 M | Same pattern, scaled: input 600                                                                                                                                                                                                                                                                                          |\n",
    "\n",
    "---\n",
    "\n",
    "### How to read the MBConv stage string\n",
    "\n",
    "Example for B0:\n",
    "\n",
    "* **MBConv6 k5×5 e6 r4 192ch s2**\n",
    "\n",
    "  * MBConv block with expansion ratio 6\n",
    "  * Kernel size 5×5\n",
    "  * Repeated 4 times\n",
    "  * Output channels 192\n",
    "  * First block in stage stride 2 (downsample); subsequent repeats stride 1\n",
    "\n",
    "All B1–B7 keep the same stage *types* but multiply:\n",
    "\n",
    "* **Depth**: more repeats per stage.\n",
    "* **Width**: slightly higher channels.\n",
    "* **Resolution**: bigger input image.\n",
    "\n",
    "That’s exactly the compound scaling you saw with (\\alpha,\\beta,\\gamma).\n",
    "\n",
    "---\n",
    "\n",
    "### Quick parameter reference\n",
    "\n",
    "| Model | Parameters (M) | FLOPs (B) | Top-1 Accuracy (ImageNet) |\n",
    "| ----- | -------------: | --------: | ------------------------: |\n",
    "| B0    |            5.3 |      0.39 |                     77.1% |\n",
    "| B1    |            7.8 |      0.70 |                     79.1% |\n",
    "| B2    |            9.2 |       1.0 |                     80.1% |\n",
    "| B3    |             12 |       1.8 |                     81.6% |\n",
    "| B4    |             19 |       4.2 |                     83.0% |\n",
    "| B5    |             30 |       9.9 |                     83.6% |\n",
    "| B6    |             43 |        19 |                     84.0% |\n",
    "| B7    |             66 |        37 |                     84.3% |\n",
    "\n",
    "*(Values from the original paper; can vary slightly by implementation.)*\n",
    "\n",
    "---\n",
    "\n",
    "### In words\n",
    "\n",
    "* **B0**: baseline MBConv layout.\n",
    "* **B1–B7**: same layout but systematically scaled deeper/wider/higher-res.\n",
    "* All include **Squeeze-and-Excitation** inside MBConv blocks and **Swish/SiLU** activations.\n",
    "\n",
    "Would you like me to make a **visual block diagram** showing B0’s stages stacked from top to bottom? (That often helps see the pattern.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
