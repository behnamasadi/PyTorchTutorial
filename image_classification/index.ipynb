{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3eb8f3-dce6-4537-85c9-a1f4cbc5d2a0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "A Convolutional Neural Network (CNN) processes an input image through several **convolutional layers**, each applying multiple learnable filters (kernels). For example, the first convolution layer might use 10 filters of size $6 \\times 6 \\times 3$ (height × width × depth), producing **10 feature maps** that capture different local patterns such as edges or textures.\n",
    "\n",
    "These feature maps then undergo **subsampling** (often via max pooling) to reduce spatial dimensions while keeping the most prominent features, improving translation invariance and reducing computation.\n",
    "\n",
    "The process of **convolution → subsampling** repeats multiple times, with feature maps getting deeper (more channels) but spatially smaller as we move through the network.\n",
    "\n",
    "Eventually, the resulting high-level feature maps are **flattened** into a vector and passed to one or more **fully connected layers**, which perform the final classification or regression task.\n",
    "\n",
    "In short:\n",
    "\n",
    "* **Width & height** of feature maps → decrease as we go deeper.\n",
    "* **Depth (number of channels)** → increases as we go deeper.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b1e48-884d-4f98-9dee-396fcfe6cc43",
   "metadata": {},
   "source": [
    "![](images/Typical_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ac4bf-ff22-4b6a-ab6e-e96576f2b41c",
   "metadata": {},
   "source": [
    "![](images/vgg-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c482b2-b0db-4544-ad3f-45e2f152bc00",
   "metadata": {},
   "source": [
    "## 1. The Structural Change: Resolution ↓, Depth ↑\n",
    "\n",
    "As you move **deeper into a CNN**, two things typically happen:\n",
    "\n",
    "1. **Spatial resolution decreases** —\n",
    "   The width and height of the feature maps become smaller.\n",
    "   This happens due to **strided convolutions** or **pooling** (e.g. max pooling with stride 2).\n",
    "\n",
    "2. **Depth (number of channels) increases** —\n",
    "   The number of filters in each layer increases (e.g. 64 → 128 → 256 → 512).\n",
    "   Each filter captures a **different type of pattern** or **feature dimension**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What this means semantically\n",
    "\n",
    "### Early layers — High resolution, low semantics\n",
    "\n",
    "* Each neuron sees a **small receptive field** (a few pixels).\n",
    "* The features represent **low-level visual cues**, such as:\n",
    "\n",
    "  * Edges\n",
    "  * Corners\n",
    "  * Color blobs\n",
    "  * Simple textures\n",
    "* Because the resolution is high, spatial precision is retained — you know *where* the feature is.\n",
    "\n",
    "Mathematically, if input is $ X \\in \\mathbb{R}^{H \\times W \\times 3} $, after first convolution:\n",
    "$$\n",
    "X_1 = f(W_1 * X + b_1)\n",
    "$$\n",
    "with small receptive field (e.g., $3 \\times 3$) → local features.\n",
    "\n",
    "---\n",
    "\n",
    "### Middle layers — Medium resolution, medium semantics\n",
    "\n",
    "* Receptive fields expand: neurons start combining lower-level edges and patterns.\n",
    "* They detect **parts of objects** (e.g. corners of a mouth, eyes, wheels).\n",
    "* Still some spatial detail, but less than before.\n",
    "* Representations become **more invariant** to small translations, rotations, lighting.\n",
    "\n",
    "---\n",
    "\n",
    "### Deep layers — Low resolution, high semantics\n",
    "\n",
    "* Receptive fields cover **almost the entire input image**.\n",
    "* Each neuron responds to **high-level, abstract concepts** like:\n",
    "\n",
    "  * “dog’s face”\n",
    "  * “wheel”\n",
    "  * “human torso”\n",
    "  * “text region”\n",
    "* You lose precise location information — instead, you gain **semantic meaning**.\n",
    "* This is why deep features are useful for classification, but not directly for tasks that need fine localization (e.g. segmentation or detection).\n",
    "\n",
    "Formally, the **effective receptive field** grows approximately as:\n",
    "$$\n",
    "r_l = r_{l-1} + (k_l - 1) \\prod_{i=1}^{l-1} s_i\n",
    "$$\n",
    "where:\n",
    "\n",
    "* $ r_l $: receptive field size at layer $ l $\n",
    "* $ k_l $: kernel size\n",
    "* $ s_i $: stride at layer $ i $\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why this tradeoff is useful\n",
    "\n",
    "* **Decreasing spatial resolution** reduces computation and memory cost.\n",
    "* **Increasing channel depth** allows the model to learn richer, more abstract representations.\n",
    "* The combination enables hierarchical feature extraction — the hallmark of CNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Relation to downstream tasks\n",
    "\n",
    "| Task               | Needed Info                     | Example Adaptation                     |\n",
    "| ------------------ | ------------------------------- | -------------------------------------- |\n",
    "| **Classification** | Semantic meaning                | Deep layers only (e.g., GAP + Linear)  |\n",
    "| **Detection**      | Both semantics & spatial        | Multi-scale feature maps (FPN, SSD)    |\n",
    "| **Segmentation**   | High semantics + spatial detail | Encoder–Decoder (e.g., U-Net, DeepLab) |\n",
    "\n",
    "In segmentation, for example, **skip connections** are used to recover fine-grained spatial info from earlier layers that was lost in downsampling.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
