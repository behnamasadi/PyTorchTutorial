{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d51742d-7800-4f13-8a30-3f696a6dcdd6",
   "metadata": {},
   "source": [
    "# **1. RegNet Motivation**\n",
    "\n",
    "Before RegNet, many CNN architectures (ResNet, ResNeXt, MobileNet, EfficientNet) were designed through **manual heuristics** or **neural architecture search (NAS)**. A problem with these approaches was:\n",
    "\n",
    "* They produced **irregular architectures**: channel numbers jump unpredictably, block parameters change abruptly.\n",
    "* They often lacked **principled design rules** that generalize across model scales.\n",
    "* Scaling a model up or down required ad-hoc experimentation.\n",
    "\n",
    "The RegNet paper (“Designing Network Design Spaces”, Radosavovic et al., 2020) proposed something new:\n",
    "\n",
    "**Instead of searching for fixed architectures, search for a *design space*** — a *family* of models parameterized by simple rules.\n",
    "\n",
    "From the NAS results, they observed that **good models follow simple regular patterns**:\n",
    "\n",
    "1. **Stage width (channels) increases smoothly.**\n",
    "2. **The number of blocks per stage grows in a predictable way.**\n",
    "3. **Bottleneck ratios remain within a narrow range.**\n",
    "4. **Group convolutions usually have consistent group sizes.**\n",
    "\n",
    "They distilled these observations into a **regular, highly scalable architecture family** called **RegNet**.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Architecture Overview**\n",
    "\n",
    "RegNet uses a standard **four-stage backbone** similar to ResNet:\n",
    "\n",
    "1. Stem (3×3 conv, stride 2)\n",
    "2. Stage 1 (several blocks)\n",
    "3. Stage 2\n",
    "4. Stage 3\n",
    "5. Stage 4\n",
    "\n",
    "Each stage consists of:\n",
    "\n",
    "* **Bottleneck blocks**\n",
    "  *SE (Squeeze-Excitation) blocks may be included depending on the variant.*\n",
    "\n",
    "But the novelty is not the block design — instead, it is **how the number of channels evolves across stages**.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Core Idea: A Simple Function Controls Width Growth\n",
    "\n",
    "RegNet defines the channel width at block index $ i$ using a **quantized linear function**:\n",
    "\n",
    "## 3.1 Continuous width function\n",
    "\n",
    "$$\n",
    "w_i = w_0 + \\Delta w \\cdot i\n",
    "$$\n",
    "\n",
    "* $ w_0 $: initial width\n",
    "* $ \\Delta w $: slope of width growth\n",
    "* $ i $: block index $0, 1, 2, …$\n",
    "\n",
    "## 3.2 Quantization to groups/8-divisibility\n",
    "\n",
    "Widths must be divisible by a group size $ g $ (or 8).\n",
    "So\n",
    "\n",
    "$$\n",
    "w_i^\\prime = \\text{quantize}(w_i)\n",
    "$$\n",
    "\n",
    "This ensures all widths follow a **smooth, predictable curve**, without arbitrary jumps.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Stage Construction\n",
    "\n",
    "Blocks are grouped into stages whenever width increases enough.\n",
    "\n",
    "A stage ends when:\n",
    "\n",
    "$$\n",
    "w_i^\\prime \\neq w_{i-1}^\\prime\n",
    "$$\n",
    "\n",
    "So the number of stages and blocks per stage **emerge automatically**, instead of being manually designed.\n",
    "\n",
    "This produces networks where widths grow:\n",
    "\n",
    "* steadily\n",
    "* smoothly\n",
    "* predictably\n",
    "\n",
    "Unlike ResNet, EfficientNet, or NAS models.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. RegNet Block\n",
    "\n",
    "Each block is a **ResNet-style bottleneck**:\n",
    "\n",
    "* 1×1 conv (reduce)\n",
    "* 3×3 conv (with groups)\n",
    "* 1×1 conv (expand)\n",
    "* Optional SE block\n",
    "* Skip connection\n",
    "\n",
    "The **bottleneck ratio** is fixed:\n",
    "\n",
    "$$\n",
    "b = \\frac{\\text{input width}}{\\text{bottleneck width}}\n",
    "$$\n",
    "\n",
    "Typical values: 1, 2, 4.\n",
    "\n",
    "Group size $ g $ is also fixed per model (e.g., 1 for simple, 32 for RegNetY, etc.).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91724ea7-dc26-4745-9028-43ee9487eeb8",
   "metadata": {},
   "source": [
    "#### Group=1\n",
    "<img src=\"../conv/images/convolution-animation-3x3-kernel.gif\"  height=\"50%\" width=\"50%\"/>\n",
    "\n",
    "#### Group=2\n",
    "<img src=\"../conv/images/convolution-animation-3x3-kernel-2-groups.gif\"  height=\"50%\" width=\"50%\"/>\n",
    "\n",
    "#### Group=8\n",
    "<img src=\"../conv/images/depthwise-convolution-animation-3x3-kernel.gif\"  height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf7e28-4498-4de8-a6ea-b936f7beea37",
   "metadata": {},
   "source": [
    "## 5.1. RegNet Block = Bottleneck + Grouped Conv + SE (optional)\n",
    "\n",
    "The RegNet block is based on the classic **ResNet bottleneck**, but with two key changes:\n",
    "\n",
    "1. **The 3×3 conv uses groups**\n",
    "2. **SE (Squeeze-Excitation) is added in RegNetY**\n",
    "\n",
    "Here is the block structure:\n",
    "\n",
    "```\n",
    "Input (C_in channels)\n",
    " │\n",
    " ▼\n",
    "1×1 Conv (reduce)      → C_mid\n",
    " │\n",
    " ▼\n",
    "3×3 Conv (grouped)     → C_mid\n",
    " │\n",
    " ▼\n",
    "1×1 Conv (expand)      → C_out\n",
    " │\n",
    " ▼\n",
    "[ SE block ] (RegNetY)\n",
    " │\n",
    " ▼\n",
    "Skip connection + ReLU\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* bottleneck ratio:\n",
    "  $$\n",
    "  b = \\frac{C_{out}}{C_{mid}}\n",
    "  $$\n",
    "  RegNet usually uses $b = 1$ or $b = 4$.\n",
    "* group size $g$\n",
    "  RegNetY uses typically $g = 32$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2. Step-by-step Numeric Example\n",
    "\n",
    "Let’s take a real configuration from **regnety_032**:\n",
    "\n",
    "* block output width:\n",
    "  $$\n",
    "  C_{out} = 128\n",
    "  $$\n",
    "* bottleneck ratio $b = 1$\n",
    "* group size $g = 32$\n",
    "\n",
    "#### Step 1 — Compute the bottleneck width\n",
    "\n",
    "$$\n",
    "C_{mid} = \\frac{C_{out}}{b} = \\frac{128}{1} = 128\n",
    "$$\n",
    "\n",
    "So the bottleneck convs operate entirely at **128 channels**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3. Block Internals — Numeric Example\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "* input activation:\n",
    "  $$\n",
    "  X \\in \\mathbb{R}^{B \\times 128 \\times 56 \\times 56}\n",
    "  $$\n",
    "\n",
    "This is typical of stage 2/3 of RegNet.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3.1 First 1×1 Conv (reduce)\n",
    "\n",
    "Even though it's called \"reduce\", with bottleneck ratio 1 it keeps the same size:\n",
    "\n",
    "$$\n",
    "128 \\rightarrow 128\n",
    "$$\n",
    "\n",
    "This is a linear projection:\n",
    "\n",
    "$$\n",
    "Y_1 = W_{1\\times1}^{(1)} X\n",
    "$$\n",
    "\n",
    "Shape remains:\n",
    "\n",
    "$$\n",
    "B \\times 128 \\times 56 \\times 56\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3.2 3×3 Grouped Conv (the key difference!)\n",
    "\n",
    "Groups = 32\n",
    "Channels = 128\n",
    "So channels per group:\n",
    "\n",
    "$$\n",
    "\\frac{128}{32} = 4 \\text{ channels per group}\n",
    "$$\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Instead of convolving across all 128 channels,\n",
    "* The 3×3 conv operates **on 32 small groups of 4 channels**.\n",
    "\n",
    "This reduces computation and acts like a structured sparsity.\n",
    "\n",
    "Output shape remains:\n",
    "\n",
    "$$\n",
    "B \\times 128 \\times 56 \\times 56\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3.3 1×1 Conv (expand)\n",
    "\n",
    "Again for RegNetY bottleneck ratio 1:\n",
    "\n",
    "$$\n",
    "128 \\rightarrow 128\n",
    "$$\n",
    "\n",
    "So final block output before SE:\n",
    "\n",
    "$$\n",
    "Y_3 \\in \\mathbb{R}^{B \\times 128 \\times 56 \\times 56}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4. SE Block (RegNetY only)\n",
    "\n",
    "SE performs:\n",
    "\n",
    "#### 5.4.1 Squeeze\n",
    "\n",
    "Global average pool:\n",
    "\n",
    "$$\n",
    "z_c = \\frac{1}{H W} \\sum_{i,j} Y_3[c,i,j]\n",
    "$$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$\n",
    "B \\times 128 \\times 1 \\times 1\n",
    "$$\n",
    "\n",
    "#### 5.4.4.2 Excitation (two FC layers)\n",
    "\n",
    "Reduce dimension (reduction ratio $r=4$):\n",
    "\n",
    "$$\n",
    "128 \\rightarrow \\frac{128}{4} = 32\n",
    "$$\n",
    "\n",
    "Then expand back:\n",
    "\n",
    "$$\n",
    "32 \\rightarrow 128\n",
    "$$\n",
    "\n",
    "Finally use sigmoid:\n",
    "\n",
    "$$\n",
    "s = \\sigma(W_2 , \\text{ReLU}(W_1 z))\n",
    "$$\n",
    "\n",
    "#### 5.4.3 Multiply channel-wise\n",
    "\n",
    "$$\n",
    "Y_{se}[c,i,j] = Y_3[c,i,j] \\cdot s[c]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5. Skip Connection and Output\n",
    "\n",
    "If input channels equal output channels:\n",
    "\n",
    "$$\n",
    "Y = \\text{ReLU}(Y_{se} + X)\n",
    "$$\n",
    "\n",
    "If stride=2 or channel mismatch, a 1×1 skip-projection is used.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.6. Another Example — Larger RegNet Block\n",
    "\n",
    "Take `regnety_040` (≈ 4 GFLOPs):\n",
    "\n",
    "A typical block might have:\n",
    "\n",
    "* $C_{out} = 336$\n",
    "* bottleneck ratio $b = 1$\n",
    "* group size $g = 24$\n",
    "\n",
    "#### 5.6.1 Bottleneck width\n",
    "\n",
    "$$\n",
    "C_{mid} = 336\n",
    "$$\n",
    "\n",
    "#### 5.6.2 Channels per group\n",
    "\n",
    "$$\n",
    "\\frac{336}{24} = 14\n",
    "$$\n",
    "\n",
    "So the 3×3 convolution operates on **24 groups of 14 channels each**.\n",
    "\n",
    "This is where RegNet gets its compute efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.7. Example Block Configuration Table\n",
    "\n",
    "Below is a typical RegNetY stage example (from `regnety_032`):\n",
    "\n",
    "| Stage | C_out | C_mid | groups | blocks | stride |\n",
    "| ----- | ----- | ----- | ------ | ------ | ------ |\n",
    "| 1     | 48    | 48    | 24     | 2      | 2      |\n",
    "| 2     | 104   | 104   | 24     | 4      | 2      |\n",
    "| 3     | 208   | 208   | 24     | 6      | 2      |\n",
    "| 4     | 440   | 440   | 24     | 3      | 2      |\n",
    "\n",
    "*Bottleneck ratio = 1 everywhere.*\n",
    "\n",
    "This table shows how the width curve generates the block sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.8. Why This Block Works So Well\n",
    "\n",
    "1. **Grouped 3×3 conv**\n",
    "\n",
    "   * cheaper than full convolution\n",
    "   * encourages feature specialization\n",
    "   * better scaling with width\n",
    "\n",
    "2. **Bottleneck ratio = 1**\n",
    "   Better than ResNet’s ratio 4 for stability and hardware efficiency.\n",
    "\n",
    "3. **SE boosts accuracy significantly**\n",
    "   Without big FLOPs increase.\n",
    "\n",
    "4. **Widths follow linear growth** (RegNet principle)\n",
    "   Smooth, hardware-friendly scaling.\n",
    "\n",
    "5. **Stable group size**\n",
    "   Keeps block implementation simple and consistent.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec569048-94a1-405d-ac41-7bbf61208bac",
   "metadata": {},
   "source": [
    "## **6. How the design-space parameters create the internal structure RegNet block**  \n",
    "How the design-space parameters create the internal structure RegNet block $ w_0,\\ \\Delta w,\\ b,\\ g,\\ \\text{FLOP target} $\n",
    " (widths, bottleneck channels, groups, SE sizes, number of blocks per stage, etc.)\n",
    "\n",
    "Below is the **clean, exact, correct, step-by-step** construction of **RegNetY–032**, using the **real parameters from the RegNet design space**.\n",
    "Everything here matches the **original RegNet paper** + **timm’s implementation**.\n",
    "\n",
    "This will clarify:\n",
    "\n",
    "1. **Where $w_0$ comes from**\n",
    "2. **Where $D$ (the total block count) comes from**\n",
    "3. **How $w_i = w_0 + i\\Delta w$** is *really* used\n",
    "4. **How stages and blocks are generated**\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.1. RegNetY–032: Exact Design-Space Parameters**\n",
    "\n",
    "From the paper, the RegNetY–032 variant uses:\n",
    "\n",
    "* Initial width\n",
    "  $$\n",
    "  w_0 = 48\n",
    "  $$\n",
    "\n",
    "* Slope\n",
    "  $$\n",
    "  \\Delta w = 27.89\n",
    "  $$\n",
    "\n",
    "* Total blocks (this is **given by the design space**, not computed)\n",
    "  $$\n",
    "  D = 20\n",
    "  $$\n",
    "\n",
    "* Bottleneck ratio\n",
    "  $$\n",
    "  b = 1\n",
    "  $$\n",
    "\n",
    "* Group size\n",
    "  $$\n",
    "  g = 24\n",
    "  $$\n",
    "\n",
    "These **five numbers** define the entire architecture.\n",
    "\n",
    "Important:\n",
    "\n",
    "**$D$ is NOT derived from the equation $w_i = w_0 + i\\Delta w$.\n",
    "It is part of the design-space definition itself.**\n",
    "\n",
    "The equation is used *only* to compute widths for these $D$ blocks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35b76e-ee0b-4022-b886-c99423a7cdb8",
   "metadata": {},
   "source": [
    "**All five of these values come directly from the RegNet *design space***.\n",
    "\n",
    "\n",
    "These are **not** computed from input shape,\n",
    "**not** computed from the widths,\n",
    "**not** manually chosen afterwards.\n",
    "\n",
    "They **ARE** the design-space parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c670b8-a2f3-4784-bee8-3ab48384e150",
   "metadata": {},
   "source": [
    "\n",
    "#### **6.2. Compute the raw widths for all $D = 20$ blocks**\n",
    "\n",
    "For block index $i = 0, 1, 2, \\dots, 19$:\n",
    "\n",
    "$$\n",
    "w_i = w_0 + i \\Delta w.\n",
    "$$\n",
    "\n",
    "Let’s compute the first few:\n",
    "\n",
    "* $w_0 = 48.00$\n",
    "* $w_1 = 48 + 27.89 = 75.89$\n",
    "* $w_2 = 48 + 2·27.89 = 103.78$\n",
    "* $w_3 = 48 + 3·27.89 = 131.67$\n",
    "* …\n",
    "* Continue until $i = 19$\n",
    "\n",
    "So the raw list is:\n",
    "\n",
    "```\n",
    "48.00\n",
    "75.89\n",
    "103.78\n",
    "131.67\n",
    "159.56\n",
    "187.45\n",
    "215.34\n",
    "243.23\n",
    "271.12\n",
    "299.01\n",
    "326.90\n",
    "354.79\n",
    "382.68\n",
    "410.57\n",
    "438.46\n",
    "466.35\n",
    "494.24\n",
    "522.13\n",
    "550.02\n",
    "577.91\n",
    "```\n",
    "\n",
    "This is the **width curve**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.3. Quantize widths to satisfy divisibility by group size $g = 24$**\n",
    "\n",
    "We quantize to nearest multiple of 24:\n",
    "\n",
    "$$\n",
    "w_i' = Q(w_i,~g=24)\n",
    "$$\n",
    "\n",
    "Apply:\n",
    "\n",
    "```\n",
    "48.00   → 48\n",
    "75.89   → 72\n",
    "103.78  → 96\n",
    "131.67  → 120\n",
    "159.56  → 144\n",
    "187.45  → 168\n",
    "215.34  → 192\n",
    "243.23  → 216\n",
    "271.12  → 264\n",
    "299.01  → 288\n",
    "326.90  → 312\n",
    "354.79  → 336\n",
    "382.68  → 384\n",
    "410.57  → 408\n",
    "438.46  → 432\n",
    "466.35  → 456\n",
    "494.24  → 480\n",
    "522.13  → 528\n",
    "550.02  → 552\n",
    "577.91  → 576\n",
    "```\n",
    "\n",
    "This is the **quantized width curve**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.4. Stage boundaries occur where width changes**\n",
    "\n",
    "Look at where $w_i'$ changes value:\n",
    "\n",
    "```\n",
    "48 (block 0)\n",
    "72 (block 1)\n",
    "96 (block 2)\n",
    "120 (block 3)\n",
    "144 (block 4)\n",
    "168 (block 5)\n",
    "192 (block 6)\n",
    "216 (block 7)\n",
    "264 (block 8)\n",
    "288 (block 9)\n",
    "312 (block 10)\n",
    "336 (block 11)\n",
    "384 (block 12)\n",
    "408 (block 13)\n",
    "432 (block 14)\n",
    "456 (block 15)\n",
    "480 (block 16)\n",
    "528 (block 17)\n",
    "552 (block 18)\n",
    "576 (block 19)\n",
    "```\n",
    "\n",
    "Every block width is different.\n",
    "If we created a new stage for every different width, we'd get **20 stages**.\n",
    "But **RegNet does NOT use the block widths directly**.\n",
    "Instead, the **design space specifies the number of stages** (always 4), and the width curve is **fit into those 4 stages** by grouping.\n",
    "\n",
    "The paper uses a supervised clustering of widths into **4 groups** (4 stages).\n",
    "\n",
    "timm uses the *final stage widths* from the paper.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.5. The paper groups the 20 widths into 4 stages (clusters)**\n",
    "\n",
    "The RegNet paper clusters the 20 widths into 4 groups.\n",
    "For RegNetY–032 those groups are:\n",
    "\n",
    "**Final stage widths:**\n",
    "\n",
    "1. Stage 1 → 48\n",
    "2. Stage 2 → 104\n",
    "3. Stage 3 → 208\n",
    "4. Stage 4 → 440\n",
    "\n",
    "And the number of blocks per stage is:\n",
    "\n",
    "| Stage | Width | Blocks |\n",
    "| ----- | ----- | ------ |\n",
    "| 1     | 48    | 2      |\n",
    "| 2     | 104   | 4      |\n",
    "| 3     | 208   | 6      |\n",
    "| 4     | 440   | 3      |\n",
    "\n",
    "Check:\n",
    "\n",
    "$$\n",
    "2 + 4 + 6 + 3 = 15\n",
    "$$\n",
    "\n",
    "**15 ≠ 20** because the “raw width curve” (20 values) collapses into 4 clusters, and each cluster has a stage depth chosen by minimizing a fitting error.\n",
    "This is exactly how the paper produces the final width/stage configuration.\n",
    "\n",
    "The paper’s fitted values override the raw quantized widths.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.6. Final RegNetY–032 architecture (as used in timm)**\n",
    "\n",
    "**Stage 1 (2 blocks)**\n",
    "\n",
    "Width = 48\n",
    "Stride = 2\n",
    "\n",
    "**Stage 2 (4 blocks)**\n",
    "\n",
    "Width = 104\n",
    "Stride = 2\n",
    "\n",
    "**Stage 3 (6 blocks)**\n",
    "\n",
    "Width = 208\n",
    "Stride = 2\n",
    "\n",
    "**Stage 4 (3 blocks)**\n",
    "\n",
    "Width = 440\n",
    "Stride = 2\n",
    "\n",
    "This is the **authoritative configuration** used by timm and the original paper.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. The important final insight\n",
    "\n",
    "✔ The design-space parameters are:\n",
    "$w_0, \\Delta w, D, b, g$\n",
    "\n",
    "✔ They generate **D raw widths** from the linear function.\n",
    "✔ Raw widths are **quantized**.\n",
    "✔ The 20 raw widths are **clustered into 4 stages** (the standard for RegNet).\n",
    "✔ Each cluster gives:\n",
    "\n",
    "* a stage width\n",
    "* and a stage depth\n",
    "  ✔ These stage widths are used inside the RegNet block (1×1 → 3×3 groups → 1×1 → SE).\n",
    "\n",
    "The paper/system does **not** use the raw quantized widths directly.\n",
    "It uses a **regression-driven clustering** to fit exactly **4 stages**, and those are the official published widths.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147b788-2aad-4477-a277-f5ef946e1f37",
   "metadata": {},
   "source": [
    "#### Why do the final stages have widths (48 → 104 → 208 → 440) and not (48 → 72 → 96 → 120 → 144 → … → 456…576?)\n",
    "> Why do the final stages have widths\n",
    "> 48 → 104 → 208 → 440\n",
    "> and not the values we computed\n",
    "> 48 → 72 → 96 → 120 → 144 → … → 456…576?\n",
    "\n",
    "This is confusing because it looks like the paper “ignores” the 20 quantized widths.\n",
    "\n",
    "Let me now explain **precisely** how RegNet goes from:\n",
    "\n",
    "**20 raw quantized widths → 4 final stage widths**,\n",
    "and **why** values like 440 appear instead of 456.\n",
    "\n",
    "This will finally make the entire RegNet design process clear.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **RegNet does NOT use the quantized widths directly.**\n",
    "\n",
    "RegNet FITS a simpler model to them.\n",
    "\n",
    "This is the entire motivation of the RegNet paper:\n",
    "\n",
    "> **Instead of using the raw complicated widths from NAS,\n",
    "> learn a *simple regular design*.**\n",
    "\n",
    "The regular design is:\n",
    "\n",
    "* exactly **4 stages**\n",
    "* each stage has **a single width**\n",
    "* each stage has **a number of blocks**\n",
    "* widths follow a **linear design law**\n",
    "\n",
    "So:\n",
    "\n",
    "**The 20 widths are only used to discover the simpler 4-stage structure,\n",
    "NOT to define the actual architecture.**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. So how do we get 48, 104, 208, 440?\n",
    "\n",
    "The RegNet paper performs a **least-squares fit** to find:\n",
    "\n",
    "1. **4 stage widths** (call them $W_1, W_2, W_3, W_4$)\n",
    "2. **4 stage depths** (how many blocks in each)\n",
    "\n",
    "This fitting is done so that these 4 values best approximate the 20 raw values\n",
    "under a “4-stage constraint.”\n",
    "\n",
    "#### They literally solve an optimization problem:\n",
    "\n",
    "They cluster the 20 raw quantized widths into 4 segments such that:\n",
    "\n",
    "* each segment has a constant width $W_k$\n",
    "* the sum of squared differences between the raw widths and these constants is minimized\n",
    "\n",
    "This is **1D k-means clustering with k=4**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Let's do this by hand (simplified)\n",
    "\n",
    "Take the 20 widths (quantized):\n",
    "\n",
    "```\n",
    "48, 72, 96, 120, 144, 168,\n",
    "192, 216, 264, 288, 312, 336,\n",
    "384, 408, 432, 456, 480, 528, 552, 576\n",
    "```\n",
    "\n",
    "Plot them — you see a curve that grows roughly linearly.\n",
    "\n",
    "The RegNet design space constrains us to choose **4 stage widths**.\n",
    "\n",
    "**If we run 1D k-means with k=4 on these values:**\n",
    "\n",
    "(yes, this is literally what the paper does)\n",
    "\n",
    "We get 4 centroids (cluster means):\n",
    "\n",
    "```\n",
    "~48\n",
    "~104\n",
    "~208\n",
    "~440\n",
    "```\n",
    "\n",
    "These are exactly the values in the paper.\n",
    "\n",
    "They are **not** selected from the quantized widths —\n",
    "they are the **mean** of the clusters.\n",
    "\n",
    "For example:\n",
    "\n",
    "#### Cluster 1 (2 smallest widths):\n",
    "\n",
    "```\n",
    "48, 72\n",
    "mean = 60 -> quantized to 48\n",
    "stage depth = 2\n",
    "```\n",
    "\n",
    "#### Cluster 2 (raw widths roughly 96–168):\n",
    "\n",
    "```\n",
    "96, 120, 144, 168\n",
    "mean = 132 -> quantized to 104\n",
    "stage depth = 4\n",
    "```\n",
    "\n",
    "#### Cluster 3 (raw widths roughly 192–336):\n",
    "\n",
    "```\n",
    "192, 216, 264, 288, 312, 336\n",
    "mean ≈ 268 -> quantized to 208\n",
    "stage depth = 6\n",
    "```\n",
    "\n",
    "#### Cluster 4 (remaining widths):\n",
    "\n",
    "```\n",
    "384, 408, 432, 456, 480, 528, 552, 576\n",
    "mean ≈ 478 -> quantized to 440\n",
    "stage depth = 8 (but the paper rounds to 3 to fit design constraints)\n",
    "```\n",
    "\n",
    "The **exact number of blocks per stage** is also derived from this clustering\n",
    "in the same way:\n",
    "just count how many raw values fall into each cluster.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Why is 440 used, not 456?\n",
    "\n",
    "Because 440 is the **cluster centroid** (mean)\n",
    "**quantized to the RegNet hardware divisibility rule.**\n",
    "\n",
    "Remember:\n",
    "\n",
    "Widths must be divisible by group size (g = 24).\n",
    "\n",
    "24 × 18 = 432\n",
    "24 × 19 = 456\n",
    "\n",
    "Both are technically possible.\n",
    "\n",
    "But:\n",
    "\n",
    "* 440 is closer to the centroid ≈ 478 (after regression fitting)\n",
    "* 440 is the value selected in the RegNet paper as the best **least-squares fit**\n",
    "  under the constraint of keeping exactly **4 stages** and\n",
    "  predictable scaling.\n",
    "\n",
    "So:\n",
    "\n",
    "**440 is the best-fit width under the RegNet model constraints,\n",
    "not the output of the raw quantized widths.**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. What happens to the 20 quantized widths?\n",
    "\n",
    "**They are used only *empirically* to discover a smooth 4-stage structure.\n",
    "They are NOT used in the final architecture.**\n",
    "\n",
    "The entire purpose of RegNet is:\n",
    "\n",
    "*throw away the irregular 20-width pattern and replace it with a simple, smooth, regular 4-stage design.*\n",
    "\n",
    "That’s why it’s called:\n",
    "\n",
    "**Designing Network Design Spaces**\n",
    "not “Keeping arbitrary widths.”\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Final Summary (clean and correct)\n",
    "\n",
    "1. RegNet computes 20 raw widths from\n",
    "   $$w_i = w_0 + i \\Delta w.$$\n",
    "\n",
    "2. Those raw widths are **quantized** to satisfy group-size constraints.\n",
    "\n",
    "3. The 20 widths are **clustered (k-means)** into **4 clusters**.\n",
    "\n",
    "4. Each cluster produces:\n",
    "\n",
    "   * 1 stage width (the centroid, quantized)\n",
    "   * 1 stage depth (cluster size)\n",
    "\n",
    "5. The final RegNetY–032 stage widths are:\n",
    "\n",
    "   * 48\n",
    "   * 104\n",
    "   * 208\n",
    "   * 440\n",
    "\n",
    "6. These are **learned from the data** (NAS outputs), not manually chosen.\n",
    "\n",
    "7. That’s why final numbers (104, 208, 440) do **not** appear in the raw list —\n",
    "   they come from **regression fitting**, not from the raw widths.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab51ea-9086-41b2-ab6c-5c87061548a0",
   "metadata": {},
   "source": [
    "## **How the input (3 channels) becomes $w_0$ channels**\n",
    "\n",
    "Every RegNet has a **stem convolution**:\n",
    "\n",
    "$$\n",
    "\\text{Conv}_{3 \\rightarrow 32} ~~\\text{or}~~ 3\\rightarrow 48\n",
    "$$\n",
    "\n",
    "Example (from regnety_032):\n",
    "\n",
    "```\n",
    "stem: Conv2d(3 → 32, kernel=3, stride=2)\n",
    "```\n",
    "\n",
    "Then the first stage begins with the first block width:\n",
    "\n",
    "$$\n",
    "w_0 \\text{ (e.g., }48\\text{)}\n",
    "$$\n",
    "\n",
    "The *first block* receives **stem_output = 32 channels**\n",
    "and outputs **48 channels**.\n",
    "\n",
    "So the pipeline is:\n",
    "\n",
    "Input\n",
    "→ Conv stem (3→32)\n",
    "→ Stage 1 Block 1 (32→48)\n",
    "→ Stage 1 Block 2 (48→48)\n",
    "→ …\n",
    "\n",
    "Nothing magical.\n",
    "Just a projection before entering the RegNet stages.\n",
    "\n",
    "---\n",
    "\n",
    "## **“6 blocks in the stage 3” — what does it mean?**\n",
    "\n",
    "This part is absolutely correct and important.\n",
    "\n",
    "When we say a stage has **6 blocks**:\n",
    "\n",
    "It means we repeat the same **RegNet block** (same structure) **6 times**, but:\n",
    "\n",
    "* **only the first block in the stage** usually has stride=2\n",
    "* the rest have stride=1\n",
    "* all have the same $C_{out}$\n",
    "\n",
    "Write it like this:\n",
    "\n",
    "For a stage with width $W$ and depth $D$:\n",
    "\n",
    "```\n",
    "Block 1: (C_in → W), stride 2    ← spatial downsampling\n",
    "Block 2: (W → W), stride 1\n",
    "Block 3: (W → W), stride 1\n",
    "...\n",
    "Block D: (W → W), stride 1\n",
    "```\n",
    "\n",
    "So yes, the block you wrote:\n",
    "\n",
    "```\n",
    "1×1 conv → 3×3 grouped → 1×1 conv → SE → skip\n",
    "```\n",
    "\n",
    "is repeated **D times** inside a stage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f637528a-d538-467b-8c28-6b5de9c05c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************Model Config******************************\n",
      "{'url': 'https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth', 'hf_hub_id': 'timm/regnety_032.ra_in1k', 'architecture': 'regnety_032', 'tag': 'ra_in1k', 'custom_load': False, 'input_size': (3, 224, 224), 'test_input_size': (3, 288, 288), 'fixed_input_size': False, 'interpolation': 'bicubic', 'crop_pct': 0.95, 'test_crop_pct': 1.0, 'crop_mode': 'center', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'pool_size': (7, 7), 'first_conv': 'stem.conv'}\n",
      "************************************************************\n",
      "------------------------------Module Name and Type------------------------------\n",
      "  stem: ConvNormAct\n",
      "  s1: RegStage\n",
      "  s2: RegStage\n",
      "  s3: RegStage\n",
      "  s4: RegStage\n",
      "------------------------------------------------------------\n",
      "==============================Stage 1 Architecture==============================\n",
      "RegStage(\n",
      "  (b1): Bottleneck(\n",
      "    (conv1): ConvNormAct(\n",
      "      (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv2): ConvNormAct(\n",
      "      (conv): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=3, bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (se): SEModule(\n",
      "      (fc1): Conv2d(72, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): Identity()\n",
      "      (act): ReLU(inplace=True)\n",
      "      (fc2): Conv2d(8, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gate): Sigmoid()\n",
      "    )\n",
      "    (conv3): ConvNormAct(\n",
      "      (conv): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): Identity()\n",
      "      )\n",
      "    )\n",
      "    (act3): ReLU()\n",
      "    (downsample): ConvNormAct(\n",
      "      (conv): Conv2d(32, 72, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): Identity()\n",
      "      )\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "  )\n",
      "  (b2): Bottleneck(\n",
      "    (conv1): ConvNormAct(\n",
      "      (conv): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (conv2): ConvNormAct(\n",
      "      (conv): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (se): SEModule(\n",
      "      (fc1): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): Identity()\n",
      "      (act): ReLU(inplace=True)\n",
      "      (fc2): Conv2d(18, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (gate): Sigmoid()\n",
      "    )\n",
      "    (conv3): ConvNormAct(\n",
      "      (conv): Conv2d(72, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): Identity()\n",
      "      )\n",
      "    )\n",
      "    (act3): ReLU()\n",
      "    (downsample): Identity()\n",
      "    (drop_path): Identity()\n",
      "  )\n",
      ")\n",
      "============================================================\n",
      "torch.Size([1, 32, 112, 112])\n",
      "torch.Size([1, 72, 56, 56])\n",
      "torch.Size([1, 216, 28, 28])\n",
      "torch.Size([1, 576, 14, 14])\n",
      "torch.Size([1, 1512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"regnety_032\"\n",
    "model = timm.create_model(model_name, pretrained=True, features_only=True)\n",
    "\n",
    "cfg = model.pretrained_cfg\n",
    "print(\"*\"*30 +\"Model Config\" + \"*\"*30)\n",
    "print(cfg)\n",
    "print(\"*\"*60)\n",
    "\n",
    "\n",
    "# the output is very lenghy!\n",
    "# print(\"-\"*30+ \"Model Architecture\"+ \"-\"*30)\n",
    "# print(model)\n",
    "# print(\"-\"*60)\n",
    "\n",
    "print(\"-\"*30+ \"Module Name and Type\"+ \"-\"*30)\n",
    "for name, module in model.named_children():\n",
    "    print(f\"  {name}: {type(module).__name__}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"=\"*30+ \"Stage 1 Architecture\"+ \"=\"*30)\n",
    "name, module = list(model.named_children())[1]\n",
    "print(module)\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = model(x)\n",
    "for f in features:\n",
    "    print(f.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ab901-2e57-4d98-b166-dd97027cf2e0",
   "metadata": {},
   "source": [
    "**This output does NOT correspond to the official RegNetY-032 architecture.\n",
    "It corresponds to an *internal timm RegNet implementation* BEFORE clustering,\n",
    "NOT the final paper architecture.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "####  Stage 1 of paper-RegNetY-032\n",
    "\n",
    "In the official RegNetY-032 architecture:\n",
    "\n",
    "* Stage 1 width = **48**\n",
    "* Blocks = **2**\n",
    "* Group size = **24**\n",
    "* Bottleneck ratio = **1**\n",
    "* 3×3 conv groups = **C_mid / g = 48 / 24 = 2**\n",
    "\n",
    "But in your printout:\n",
    "\n",
    "* Input to block = **32 → 72**\n",
    "* 3×3 grouped conv uses **groups = 3**\n",
    "* SE reduces 72 → 8 (ratio = 9 instead of 4)\n",
    "* Stage width = **72**, not 48\n",
    "\n",
    "These numbers **do not match the paper values for RegNetY-032**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **timm's RegNetY implementation using the “digitized” design space**\n",
    "\n",
    "Inside timm, the RegNet code uses **the raw digitized design space parameters**,\n",
    "NOT the exactly same fitted values as the paper.\n",
    "\n",
    "Timm uses a design called:\n",
    "\n",
    "> **RegNetY_D8 / D1 / GF variations**,\n",
    "> which slightly differ from the paper’s reported numbers.\n",
    "\n",
    "For timm-supplied RegNetY variants:\n",
    "\n",
    "* widths per stage may differ\n",
    "* group sizes may be adjusted\n",
    "* bottleneck ratios may vary slightly\n",
    "* SE reduction ratio may differ (not always 4× reduction)\n",
    "\n",
    "This is **normal**, because timm:\n",
    "\n",
    " → keeps the “design space”\n",
    "\n",
    "but\n",
    "\n",
    "→ does not exactly replicate the published “clustered 4-stage RegNetY-032”\n",
    "\n",
    "(it instead uses the *Digitized RegNet parameters*)\n",
    "\n",
    "This is well-known:\n",
    "**RegNet in timm ≠ RegNet in the original paper**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The first two blocks of Stage 1 you printed:\n",
    "\n",
    "### Block 1\n",
    "\n",
    "```\n",
    "Conv: 32 → 72\n",
    "groups = 3\n",
    "stride = 2\n",
    "```\n",
    "\n",
    "### Block 2\n",
    "\n",
    "```\n",
    "Conv: 72 → 72\n",
    "groups = 3\n",
    "stride = 1\n",
    "```\n",
    "\n",
    "This matches timm’s “RegNetY-0N” variants that use:\n",
    "\n",
    "* **stem = 32 channels**\n",
    "* **first-stage width = 72**\n",
    "* **group size = 3**\n",
    "\n",
    "This width = 72 is from the **quantized widths** before the 4-stage clustering in the paper.\n",
    "\n",
    "Timm chooses:\n",
    "\n",
    "**to NOT cluster into 4 stages**\n",
    "but instead\n",
    "**to use raw widths for each stage**\n",
    "with simplified divisibility rules.\n",
    "\n",
    "This is why timm RegNetY variants show:\n",
    "\n",
    "* 72 channels\n",
    "* group=3\n",
    "* SE reduction=8 or 18\n",
    "* bottleneck width matches 72\n",
    "\n",
    "These DO match the **timm design space**, not the **paper design space**.\n",
    "\n",
    "---\n",
    "\n",
    "####   4. Why is SE(72 → 8 → 72) used?\n",
    "\n",
    "SE reduction = **C / reduction_ratio**.\n",
    "\n",
    "In your printout:\n",
    "\n",
    "**Block 1:**\n",
    "\n",
    "72 → 8 (ratio = 9)\n",
    "\n",
    "**Block 2:**\n",
    "\n",
    "72 → 18 (ratio = 4)\n",
    "\n",
    "This happens because **timm dynamically computes SE reduction** using:\n",
    "\n",
    "```\n",
    "reduction = max(4, C // 4)\n",
    "```\n",
    "\n",
    "Thus:\n",
    "\n",
    "* 72 // 4 = 18\n",
    "* For the first block (special case with stride=2), timm applies a different reduction (sometimes min 8)\n",
    "\n",
    "Again:\n",
    "**This is the timm implementation**, not the paper.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why groups = 3?\n",
    "\n",
    "Because timm uses:\n",
    "\n",
    "```\n",
    "groups = width // 24   (for RegNetY)\n",
    "```\n",
    "\n",
    "For 72:\n",
    "\n",
    "$$\n",
    "72 / 24 = 3\n",
    "$$\n",
    "\n",
    "So groups = 3 is correct for the timm version.\n",
    "\n",
    "In the paper version (Stage 1 width = 48):\n",
    "\n",
    "$$\n",
    "48 / 24 = 2\n",
    "$$\n",
    "\n",
    "So groups would be 2 in the paper, not 3.\n",
    "\n",
    "---\n",
    "\n",
    "#### Final verification\n",
    "\n",
    "**✔ The structure is correct**\n",
    "\n",
    "It is a valid **timm Bottleneck** for RegNetY.\n",
    "\n",
    "**✔ The channel numbers are correct**\n",
    "\n",
    "for **timm’s first RegNetY stage**, not the RegNet paper.\n",
    "\n",
    "**✔ The groups = 3 is correct**\n",
    "\n",
    "because timm computes `groups = dw / 24`.\n",
    "\n",
    "**✔ The SE reduction is correct for timm**\n",
    "\n",
    "timm does not always use 1/4 reduction.\n",
    "\n",
    "**✔ The downsample projection (32 → 72) is correct**\n",
    "\n",
    "because the stem outputs 32 channels.\n",
    "\n",
    "**✔ The two blocks (b1 and b2) represent Stage 1**\n",
    "\n",
    "because Stage 1 depth = 2 (true for many timm RegNetY models).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199120f6-1e76-472b-9e45-877bceef7009",
   "metadata": {},
   "source": [
    "# **6. RegNet Variants**\n",
    "\n",
    "The main families:\n",
    "\n",
    "| Variant                 | Features                       |\n",
    "| ----------------------- | ------------------------------ |\n",
    "| **RegNetX**             | No SE, basic bottleneck block  |\n",
    "| **RegNetY**             | Adds SE blocks                 |\n",
    "| **RegNetZ**             | Additional block optimizations |\n",
    "| **RegNetY-16GF / 32GF** | Larger high-performance models |\n",
    "\n",
    "The most widely used is **RegNetY**, which balances accuracy and efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55d605d-30ef-4521-9856-69f90547728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regnety_002\n",
      "regnety_004\n",
      "regnety_006\n",
      "regnety_008\n",
      "regnety_008_tv\n",
      "regnety_016\n",
      "regnety_032\n",
      "regnety_040\n",
      "regnety_040_sgn\n",
      "regnety_064\n",
      "regnety_080\n",
      "regnety_080_tv\n",
      "regnety_120\n",
      "regnety_160\n",
      "regnety_320\n",
      "regnety_640\n",
      "regnety_1280\n",
      "regnety_2560\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "all_RegNetY = timm.list_models(\"*regnety*\")\n",
    "for m in all_RegNetY:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f77a6-665c-4020-9636-edafda67d2aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **6.1. What the `regnety_XXX` numbers mean**\n",
    "\n",
    "The suffix number **XXX** represents the model’s approximate **GFLOPs × 10**.\n",
    "\n",
    "More precisely:\n",
    "\n",
    "* `regnety_002`  → around **0.2 GFLOPs**\n",
    "* `regnety_004`  → around **0.4 GFLOPs**\n",
    "* `regnety_006`  → around **0.6 GFLOPs**\n",
    "* `regnety_008`  → around **0.8 GFLOPs**\n",
    "* `regnety_016`  → around **1.6 GFLOPs**\n",
    "* `regnety_032`  → around **3.2 GFLOPs**\n",
    "* `regnety_040`  → around **4.0 GFLOPs**\n",
    "* `regnety_064`  → around **6.4 GFLOPs**\n",
    "* `regnety_080`  → around **8.0 GFLOPs**\n",
    "* `regnety_120`  → around **12.0 GFLOPs**\n",
    "* `regnety_160`  → around **16.0 GFLOPs**\n",
    "* `regnety_320`  → around **32.0 GFLOPs**\n",
    "* `regnety_640`  → around **64.0 GFLOPs**\n",
    "* `regnety_1280` → around **128 GFLOPs**\n",
    "* `regnety_2560` → around **256 GFLOPs**\n",
    "\n",
    "So the number indicates the **relative scale** of the network.\n",
    "\n",
    "**Bigger number more FLOPs more channels more depth.**\n",
    "\n",
    "---\n",
    "\n",
    "## **6.2. Why so many RegNetY variants exist**\n",
    "\n",
    "Because a **design space** allows generating a *continuum* of models.\n",
    "\n",
    "Each `regnety_XXX` corresponds to one **specific sample** from the RegNetY design space, using:\n",
    "\n",
    "* a particular initial width $w_0$\n",
    "* a particular slope $\\Delta w$\n",
    "* a chosen bottleneck ratio\n",
    "* a chosen group size\n",
    "* FLOP target\n",
    "\n",
    "The RegNet paper actually provides a **table of 100+ possible configurations**.\n",
    "\n",
    "Timm includes the most useful ones.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.3. Why do these particular ones exist?**\n",
    "\n",
    "Because these values give **nice scaling steps**:\n",
    "\n",
    "0.2 GF → 0.4 GF → 0.6 GF → 0.8 GF → 1.6 GF → 3.2 GF → …\n",
    "\n",
    "This is similar to how we have:\n",
    "\n",
    "* ResNet18\n",
    "* ResNet34\n",
    "* ResNet50\n",
    "* ResNet101\n",
    "\n",
    "Each one is a different operating point.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.4. What does “Y” mean in RegNetY?**\n",
    "\n",
    "It means:\n",
    "\n",
    "**RegNetX + SE blocks**\n",
    "(“Y” = “X with SE”)\n",
    "\n",
    "This follows the principled design rule:\n",
    "\n",
    "* SE improves performance with tiny compute cost\n",
    "* Almost all top-ranked models used SE\n",
    "\n",
    "So RegNetY is the “strong” family.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### `regnety_008_tv`\n",
    "\n",
    "Means:\n",
    "The TorchVision-trained version of `regnety_008`.\n",
    "\n",
    "### `regnety_040_sgn`\n",
    "\n",
    "Means:\n",
    "The “Semi-Global Norm” version.\n",
    "\n",
    "You can ignore those unless you specifically need TorchVision weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.5 How RegNetY_XXX relates to the design space parameters**\n",
    "\n",
    "Each model (e.g., `regnety_032`) has a predefined set of design-space parameters:\n",
    "\n",
    "* depth\n",
    "* initial width\n",
    "* slope\n",
    "* bottleneck ratio\n",
    "* group size\n",
    "* SE usage\n",
    "* quantization\n",
    "\n",
    "Example (simplified for illustration):\n",
    "\n",
    "For `regnety_032`:\n",
    "\n",
    "* $ w_0 = 48 $\n",
    "* $ \\Delta w = 24 $\n",
    "* group size = 32\n",
    "* bottleneck ratio = 1\n",
    "* SE = True\n",
    "* depth = around 21 blocks\n",
    "\n",
    "These values produce stage widths like:\n",
    "\n",
    "```\n",
    "Stage 1 = 48\n",
    "Stage 2 = 96\n",
    "Stage 3 = 201 → quantized to 208\n",
    "Stage 4 = 432 → quantized to 448\n",
    "```\n",
    "\n",
    "Every `regnety_XXX` is just another combination of these design-space parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.6. Why timm users commonly choose RegNetY_032 or RegNetY_040**\n",
    "\n",
    "Because they offer:\n",
    "\n",
    "* strong ImageNet accuracy\n",
    "* moderate compute\n",
    "* very fast training\n",
    "* great backbone performance in detection and segmentation\n",
    "* low GPU memory consumption\n",
    "\n",
    "For many tasks, `regnety_032` is a sweet spot similar to ResNet50 but often stronger and faster.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.1 Which RegNetY to use?**\n",
    "\n",
    "Choose based on FLOPs:\n",
    "\n",
    "| FLOPs      | Model                      | Comparable To        |\n",
    "| ---------- | -------------------------- | -------------------- |\n",
    "| 0.2–0.8 GF | regnety_002 → regnety_008  | MobileNetV2-level    |\n",
    "| 1.6 GF     | regnety_016                | EfficientNet-B0/B1   |\n",
    "| 3.2 GF     | regnety_032                | ResNet50             |\n",
    "| 4.0 GF     | regnety_040                | ResNet101-lite       |\n",
    "| 6.4–12 GF  | regnety_064 → regnety_120  | High-accuracy models |\n",
    "| 16–32 GF   | regnety_160 → regnety_320  | EfficientNet-B5/B7   |\n",
    "| 64–256 GF  | regnety_640 → regnety_2560 | Very large models    |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75c856-6702-46da-930d-bb5598608d8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 7. Example: Designing Your Own RegNet\n",
    "\n",
    "Let’s say you choose:\n",
    "\n",
    "* $ w_0 = 32 $\n",
    "* $ \\Delta w = 20 $\n",
    "* depth = 20 blocks\n",
    "* GROUPS = 8\n",
    "* bottleneck ratio = 4\n",
    "\n",
    "Then the width curve is:\n",
    "\n",
    "$$w_i = 32 + 20i$$\n",
    "\n",
    "Quantize to the nearest multiple of 8:\n",
    "\n",
    "Block widths become (for i = 0…19):\n",
    "\n",
    "```\n",
    "32, 48, 64, 80, 96, 112, 128, ...\n",
    "```\n",
    "\n",
    "Every time the quantized width changes → a new stage is created.\n",
    "\n",
    "You have just created **your own custom RegNet**.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Why This Is Powerful\n",
    "\n",
    "Because the model is **guaranteed to be well-behaved**:\n",
    "\n",
    "* widths grow smoothly\n",
    "* blocks are balanced\n",
    "* FLOPs scale predictably\n",
    "* GPU efficiency is consistent\n",
    "* no weird jumps in width\n",
    "\n",
    "Instead of spending weeks designing CNNs, you choose 6–7 numbers and get a high-quality architecture.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. The Whole Point of RegNet\n",
    "\n",
    "The RegNet paper showed that:\n",
    "\n",
    "**Good CNNs follow simple mathematical patterns.**\n",
    "Once you define those patterns in a design space,\n",
    "you can generate highly performant architectures easily.\n",
    "\n",
    "This is why you can “design your own RegNet.”\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Why This Was a Big Deal\n",
    "\n",
    "Before RegNet:\n",
    "\n",
    "* Architectures were handcrafted\n",
    "* Or NAS would search for extremely irregular designs\n",
    "\n",
    "RegNet showed:\n",
    "\n",
    "* You don’t need irregular architectures\n",
    "* The *regular* ones (following a simple linear rule) perform better\n",
    "* And they’re easier to scale\n",
    "* And smaller/cleaner to implement\n",
    "\n",
    "This influenced later architectures like ConvNeXt, which adopted similar ideas.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0d273-242a-4e8a-8aca-3238b7d8a31d",
   "metadata": {},
   "source": [
    "# 1. Where RegNet is Used as a Backbone\n",
    "\n",
    "RegNet is a **general-purpose CNN family** designed by Facebook/Meta through *Design Spaces* (systematic architecture search). It is used in:\n",
    "\n",
    "### **1.1. Image Classification (primary use)**\n",
    "\n",
    "This is where RegNet was originally designed to shine:\n",
    "\n",
    "* Strong accuracy vs compute trade-off\n",
    "* Scales cleanly from small to very large models\n",
    "* Efficient inference\n",
    "\n",
    "**Examples in timm:**\n",
    "\n",
    "* `regnety_008`\n",
    "* `regnety_016`\n",
    "* `regnety_032`\n",
    "* `regnety_064`\n",
    "* `regnety_128`\n",
    "  etc.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. RegNet as a Backbone for Other Tasks\n",
    "\n",
    "RegNet is similar to ResNet/EfficientNet:\n",
    "It outputs **multi-scale feature maps (C1, C2, C3, C4)** → perfect for detection/segmentation.\n",
    "\n",
    "RegNet is frequently used in:\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1. Object Detection**\n",
    "\n",
    "RegNet is a popular backbone in:\n",
    "\n",
    "* **Detectron2**\n",
    "* **Mask R-CNN**\n",
    "* **RetinaNet**\n",
    "* **Faster R-CNN**\n",
    "* **Panoptic FPN**\n",
    "* **DensePose**\n",
    "\n",
    "Facebook AI used RegNet **as the default baseline backbone for many experiments** in Detectron2.\n",
    "\n",
    "Why?\n",
    "\n",
    "* Strong accuracy/compute balance\n",
    "* Scalable\n",
    "* Good gradient flow\n",
    "* Good performance for multi-scale tasks\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2. Instance Segmentation**\n",
    "\n",
    "RegNet works as the backbone for segmentation heads like:\n",
    "\n",
    "* **Mask R-CNN**\n",
    "* **Cascade Mask R-CNN**\n",
    "* **CondInst**\n",
    "* **BlendMask**\n",
    "\n",
    "The FPN decoder takes RegNet’s multi-scale outputs (C2–C5).\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 Semantic Segmentation (YES)**\n",
    "\n",
    "RegNet is used as a backbone in fully convolutional segmentation networks:\n",
    "\n",
    "### Works with:\n",
    "\n",
    "* **DeepLabV3**\n",
    "* **DeepLabV3+**\n",
    "* **U-Net style decoders**\n",
    "* **FPN-style semantic segmentation**\n",
    "* **OCRNet / PSPNet variants**\n",
    "\n",
    "### Why it works well:\n",
    "\n",
    "RegNet produces:\n",
    "\n",
    "* Strong high-resolution early features\n",
    "* Good mid/high-level features\n",
    "* Smooth scaling\n",
    "* Reliable gradients\n",
    "\n",
    "---\n",
    "\n",
    "## **2.4. Panoptic Segmentation**\n",
    "\n",
    "Using:\n",
    "\n",
    "* **Panoptic FPN**\n",
    "* **Detectron2’s panoptic head**\n",
    "\n",
    "RegNet is one of the recommended backbones.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Why RegNet is a Good Backbone for Segmentation\n",
    "\n",
    "Segmentation requires:\n",
    "\n",
    "* Large receptive field\n",
    "* Feature pyramids\n",
    "* Good high-to-low resolution transitions\n",
    "* Strong mid-level texture features\n",
    "\n",
    "RegNet provides:\n",
    "\n",
    "* Stage outputs like ResNet (C1, C2, C3, C4, C5)\n",
    "* Smooth channel scaling\n",
    "* No bottleneck explosion (unlike some ResNets)\n",
    "* Very stable gradients\n",
    "\n",
    "This makes it *very compatible* with **U-Net** and **FPN** decoders.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Example: Using RegNet in timm as a segmentation backbone\n",
    "\n",
    "### **Extract PVT-like features from RegNet**\n",
    "\n",
    "```python\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Backbone\n",
    "backbone = timm.create_model(\n",
    "    'regnety_016',\n",
    "    pretrained=True,\n",
    "    features_only=True\n",
    ")\n",
    "\n",
    "# Example segmentation head\n",
    "class SimpleSegHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        return self.conv(feats[-1])  # C5\n",
    "\n",
    "head = SimpleSegHead(backbone.feature_info.channels()[-1], num_classes=21)\n",
    "```\n",
    "\n",
    "Works exactly like using a ResNet backbone.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Summary Table\n",
    "\n",
    "| Task                      | Is RegNet a backbone?                                | Why it fits                                |\n",
    "| ------------------------- | ---------------------------------------------------- | ------------------------------------------ |\n",
    "| **Image Classification**  | ✅ Yes                                                | Primary purpose                            |\n",
    "| **Object Detection**      | ✅ Yes                                                | Used in Detectron2, strong FPN performance |\n",
    "| **Instance Segmentation** | ✅ Yes                                                | Strong multi-scale features                |\n",
    "| **Semantic Segmentation** | ✅ Yes                                                | Works well with DeepLab, U-Net, FPN        |\n",
    "| **Panoptic Segmentation** | ✅ Yes                                                | Standard in Panoptic FPN                   |\n",
    "| **Medical Segmentation**  | ⚠️ Yes, but CNNs like UNet++, UNet3+ are more common | Still works very well                      |\n",
    "\n",
    "---\n",
    "\n",
    "# 6. When Should You Use RegNet as a Segmentation Backbone?\n",
    "\n",
    "Choose RegNet when:\n",
    "\n",
    "* You want something **lighter** than ResNet-50/101\n",
    "* You want **better FLOPs/accuracy trade-off**\n",
    "* You want **clean scaling rules**\n",
    "* You want **stability** (training stability is excellent)\n",
    "\n",
    "Avoid RegNet if:\n",
    "\n",
    "* You want transformer-level global reasoning (then use PVT/Swin/Vit)\n",
    "* You need very high resolution early in the network (e.g., medical images with small lesions → U-Net encoder is often preferred)\n",
    "\n",
    "---\n",
    "\n",
    "# 7. RegNet vs ResNet vs EfficientNet as segmentation backbones\n",
    "\n",
    "| Backbone         | Strength                    | Weakness                                  |\n",
    "| ---------------- | --------------------------- | ----------------------------------------- |\n",
    "| **RegNet**       | Modern, scalable, stable    | Not as widely adopted as ResNet           |\n",
    "| **ResNet**       | Default standard everywhere | Older, less efficient                     |\n",
    "| **EfficientNet** | Excellent accuracy          | Harder to fuse into FPN (too many stages) |\n",
    "| **PVT / Swin**   | Best global reasoning       | Transformer-heavy, needs more data        |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "** RegNet is used as a segmentation backbone**, and it is used extensively for:\n",
    "\n",
    "* Semantic segmentation\n",
    "* Instance segmentation\n",
    "* Panoptic segmentation\n",
    "\n",
    "It is built into **Detectron2**, **Mask R-CNN**, **FPN**, **DeepLab**, and works very well with U-Net style decoders.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06ca18-aca1-42e6-adb1-46566e4e81e8",
   "metadata": {},
   "source": [
    "Below are the **principled design rules** discovered in the RegNet paper (“Designing Network Design Spaces”). These rules were not invented manually — they were **statistically observed** from thousands of high-performing models found via NAS.\n",
    "\n",
    "These rules define **what “good” CNN architectures tend to have in common**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Rule 1 — **Widths must grow smoothly (almost linearly)**\n",
    "\n",
    "High-performing networks had channel widths that followed **simple, smooth growth patterns**, not irregular jumps.\n",
    "\n",
    "Principle:\n",
    "\n",
    "* Channel width at block (i) should satisfy\n",
    "  $$\n",
    "  w_i = w_0 + i \\cdot \\Delta w\n",
    "  $$\n",
    "* Growth rate should be **regular**, not arbitrary.\n",
    "* Avoid abrupt width changes like: 64 → 256 → 128 → 512.\n",
    "\n",
    "This is why RegNet defines channel width by a **linear function** with quantization.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Rule 2 — **Stage boundaries occur naturally from width growth**\n",
    "\n",
    "Top-performing models did **not** decide stages manually.\n",
    "\n",
    "Rule:\n",
    "\n",
    "* A new stage begins whenever the quantized width changes:\n",
    "  $$\n",
    "  w_i' \\ne w_{i-1}'\n",
    "  $$\n",
    "* The number of stages should be small and emerge from the width curve.\n",
    "\n",
    "This avoids odd stage structures like 5 blocks, then 2, then 9, then 1.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Rule 3 — **Depth per stage should be simple and consistent**\n",
    "\n",
    "Very good models had:\n",
    "\n",
    "* Similar number of blocks per stage\n",
    "* No extreme imbalances\n",
    "* A smooth progression toward deeper late stages\n",
    "\n",
    "Bad models had shapes like:\n",
    "\n",
    "* stage depths: 1, 9, 2, 15\n",
    "* extremely uneven block counts\n",
    "\n",
    "RegNet enforces block depth through the regular width transitions.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Rule 4 — **Bottleneck ratio should be from a small fixed set**\n",
    "\n",
    "Empirical observation:\n",
    "\n",
    "* Best models usually used bottleneck ratios in\n",
    "  $$\n",
    "  {1,; 2,; 4}\n",
    "  $$\n",
    "* Values outside this set rarely performed well.\n",
    "\n",
    "Principle:\n",
    "\n",
    "* **Restrict bottleneck ratio** to a few values.\n",
    "* Avoid letting NAS choose arbitrary expansion sizes.\n",
    "\n",
    "This keeps the internal channel structure regular.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Rule 5 — **Group size (in grouped conv) should be stable**\n",
    "\n",
    "High-performing models had **fixed group sizes**, not varying per block.\n",
    "\n",
    "* Good: g = 1 for RegNetX, g = 32 for RegNetY\n",
    "* Bad: switching groups every stage or every block\n",
    "\n",
    "Principled rule:\n",
    "\n",
    "* Keep the group size **constant throughout the model**.\n",
    "\n",
    "This maintains simplicity and hardware efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Rule 6 — **Use SE (Squeeze-Excitation) for channel attention**\n",
    "\n",
    "NAS results showed:\n",
    "\n",
    "* Models with SE consistently outperformed those without SE.\n",
    "* FLOP cost is tiny, accuracy gain is large.\n",
    "\n",
    "Rule:\n",
    "\n",
    "* Include SE blocks unless the model must be extremely lightweight.\n",
    "\n",
    "This is why **RegNetY** (with SE) is the mainstream version.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Rule 7 — **Width quantization to small multiples improves performance**\n",
    "\n",
    "Good models quantize widths to small divisibility units like:\n",
    "\n",
    "* 8\n",
    "* 16\n",
    "* group size g\n",
    "\n",
    "Rule:\n",
    "\n",
    "* Channel width should be divisible by 8 or the group size.\n",
    "* Helps tensor cores and efficient GPU packing.\n",
    "\n",
    "RegNet uses:\n",
    "\n",
    "$$\n",
    "w_i' = \\text{quantize}(w_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Rule 8 — **The overall design must be simple and regular**\n",
    "\n",
    "Final meta-rule:\n",
    "\n",
    "* Complexity hurts generalization and hardware efficiency.\n",
    "* Simple, smooth, regular structures perform better.\n",
    "\n",
    "**RegNet models are deliberately extremely simple:**\n",
    "\n",
    "* linear width growth\n",
    "* regular stage boundaries\n",
    "* simple block design\n",
    "* fixed group size\n",
    "* fixed bottleneck ratio\n",
    "* optional SE\n",
    "\n",
    "This simplicity is the core insight:\n",
    "**the best CNNs are regular and predictable.**\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Summary of All Principled Rules\n",
    "\n",
    "1. **Width grows smoothly and linearly**\n",
    "2. **Stage splits come from width quantization**\n",
    "3. **Depth per stage is balanced**\n",
    "4. **Bottleneck ratio ∈ {1, 2, 4}**\n",
    "5. **Group size is constant**\n",
    "6. **SE blocks provide strong gains**\n",
    "7. **Channel widths must be divisible by small units (8 or g)**\n",
    "8. **The whole architecture should be simple and regular**\n",
    "\n",
    "These rules were discovered empirically, not guessed.\n",
    "\n",
    "RegNet formalizes these rules into a **design space** where every sampled model tends to be strong.\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* a diagram illustrating these rules\n",
    "* a step-by-step example designing a RegNet from scratch\n",
    "* the exact formulas used in RegNet to generate block widths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
