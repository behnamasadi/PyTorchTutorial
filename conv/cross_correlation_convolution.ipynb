{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Cross-Correlation**\n",
    "Each output unit is a linear function of a localized subset of input units.\n",
    "Cross-correlation is an operation used to measure the **similarity** between a filter/kernel and a region of an image (or feature map). It's defined as:\n",
    "\n",
    "$\n",
    "(f \\star g)[i, j] = \\sum_m \\sum_n f[i + m, j + n] \\cdot g[m, n]\n",
    "$\n",
    "\n",
    "- You slide the kernel `g` over the input `f`.\n",
    "- You **do not flip** the kernel.\n",
    "- Multiply corresponding elements and sum them.\n",
    "\n",
    "This is the **actual operation** used in deep learning frameworks like PyTorch and TensorFlow when we say \"convolution\".\n",
    "\n",
    "---\n",
    "\n",
    "# 2. **Convolution**\n",
    "\n",
    "Mathematically, convolution involves **flipping the kernel** both horizontally and vertically before applying it:\n",
    "\n",
    "$\n",
    "(f * g)[i, j] = \\sum_m \\sum_n f[i + m, j + n] \\cdot g[-m, -n]\n",
    "$\n",
    "\n",
    "- Used in signal processing.\n",
    "- Has properties like **commutativity** and **associativity** that are useful in theory.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Why DL Libraries Use Cross-Correlation?**\n",
    "\n",
    "Because in learning, the **filter is learned**, and flipping it doesn’t add value—so it’s simpler to define the operation as cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn-explainer.gif)\n",
    "[credit](https://poloclub.github.io/cnn-explainer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "Cross-correlation result:\n",
      " tensor([[[[ 0.,  0., -7.],\n",
      "          [ 4.,  0., -1.],\n",
      "          [ 0.,  6.,  1.]]]])\n",
      "\n",
      "Convolution result:\n",
      " tensor([[[[ 0.,  0., -7.],\n",
      "          [ 4.,  0., -1.],\n",
      "          [ 0.,  6.,  1.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor (1 image, 1 channel, 5x5)\n",
    "input = torch.tensor([[[[1., 2., 3., 0., 1.],\n",
    "                        [0., 1., 2., 3., 1.],\n",
    "                        [1., 0., 1., 2., 2.],\n",
    "                        [2., 1., 0., 1., 3.],\n",
    "                        [1., 2., 3., 0., 1.]]]])\n",
    "print(input.shape)\n",
    "# Kernel (1 filter, 1 channel, 3x3)\n",
    "kernel = torch.tensor([[[[0., 1., 0.],\n",
    "                         [1., -4., 1.],\n",
    "                         [0., 1., 0.]]]])\n",
    "\n",
    "# Apply cross-correlation (no flip), In CNNs, we don't usually flip the kernel because weights are learned.\n",
    "cross_corr = F.conv2d(input, kernel)\n",
    "print(\"Cross-correlation result:\\n\", cross_corr)\n",
    "\n",
    "# Flip kernel to do convolution\n",
    "flipped_kernel = torch.flip(kernel, [2, 3])  # Flip height and width\n",
    "conv_result = F.conv2d(input, flipped_kernel)\n",
    "print(\"\\nConvolution result:\\n\", conv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **3. Most Common Types of Convolution in Deep Learning**\n",
    "\n",
    "###  **3.1. Standard (2D) Convolution**\n",
    "\n",
    "####  What it does:\n",
    "- Applies a **learnable kernel** over the height and width of the input.\n",
    "- Each output channel is a sum of convolutions with all input channels.\n",
    "\n",
    "####  Mechanism:\n",
    "$\n",
    "\\text{Output}(x, y) = \\sum_c \\sum_{i,j} W_{c,i,j} \\cdot \\text{Input}_{c, x+i, y+j}\n",
    "$\n",
    "\n",
    "#### Use Case:\n",
    "- Image classification, detection, segmentation.\n",
    "- Basic layer in almost every CNN (e.g., VGG, ResNet).\n",
    "\n",
    "\n",
    "<img src='images/no_padding_no_strides.gif'>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **3.2. Depthwise Separable Convolution**\n",
    "\n",
    "####  What it does:\n",
    "Breaks standard convolution into two parts:\n",
    "1. **Depthwise convolution** – One filter per input channel.\n",
    "2. **Pointwise convolution** – 1×1 conv to mix channels.\n",
    "\n",
    "####  Why:\n",
    "- Greatly reduces parameters and computation.\n",
    "\n",
    "####  Use Case:\n",
    "- Mobile-friendly models: MobileNet, EfficientNet.\n",
    "\n",
    "####  Analogy:\n",
    "Instead of mixing spatial and channel info together, it **processes each channel separately**, then combines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **3.3. Dilated (Atrous) Convolution**\n",
    "\n",
    "####  What it does:\n",
    "Adds **spaces (holes)** between kernel elements, expanding the **receptive field** without increasing parameters.\n",
    "\n",
    "####  Example:\n",
    "A 3x3 kernel with dilation rate 2 covers a 5x5 region.\n",
    "\n",
    "####  Use Case:\n",
    "- Semantic segmentation (DeepLab)\n",
    "- Tasks needing large context without downsampling\n",
    "\n",
    "####  Analogy:\n",
    "Like using a zoomed-out lens to look at a bigger picture.\n",
    "\n",
    "\n",
    "<img src='images/dilation.gif'>\n",
    "\n",
    "Refs: [1](https://arxiv.org/abs/1511.07122)\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  **3.4. Transposed Convolution (Deconvolution)**\n",
    "\n",
    "####  What it does:\n",
    "Performs the **opposite** of convolution: turns a smaller feature map into a larger one.\n",
    "\n",
    "####  Common in:\n",
    "- Upsampling layers in autoencoders or GANs.\n",
    "\n",
    "####  Use Case:\n",
    "- Image generation\n",
    "- Semantic segmentation (upsampling to original image size)\n",
    "\n",
    "####  Note:\n",
    "May cause checkerboard artifacts if not carefully handled.\n",
    "\n",
    "<img src='images/no_padding_no_strides_transposed.gif'>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  **3.5. Grouped Convolution**\n",
    "\n",
    "####  What it does:\n",
    "Splits input and filters into **groups**, performs convolution **separately**, and concatenates results.\n",
    "\n",
    "####  When groups = number of channels → **depthwise convolution**.\n",
    "\n",
    "####  Use Case:\n",
    "- ResNeXt, ShuffleNet (boost performance, reduce cost)\n",
    "\n",
    "####  Analogy:\n",
    "Each group focuses on its own part of the task.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **6. Pointwise Convolution (1×1 Convolution, Network-in-Network)**\n",
    "\n",
    "####  What it does:\n",
    "Uses a $1\\times1$ kernel to **change the number of channels** or mix them.\n",
    "\n",
    "Let's say you have tensor $(N, C, H, W)$, ($N$ is the batch size, $C$ is the number of channel, $H,W$ are the spatial dimensions). Suppose this output is fed into a conv layer with $F_1$ :$1\\times1\\times C$ with zero padding and stride 1. Then the output of this $1\\times1$ conv layer will have shape $(N,1,H,W)$. We dot product every element of the filter with the tensor and apply a RelU function on the output. You can imagine this a single neuron which has $C$ input. That's why it is called **Network-in-Network**.\n",
    "\n",
    "\n",
    "You can use a $1\\times1$ convolutional layer to reduce $n_C$ but not $n_H, n_W$.\n",
    "\n",
    "You can use a pooling layer to reduce $n_H$, $n_W$, and $n_C$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####  Use Case:\n",
    "- Channel reduction/expansion\n",
    "- Bottlenecks in ResNet, Inception, MobileNet\n",
    "\n",
    "####  Note:\n",
    "No spatial context — acts purely on depth.\n",
    "\n",
    "Refs: [1](https://arxiv.org/abs/1312.4400), [2](https://www.youtube.com/watch?v=vcp0XvDAX68), [3](https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **3.7. Circular (Padding) Convolution**\n",
    "\n",
    "####  What it does:\n",
    "Instead of padding with zeros, it **wraps around** the input.\n",
    "\n",
    "####  Use Case:\n",
    "- Periodic signals (e.g., angles, time series)\n",
    "- Rarely used in vision, but can be handy for special cases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **3.8. Separable Convolution (Spatial)**\n",
    "\n",
    "####  What it does:\n",
    "Decomposes a 2D convolution into **two 1D convolutions**:\n",
    "- First along rows (horizontal),\n",
    "- Then along columns (vertical).\n",
    "\n",
    "####  Use Case:\n",
    "- Computational optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "| Type                        | What It Does                         | Key Use Case                              |\n",
    "|----------------------------|--------------------------------------|-------------------------------------------|\n",
    "| Standard Convolution       | Learns spatial + channel features    | All CNNs                                  |\n",
    "| Depthwise Separable        | Splits spatial and channel conv      | MobileNet, fast models                    |\n",
    "| Dilated Convolution        | Larger receptive field w/o pooling   | Segmentation (DeepLab)                    |\n",
    "| Transposed Convolution     | Upsamples feature maps               | GANs, Autoencoders                        |\n",
    "| Grouped Convolution        | Parallel smaller convolutions        | ResNeXt, ShuffleNet                       |\n",
    "| Pointwise (1x1) Convolution| Channel mixing, bottleneck layer     | ResNet, MobileNet                         |\n",
    "| Circular Convolution       | Wrap-around padding                  | Periodic data, special math tasks         |\n",
    "| Separable Spatial Conv     | Decomposes 2D conv to 1D ops         | Optimizations (less common)               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.Shape of the Convolution Output**\n",
    "$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2 \\cdot \\text{padding}_h - \\text{dilation}_h \\cdot (\\text{kernel}_h - 1) - 1}{\\text{stride}_h} + 1 \\right\\rfloor\n",
    "$\n",
    "\n",
    "$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2 \\cdot \\text{padding}_w - \\text{dilation}_w \\cdot (\\text{kernel}_w - 1) - 1}{\\text{stride}_w} + 1 \\right\\rfloor\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue maps are inputs, and cyan maps are outputs.\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"images/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/arbitrary_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/same_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"images/no_padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/padding_strides_odd.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.Convolution in RGB Images**\n",
    "The number of channels in our image must match the number of channels in our filter, so these two numbers have to be equal. The output of this will be a $4 \\times 4 \\times 1$. We often have $k$ filters of size $3\\times3\\times3$, so the output would be $k$ images of size $4 \\times 4 \\times 1$\n",
    "<img src='images/06_03.png'/>\n",
    "<img src='images/06_09.png'>\n",
    "<img src='images/3_channel_conv.gif'>\n",
    "\n",
    "\n",
    "Refs: [1](http://datahacker.rs/convolution-rgb-image/), [2](https://cs231n.github.io/convolutional-networks/#conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/convolution-animation-3x3-kernel.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch's `torch.nn.Conv2d`, the parameters are:\n",
    "\n",
    "```python\n",
    "Conv2d(in_channels, out_channels, kernel_size, ...)\n",
    "```\n",
    "\n",
    "* **`in_channels`**: Number of channels in the input image (e.g., 3 for RGB).\n",
    "* **`out_channels`**: Number of **output feature maps**, i.e., the number of **filters** you want to learn.\n",
    "* The **kernel weights are learnable parameters**, initialized internally by PyTorch using something like Kaiming initialization.\n",
    "* You **don’t set the kernel manually** — it’s learned during training.\n",
    "* Each output channel in this Conv2D becomes a **dimension in the embedding vector** for each patch.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**What happens under the hood?**\n",
    "\n",
    "Each **output channel** (i.e., filter) is not just a single 2D kernel. Instead, it is a **stack of 2D kernels** — one for **each input channel**. So:\n",
    "\n",
    "* For `in_channels = 3` and `out_channels = 4`, each output channel (filter) has 3 separate 2D kernels — one per input channel.\n",
    "* Therefore, the total number of kernels = `out_channels × in_channels = 4 × 3 = 12`.\n",
    "\n",
    "Each output feature map is computed as:\n",
    "\n",
    "1. Multiply each of the 3 input channels with its corresponding 2D kernel (element-wise).\n",
    "2. Sum the results (over the 3 channels).\n",
    "3. Add a bias (optional).\n",
    "4. That gives 1 feature map — one per output channel.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* If `in_channels = 3` and `out_channels = 4`, you have:\n",
    "\n",
    "  * 4 filters (one for each output channel),\n",
    "  * Each filter contains 3 kernels (one per input channel),\n",
    "  * So **total of 12 kernels**,\n",
    "  * Output: 4 feature maps (one per filter).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Transposed Convolution (Deconvolution)\n",
    "\n",
    "**ConvTranspose2d** is the **NOT** opposite of **maxpooling2d**\n",
    "\n",
    "#### What `MaxPool2d` does\n",
    "Downsamples feature maps (reduces spatial resolution), For each pooling window (e.g., 2×2), it outputs the **maximum** value,\n",
    "**No learnable parameters.**.\n",
    "\n",
    "---\n",
    "\n",
    "#### What `ConvTranspose2d` does\n",
    "\n",
    "Learns to upsample (increase spatial resolution), Performs a *learnable* transposed convolution (sometimes called “deconvolution”),  * **Has learnable weights** — just like `nn.Conv2d`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  Numerical Example of `ConvTranspose2d`\n",
    "\n",
    "Imagine we have:\n",
    "\n",
    "* Input feature map: **1×1×2×2** (N=1, C=1, H=2, W=2)\n",
    "* A single learnable kernel: **1×1×2×2**\n",
    "* Stride = 2 → we want to upsample from 2×2 → 4×4\n",
    "\n",
    "**When there is overlap** (e.g., stride < kernel\\_size, so outputs sum in overlapping regions), This is where ConvTranspose2d really behaves differently from naive \"nearest neighbor upsampling\".\n",
    "\n",
    "---\n",
    "\n",
    "**Input & Kernel**\n",
    "\n",
    "Input:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Kernel (weight):\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Transposed Convolution:**\n",
    "\n",
    "Each element of the input is multiplied by the kernel and **placed into the output grid with stride spacing**.\n",
    "* Overlapping regions are **summed**.\n",
    "\n",
    "Output (4×4) construction:\n",
    "\n",
    "| Position                 | Contribution | Partial Result                                       |\n",
    "| ------------------------ | ------------ | ---------------------------------------------------- |\n",
    "| Top-left (value = 1)     | $1 \\times W$ | Places `[[1,0],[0,1]]` at top-left corner            |\n",
    "| Top-right (value = 2)    | $2 \\times W$ | Places `[[2,0],[0,2]]` starting 2 steps to the right |\n",
    "| Bottom-left (value = 3)  | $3 \\times W$ | Places `[[3,0],[0,3]]` starting 2 steps down         |\n",
    "| Bottom-right (value = 4) | $4 \\times W$ | Places `[[4,0],[0,4]]` starting 2 down, 2 right      |\n",
    "\n",
    "Final **4×4 output** after summation:\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 1 & 0 & 2 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 3 & 0 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Learnable Parameters\n",
    "\n",
    "Each `nn.ConvTranspose2d` layer has **its own set of learnable parameters** (weights + optional bias).\n",
    "If you create two separate layers, they each have their own parameters and **don’t share weights by default**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "deconv1 = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2)\n",
    "deconv2 = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2)\n",
    "\n",
    "print(id(deconv1.weight), id(deconv2.weight))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 1x1 Convolution\n",
    "\n",
    "* Kernel size = **1×1** (just one pixel).\n",
    "* Still slides across the whole feature map, but since the filter covers only one pixel spatially, it **doesn’t look at neighbors**.\n",
    "* **It mixes information only across channels**, not across spatial locations.\n",
    "* So, it’s basically a **fully connected layer applied independently at each pixel location**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.1. Why Do We Need It?\n",
    "\n",
    "- **Dimensionality Reduction / Expansion**\n",
    "\n",
    "   * Example: Input has 256 channels, but you only need 64 → use a `1×1` conv to reduce computation and memory.\n",
    "   * This is exactly what happens in **ResNet bottleneck blocks**.\n",
    "\n",
    "   ```\n",
    "   [H, W, 256] --1x1 conv--> [H, W, 64]\n",
    "   ```\n",
    "\n",
    "- **Introduce Non-Linearity**\n",
    "\n",
    "   * If you follow with ReLU/BatchNorm, you add non-linearity without increasing spatial receptive field.\n",
    "\n",
    "- **Cross-Channel Interaction**\n",
    "\n",
    "   * Lets the network learn **linear combinations of feature maps**.\n",
    "   * Example: Combine “edge in x” + “edge in y” into “corner detector” at each pixel.\n",
    "\n",
    "- **Depthwise Separable Convolutions (MobileNet, Xception)**\n",
    "\n",
    "   * First apply cheap **depthwise conv** (per-channel spatial filtering).\n",
    "   * Then apply `1×1` conv to mix channel information (called **pointwise convolution**).\n",
    "\n",
    "- **Adjust Channel Size for Skip Connections**\n",
    "\n",
    "   * In UNet / ResNet, sometimes input and output channel counts don’t match → `1×1` conv is used to align dimensions so you can add/concat.\n",
    "\n",
    "![](images/1x1conv.png)\n",
    "[Image courtesy](https://www.youtube.com/watch?v=wf2HblQbP-U)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([1, 4, 2, 2])\n",
      "Output shape: torch.Size([1, 3, 2, 2])\n",
      "torch.Size([3, 4, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input tensor: (batch_size=1, channels=4, height=2, width=2)\n",
    "x = torch.randn(1, 4, 2, 2)\n",
    "\n",
    "# Define a 1x1 convolution layer\n",
    "conv1x1 = nn.Conv2d(in_channels=4, out_channels=3, kernel_size=1)\n",
    "\n",
    "# Apply convolution\n",
    "y = conv1x1(x)\n",
    "\n",
    "print(\"Input shape :\", x.shape)   # (1, 4, 2, 2)\n",
    "print(\"Output shape:\", y.shape)   # (1, 3, 2, 2)\n",
    "\n",
    "# Get the kernels (weights) from the Conv2d layer\n",
    "kernels = conv1x1.weight\n",
    "print(kernels.shape)\n",
    "# [out_channels, in_channels, kernel_h, kernel_w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The kernel size $1\\times1$ means each output pixel is a **linear combination** of the 3 input channels at that location.\n",
    "* Spatial dimensions $4×4$ remain **unchanged**.\n",
    "* Only the number of channels changes: $3 \\to 8$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.2. Verify mathematically\n",
    "\n",
    "Let’s manually compute one output pixel:\n",
    "\n",
    "$$\n",
    "y_{c_\\text{out}}(h,w) = \\sum_{c_\\text{in}=1}^{3} W_{c_\\text{out}, c_\\text{in}} \\cdot x_{c_\\text{in}}(h,w) + b_{c_\\text{out}}\n",
    "$$\n",
    "\n",
    "So each $1\\times1 $ kernel acts as a **fully connected layer applied channel-wise per pixel**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Numerical Example of `1×1` Convolution\n",
    "\n",
    "A  **1×1 convolution example** with an actual **2×2 feature map, 4 channels**, and **3 output channels**.\n",
    "\n",
    "\n",
    "\n",
    "**Setup**\n",
    "\n",
    "* Input shape: `[H=2, W=2, C_in=4]`\n",
    "* Output: `[H=2, W=2, C_out=3]`\n",
    "  (so every pixel’s **4 values** get linearly combined into **3 new values**).\n",
    "\n",
    "Let’s write the input:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "\\text{Pixel}(0,0) & \\text{Pixel}(0,1) \\\\\n",
    "\\text{Pixel}(1,0) & \\text{Pixel}(1,1)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each pixel has **4 channels**:\n",
    "\n",
    "* $X(0,0) = [1, 2, 3, 4]$\n",
    "* $X(0,1) = [5, 6, 7, 8]$\n",
    "* $X(1,0) = [9, 10, 11, 12]$\n",
    "* $X(1,1) = [13, 14, 15, 16]$\n",
    "\n",
    "---\n",
    "\n",
    "**The 1×1 Convolution Weights**\n",
    "\n",
    "Since we go from 4 → 3 channels, the weight matrix is:\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let’s pick simple numbers for clarity:\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\   % picks only ch1\n",
    "0 & 1 & 0 & 0 \\\\   % picks only ch2\n",
    "0 & 0 & 1 & 1      % adds ch3+ch4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "No bias, for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "**Apply to Each Pixel**\n",
    "\n",
    "Formula for each pixel:\n",
    "\n",
    "$$\n",
    "y = W \\cdot x\n",
    "$$\n",
    "\n",
    "\n",
    "Pixel (0,0): \\[1,2,3,4]\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 2 \\\\ 3 \\\\ 4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3+4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 2 \\\\ 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Pixel (0,1): \\[5,6,7,8]\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "5 \\\\ 6 \\\\ 7+8\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "5 \\\\ 6 \\\\ 15\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Pixel (1,0): \\[9,10,11,12]\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "9 \\\\ 10 \\\\ 11+12\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "9 \\\\ 10 \\\\ 23\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Pixel (1,1): \\[13,14,15,16]\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "13 \\\\ 14 \\\\ 15+16\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "13 \\\\ 14 \\\\ 31\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Final Output\n",
    "\n",
    "So the output `[H=2, W=2, C_out=3]` is:\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "[1,2,7] & [5,6,15] \\\\\n",
    "[9,10,23] & [13,14,31]\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Input: `[2,2,4]` (height 2, width 2, 4 channels).\n",
    "* 1×1 conv with 3 filters → Output: `[2,2,3]`.\n",
    "* Each pixel’s **4 input values** got projected into **3 new features** using the learned weights.\n",
    "\n",
    "This is exactly how **channel mixing** works: spatial resolution (`H×W`) is unchanged, but the **depth (channels)** changes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. Fully Connected (linear) Layer Using a Convolutional Layer\n",
    "You can mimic a fully connected (linear) layer using a convolutional layer in PyTorch. The trick is that a fully connected layer can be seen as a convolution with a kernel that matches the entire input size.\n",
    "\n",
    "So if you have a 1D input of length `N`, and you want a linear layer that outputs `M` values, you can use a 1D convolution with a kernel size equal to `N` and `M` output channels. In other words, the convolution is basically doing a dot product over the whole input, just like a linear layer would.\n",
    "\n",
    "The same logic applies for 2D or 3D inputs as well. You just set the kernel size to match the entire spatial dimension of the input. In that way, a convolutional layer can completely replace a linear layer if you configure it that way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 7.5. Problem Setup\n",
    "\n",
    "You have:\n",
    "\n",
    "* A **linear layer** connecting layer $\\mathbf{a}^{[l-1]}$ (size $N$) to layer $\\mathbf{a}^{[l]}$ (size $M$).\n",
    "* The operation is:\n",
    "  $\n",
    "  \\mathbf{a}^{[l]} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "  $\n",
    "  Where:\n",
    "\n",
    "  * $\\mathbf{x} \\in \\mathbb{R}^{N}$ is the input (i.e., $\\mathbf{a}^{[l-1]})$\n",
    "  * $\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$ is the weight matrix\n",
    "  * $\\mathbf{b} \\in \\mathbb{R}^{M}$ is the bias vector\n",
    "  * $\\mathbf{a}^{[l]} \\in \\mathbb{R}^{M}$ is the output\n",
    "\n",
    "This is a **fully connected layer** (dense/linear).\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.6. Linear Transformation Equation\n",
    "\n",
    "$\n",
    "\\boxed{\n",
    "\\mathbf{a}^{[l]} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "}\n",
    "$\n",
    "\n",
    "Each output neuron:\n",
    "$\n",
    "a^{[l]}_i = \\sum_{j=1}^{N} W_{ij} \\cdot x_j + b_i\n",
    "\\quad \\text{for } i = 1, \\dots, M\n",
    "$\n",
    "\n",
    "This is **matrix-vector multiplication + bias**, applied once for each layer.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 7.7. Same Using 1D Convolution\n",
    "\n",
    "Now let’s model the same thing using a **1D convolution**.\n",
    "\n",
    "\n",
    "\n",
    "Assume:\n",
    "\n",
    "* Input: $\\mathbf{x} \\in \\mathbb{R}^{C_{\\text{in}} \\times L_{\\text{in}}}$ (e.g., a \"flattened\" 1D input)\n",
    "* Use `Conv1d` with:\n",
    "\n",
    "  * `in_channels `= $C_{\\text{in}}$\n",
    "  * `out_channels` = $C_{\\text{out}}$\n",
    "  * `kernel_size `= $L_{\\text{in}}$\n",
    "  * `stride = 1`\n",
    "  * No padding\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "* $C_{\\text{in}} = 1$, $L_{\\text{in}} = N$ → input shape is $[1 \\times N]$\n",
    "* $C_{\\text{out}} = M$, so we want to output $M$ values\n",
    "* Kernel size = $N$, i.e., full width\n",
    "\n",
    "Then each filter $W_i$ spans the whole input and produces a **scalar** output via inner product:\n",
    "\n",
    "$\n",
    "\\boxed{\n",
    "a^{[l]}_i = \\sum_{j=1}^{N} W_{i,j} \\cdot x_j + b_i\n",
    "}\n",
    "\\quad \\text{for } i = 1, \\dots, M\n",
    "$\n",
    "\n",
    "**Identical equation** to the linear layer!\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.8. Flattened Input Comparison\n",
    "\n",
    "If the original input is a 2D image $(C_{\\text{in}}, H, W)$, and you:\n",
    "\n",
    "1. **Flatten it:**\n",
    "   $\n",
    "   x \\in \\mathbb{R}^{N}, \\quad \\text{where } N = C_{\\text{in}} \\cdot H \\cdot W\n",
    "   $\n",
    "   Then the linear layer is:\n",
    "   $\n",
    "   \\mathbf{a}^{[l]} = \\mathbf{W}_{M \\times N} \\cdot \\mathbf{x}_{N \\times 1} + \\mathbf{b}\n",
    "   $\n",
    "\n",
    "2. **Use convolution:**\n",
    "\n",
    "   * Input shape: $[C_{\\text{in}}, H, W]$\n",
    "   * Conv2d:\n",
    "\n",
    "     * `in_channels = C_in`\n",
    "     * `out_channels = M`\n",
    "     * `kernel_size = (H, W)`\n",
    "     * Output: $[M, 1, 1]$ → squeeze to $[M]$\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.9. Summary Table\n",
    "\n",
    "| Method       | Input Shape | Weights Shape  | Output Shape | Operation Type          |\n",
    "| ------------ | ----------- | -------------- | ------------ | ----------------------- |\n",
    "| Linear Layer | $[N]$       | $[M \\times N]$ | $[M]$        | Matrix-vector product   |\n",
    "| Conv1D       | $[1, N]$    | $[M, 1, N]$    | $[M, 1]$     | Sliding dot product     |\n",
    "| Conv2D       | $[C, H, W]$ | $[M, C, H, W]$ | $[M, 1, 1]$  | Dot over spatial extent |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Depthwise Separable Convolution\n",
    " \n",
    "\n",
    "A **Depthwise Separable Convolution** breaks a standard convolution into two operations:\n",
    "\n",
    "1. **Depthwise Convolution** – each input channel is convolved independently.\n",
    "\n",
    "   * Kernel: one per input channel.\n",
    "   * No mixing between channels.\n",
    "2. **Pointwise Convolution (1×1)** – mixes information across channels.\n",
    "\n",
    "   * Kernel size $1\\times1$.\n",
    "   * Changes the number of channels.\n",
    "\n",
    "This is used in **MobileNet**, **EfficientNet**, and **MBConv** to reduce computation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.1. Standard Convolution (for comparison)\n",
    "\n",
    "A standard convolution with:\n",
    "\n",
    "* input: $C_{in}$\n",
    "* output: $C_{out}$\n",
    "* kernel size: $k\\times k$\n",
    "\n",
    "requires\n",
    "$$\n",
    "\\text{Params} = C_{in} \\times C_{out} \\times k \\times k\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.2.  Depthwise Separable Convolution Parameters\n",
    "\n",
    "It does:\n",
    "\n",
    "1. **Depthwise:**\n",
    "   Each input channel has its own filter → $C_{in}$ filters.\n",
    "   $$\\text{Params}_{dw} = C_{in} \\times k \\times k$$\n",
    "\n",
    "2. **Pointwise (1×1):**\n",
    "   Mix across channels.\n",
    "   $$\\text{Params}_{pw} = C_{in} \\times C_{out} \\times 1 \\times 1$$\n",
    "\n",
    "So total parameters:\n",
    "\n",
    "$$\n",
    "\\text{Params}_{sep} = C_{in}\\times k\\times k + C_{in}\\times C_{out}\n",
    "$$\n",
    "\n",
    "which is much smaller than $C_{in}\\times C_{out}\\times k\\times k.$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. Standard Convolution  Group=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_channels, in_channels, kernel_h, kernel_w\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conv_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Normal convolution (groups=1)\u001b[39;00m\n\u001b[32m     10\u001b[39m conv_standard = torch.nn.Conv2d(in_channels=\u001b[32m8\u001b[39m, out_channels=\u001b[32m8\u001b[39m, kernel_size=\u001b[32m3\u001b[39m, stride=\u001b[32m1\u001b[39m, groups=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m conv_standard_weight = \u001b[43mconv_normal\u001b[49m.weight\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(conv_standard_weight.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'conv_normal' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input tensor (batch=1, channels=8, height=4, width=4)\n",
    "input = torch.randn(1, 8, 4, 4)\n",
    "\n",
    "print(\"out_channels, in_channels, kernel_h, kernel_w\")\n",
    "\n",
    "# Normal convolution (groups=1)\n",
    "conv_standard = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, groups=1)\n",
    "conv_standard_weight = conv_normal.weight\n",
    "print(conv_standard_weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/convolution-animation-3x3-kernel.gif\"  height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4. Grouped Convolution Groups=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise convolution (groups=2)\n",
    "conv_depthwise_2_groups = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, groups=2)\n",
    "conv_depthwise_2_groups_weight = conv_depthwise_2_groups.weight\n",
    "print(conv_depthwise_2_groups_weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/convolution-animation-3x3-kernel-2-groups.gif\"  height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Depthwise Groups=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise convolution (groups=in_channels)\n",
    "conv_depthwise_8_groups = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, groups=8)\n",
    "conv_depthwise_8_groups_weight = conv_depthwise_8_groups.weight\n",
    "print(conv_depthwise_8_groups_weight.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/depthwise-convolution-animation-3x3-kernel.gif\"  height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6. Depthwise-Separable  Groups=8 Followed by 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1x1 = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=1, stride=1)\n",
    "output = con1x1(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "kernels = con1x1.weight\n",
    "print(f\"Kernel shape: {kernels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/depthwise-separable-convolution-animation-3x3-kernel.gif\"  height=\"50%\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 8.7 Groups logic\n",
    "\n",
    "Let's have a look how exactly the `groups` parameter works in `torch.nn.Conv2d`, how it affects `in_channels`, `out_channels`, \n",
    "\n",
    "In PyTorch,\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels=C_in, out_channels=C_out, groups=G)\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "* When $G=1$: standard convolution (all channels mixed).\n",
    "* When $G=C_{in}$: **depthwise convolution** (each channel handled separately).\n",
    "* When $1 < G < C_{in}$: **grouped convolution**.\n",
    "\n",
    "This means the convolution **splits** the input and output channels into `G` separate groups, and each group only connects to the corresponding subset of input channels.\n",
    "\n",
    "So the rules are:\n",
    "\n",
    "$$\n",
    "\\text{Each group has } \\frac{\\text{in\\_channels}}{G} \\text{ input channels and } \\frac{\\text{out\\_channels}}{G} \\text{ output channels.}\n",
    "$$\n",
    "\n",
    "Therefore, both `in_channels` and `out_channels` **must be divisible by** `groups`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8. Efficiency Comparison Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_in, C_out, k = 32, 64, 3\n",
    "\n",
    "params_standard = C_in * C_out * k * k\n",
    "params_depthwise = C_in * k * k + C_in * C_out\n",
    "\n",
    "print(f\"Standard Conv params: {params_standard}\")\n",
    "print(f\"Depthwise Separable Conv params: {params_depthwise}\")\n",
    "print(f\"Reduction ratio: {params_standard / params_depthwise:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyTorch Conv2d class vs conv2d function\n",
    "\n",
    "In PyTorch, both `torch.nn.Conv2d` (the **class**) and `torch.nn.functional.conv2d` (the **function**) perform 2D convolution, but they're used in **different contexts**. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "####  `torch.nn.Conv2d` — **Layer Class**\n",
    "\n",
    "Used when defining **learnable layers** in `nn.Module`.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "output = conv(input_tensor)\n",
    "```\n",
    "\n",
    "- Automatically **creates learnable parameters** (weights and bias).\n",
    "- Integrates well into neural networks using `nn.Sequential` or `nn.Module`.\n",
    "- Automatically registered in `model.parameters()` (needed for optimization).\n",
    "- Handles initialization internally.\n",
    "\n",
    "Use this when you're building a model and want to **train** the convolutional kernel.\n",
    "\n",
    "---\n",
    "\n",
    "####  `torch.nn.functional.conv2d` — **Functional API**\n",
    "\n",
    "Used when you want **more control** or are implementing something manually.\n",
    "\n",
    "```python\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output = F.conv2d(input_tensor, weight=weight_tensor, bias=bias_tensor, stride=1, padding=1)\n",
    "```\n",
    "\n",
    "- You **must provide the weights and bias manually**.\n",
    "- No automatic parameter registration (not trainable unless you do extra work).\n",
    "- Useful for **custom layers**, weight sharing, or manually tweaking weights.\n",
    "\n",
    "Use this when you’re doing **custom forward passes**, writing **experimental architectures**, or creating layers that share weights.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Feature                         | `nn.Conv2d`                    | `F.conv2d`                     |\n",
    "|-------------------------------|-------------------------------|-------------------------------|\n",
    "| Learnable parameters           | ✅ Yes                        | ❌ No (you provide them)       |\n",
    "| Suitable for `nn.Module`       | ✅ Yes                        | ⚠️ Manual parameter handling   |\n",
    "| Autograd support               | ✅ Yes                        | ✅ Yes                         |\n",
    "| Custom weight handling         | ❌ Limited                    | ✅ Full control                |\n",
    "| Registered in `model.parameters()` | ✅ Yes                  | ❌ No                          |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `torch.nn.Unfold`, the output shape is:\n",
    "\n",
    "$\n",
    "\\text{patches.shape} = [N,\\; C_{\\text{in}} \\times k_H \\times k_W,\\; L]\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ N $ = batch size  \n",
    "- $ C_{\\text{in}} $ = number of input channels (e.g., 3 for RGB)  \n",
    "- $ k_H, k_W $ = kernel height and width  \n",
    "- $ L = H_{\\text{out}} \\times W_{\\text{out}} $ = number of sliding positions (output pixels)\n",
    "\n",
    "So for a kernel of $ 3 \\times 3 $, and $ C_{\\text{in}} = 3 $:\n",
    "\n",
    "$\n",
    "\\text{patches.shape} = [N,\\; 3 \\times 3 \\times 3 = 27,\\; H_{\\text{out}} \\times W_{\\text{out}}]\n",
    "$\n",
    "\n",
    "For a kernel of $ 5 \\times 5 $:\n",
    "\n",
    "$\n",
    "\\text{patches.shape} = [N,\\; 3 \\times 5 \\times 5 = 75,\\; H_{\\text{out}} \\times W_{\\text{out}}]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- `C_in = 3` (input has 3 channels)\n",
    "- `kernel size = 3x3 → kH = 3, kW = 3`\n",
    "\n",
    "So:\n",
    "\n",
    "$\n",
    "\\text{patches.shape} = [N,\\; 3 (C_{\\text{in}})  \\times 3 (k_H) \\times 3(k_W) = 27,\\; L]\n",
    "$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. **Feature Map**\n",
    "\n",
    "A **feature map** is the output of a convolutional layer after applying **filters** (also called kernels) to an input (like an image or another feature map).\n",
    "\n",
    "In simple terms, it's:\n",
    "> A grid (matrix) that represents **detected features** (like edges, textures, or patterns) in different parts of the input.\n",
    "\n",
    "---\n",
    "\n",
    "**Visual Intuition**\n",
    "\n",
    "Suppose you give a grayscale image to a CNN. It might look like this (in pixel values):\n",
    "\n",
    "```\n",
    "Input image:  [1, 2, 3, ...]\n",
    "```\n",
    "\n",
    "A convolutional filter (kernel) slides over this image and detects something — like a **vertical edge**.\n",
    "\n",
    "The result is a **new 2D array** — that's your **feature map** — showing **where that pattern exists** in the image.\n",
    "\n",
    ">  Input →  Convolution →  Feature Map\n",
    "\n",
    "---\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let’s say you have an input image of size **[3, 224, 224]** (3 color channels: RGB).\n",
    "\n",
    "You apply **64 filters** in the first conv layer → each filter produces a **feature map**.\n",
    "\n",
    "You now have:\n",
    "```\n",
    "Output shape = [64, 112, 112]\n",
    "```\n",
    "\n",
    "That means:  \n",
    "- You have **64 feature maps** (one per filter)  \n",
    "- Each feature map is **112×112** (spatially)\n",
    "\n",
    "---\n",
    "\n",
    "**What Do Feature Maps Represent?**\n",
    "\n",
    "- In **early layers**, they detect simple patterns:\n",
    "  - Edges, corners, color gradients\n",
    "- In **middle layers**, they capture more abstract features:\n",
    "  - Shapes, textures\n",
    "- In **deeper layers**, they focus on **semantic features**:\n",
    "  - Eyes, wheels, faces, dog ears, etc.\n",
    "\n",
    "Think of them like **snapshots of \"what the network sees\"** at different levels of abstraction.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Maps vs. Activation Maps**\n",
    "\n",
    "These are often used **interchangeably**, but technically:\n",
    "- **Feature Map**: Raw output of convolution\n",
    "- **Activation Map**: After applying non-linearity (like ReLU)\n",
    "\n",
    "But people usually just say “feature map” for both.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Are Feature Maps Important?**\n",
    "\n",
    "- They **drive the learning** of CNNs.\n",
    "- Let you **visualize what your model is learning** (e.g., via Grad-CAM or activation maximization).\n",
    "- They **reduce dimensionality** while retaining important features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load sample image from CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet expects 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "sample_img, label = dataset[0]  # Get the first image and its label\n",
    "input_tensor = sample_img.unsqueeze(0)  # Add batch dimension [1, 3, 224, 224]\n",
    "\n",
    "# 2. Load pretrained ResNet18\n",
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 3. Hook into the first convolution layer to extract feature maps\n",
    "feature_maps = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output.detach())\n",
    "\n",
    "hook = model.conv1.register_forward_hook(hook_fn)\n",
    "\n",
    "# 4. Forward pass\n",
    "_ = model(input_tensor)\n",
    "\n",
    "# 5. Visualize first 8 feature maps\n",
    "activation = feature_maps[0].squeeze(0)  # Shape: [64, H, W]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(activation[i].cpu(), cmap='viridis')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Feature Map {i}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Cleanup\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Convolution is translation-Equivariant, not Translation-Invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
