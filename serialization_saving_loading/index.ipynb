{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511abc29-ad3f-4302-bc5e-5cb9d03db5e7",
   "metadata": {},
   "source": [
    "##  Two ways to save models in PyTorch\n",
    "\n",
    "### 1. `torch.save(model.state_dict(), PATH)`\n",
    "\n",
    "This saves **only the parameters** (weights & biases) of the model.\n",
    "\n",
    "```python\n",
    "# Save\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "# Load\n",
    "model = TheModelClass(*args, **kwargs)  # must recreate same architecture\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* **Pros:**\n",
    "\n",
    "  * Recommended / common way.\n",
    "  * File is lighter (only contains tensors).\n",
    "  * Flexible: you can change some parts of the model code and still load weights.\n",
    "  * More portable (different machines, PyTorch versions).\n",
    "*  **Cons:**\n",
    "\n",
    "  * You must have the model class definition available when loading.\n",
    "  * Requires you to manually re-create the model object before loading weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `torch.save(model, PATH)`\n",
    "\n",
    "This saves the **entire model object**, including:\n",
    "\n",
    "* Architecture definition\n",
    "* Parameters\n",
    "\n",
    "```python\n",
    "# Save\n",
    "torch.save(model, \"entire_model.pth\")\n",
    "\n",
    "# Load\n",
    "model = torch.load(\"entire_model.pth\")\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "*  **Pros:**\n",
    "\n",
    "  * Super easy to load — you don’t need to redefine the model class.\n",
    "  * Useful for quick experiments or when sharing with colleagues who won’t re-create the model code.\n",
    "*  **Cons:**\n",
    "\n",
    "  * Not portable: tightly coupled to the code (class name, location in module, etc.).\n",
    "  * May break if:\n",
    "\n",
    "    * Code changes (e.g., you rename a class or move files).\n",
    "    * Different PyTorch versions are used.\n",
    "  * Larger file size.\n",
    "\n",
    "---\n",
    "\n",
    "##  Which one is more common?\n",
    "\n",
    " **Best practice (most common in research & production):**\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), PATH)\n",
    "```\n",
    "\n",
    "because it’s more **robust, portable, and reproducible**.\n",
    "\n",
    "**When to use `torch.save(model, PATH)`:**\n",
    "\n",
    "* Quick experiments, debugging, or toy projects.\n",
    "* When you’re not planning to refactor the code and just want to “freeze” everything.\n",
    "\n",
    "---\n",
    "\n",
    " **Summary:**\n",
    "\n",
    "* Use `state_dict()` in 99% of cases (production, research, sharing).\n",
    "* Use `torch.save(model)` only if you want the *entire object snapshot* and you control the environment.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82640f-322a-433d-bf45-b398f08ebddf",
   "metadata": {},
   "source": [
    "```python\n",
    "class SimpleMNISTModel(nn.Module):\n",
    "    def __init__(self, num_classes=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Feature extraction layers (conv layers) - can be frozen/unfrozen\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))  # 7x7 -> 4x4\n",
    "        )\n",
    "\n",
    "        # Classifier layers (MLP)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128*4*4, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.features(x)\n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**`features.0.weight torch.Size([32, 1, 3, 3])`**\n",
    "- This is the **first convolutional layer** (line 21-22 in your code)\n",
    "- **Shape breakdown**: `[out_channels, in_channels, kernel_height, kernel_width]`\n",
    "  - `32`: Output channels (number of feature maps this layer produces)\n",
    "  - `1`: Input channels (grayscale MNIST images have 1 channel)\n",
    "  - `3, 3`: Kernel size (3×3 convolution filter)\n",
    "\n",
    "**`features.0.bias torch.Size([32])`**\n",
    "- **Bias terms** for the first conv layer\n",
    "- One bias value per output channel (32 biases for 32 output channels)\n",
    "\n",
    "**`features.3.weight torch.Size([64, 32, 3, 3])`**\n",
    "- This is the **second convolutional layer** (line 26-27 in your code)\n",
    "- **Shape breakdown**:\n",
    "  - `64`: Output channels\n",
    "  - `32`: Input channels (matches the output of the previous layer)\n",
    "  - `3, 3`: Kernel size\n",
    "\n",
    "**`features.3.bias torch.Size([64])`**\n",
    "- **Bias terms** for the second conv layer (64 biases)\n",
    "\n",
    "**`features.6.weight torch.Size([128, 64, 3, 3])`**\n",
    "- This is the **third convolutional layer** (line 31-32 in your code)\n",
    "- **Shape breakdown**:\n",
    "  - `128`: Output channels\n",
    "  - `64`: Input channels (from previous layer)\n",
    "  - `3, 3`: Kernel size\n",
    "\n",
    "**`features.6.bias torch.Size([128])`**\n",
    "- **Bias terms** for the third conv layer (128 biases)\n",
    "\n",
    "---\n",
    "\n",
    "The indices (0, 3, 6) correspond to the position of each Conv2d layer in your `nn.Sequential`:\n",
    "- Index 0: First `nn.Conv2d`\n",
    "- Index 1: `nn.ReLU()` (no parameters)\n",
    "- Index 2: `nn.MaxPool2d()` (no parameters)\n",
    "- Index 3: Second `nn.Conv2d`\n",
    "- Index 4: `nn.ReLU()` (no parameters)\n",
    "- Index 5: `nn.MaxPool2d()` (no parameters)\n",
    "- Index 6: Third `nn.Conv2d`\n",
    "\n",
    "---\n",
    "\n",
    "Notice how the channels flow through your network:\n",
    "```\n",
    "Input: 1 channel (grayscale)\n",
    "   ↓\n",
    "Conv1: 1 → 32 channels\n",
    "   ↓\n",
    "Conv2: 32 → 64 channels\n",
    "   ↓\n",
    "Conv3: 64 → 128 channels\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e770d83-6707-4789-a050-efb5414999f1",
   "metadata": {},
   "source": [
    "### 1. **Automatic Parameter Registration**\n",
    "\n",
    "When you create layers like `nn.Conv2d` and `nn.Linear`, they internally create `nn.Parameter` objects for their weights and biases. These parameters are automatically registered with the parent module:\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Module Hierarchy and Parameter Discovery**\n",
    "\n",
    "When you assign modules to `self.features` and `self.classifier`, PyTorch's `nn.Module` class:\n",
    "\n",
    "1. **Detects all child modules** recursively\n",
    "2. **Collects all parameters** from each child module\n",
    "3. **Makes them accessible** via `model.parameters()` and `model.named_parameters()`\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "✅ **No manual tracking needed**: You don't need to register parameters yourself\n",
    "\n",
    "✅ **Automatic `requires_grad=True`**: All layer parameters are trainable by default\n",
    "\n",
    "✅ **Hierarchical access**: You can access parameters at different levels:\n",
    "- `model.parameters()` - all parameters\n",
    "- `model.features.parameters()` - only conv layer parameters  \n",
    "- `model.classifier.parameters()` - only linear layer parameters\n",
    "\n",
    "✅ **Works with any nn.Module structure**: Whether you use `nn.Sequential`, individual layers, or custom modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
