{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3604bb9-3e58-451f-a667-7fd10afda2fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **DINO — Distillation with NO Labels**\n",
    "\n",
    "**DINO (Distillation with NO Labels)** is a **self-supervised learning method** introduced by Facebook AI Research (FAIR) in 2021 in the paper:\n",
    "\n",
    "> **“Emerging Properties in Self-Supervised Vision Transformers”**\n",
    "> *Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin, 2021*\n",
    "\n",
    "DINO demonstrated that **Vision Transformers (ViTs)** can learn **powerful visual representations without labeled data** — and more importantly, **without collapsing** (i.e., without all representations becoming identical).\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Main Idea**\n",
    "\n",
    "DINO trains a model to produce **consistent representations for different augmented views** of the same image — without labels.\n",
    "\n",
    "It does this via **knowledge distillation** between two networks:\n",
    "\n",
    "* **Student network** — learns by gradient descent.\n",
    "* **Teacher network** — provides *soft targets* but is **not trained via gradients**.\n",
    "\n",
    "The teacher’s weights are an **Exponential Moving Average (EMA)** of the student’s:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{teacher}} \\leftarrow \\tau \\theta_{\\text{teacher}} + (1 - \\tau) \\theta_{\\text{student}}\n",
    "$$\n",
    "\n",
    "where  $\\tau \\in [0,1) $ is a momentum parameter (typically 0.996–0.9995).\n",
    "Hence the name: **Distillation with NO labels (DINO).**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Training Setup**\n",
    "\n",
    "Each input image is augmented into **multiple crops**:\n",
    "\n",
    "| Crop Type    | Input Size | Used By           | Number per Image |\n",
    "| ------------ | ---------- | ----------------- | ---------------- |\n",
    "| Global crops | 224×224    | Teacher + Student | 2                |\n",
    "| Local crops  | 96×96      | Student only      | 8                |\n",
    "\n",
    "So, for each image:\n",
    "\n",
    "1. The **teacher** sees only the 2 global crops.\n",
    "2. The **student** sees both 2 global + 8 local crops.\n",
    "3. The student’s predictions for *all* crops are trained to match the teacher’s outputs on the global crops.\n",
    "\n",
    "This **multi-crop strategy** enforces **scale- and viewpoint-invariance** — forcing the model to focus on *semantic content* rather than low-level pixels.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Outputs and Objective**\n",
    "\n",
    "Both teacher and student end with an **MLP projection head** that produces a **probability distribution** via softmax:\n",
    "\n",
    "$$\n",
    "p_t = \\text{softmax}\\left(\\frac{z_t - c}{T_t}\\right), \\qquad p_s = \\text{softmax}\\left(\\frac{z_s}{T_s}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ z_t, z_s $ = projection head outputs\n",
    "* $ T_t, T_s $ = temperature scalars\n",
    "* $ c $ = center vector (running mean of teacher outputs)\n",
    "\n",
    "The loss is the **cross-entropy** between teacher and student distributions:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DINO}} = -\\sum_i p_t^{(i)} \\log p_s^{(i)}\n",
    "$$\n",
    "\n",
    "Each **student crop** is matched with **each teacher global crop**, encouraging all crops of the same image to yield similar embeddings:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{N_t} \\sum_{\\substack{j=1 \\ j \\neq i}}^{N_s} H(p_t^{(i)}, p_s^{(j)})\n",
    "$$\n",
    "\n",
    "with ( N_t=2 ) (teacher global crops) and ( N_s=10 ) (student crops).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Avoiding Collapse**\n",
    "\n",
    "To prevent trivial solutions (identical embeddings for all inputs), DINO stabilizes training via:\n",
    "\n",
    "1. **Temperature scaling:** controls sharpness of teacher/student distributions.\n",
    "2. **Centering:** subtracts a running mean ( c ) from teacher outputs:\n",
    "   $$\n",
    "   p_t = \\text{softmax}\\left(\\frac{z_t - c}{T_t}\\right)\n",
    "   $$\n",
    "3. **EMA teacher:** ensures stable targets through slow updates.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. EMA Update Explained**\n",
    "\n",
    "The EMA equation:\n",
    "\n",
    "$$\n",
    "\\theta_t \\leftarrow \\tau \\theta_t + (1 - \\tau)\\theta_s\n",
    "$$\n",
    "\n",
    "means the teacher is a **momentum-averaged version** of past student weights.\n",
    "If $ \\tau = 0.996 $, the teacher changes slowly, producing stable targets.\n",
    "Thus, the teacher acts as a **temporal ensemble** — a “memory” of recent student states.\n",
    "\n",
    "**PyTorch-style implementation:**\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for t, s in zip(teacher.parameters(), student.parameters()):\n",
    "        t.data = tau * t.data + (1 - tau) * s.data\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "\n",
    "* The **student** learns fast via gradients.\n",
    "* The **teacher** evolves slowly and stably.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Iteration 1\n",
    "teacher = student\n",
    "\n",
    "Iteration 2\n",
    "teacher = 0.996 * teacher + 0.004 * new_student\n",
    "\n",
    "Iteration 3\n",
    "teacher = 0.996 * teacher + 0.004 * new_student\n",
    "...\n",
    "```\n",
    "\n",
    "So by iteration 1000, the teacher is effectively a **smoothed ensemble** of many previous student versions — acting like a “memory bank” of good representations.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30fc34-1ad4-4db3-aa19-a55187c36772",
   "metadata": {},
   "source": [
    "## **6. Architecture**\n",
    "\n",
    "Both teacher and student have **identical architectures**, often **Vision Transformers (ViT)** such as ViT-S/16 or ViT-B/16.\n",
    "\n",
    "| Role    | Architecture         | Gradient Updates | Purpose                      |\n",
    "| ------- | -------------------- | ---------------- | ---------------------------- |\n",
    "| Student | ViT-S/16 (or ResNet) | Yes (**trained with gradient descent** )    | Learns from teacher          |\n",
    "| Teacher | ViT-S/16 (same)      | No (**updated via EMA (no gradients)**)    | Provides stable soft targets |\n",
    "\n",
    "At initialization:\n",
    "$$\n",
    "\\theta_{\\text{teacher}} = \\theta_{\\text{student}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1. The original DINO paper (2021)\n",
    "\n",
    "In *“Emerging Properties in Self-Supervised Vision Transformers”* (Caron et al., 2021),\n",
    "the authors tested DINO with **two main families** of architectures:\n",
    "\n",
    "| Family                           | Examples              | Notes                                                           |\n",
    "| -------------------------------- | --------------------- | --------------------------------------------------------------- |\n",
    "| **Vision Transformers (ViT)**    | ViT-S/16, ViT-B/16    | DINO works *extremely well* — emergent segmentation, clustering |\n",
    "| **Convolutional Networks (CNN)** | ResNet-50, ResNet-101 | Works well, but ViTs show stronger semantic representations     |\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.2. Why Vision Transformers (ViT) are usually preferred\n",
    "\n",
    "DINO’s biggest discovery was that **self-supervised ViTs** (trained *without labels*)\n",
    "can automatically learn **object-level representations** and **attention maps**.\n",
    "\n",
    "Here’s why ViTs work so well in this setup:\n",
    "\n",
    "1. **Patch tokens** naturally represent local regions — ideal for multi-crop learning.\n",
    "2. **Self-attention** allows modeling global context easily.\n",
    "3. EMA teacher stabilization + multi-crop augmentations yield strong object localization signals.\n",
    "\n",
    "Result: even without labels, attention heads focus on *objects* in the image.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.3. Commonly used networks\n",
    "\n",
    "| Network        | Params | Patch size | Resolution | Typical use                                   |\n",
    "| -------------- | ------ | ---------- | ---------- | --------------------------------------------- |\n",
    "| **ViT-S/16**   | ~22M   | 16×16      | 224×224    | ✅ Default in DINO (good balance of size/perf) |\n",
    "| **ViT-B/16**   | ~86M   | 16×16      | 224×224    | High accuracy, heavier                        |\n",
    "| **ViT-B/8**    | ~86M   | 8×8        | 224×224    | Better fine-grained detail, more compute      |\n",
    "| **ResNet-50**  | ~25M   | –          | 224×224    | Works fine but weaker object-level features   |\n",
    "| **ResNet-101** | ~44M   | –          | 224×224    | Higher performance CNN baseline               |\n",
    "\n",
    "Later versions (like **DINOv2**) scale this up to huge models like **ViT-L/14**, **ViT-G/14**,\n",
    "trained on hundreds of millions of images.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.4. Architecture summary in code (simplified)\n",
    "\n",
    "```python\n",
    "# Teacher and student both ViT-S/16\n",
    "teacher = VisionTransformer(patch_size=16, embed_dim=384)\n",
    "student = VisionTransformer(patch_size=16, embed_dim=384)\n",
    "\n",
    "# Add projection heads\n",
    "teacher_head = DINOHead(in_dim=384, out_dim=65536)\n",
    "student_head = DINOHead(in_dim=384, out_dim=65536)\n",
    "```\n",
    "\n",
    "The **DINOHead** is an MLP with normalization and a softmax output used for the loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.5. Which to use (practical guide)\n",
    "\n",
    "| GPU Memory         | Recommended Backbone                        | Notes                       |\n",
    "| ------------------ | ------------------------------------------- | --------------------------- |\n",
    "| ≤ 4 GB             | ViT-Ti/16 or ViT-S/16 with small batch size | Possible on laptop GPU      |\n",
    "| 8–16 GB            | ViT-S/16 or ViT-B/16                        | Standard DINO               |\n",
    "| 24 GB+             | ViT-B/16 or ViT-L/16                        | High-performance            |\n",
    "| CPU or low compute | ResNet-50                                   | Simpler, slower convergence |\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.6. For DINOv2 (2023, Meta)\n",
    "\n",
    "The updated version **DINOv2** uses much larger models:\n",
    "\n",
    "| Model    | Layers | Hidden dim | Patch size | Params |\n",
    "| -------- | ------ | ---------- | ---------- | ------ |\n",
    "| ViT-S/14 | 12     | 384        | 14         | ~22M   |\n",
    "| ViT-B/14 | 12     | 768        | 14         | ~86M   |\n",
    "| ViT-L/14 | 24     | 1024       | 14         | ~304M  |\n",
    "| ViT-G/14 | 40     | 1536       | 14         | ~1B    |\n",
    "\n",
    "All trained on **142 million curated images** (without labels).\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.7. TL;DR Summary\n",
    "\n",
    "| Aspect          | DINO (2021)           | DINOv2 (2023)                         |\n",
    "| --------------- | --------------------- | ------------------------------------- |\n",
    "| Typical network | ViT-S/16 or ViT-B/16  | ViT-L/14 or ViT-G/14                  |\n",
    "| Input size      | 224×224               | 518×518                               |\n",
    "| Patch size      | 16                    | 14                                    |\n",
    "| Teacher/Student | same architecture     | same architecture                     |\n",
    "| Supervision     | none (self-distilled) | none (self-distilled, larger dataset) |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* You can use **any network**, but **Vision Transformers (ViT-S/16)** are the **standard** in DINO.\n",
    "* The **teacher and student** share the *same architecture*.\n",
    "* Only the **student** is trained by gradient descent; the **teacher** updates via EMA.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f886cf9-7654-468f-9f97-a28614f5898d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 6.1. Why This Works\n",
    "\n",
    "This EMA update makes the teacher a **temporal ensemble** — it slowly tracks the student’s improvements, producing **stable targets**.\n",
    "\n",
    "If both teacher and student were trained by gradients, the targets would change too fast → **training collapse**.\n",
    "\n",
    "By keeping the teacher **slow and stable**, DINO ensures:\n",
    "\n",
    "* The student always learns from a consistent signal.\n",
    "* The teacher gradually improves as the student improves.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.2 Initialization\n",
    "\n",
    "At the start:\n",
    "\n",
    "* The **teacher** is initialized as a **copy** of the student:\n",
    "  $$\n",
    "  \\theta_{\\text{teacher}} = \\theta_{\\text{student}}\n",
    "  $$\n",
    "\n",
    "Then training begins — student learns, teacher slowly follows via EMA.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.3 Visual Summary\n",
    "\n",
    "```\n",
    "             ┌───────────────────────┐\n",
    "             │   Same ViT backbone   │\n",
    "             └───────────────────────┘\n",
    "                    ▲           ▲\n",
    "                    │           │\n",
    "         EMA update │           │ Gradient descent\n",
    "                    │           │\n",
    "              ┌───────────┐     │\n",
    "              │  Teacher  │     │\n",
    "              └───────────┘     │\n",
    "                  │             │\n",
    "    Global crops  │             │ Global + Local crops\n",
    "                  ▼             ▼\n",
    "              ┌───────────┐\n",
    "              │  Student  │\n",
    "              └───────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618f8d7-1d49-4aeb-9680-07983525d2e8",
   "metadata": {},
   "source": [
    "## **7. Cropping Images**\n",
    "For each image:\n",
    "\n",
    "1. Create 2 global and 8 local crops via random cropping and resizing.\n",
    "2. Teacher processes **only** the 2 global crops.\n",
    "3. Student processes **all 10 crops**.\n",
    "\n",
    "```python\n",
    "teacher_outputs = [teacher(c) for c in global_crops]\n",
    "student_outputs = [student(c) for c in (global_crops + local_crops)]\n",
    "```\n",
    "\n",
    "Each output is a probability vector (e.g., 65,536 dimensions).\n",
    "Loss is computed between every teacher and student pair (excluding same-view pairs).\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf9f90-2c2f-4074-9a71-cc2cec650906",
   "metadata": {},
   "source": [
    "#### 7.1. What “crop” means here\n",
    "\n",
    "When we say *“crop”*, we mean we **take a sub-region** of the original image —\n",
    "so yes, we *remove part of the image* and then **resize that sub-region** to the network’s input size.\n",
    "\n",
    "This is **data augmentation** — not truncation of data, but creation of *different views* of the same image.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.2. Step-by-step example\n",
    "\n",
    "Suppose your **original image** is\n",
    "$$\n",
    "\\text{Original size: } 480 \\times 480\n",
    "$$\n",
    "\n",
    "DINO will create **multiple random crops** from it:\n",
    "\n",
    "**(a) Global crops**\n",
    "\n",
    "* Two random sub-regions (large portions of the image).\n",
    "* Each crop might cover, for example, 50–100% of the area.\n",
    "* These are then **resized** to\n",
    "  $$224 \\times 224$$\n",
    "  before being fed to the network.\n",
    "\n",
    "**(b) Local crops**\n",
    "\n",
    "* Smaller sub-regions (e.g., 15–50% of the area).\n",
    "* These are resized to\n",
    "  $$96 \\times 96$$\n",
    "  before being fed to the network.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.3. Who sees what\n",
    "\n",
    "| Network     | Crop type            | Input size        | Number per image              | Purpose               |\n",
    "| ----------- | -------------------- | ----------------- | ----------------------------- | --------------------- |\n",
    "| **Teacher** | Global crops only    | 224×224           | 2                             | Stable supervision    |\n",
    "| **Student** | Global + Local crops | 224×224 and 96×96 | 2 global + 8 local = 10 total | Learns from all views |\n",
    "\n",
    "So:\n",
    "\n",
    "* The **teacher** only sees the two *large* views.\n",
    "* The **student** sees all views — two global and eight local.\n",
    "\n",
    "The student’s outputs for *each local/global crop* are matched (via cross-entropy) to the corresponding teacher outputs of the *global crops*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.4. Concrete example\n",
    "\n",
    "Let’s visualize one sample numerically.\n",
    "\n",
    "**Original image: 480×480**\n",
    "\n",
    "```\n",
    "+------------------------------------------------+\n",
    "|                                                |\n",
    "|          [ Original image 480×480 ]            |\n",
    "|                                                |\n",
    "+------------------------------------------------+\n",
    "```\n",
    "\n",
    "**Cropping (random locations & scales)**\n",
    "\n",
    "1. Global crop 1: region (x=20,y=10,width=420,height=420)\n",
    "   → resize → (224×224)\n",
    "2. Global crop 2: region (x=80,y=50,width=350,height=350)\n",
    "   → resize → (224×224)\n",
    "3. Local crop 1: region (x=200,y=100,width=150,height=150)\n",
    "   → resize → (96×96)\n",
    "4. Local crop 2: region (x=50,y=300,width=120,height=120)\n",
    "   → resize → (96×96)\n",
    "5. … and so on up to 8 local crops.\n",
    "\n",
    "Now we have:\n",
    "\n",
    "* 2 crops of 224×224 (global)\n",
    "* 8 crops of 96×96 (local)\n",
    "\n",
    "**Feeding to networks**\n",
    "\n",
    "```python\n",
    "# Example (pseudo-code)\n",
    "global_crops = [crop1_224, crop2_224]\n",
    "local_crops  = [crop3_96, crop4_96, ..., crop10_96]\n",
    "\n",
    "teacher_outputs = [teacher(c) for c in global_crops]   # 2 × 224×224\n",
    "student_outputs = [student(c) for c in (global_crops + local_crops)]  # 10 total\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.5. Why DINO Does This\n",
    "\n",
    "The **multi-crop strategy** helps the model learn **scale-invariant** and **location-invariant** representations:\n",
    "\n",
    "* The same object seen in large or small view → should produce similar embeddings.\n",
    "* Forces the model to focus on **semantics**, not exact pixels.\n",
    "\n",
    "So for example:\n",
    "\n",
    "* A cat’s face in a global crop and a zoomed-in cat’s ear (local crop)\n",
    "  should still map to similar feature space representations.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 7.7. Intuitive Analogy\n",
    "\n",
    "Imagine you’re showing two people (teacher and student) pictures of the same scene:\n",
    "\n",
    "* The **teacher** sees two big photos of the whole scene.\n",
    "* The **student** sees those same big photos plus several **zoomed-in patches** of details.\n",
    "\n",
    "Then you ask the student to make their representations consistent with the teacher’s interpretation — even when looking at smaller parts.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb8002-fea4-4ea0-bae8-cb93c1c93359",
   "metadata": {},
   "source": [
    "## **8. Feeding Crops to the Network**\n",
    "\n",
    "1. **Architecture setup (teacher vs student)**\n",
    "2. **What the inputs are**\n",
    "3. **How the inputs are fed (serial vs batch)**\n",
    "4. **How the loss is computed when the number of crops differ**\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.1. Architecture setup\n",
    "\n",
    "**Both teacher and student have the same architecture** (e.g. ViT-S/16 or ViT-B/16).\n",
    "\n",
    "They both can process any image size, but in DINO:\n",
    "\n",
    "| Network     | Inputs                   | Input Size(s)     |\n",
    "| ----------- | ------------------------ | ----------------- |\n",
    "| **Teacher** | 2 global crops           | 224×224           |\n",
    "| **Student** | 2 global + 8 local crops | 224×224 and 96×96 |\n",
    "\n",
    "They share the same patch embedding structure (e.g., ViT divides image into 16×16 patches),\n",
    "so smaller crops just yield fewer tokens — but the same network can process them.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.2. What the inputs are\n",
    "\n",
    "For each original image (say 480×480):\n",
    "\n",
    "* **Global crops (x2)** → resized to **224×224**\n",
    "* **Local crops (x8)** → resized to **96×96**\n",
    "\n",
    "So total 10 crops (views).\n",
    "\n",
    "Let’s denote them:\n",
    "\n",
    "$$\n",
    "\\text{Global views: } {g_1, g_2}, \\quad \\text{Local views: } {l_1, l_2, \\dots, l_8}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.3. How the inputs are fed\n",
    "\n",
    "You can’t directly feed all of them into a single forward pass because:\n",
    "\n",
    "* The teacher gets **only global crops (2)**.\n",
    "* The student gets **both global (2) + local (8) crops = 10**.\n",
    "\n",
    "So we usually **loop or batch** them separately:\n",
    "\n",
    "**Example (pseudo-code)**\n",
    "\n",
    "```python\n",
    "# Teacher: only global crops\n",
    "teacher_outputs = [teacher(crop) for crop in global_crops]  # 2 outputs\n",
    "\n",
    "# Student: global + local crops\n",
    "student_outputs = [student(crop) for crop in (global_crops + local_crops)]  # 10 outputs\n",
    "```\n",
    "\n",
    "Each call produces a feature vector or a probability distribution, e.g. `[batch_size, dim]`.\n",
    "\n",
    "In practice, DINO implements this efficiently by **concatenating** the crops along the batch dimension,\n",
    "so they still go through one or two vectorized forward passes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.4. How the loss is computed\n",
    "\n",
    "The teacher provides **reference distributions** for the global crops:\n",
    "$$\n",
    "p_t^{(1)}, ; p_t^{(2)}\n",
    "$$\n",
    "\n",
    "The student provides **predictions** for all 10 crops:\n",
    "$$\n",
    "p_s^{(1)}, p_s^{(2)}, \\dots, p_s^{(10)}\n",
    "$$\n",
    "\n",
    "**Matching rule:**\n",
    "\n",
    "For **each student crop**, you compute a loss with **each teacher global crop**.\n",
    "That is, every student view tries to predict what the teacher predicts for the same image (under different augmentations).\n",
    "\n",
    "Formally, for an image (x):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{N_t} \\sum_{\\substack{j=1 \\ j \\neq i}}^{N_s} H(p_t^{(i)}, p_s^{(j)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $N_t = 2$: number of teacher global crops,\n",
    "* $N_s = 10$: number of student crops (2 global + 8 local),\n",
    "* $H(p_t, p_s)$ is the cross-entropy between teacher and student distributions.\n",
    "\n",
    "The “$j \\neq i$” part simply means we don’t match a student’s global view with the *same* global view used for that teacher prediction — only with *different* ones.\n",
    "\n",
    "So, the loss encourages **all crops (local and global)** of the same image to yield similar semantic representations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.5. Example dimensions\n",
    "\n",
    "Assume:\n",
    "\n",
    "* Student ViT produces a feature vector of size 384 (for ViT-S).\n",
    "* Projection head → 65,536-dim softmax output (e.g. via MLP).\n",
    "\n",
    "Then:\n",
    "\n",
    "| Crop      | Network | Input size | Output shape |\n",
    "| --------- | ------- | ---------- | ------------ |\n",
    "| 2× global | Teacher | 224×224    | [2, 65536]   |\n",
    "| 2× global | Student | 224×224    | [2, 65536]   |\n",
    "| 8× local  | Student | 96×96      | [8, 65536]   |\n",
    "\n",
    "All teacher and student outputs are **probability vectors** over the same dimension,\n",
    "so you can compute cross-entropy pairwise.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### 8.6. Intuitive picture\n",
    "\n",
    "```\n",
    "Original image\n",
    "   │\n",
    "   ├── 2 global crops (224×224) → teacher & student\n",
    "   └── 8 local crops (96×96)   → student only\n",
    "\n",
    "teacher_outputs = [p_t1, p_t2]\n",
    "student_outputs = [p_s1 ... p_s10]\n",
    "\n",
    "Loss = Σ H(p_ti, p_sj) for i∈{1,2}, j∈{1,...,10}, i≠j\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.7. Why multi-crop matters\n",
    "\n",
    "This design enforces **consistency across scales and viewpoints**:\n",
    "\n",
    "* A small local crop should have the same representation as the global view of the same image.\n",
    "* This drives the model to learn **semantic**, not pixel-level, similarity.\n",
    "\n",
    "---\n",
    "\n",
    "So, in short:\n",
    "\n",
    "✅ **Both teacher and student** have the same architecture.\n",
    "✅ **Teacher input:** 2× 224×224 (global crops).\n",
    "✅ **Student input:** 2× 224×224 (global) + 8× 96×96 (local).\n",
    "✅ **Feeds:** Usually batched or iterated, outputs concatenated.\n",
    "✅ **Loss:** Cross-entropy between each student crop and each teacher global crop.\n",
    "✅ **Gradients:** Student only; teacher updated via EMA.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbda96a-42f4-4394-b752-5fd3e75ff6ad",
   "metadata": {},
   "source": [
    "## **8. ViT and Variable Input Sizes**\n",
    "\n",
    "DINO can feed **224×224** (global) and **96×96** (local) crops into the same ViT.\n",
    "\n",
    "### Patch calculation\n",
    "\n",
    "For ViT-S/16:\n",
    "\n",
    "* 224×224 → ( 14×14 = 196 ) patches → 197 tokens (with [CLS])\n",
    "* 96×96 → ( 6×6 = 36 ) patches → 37 tokens (with [CLS])\n",
    "\n",
    "### Positional embedding interpolation\n",
    "\n",
    "Since ViT uses **learned positional embeddings** for 14×14 patches, they must be **interpolated** for 6×6 crops:\n",
    "\n",
    "```python\n",
    "def interpolate_pos_encoding(model, x):\n",
    "    n_patches = x.shape[1] - 1\n",
    "    N = model.pos_embed.shape[1] - 1\n",
    "    if n_patches == N:\n",
    "        return model.pos_embed\n",
    "    dim = model.pos_embed.shape[-1]\n",
    "    h = w = int(N ** 0.5)\n",
    "    pos = model.pos_embed[0,1:].reshape(1, h, w, dim).permute(0,3,1,2)\n",
    "    new_hw = int(n_patches ** 0.5)\n",
    "    pos = F.interpolate(pos, size=(new_hw, new_hw), mode='bicubic', align_corners=False)\n",
    "    pos = torch.cat([model.pos_embed[:,:1], pos.flatten(2).transpose(1,2)], dim=1)\n",
    "    return pos\n",
    "```\n",
    "\n",
    "This enables one ViT model to handle **mixed resolutions** (96×96 and 224×224) in the same batch.\n",
    "\n",
    "| Crop   | Input   | # Patches | Tokens | Positional Embedding | Used by           |\n",
    "| ------ | ------- | --------- | ------ | -------------------- | ----------------- |\n",
    "| Global | 224×224 | 14×14=196 | 197    | Learned              | Teacher + Student |\n",
    "| Local  | 96×96   | 6×6=36    | 37     | Interpolated         | Student only      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de4628-3672-4f7c-9e88-9f9da6a622c2",
   "metadata": {},
   "source": [
    "####  ** How Vision Transformers handle variable input sizes**\n",
    "Excellent question — and this shows you’ve understood the subtle part of **DINO with ViT backbones**.\n",
    "\n",
    "Let’s go deep into how **Vision Transformers handle variable input sizes**, and how DINO feeds both **224×224** and **96×96** crops into the *same ViT-S/16* model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.1. Recall what ViT does\n",
    "\n",
    "In **ViT-S/16**, the input image (say 224×224) is split into **non-overlapping 16×16 patches**.\n",
    "\n",
    "So for a 224×224 input:\n",
    "\n",
    "$$\n",
    "\\text{Number of patches} = \\frac{224}{16} \\times \\frac{224}{16} = 14 \\times 14 = 196\n",
    "$$\n",
    "\n",
    "Each patch → flattened → linearly projected → **token** (dimension = 384 for ViT-S).\n",
    "Then a **[CLS] token** is prepended, giving total **197 tokens**.\n",
    "\n",
    "So the input to the transformer is:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{197 \\times 384}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.2. What happens with a 96×96 crop?\n",
    "\n",
    "Now if we input 96×96 into the *same* ViT-S/16:\n",
    "\n",
    "$$\n",
    "\\frac{96}{16} = 6 \\quad \\text{patches per dimension}\n",
    "$$\n",
    "\n",
    "So total patches:\n",
    "$$\n",
    "6 \\times 6 = 36 \\text{ patches}\n",
    "$$\n",
    "\n",
    "Add one CLS token:\n",
    "$$\n",
    "\\Rightarrow 37 \\text{ tokens in total.}\n",
    "$$\n",
    "\n",
    "So now the transformer input is:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{37 \\times 384}\n",
    "$$\n",
    "\n",
    "✅ This is perfectly fine — the ViT can handle *different numbers of tokens*, as long as patch size and embedding dimension stay consistent.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.3. The positional embedding challenge\n",
    "\n",
    "Here’s the **only tricky part**:\n",
    "ViTs use **learned positional embeddings** that depend on the number of patches (e.g. 14×14 for 224×224).\n",
    "\n",
    "So, for 224×224 input:\n",
    "$$\n",
    "\\text{pos_embed} \\in \\mathbb{R}^{1 \\times (196 + 1) \\times 384}\n",
    "$$\n",
    "\n",
    "For 96×96 input:\n",
    "$$\n",
    "\\text{pos_embed} \\in \\mathbb{R}^{1 \\times (36 + 1) \\times 384}\n",
    "$$\n",
    "\n",
    "We don’t have positional embeddings pre-trained for the 6×6 grid — so we must **resize/interpolate** the positional embeddings.\n",
    "\n",
    "That’s exactly what DINO does internally.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.4. How DINO handles this (positional embedding interpolation)\n",
    "\n",
    "DINO resizes the **2D grid** of positional embeddings via **bicubic interpolation** to match the number of patches for each input size.\n",
    "\n",
    "```python\n",
    "def interpolate_pos_encoding(model, x):\n",
    "    # Get number of patches in current input\n",
    "    n_patches = x.shape[1] - 1  # minus CLS token\n",
    "    N = model.pos_embed.shape[1] - 1\n",
    "    if n_patches == N:\n",
    "        return model.pos_embed\n",
    "    \n",
    "    # Extract 2D positional embedding grid\n",
    "    dim = model.pos_embed.shape[-1]\n",
    "    h = w = int(N ** 0.5)\n",
    "    pos_embed_2d = model.pos_embed[0, 1:].reshape(1, h, w, dim).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Interpolate to new resolution\n",
    "    new_h = new_w = int(n_patches ** 0.5)\n",
    "    pos_embed_2d = F.interpolate(pos_embed_2d, size=(new_h, new_w), mode='bicubic', align_corners=False)\n",
    "    \n",
    "    # Flatten back and concat CLS token\n",
    "    pos_embed = torch.cat([model.pos_embed[:, :1], pos_embed_2d.flatten(2).transpose(1, 2)], dim=1)\n",
    "    return pos_embed\n",
    "```\n",
    "\n",
    "This allows a **single ViT** to process both 224×224 and 96×96 crops by **adapting** positional embeddings on the fly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.5. Feeding different-sized inputs\n",
    "\n",
    "DINO processes all crops (global + local) in a single forward pass by **concatenating them** along the batch dimension:\n",
    "\n",
    "```python\n",
    "# global_crops = [B, 3, 224, 224]\n",
    "# local_crops  = [B, 3, 96, 96]\n",
    "\n",
    "all_crops = global_crops + local_crops\n",
    "student_output = student(all_crops)  # handles mixed resolutions internally\n",
    "```\n",
    "\n",
    "During the forward pass, the model interpolates the positional embeddings for each crop depending on its spatial size.\n",
    "\n",
    "So even though **teacher** always sees 224×224 inputs,\n",
    "the **student** can flexibly handle both 224×224 and 96×96 crops — same model weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.6. Why this works\n",
    "\n",
    "* Patch size = 16 is fixed → feature dimension fixed (384).\n",
    "* Only the **number of tokens** changes (196 vs 36).\n",
    "* Positional embedding is resized → keeps spatial structure aligned.\n",
    "* Transformer layers (self-attention, MLPs) can process any sequence length.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 8.7 Intuition\n",
    "\n",
    "So even though the ViT expects *“224×224”* in its original pretraining,\n",
    "the **patch embedding + interpolation trick** lets it generalize to any crop size that’s a multiple of the patch size.\n",
    "\n",
    "That’s how DINO’s **student network** can ingest both large (224×224) and small (96×96) views\n",
    "— and match them semantically to the teacher’s global representations.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* Both teacher and student = same ViT-S/16 architecture.\n",
    "* Teacher always gets 224×224 crops.\n",
    "* Student gets both 224×224 and 96×96 crops.\n",
    "* Different crop sizes work fine because:\n",
    "\n",
    "  * Patch embedding is fixed.\n",
    "  * Positional embeddings are interpolated for smaller grids.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c7628-e8a6-4676-9766-74cd5505ec14",
   "metadata": {},
   "source": [
    "#### **When and Why Interpolation is needed in ViT** \n",
    "\n",
    "\n",
    "In the ViT forward pass, you typically see something like:\n",
    "\n",
    "```python\n",
    "x = x + self.pos_embed[:, :x.size(1), :]\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `x` → token sequence (shape `[B, N_tokens, D]`)\n",
    "* `self.pos_embed` → learned positional embeddings of shape `[1, N_max, D]`, where\n",
    "  $$ N_\\text{max} = \\text{num_patches} + 1 = 197 $$ for 224×224 input (14×14 grid + CLS token)\n",
    "\n",
    "So this code assumes:\n",
    "$$\n",
    "N_\\text{tokens} = x.size(1) \\leq N_\\text{max}\n",
    "$$\n",
    "\n",
    "That is: the number of tokens (patches + CLS) in your **current input**\n",
    "must be ≤ the number of positional embeddings stored in the model.\n",
    "\n",
    "If smaller, it just slices a subset of embeddings (which is fine).\n",
    "If larger — e.g., 384×384 → 24×24 patches → 577 tokens — it would fail,\n",
    "and **interpolation is needed** to *expand* the position grid.\n",
    "\n",
    "---\n",
    "\n",
    "Why this simple slicing **does not work for smaller images**\n",
    "\n",
    "It’s true that slicing (`[:, :x.size(1)]`) *works syntactically* for smaller images —\n",
    "but **it gives the wrong spatial correspondence**.\n",
    "\n",
    "Let’s see why.\n",
    "\n",
    "\n",
    "\n",
    "| Input   | Grid          | Tokens  | Shape |\n",
    "| ------- | ------------- | ------- | ----- |\n",
    "| 224×224 | 14×14 patches | 196 + 1 | 197   |\n",
    "| 96×96   | 6×6 patches   | 36 + 1  | 37    |\n",
    "\n",
    "If you simply slice `pos_embed[:, :37]`, you’re **taking the first 37 embeddings**\n",
    "from a **1D flattening** of the 14×14 positional grid.\n",
    "\n",
    "That means you’re taking something like:\n",
    "\n",
    "```\n",
    "first row patches (14 tokens)\n",
    "second row patches (14 tokens)\n",
    "third row patches (9 tokens)\n",
    "```\n",
    "\n",
    "→ It doesn’t form a 6×6 *spatial grid* anymore!\n",
    "\n",
    "So the positional meaning (e.g., top-left → bottom-right) is **completely misaligned**.\n",
    "The network would think the patches are still 14×14 arranged, not 6×6.\n",
    "\n",
    "Hence, **simple slicing breaks spatial consistency** when the grid size changes.\n",
    "\n",
    "---\n",
    "\n",
    "**Correct fix: interpolate the 2D positional grid**\n",
    "\n",
    "To preserve spatial meaning, you must:\n",
    "\n",
    "1. Reshape the original 1D positional embeddings (except CLS token)\n",
    "   into a 2D grid → `[H_orig, W_orig, D]`\n",
    "   (for 224×224, that’s 14×14×384).\n",
    "\n",
    "2. **Interpolate** this 2D grid to the new grid size (e.g. 6×6 for 96×96 input).\n",
    "\n",
    "3. Flatten it back and concatenate with the CLS token.\n",
    "\n",
    "This keeps the “positional meaning” consistent for arbitrary input sizes.\n",
    "\n",
    "---\n",
    "\n",
    "**Visual intuition**\n",
    "\n",
    "Imagine the learned positional embeddings as a **14×14 colored grid** —\n",
    "each color corresponds to a patch location.\n",
    "\n",
    "When the crop is **smaller (6×6)**:\n",
    "\n",
    "* If you just slice the first 36 colors → you take the *wrong patches*.\n",
    "* If you interpolate → you *resample* that 14×14 map into a 6×6 version.\n",
    "\n",
    "That’s what interpolation ensures.\n",
    "\n",
    "---\n",
    "\n",
    "**When interpolation is needed**\n",
    "\n",
    "| Case                       | # Patches            | Example   | Why interpolate?         |\n",
    "| -------------------------- | -------------------- | --------- | ------------------------ |\n",
    "| **Smaller input (96×96)**  | fewer patches (6×6)  | ✅ **Yes** | preserve spatial mapping |\n",
    "| **Same input (224×224)**   | same patches (14×14) | ❌ No      | direct match             |\n",
    "| **Larger input (384×384)** | more patches (24×24) | ✅ **Yes** | extend grid spatially    |\n",
    "\n",
    "So **both smaller and larger** inputs require interpolation —\n",
    "because the **2D spatial structure** changes, not just the count.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example with numbers\n",
    "\n",
    "* Original: 224×224 → 14×14 grid → 14×14 positional map\n",
    "* Smaller: 96×96 → 6×6 grid\n",
    "\n",
    "Interpolation rescales:\n",
    "$$\n",
    "14 \\times 14 ; \\text{→} ; 6 \\times 6\n",
    "$$\n",
    "\n",
    "so each of the 36 positions in the smaller image corresponds to\n",
    "a proper spatially-aligned position in the original grid.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Code snippet recap\n",
    "\n",
    "That’s why DINO includes this in its ViT forward:\n",
    "\n",
    "```python\n",
    "# During forward:\n",
    "pos_embed = interpolate_pos_encoding(self, x)\n",
    "\n",
    "x = x + pos_embed\n",
    "```\n",
    "\n",
    "and `interpolate_pos_encoding()` handles the grid resizing automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "| Situation                             | x.size(1)    | Problem                        | Solution             |\n",
    "| ------------------------------------- | ------------ | ------------------------------ | -------------------- |\n",
    "| Input smaller than pretrained (96×96) | fewer tokens | slicing breaks spatial mapping | **interpolate down** |\n",
    "| Input equal (224×224)                 | same tokens  | works fine                     | use as is            |\n",
    "| Input larger (384×384)                | more tokens  | not enough embeddings          | **interpolate up**   |\n",
    "\n",
    "So yes — slicing only “works” numerically for smaller inputs,\n",
    "but it **destroys the geometry**, so **interpolation is required** to maintain correct positional relationships.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* Slicing keeps correct *count* but wrong *spatial layout*.\n",
    "* Interpolation preserves *2D positional meaning* for any resolution (smaller or larger).\n",
    "* That’s why DINO uses interpolation, even for smaller 96×96 crops.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd9e5d-a1c4-4b8f-8ffb-c7fce6b71071",
   "metadata": {},
   "source": [
    "## **numerical and visual example** \n",
    "Perfect. Let’s build a **numerical and visual example** to make the positional-embedding interpolation crystal clear.\n",
    "\n",
    "We’ll take a **ViT-S/16** model (patch size = 16) and compare two inputs:\n",
    "\n",
    "* A **224 × 224** image (standard size)\n",
    "* A **96 × 96** image (local crop, used by the student in DINO)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Number of patches and tokens\n",
    "\n",
    "### For input 224×224\n",
    "\n",
    "$$\n",
    "N_{h} = N_{w} = \\frac{224}{16} = 14\n",
    "$$\n",
    "$$\n",
    "N_\\text{patches} = 14 \\times 14 = 196\n",
    "$$\n",
    "Adding the `[CLS]` token:\n",
    "$$\n",
    "N_\\text{tokens} = 196 + 1 = 197\n",
    "$$\n",
    "\n",
    "### For input 96×96\n",
    "\n",
    "$$\n",
    "N_{h} = N_{w} = \\frac{96}{16} = 6\n",
    "$$\n",
    "$$\n",
    "N_\\text{patches} = 6 \\times 6 = 36\n",
    "$$\n",
    "Adding `[CLS]`:\n",
    "$$\n",
    "N_\\text{tokens} = 37\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "| Input size | Grid  | # patches | # tokens (with CLS) |\n",
    "| ---------- | ----- | --------- | ------------------- |\n",
    "| 224 × 224  | 14×14 | 196       | 197                 |\n",
    "| 96 × 96    | 6×6   | 36        | 37                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What the positional embeddings look like\n",
    "\n",
    "The ViT has a **learned 2D grid** of positional embeddings\n",
    "(ignoring the `[CLS]` token):\n",
    "\n",
    "$$\n",
    "\\text{pos_embed_2D} \\in \\mathbb{R}^{14\\times14\\times384}\n",
    "$$\n",
    "\n",
    "Flattened into 196×384 when used.\n",
    "\n",
    "For a smaller image (6×6 = 36 patches), we must **resize** this 14×14 grid → 6×6.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Visualizing the idea (grids)\n",
    "\n",
    "### Original positional grid (14×14)\n",
    "\n",
    "```\n",
    "14×14 positions\n",
    "┌───────────────────────────────┐\n",
    "│▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒│\n",
    "│▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒│\n",
    "│▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒│\n",
    "│............. 14×14 ...........│\n",
    "└───────────────────────────────┘\n",
    "```\n",
    "\n",
    "### After interpolation to 6×6\n",
    "\n",
    "```\n",
    "6×6 positions\n",
    "┌──────────────┐\n",
    "│▒▒▒▒▒▒▒▒▒▒▒▒▒▒│\n",
    "│▒▒▒▒▒▒▒▒▒▒▒▒▒▒│\n",
    "│......6×6......│\n",
    "└──────────────┘\n",
    "```\n",
    "\n",
    "This interpolation keeps the spatial meaning (top-left → bottom-right mapping) rather than just slicing off the first 36 embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Numerical example in code\n",
    "\n",
    "Below is a minimal **PyTorch snippet** to simulate this interpolation.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate learned 2D positional embeddings for 14×14 grid\n",
    "H_orig, W_orig = 14, 14\n",
    "dim = 1  # for visualization (normally 384)\n",
    "pos_embed = torch.arange(H_orig * W_orig).float().reshape(1, H_orig, W_orig, dim)\n",
    "pos_embed = pos_embed.permute(0, 3, 1, 2)  # [1, dim, H, W]\n",
    "\n",
    "# Interpolate down to 6×6 grid\n",
    "H_new, W_new = 6, 6\n",
    "pos_embed_resized = F.interpolate(pos_embed, size=(H_new, W_new),\n",
    "                                  mode='bicubic', align_corners=False)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pos_embed[0, 0].numpy(), cmap='viridis')\n",
    "plt.title(\"Original 14×14 positional grid\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pos_embed_resized[0, 0].numpy(), cmap='viridis')\n",
    "plt.title(\"Interpolated 6×6 positional grid\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original grid shape: {pos_embed.shape} -> Interpolated: {pos_embed_resized.shape}\")\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "Original grid shape: torch.Size([1, 1, 14, 14])\n",
    "Interpolated: torch.Size([1, 1, 6, 6])\n",
    "```\n",
    "\n",
    "…and you’ll see the coarse 6×6 grid maintaining the same smooth spatial pattern.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How the ViT uses this at runtime\n",
    "\n",
    "During a forward pass, DINO’s code calls something like:\n",
    "\n",
    "```python\n",
    "pos_embed = interpolate_pos_encoding(self, x)\n",
    "x = x + pos_embed\n",
    "```\n",
    "\n",
    "So each input crop (96×96 or 224×224) gets a **properly resized positional embedding**,\n",
    "keeping the network’s spatial reasoning consistent.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* 224×224 → 14×14 patches → 197 tokens\n",
    "* 96×96 → 6×6 patches → 37 tokens\n",
    "* The positional embedding grid (14×14) is **interpolated** to 6×6\n",
    "  so that patch positions correspond spatially.\n",
    "* This is why DINO can use one ViT for both large and small crops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03a68d-d8e0-4e8e-9075-2c09de7ed4b5",
   "metadata": {},
   "source": [
    "## **9. Emergent Properties**\n",
    "\n",
    "After self-supervised training:\n",
    "\n",
    "* ViT attention heads automatically focus on **objects** without labels.\n",
    "* Embedding space forms **semantic clusters** (e.g., all birds together).\n",
    "* Works effectively as a **feature extractor** for downstream tasks (classification, detection, segmentation).\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Comparison with Other SSL Methods**\n",
    "\n",
    "| Method  | Teacher Update | Contrastive? | Negatives? | Backbone | Key Feature                  |\n",
    "| ------- | -------------- | ------------ | ---------- | -------- | ---------------------------- |\n",
    "| SimCLR  | N/A            | Yes          | Yes        | CNN      | Simple contrastive loss      |\n",
    "| BYOL    | EMA            | No           | No         | CNN      | Momentum teacher + predictor |\n",
    "| DINO    | EMA            | No           | No         | ViT/CNN  | Multi-crop self-distillation |\n",
    "| MoCo v3 | EMA            | Yes          | Yes        | ViT      | Queue of negatives           |\n",
    "| MAE     | N/A            | No           | No         | ViT      | Masked autoencoding          |\n",
    "\n",
    "---\n",
    "\n",
    "## **11. DINOv2 (2023)**\n",
    "\n",
    "Meta’s **DINOv2** scales up DINO to **foundation-model level**:\n",
    "\n",
    "* Trained on **142M curated images**.\n",
    "* Models up to **ViT-G/14 (~1B parameters)**.\n",
    "* Achieves **zero-shot and transfer learning** performance comparable to supervised models.\n",
    "* Serves as a **vision backbone** for systems like CLIP and SAM.\n",
    "\n",
    "| Model    | Layers | Hidden dim | Patch | Params |\n",
    "| -------- | ------ | ---------- | ----- | ------ |\n",
    "| ViT-S/14 | 12     | 384        | 14    | ~22M   |\n",
    "| ViT-B/14 | 12     | 768        | 14    | ~86M   |\n",
    "| ViT-L/14 | 24     | 1024       | 14    | ~304M  |\n",
    "| ViT-G/14 | 40     | 1536       | 14    | ~1B    |\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Key Equations Summary**\n",
    "\n",
    "1. **Teacher update:**\n",
    "   $$\n",
    "   \\theta_t \\leftarrow \\tau \\theta_t + (1 - \\tau) \\theta_s\n",
    "   $$\n",
    "2. **Centered teacher output:**\n",
    "   $$\n",
    "   p_t = \\text{softmax}\\left(\\frac{z_t - c}{T_t}\\right)\n",
    "   $$\n",
    "3. **Student output:**\n",
    "   $$\n",
    "   p_s = \\text{softmax}\\left(\\frac{z_s}{T_s}\\right)\n",
    "   $$\n",
    "4. **Distillation loss:**\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{DINO}} = -\\sum_i p_t^{(i)} \\log p_s^{(i)}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## **13. Simplified PyTorch Pseudocode**\n",
    "\n",
    "```python\n",
    "# Multi-crop augmentations\n",
    "global_crops, local_crops = augment_multi_crop(batch)\n",
    "\n",
    "# Forward\n",
    "with torch.no_grad():\n",
    "    teacher_out = teacher(global_crops)\n",
    "student_out = student(global_crops + local_crops)\n",
    "\n",
    "# Softmax with temperature\n",
    "p_t = softmax((teacher_out - center) / T_teacher)\n",
    "p_s = softmax(student_out / T_student)\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = cross_entropy(p_s, p_t)\n",
    "\n",
    "# Train student\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Update teacher via EMA\n",
    "with torch.no_grad():\n",
    "    for t, s in zip(teacher.parameters(), student.parameters()):\n",
    "        t.data = tau * t.data + (1 - tau) * s.data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In summary:**\n",
    "\n",
    "* Both teacher and student share the same ViT architecture.\n",
    "* The teacher is a slow EMA of the student (no gradients).\n",
    "* Multi-crop augmentation and temperature scaling prevent collapse.\n",
    "* ViT-S/16 achieves strong object-level representation without labels.\n",
    "* DINOv2 scales this into a **foundation-level visual model** rivaling supervised ones.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
