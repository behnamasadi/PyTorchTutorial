{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d34838b-7b62-4674-8827-db321d1fd8bb",
   "metadata": {},
   "source": [
    "# Relative Positional Embedding vs Absolute Positional Embeddings in self-attention \n",
    "\n",
    "That’s **exactly** the key architectural difference between **absolute** and **relative** positional embeddings in Transformers such as **ViT vs. Swin**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. In vanilla ViT (absolute positional embeddings)**\n",
    "\n",
    "The ViT pipeline looks like this:\n",
    "\n",
    "```\n",
    "image → patchify → linear projection → +pos_embed → Transformer encoder\n",
    "```\n",
    "\n",
    "In code (simplified):\n",
    "\n",
    "```python\n",
    "x = self.patch_embed(img)           # [B, N, D]  (N patches, D=embed_dim)\n",
    "x = x + self.pos_embed              # add absolute positional embedding\n",
    "x = self.cls_token + x              # prepend class token\n",
    "x = self.transformer(x)             # pass through attention layers\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* The **positional embedding is added to the input tokens before any attention**.\n",
    "* Each token’s embedding = content + absolute position.\n",
    "* Inside the Transformer:\n",
    "  $$ A_{ij} = \\frac{(x_i + p_i)^\\top (x_j + p_j)}{\\sqrt{d}} $$\n",
    "  — the position is already mixed into Q and K implicitly.\n",
    "\n",
    "✅ The **positional signal is part of the input feature**.\n",
    "It travels through all layers as part of the representation.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. In Swin Transformer (relative positional bias)**\n",
    "\n",
    "In Swin, there is **no `+pos_embed` added to tokens**.\n",
    "\n",
    "Instead, relative positional information is injected **inside each attention block**, *after* Q and K are computed:\n",
    "\n",
    "### Inside one attention layer\n",
    "\n",
    "```python\n",
    "Q = x @ Wq\n",
    "K = x @ Wk\n",
    "V = x @ Wv\n",
    "\n",
    "# attention scores\n",
    "A = (Q @ K.transpose(-2, -1)) / sqrt(d)\n",
    "\n",
    "# add relative bias (based on relative spatial offsets)\n",
    "A = A + self.relative_position_bias[index_table]\n",
    "\n",
    "attention = softmax(A)\n",
    "output = attention @ V\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* You **don’t add anything** to `x` before attention.\n",
    "* Instead, you **add a bias** to the attention matrix (A_{ij}) before softmax.\n",
    "* The bias depends only on the relative position between patches (i) and (j).\n",
    "\n",
    "✅ The **positional signal enters only the attention scores**, not the token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Where in the computation they differ**\n",
    "\n",
    "| Stage            | Vanilla ViT (absolute)                                   | Swin / Relative models                             |\n",
    "| ---------------- | -------------------------------------------------------- | -------------------------------------------------- |\n",
    "| Before attention | Add `pos_embed` to `x`                                   | Nothing added                                      |\n",
    "| Q,K computation  | Position info is baked into Q and K (since added to `x`) | Purely from content features                       |\n",
    "| Attention score  | No extra bias                                            | Add `relative_position_bias` based on (Δrow, Δcol) |\n",
    "| Softmax          | Softmax over `QKᵀ / √d`                                  | Softmax over `(QKᵀ / √d) + bias`                   |\n",
    "| After attention  | Normal                                                   | Normal                                             |\n",
    "\n",
    "So, in Swin and similar models, **position enters at the attention level**,\n",
    "while in ViT, **position enters at the token level**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why this matters**\n",
    "\n",
    "| Property                               | Absolute (ViT)       | Relative (Swin, BEiT, ViTDet)      |\n",
    "| -------------------------------------- | -------------------- | ---------------------------------- |\n",
    "| Where injected                         | before attention     | inside attention (as bias)         |\n",
    "| Type of info                           | absolute coordinates | relative offset (Δh, Δw)           |\n",
    "| Translation invariance                 | ❌ No                 | ✅ Yes                              |\n",
    "| Need interpolation for new image sizes | ✅ Yes                | ❌ No                               |\n",
    "| Cost                                   | trivial (add once)   | minimal (add bias lookup per head) |\n",
    "\n",
    "So, **ViT** learns one `pos_embed` tensor of shape `[1, N+1, D]`.\n",
    "But **Swin** learns a much smaller bias table of shape `[num_heads, (2H−1)×(2W−1)]`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Intuition summary**\n",
    "\n",
    "* **Absolute positional embedding (ViT)**\n",
    "  → Position added *before* Transformer\n",
    "  → Tokens carry their “address” through the entire network\n",
    "\n",
    "* **Relative positional embedding (Swin)**\n",
    "  → Position added *inside* each attention layer\n",
    "  → Attention itself learns directional preference (“look right,” “look up”)\n",
    "  → Tokens remain purely content-based\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In one sentence:**\n",
    "\n",
    "> In **ViT**, positional embeddings are *added to patch embeddings before* attention (global absolute coordinates),\n",
    "> while in **Swin** or other relative models, positional information is *added as a bias inside each attention layer* (local relative relationships).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d1846-f66d-454d-88c7-27d4e4c0edbe",
   "metadata": {},
   "source": [
    "## **6. Fully numerical Example**\n",
    "A **fully numerical and visualizable** with a **tiny 2×2 image** example so we can explicitly see what’s happening in the **attention matrix** for both **absolute** and **relative** positional embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.1. Setup**\n",
    "\n",
    "We take a **2×2 grid of image patches**, flattened in raster order:\n",
    "\n",
    "```\n",
    "[patch0, patch1,\n",
    " patch2, patch3]\n",
    "```\n",
    "\n",
    "So $ N = 4 $, and the flattened indices correspond to:\n",
    "\n",
    "| Patch | Grid coord (row, col) |\n",
    "| ----- | --------------------- |\n",
    "| 0     | (0, 0)                |\n",
    "| 1     | (0, 1)                |\n",
    "| 2     | (1, 0)                |\n",
    "| 3     | (1, 1)                |\n",
    "\n",
    "We’ll use an embedding dimension (d=2) (to keep numbers readable),\n",
    "and define small content embeddings (x_i):\n",
    "\n",
    "| Patch | $x_i$  |\n",
    "| ----- | ------ |\n",
    "| 0     | [1, 0] |\n",
    "| 1     | [0, 1] |\n",
    "| 2     | [1, 1] |\n",
    "| 3     | [0, 0] |\n",
    "\n",
    "We’ll compute **attention logits** $ A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} $ for three cases:\n",
    "\n",
    "1. No positional encoding\n",
    "2. Absolute positional encoding\n",
    "3. Relative positional encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.2. Base (no positional encoding)**\n",
    "\n",
    "Assume $Q=K=X$.\n",
    "\n",
    "Compute dot products divided by $ \\sqrt{2} \\approx 1.414 $:\n",
    "\n",
    "| i\\j | 0                       | 1                       | 2                       | 3                       |\n",
    "| --- | ----------------------- | ----------------------- | ----------------------- | ----------------------- |\n",
    "| 0   | (1·1+0·0)/1.414=0.707   | (1·0+0·1)/1.414=0       | (1·1+0·1)/1.414=0.707   | (1·0+0·0)/1.414=0       |\n",
    "| 1   | (0·1+1·0)=0             | (0·0+1·1)=1/1.414=0.707 | (0·1+1·1)=1/1.414=0.707 | (0·0+1·0)=0             |\n",
    "| 2   | (1·1+1·0)=1/1.414=0.707 | (1·0+1·1)=1/1.414=0.707 | (1·1+1·1)=2/1.414=1.414 | (1·0+1·0)=1/1.414=0.707 |\n",
    "| 3   | (0·1+0·0)=0             | (0·0+0·1)=0             | (0·1+0·1)=1/1.414=0.707 | (0·0+0·0)=0             |\n",
    "\n",
    "$$\n",
    "A_{\\text{no-pos}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & 0 & 0.707 & 0\\\\\n",
    "0 & 0.707 & 0.707 & 0\\\\\n",
    "0.707 & 0.707 & 1.414 & 0.707\\\\\n",
    "0 & 0 & 0.707 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Without position, the model sees only patch *content* — shuffling patches wouldn’t change these scores.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.3. Absolute positional embedding**\n",
    "\n",
    "Let’s assign 2D positional embeddings for each patch based on grid coordinates:\n",
    "\n",
    "| Patch | (row, col) | pos_embed  |\n",
    "| ----- | ---------- | ---------- |\n",
    "| 0     | (0,0)      | [0.0, 0.0] |\n",
    "| 1     | (0,1)      | [0.0, 0.5] |\n",
    "| 2     | (1,0)      | [0.5, 0.0] |\n",
    "| 3     | (1,1)      | [0.5, 0.5] |\n",
    "\n",
    "Then $ Q_i = x_i + p_i, ; K_i = x_i + p_i $.\n",
    "\n",
    "Let’s compute $A_{ij} = Q_i K_j^\\top / \\sqrt{2}$.\n",
    "\n",
    "Example for (i=0):\n",
    "\n",
    "* $Q_0 = [1, 0]$\n",
    "* $K_0 = [1, 0]$\n",
    "* $K_1 = [0, 1.5]$\n",
    "* $K_2 = [1.5, 0]$\n",
    "* $K_3 = [0.5, 0.5]$\n",
    "\n",
    "Then:\n",
    "\n",
    "| j | dot               | /√2   |\n",
    "| - | ----------------- | ----- |\n",
    "| 0 | (1·1+0·0)=1       | 0.707 |\n",
    "| 1 | (1·0+0·1.5)=0     | 0     |\n",
    "| 2 | (1·1.5+0·0)=1.5   | 1.061 |\n",
    "| 3 | (1·0.5+0·0.5)=0.5 | 0.354 |\n",
    "\n",
    "Now do this for all rows (similar arithmetic), yielding approximately:\n",
    "\n",
    "$$\n",
    "A_{\\text{abs}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & 0 & 1.061 & 0.354\\\\\n",
    "0 & 0.707 & 0.707 & 0.177\\\\\n",
    "1.061 & 0.707 & 1.414 & 0.884\\\\\n",
    "0.354 & 0.177 & 0.884 & 0.707\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now each patch has a *unique identity* in space — patch (1,1) ≠ patch (0,0).\n",
    "\n",
    "But note: if you **shift the whole image**, the model’s positional encodings are now *wrong* — they’re absolute.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.4. Relative positional embedding**\n",
    "\n",
    "Now we’ll instead use **relative biases** that depend on 2D offsets (Δrow, Δcol):\n",
    "\n",
    "| Offset (Δrow, Δcol) | Bias |\n",
    "| ------------------- | ---- |\n",
    "| (0, 0)              | 0.0  |\n",
    "| (0, 1)              | −0.2 |\n",
    "| (1, 0)              | −0.1 |\n",
    "| (1, 1)              | −0.3 |\n",
    "| (−1, 0)             | +0.1 |\n",
    "| (0, −1)             | +0.2 |\n",
    "| (−1, −1)            | +0.3 |\n",
    "\n",
    "Now for each pair (i,j), find their grid coordinates, compute (Δrow, Δcol), and add corresponding bias to the *content-only* logits (A_{\\text{no-pos}}).\n",
    "\n",
    "For example:\n",
    "\n",
    "* i=2 (1,0), j=0 (0,0) → Δ=(+1,0) → bias −0.1\n",
    "* i=0 (0,0), j=2 (1,0) → Δ=(−1,0) → bias +0.1\n",
    "* i=3 (1,1), j=0 (0,0) → Δ=(+1,+1) → bias −0.3\n",
    "\n",
    "Adding biases gives (rounded):\n",
    "\n",
    "$$\n",
    "A_{\\text{rel}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & +0.2 & 0.807 & 0\\\\\n",
    "0 & 0.707 & 0.507 & 0.2\\\\\n",
    "0.607 & 0.707 & 1.414 & 0.607\\\\\n",
    "-0.3 & 0 & 0.507 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, instead of unique IDs per patch, the **relative offsets** define attention bias:\n",
    "\n",
    "* patch 0 prefers looking *below* itself (+Δrow),\n",
    "* patch 1 prefers *left-right* neighbors,\n",
    "* and so on.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.5. Compare**\n",
    "\n",
    "| Model        | Encodes                    | Shift-invariant? | Key difference                 |\n",
    "| ------------ | -------------------------- | ---------------- | ------------------------------ |\n",
    "| **Absolute** | Position ID for each patch | ❌ No             | Learns fixed 14×14 coordinates |\n",
    "| **Relative** | Offset between patches     | ✅ Yes            | Learns (Δrow, Δcol) biases     |\n",
    "\n",
    "If you shift the entire image by one patch:\n",
    "\n",
    "* **Absolute**: every patch now gets the wrong (p_i).\n",
    "* **Relative**: biases depend only on offsets → attention pattern remains the same.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.6. Intuition**\n",
    "\n",
    "In ViT:\n",
    "\n",
    "* $p_i$ = \"I am patch (r,c)\"\n",
    "  → model knows *where you are*.\n",
    "\n",
    "In Relative (Swin, ViTDet):\n",
    "\n",
    "* $b_{Δr,Δc}$ = \"this patch is Δr down, Δc right from me\"\n",
    "  → model knows *how you are related* to others.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Takeaway**\n",
    "\n",
    "> Absolute embeddings learn **where** patches are (index-based).\n",
    "> Relative embeddings learn **how far and in which direction** patches are from each other.\n",
    "> That’s why relative embeddings make attention **translation-invariant** and **resolution-flexible** — crucial for dense prediction tasks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e1ec7-6f3d-49dd-aa70-47140b5d15fe",
   "metadata": {},
   "source": [
    "## 7. **What does patch 0 prefers looking below itself (+Δrow) mean?**\n",
    "---\n",
    "\n",
    "#### **7.1. Recall the setup**\n",
    "\n",
    "We had a **2×2 grid**:\n",
    "\n",
    "```\n",
    "(0,0): patch 0   (0,1): patch 1\n",
    "(1,0): patch 2   (1,1): patch 3\n",
    "```\n",
    "\n",
    "and our **relative attention logits** were:\n",
    "\n",
    "$$\n",
    "A_{\\text{rel}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & 0.2 & 0.807 & 0.0\\\n",
    "0.0 & 0.707 & 0.507 & 0.2\\\n",
    "0.607 & 0.707 & 1.414 & 0.607\\\n",
    "-0.3 & 0.0 & 0.507 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each **row i** corresponds to *query patch i*\n",
    "and each **column j** corresponds to *key patch j*.\n",
    "So row i tells you how much patch i “looks at” (attends to) patch j.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.2. Relative bias meaning**\n",
    "\n",
    "We had relative bias depending on **(Δrow, Δcol)**:\n",
    "\n",
    "| Offset (Δrow, Δcol) | Bias | Interpretation        |\n",
    "| ------------------- | ---- | --------------------- |\n",
    "| (0, 0)              | 0.0  | same position         |\n",
    "| (0, +1)             | −0.2 | neighbor to the right |\n",
    "| (+1, 0)             | −0.1 | neighbor below        |\n",
    "| (+1, +1)            | −0.3 | diagonal below-right  |\n",
    "| (−1, 0)             | +0.1 | neighbor above        |\n",
    "| (0, −1)             | +0.2 | neighbor left         |\n",
    "| (−1, −1)            | +0.3 | diagonal above-left   |\n",
    "\n",
    "Positive bias means attention is *encouraged* toward that direction; negative means *discouraged*.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.3. Interpret row by row**\n",
    "\n",
    "Let’s read each **row of (A_{\\text{rel}})**, and link values to spatial neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "**Patch 0 (top-left, row 0)**\n",
    "\n",
    "Row 0 = `[0.707, 0.2, 0.807, 0.0]`\n",
    "\n",
    "| Attends to j | Grid  | Δ(row,col)                | Bias sign | Logit | Interpretation                  |\n",
    "| ------------ | ----- | ------------------------- | --------- | ----- | ------------------------------- |\n",
    "| 0 (self)     | (0,0) | (0,0)                     | 0         | 0.707 | moderate self-attention         |\n",
    "| 1            | (0,1) | (0,+1) → right            | −         | 0.2   | low — looks *less* to the right |\n",
    "| 2            | (1,0) | (+1,0) → below            | −         | 0.807 | **highest** — prefers below     |\n",
    "| 3            | (1,1) | (+1,+1) → diag down-right | −         | 0.0   | lowest — diagonal disfavored    |\n",
    "\n",
    "✅ **Interpretation:**\n",
    "Patch 0 (top-left) most strongly attends **downward** (patch 2), less to right (patch 1), and least to diagonal (patch 3).\n",
    "So we can say *“patch 0 prefers looking below itself.”*\n",
    "\n",
    "---\n",
    "\n",
    "**Patch 1 (top-right, row 0, col 1)**\n",
    "\n",
    "Row 1 = `[0.0, 0.707, 0.507, 0.2]`\n",
    "\n",
    "| Attends to j | Grid         | Δ              | Bias sign          | Logit                           | Interpretation          |      |\n",
    "| ------------ | ------------ | -------------- | ------------------ | ------------------------------- | ----------------------- | ---- |\n",
    "| 0            | (0,0) → left | (0,−1)         | +                  | 0.0                             | (content canceled bias) |      |\n",
    "| 1 (self)     | (0,1)        | (0,0)          | 0                  | 0.707                           | high self-attn          |      |\n",
    "| 2            | (1,0)        | (+1,−1)        | diagonal down-left | bias probably slightly negative | 0.507                   | okay |\n",
    "| 3            | (1,1)        | (+1,0) → below | −                  | 0.2                             | small                   |      |\n",
    "\n",
    "✅ **Interpretation:**\n",
    "Patch 1 (top-right) attends moderately to **self and left neighbor**, and somewhat downward — hence “patch 1 prefers left–right neighbors.”\n",
    "\n",
    "---\n",
    "\n",
    "**Patch 2 (bottom-left, row 1, col 0)**\n",
    "\n",
    "Row 2 = `[0.607, 0.707, 1.414, 0.607]`\n",
    "\n",
    "| j        | Grid                | Δ(row,col) | Bias | Logit | Interpretation              |\n",
    "| -------- | ------------------- | ---------- | ---- | ----- | --------------------------- |\n",
    "| 0        | (0,0) → above       | (−1,0)     | +0.1 | 0.607 | likes above                 |\n",
    "| 1        | (0,1) → above-right | (−1,+1)    | +0.3 | 0.707 | also likes up-right         |\n",
    "| 2 (self) | (1,0)               | (0,0)      | 0    | 1.414 | highest — strong self-focus |\n",
    "| 3        | (1,1) → right       | (0,+1)     | −0.2 | 0.607 | neutral/weak right          |\n",
    "\n",
    "✅ **Interpretation:**\n",
    "Patch 2 (bottom-left) mainly attends to itself, slightly upward, and to up-right — consistent with preferring **upper neighbors**.\n",
    "\n",
    "---\n",
    "\n",
    "**Patch 3 (bottom-right, row 1, col 1)**\n",
    "\n",
    "Row 3 = `[−0.3, 0.0, 0.507, 0.0]`\n",
    "\n",
    "| j | Grid            | Δ(row,col) | Bias | Logit | Interpretation    |\n",
    "| - | --------------- | ---------- | ---- | ----- | ----------------- |\n",
    "| 0 | (0,0) → up-left | (−1,−1)    | +0.3 | −0.3  | base content weak |\n",
    "| 1 | (0,1) → up      | (−1,0)     | +0.1 | 0.0   | neutral           |\n",
    "| 2 | (1,0) → left    | (0,−1)     | +0.2 | 0.507 | stronger left     |\n",
    "| 3 | (self)          | (0,0)      | 0    | 0.0   | neutral self      |\n",
    "\n",
    "✅ **Interpretation:**\n",
    "Patch 3 (bottom-right) attends mostly **to the left (patch 2)**.\n",
    "So it “prefers leftward neighbors.”\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.4. Summary of directional attention**\n",
    "\n",
    "| Query patch (position) | Highest attention target | Spatial relation | Interpretation                        |\n",
    "| ---------------------- | ------------------------ | ---------------- | ------------------------------------- |\n",
    "| Patch 0 (top-left)     | patch 2                  | ↓ (below)        | prefers looking below                 |\n",
    "| Patch 1 (top-right)    | patch 0 / 2              | ← or ↓           | prefers horizontal/vertical neighbors |\n",
    "| Patch 2 (bottom-left)  | self / patch 1           | ↑ or ↗           | prefers upward neighbors              |\n",
    "| Patch 3 (bottom-right) | patch 2                  | ←                | prefers left neighbor                 |\n",
    "\n",
    "So each patch’s **attention direction** emerges from the **biases associated with Δ(row, col)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.5. Why this matters**\n",
    "\n",
    "* In absolute embeddings: the model learns each patch’s fixed ID, so it doesn’t generalize well when the object shifts.\n",
    "* In relative embeddings: the *pattern* of attention (downward, leftward, etc.) is learned once and reused everywhere —\n",
    "  this gives **translation invariance**.\n",
    "\n",
    "Thus “patch 0 prefers looking below itself” really means:\n",
    "\n",
    "> when this attention head fires, it tends to favor queries looking downward relative to themselves — a pattern that repeats anywhere in the image.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary insight**\n",
    "\n",
    "* The **sign and magnitude** of biases for each offset (Δrow, Δcol) control the **directional attention preference**.\n",
    "* These directional patterns let each attention head specialize — one might focus horizontally, another vertically, another diagonally.\n",
    "* That’s why **relative positional bias** is so powerful in ViT/Swin: it gives the model spatial inductive biases similar to CNNs, but learned directly through attention.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa88cfb-db99-4542-a58e-0e08b851a075",
   "metadata": {},
   "source": [
    "<img src=\"images/RPE_learned_biases.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec120b-fac8-4897-acb7-805d6a608ff0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **8. The meaning of a bias for (Δrow, Δcol)**\n",
    "\n",
    "If you define a learned bias table like this:\n",
    "\n",
    "| (Δrow, Δcol) | Bias |\n",
    "| ------------ | ---- |\n",
    "| (0, 0)       | 0.0  |\n",
    "| (0, +1)      | −0.2 |\n",
    "| (0, −1)      | +0.2 |\n",
    "| (+1, 0)      | −0.1 |\n",
    "| (−1, 0)      | +0.1 |\n",
    "| (+1, +1)     | −0.3 |\n",
    "| (−1, −1)     | +0.3 |\n",
    "\n",
    "it means:\n",
    "\n",
    "* “Neighbor **to the right** (Δrow=0, Δcol=+1)” always gets a **−0.2** bias —\n",
    "  no matter whether we’re in the top row or bottom row.\n",
    "* “Neighbor **below** (Δrow=+1, Δcol=0)” always gets **−0.1**, etc.\n",
    "\n",
    "This is exactly what gives **translation invariance**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.1. Applying to patch₀ and patch₂**\n",
    "\n",
    "Let’s recall our patch grid:\n",
    "\n",
    "```\n",
    "(0,0): patch₀     (0,1): patch₁\n",
    "(1,0): patch₂     (1,1): patch₃\n",
    "```\n",
    "\n",
    "For **patch₀ (0,0)**\n",
    "\n",
    "* Right neighbor is patch₁ (0,1).\n",
    "  Offset = (Δrow=0, Δcol=+1).\n",
    "  Bias = **−0.2**.\n",
    "  → so attention from patch₀ → patch₁ gets **−0.2** added to its logit.\n",
    "\n",
    "For **patch₂ (1,0)**\n",
    "\n",
    "* Right neighbor is patch₃ (1,1).\n",
    "  Offset = (Δrow=0, Δcol=+1).\n",
    "  Bias = **−0.2** again.\n",
    "  → so attention from patch₂ → patch₃ also gets **−0.2**.\n",
    "\n",
    "✅ **They share the same bias**, because both represent\n",
    "“look at the patch to my right.”\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Interpretation**\n",
    "\n",
    "That means the **attention head** has learned a general rule:\n",
    "\n",
    "> “When looking to the right, reduce the attention score slightly (−0.2).”\n",
    "\n",
    "This rule applies **uniformly everywhere** in the image —\n",
    "top-left, bottom-left, middle, etc.\n",
    "\n",
    "So yes — both patch₀ and patch₂ have equally reduced attention to their right neighbors.\n",
    "\n",
    "If we had set the bias for (0, +1) to **+0.2**, then *both* would have their attention **encouraged** toward the right.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.3. Why this is powerful**\n",
    "\n",
    "Because it means the model’s **behavior is spatially consistent**:\n",
    "\n",
    "* It doesn’t need to relearn “looking right” separately for each row.\n",
    "* The same learned weights apply globally.\n",
    "\n",
    "In CNN terms, this is equivalent to **weight sharing** —\n",
    "the same convolutional kernel slides everywhere in the image.\n",
    "\n",
    "So relative positional bias gives the transformer a **spatial inductive bias**:\n",
    "\n",
    "> same directional relationship → same learned effect.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.4. Relation to ViT vs Swin**\n",
    "\n",
    "| Model               | Uses bias table                       | Property                                                                          |\n",
    "| ------------------- | ------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **ViT (absolute)**  | No — each patch has independent `p_i` | Attention pattern depends on absolute image coordinates (top-left ≠ bottom-left). |\n",
    "| **Swin (relative)** | Yes — bias by Δrow, Δcol              | Attention pattern depends only on direction and distance; same everywhere.        |\n",
    "\n",
    "That’s why Swin can generalize to **larger image resolutions** —\n",
    "you don’t have to interpolate positional embeddings,\n",
    "because the bias table already defines relationships in a **relative way**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "* You’re correct: the bias for (Δrow=0, Δcol=+1) = −0.2\n",
    "  applies **identically** to patch₀→patch₁ and patch₂→patch₃.\n",
    "* This shared bias means: “looking right” is discouraged slightly everywhere.\n",
    "* The model thus learns direction-sensitive, but **location-agnostic**, attention behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fcec1-9ebb-4c54-9f9a-2a224859d51c",
   "metadata": {},
   "source": [
    "## **9. Same bias for all patches?**\n",
    "That’s an **excellent and very deep observation** — and yes, you’re absolutely right to question it.\n",
    "\n",
    "At first glance, using the **same bias for all patches** might seem restrictive —\n",
    "because, as you said, sometimes a patch should look **left**, sometimes **right**.\n",
    "But the key is this: **the relative bias doesn’t dictate where a patch *must* look** — it only provides a *soft spatial prior*.\n",
    "\n",
    "Let’s unpack this carefully.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.1. What the bias actually does**\n",
    "\n",
    "In relative positional encoding, the bias term is added to the **attention logits**:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} + b_{\\Delta h(i,j)} + b_{\\Delta w(i,j)}.\n",
    "$$\n",
    "\n",
    "That means the final attention weight depends on **two things**:\n",
    "\n",
    "1. the **content similarity** (dot product of (q_i) and (k_j))\n",
    "2. the **spatial bias** (based on relative offset)\n",
    "\n",
    "So, even if the bias for “looking right” is −0.2,\n",
    "if the content similarity between patch (i) and its right neighbor is very high,\n",
    "the attention will still be large after softmax.\n",
    "\n",
    "✅ **In short:**\n",
    "The bias *nudges* the model toward or away from certain directions,\n",
    "but the actual attention is still **content-driven**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.2. Analogy: CNN kernel vs RPE bias**\n",
    "\n",
    "Think of this like a **convolutional kernel’s receptive field**.\n",
    "\n",
    "In a CNN:\n",
    "\n",
    "* The kernel weights are the same everywhere.\n",
    "* But the activations depend on local content — edges, corners, etc.\n",
    "\n",
    "In a Transformer with **relative positional bias**:\n",
    "\n",
    "* The bias table $b_{Δh,Δw}$ is shared everywhere.\n",
    "* But the attention weights depend on $q_i^\\top k_j$, which depends on the **patch features**.\n",
    "\n",
    "So “−0.2 for right neighbor” doesn’t mean “never look right.”\n",
    "It means “unless the content gives me a good reason, looking right is slightly discouraged.”\n",
    "\n",
    "That’s how **both local inductive bias and flexibility** coexist.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.3. Why uniform biases don’t hurt expressivity**\n",
    "\n",
    "Because:\n",
    "\n",
    "* Each **attention head** has its **own bias table**.\n",
    "  (In Swin, T5, etc., `num_heads × (2H−1)×(2W−1)` parameters.)\n",
    "* Heads can specialize:\n",
    "\n",
    "  * One head might learn to look mostly **horizontally**,\n",
    "  * Another might specialize in **vertical** context,\n",
    "  * Another might attend **globally**.\n",
    "\n",
    "So, even though the bias is spatially shared,\n",
    "different heads learn different directional behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.4. Example**\n",
    "\n",
    "Let’s say we have 4 heads:\n",
    "\n",
    "| Head | Learned pattern                                   |\n",
    "| ---- | ------------------------------------------------- |\n",
    "| 1    | looks right (bias +0.2 for Δcol=+1)               |\n",
    "| 2    | looks left (bias +0.2 for Δcol=−1)                |\n",
    "| 3    | looks downward (bias +0.3 for Δrow=+1)            |\n",
    "| 4    | looks diagonally (bias +0.3 for Δrow=+1, Δcol=+1) |\n",
    "\n",
    "So different heads handle different spatial relations —\n",
    "together they cover all directions.\n",
    "\n",
    "That’s how the model can decide:\n",
    "\n",
    "* “In this image region, use horizontal attention,”\n",
    "* “In that region, vertical,” etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.5. When global or flexible relations are needed**\n",
    "\n",
    "Relative bias works *locally* (like windows in Swin).\n",
    "If global relations are important (e.g., ViT global attention),\n",
    "the **content term $q_i^\\top k_j$** dominates and can override spatial biases completely.\n",
    "\n",
    "That’s why Swin Transformer mixes:\n",
    "\n",
    "* **local relative bias** (inside shifted windows)\n",
    "* **content attention** (which can connect across windows when shifted)\n",
    "\n",
    "---\n",
    "\n",
    "## **9.6. Intuitive summary**\n",
    "\n",
    "| Question                                                             | Answer                                                                                                                |\n",
    "| -------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| “Does the same bias apply everywhere?”                               | Yes — for the same relative offset.                                                                                   |\n",
    "| “Doesn’t that make the model look in the same direction everywhere?” | No — because each head has its own bias table and attention is still content-based.                                   |\n",
    "| “So what does the bias actually do?”                                 | It provides a *consistent spatial prior* — like “prefer nearby patches,” “favor upward,” etc. — but doesn’t force it. |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In one sentence:**\n",
    "\n",
    "> The relative bias doesn’t *force* the model to look in one direction;\n",
    "> it just gives every attention head a consistent **spatial preference**,\n",
    "> while the **content similarity** term decides where to actually look.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b39e1-cb43-45b2-842e-64a57b207414",
   "metadata": {},
   "source": [
    "##  **10. Heatmap Example**\n",
    "A **heatmap example**, showing how a content-dominant case can override a negative bias (i.e., the model still attends strongly to the right if the content is relevant)\n",
    "\n",
    "<img src=\"images/heatmap_showing_a_content-dominant_case_can_override_a_negative_bias.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836e961-98a2-4955-a7ab-a0e51c9a8dba",
   "metadata": {},
   "source": [
    "## **11. Word Embeddings vs. Positional Embeddings**\n",
    "These are **two different uses of embeddings** (word embeddings vs. positional embeddings),\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.1. What “absolute axis” means**\n",
    "\n",
    "When we say:\n",
    "\n",
    "> “Absolute positional embeddings tell the model *where* each token is on an absolute axis”\n",
    "\n",
    "the **axis** here refers to the **sequence index** — the fixed ordering of tokens in the input.\n",
    "\n",
    "For example, for the sentence:\n",
    "`The cat sat on the mat.`\n",
    "\n",
    "| Token | Index (Position) | Absolute Positional Embedding (example) |\n",
    "| ----- | ---------------- | --------------------------------------- |\n",
    "| The   | 0                | [0.1, 0.3, 0.2, …]                      |\n",
    "| cat   | 1                | [0.4, 0.7, 0.9, …]                      |\n",
    "| sat   | 2                | [0.8, 0.1, 0.5, …]                      |\n",
    "| on    | 3                | [0.3, 0.2, 0.6, …]                      |\n",
    "| the   | 4                | [0.7, 0.4, 0.1, …]                      |\n",
    "| mat   | 5                | [0.5, 0.9, 0.7, …]                      |\n",
    "\n",
    "Each token gets a unique vector *based on its position index*.\n",
    "That index (0, 1, 2, 3, 4, 5) is the **absolute axis** — the “ruler” along which tokens are placed.\n",
    "\n",
    "So “absolute axis” simply means:\n",
    "→ *a fixed coordinate system based on token index in the sequence.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.2. Why we need position embeddings at all**\n",
    "\n",
    "Transformers take a set of tokens and compute attention as:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}}\n",
    "$$\n",
    "\n",
    "But without any positional signal, this operation is **permutation-invariant**:\n",
    "if you shuffle the tokens, the result doesn’t change.\n",
    "That’s bad, because word order matters!\n",
    "\n",
    "Hence, we add **positional embeddings** — absolute or relative — to inject the notion of *order*.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.3. Absolute vs. Relative (geometric analogy)**\n",
    "\n",
    "Let’s visualize with a simple 1D line (the “absolute axis”):\n",
    "\n",
    "```\n",
    "0----1----2----3----4----5---->\n",
    "|    |    |    |    |    |\n",
    "The cat sat on  the  mat\n",
    "```\n",
    "\n",
    "* **Absolute position**: “This word is at coordinate 0, 1, 2, etc.”\n",
    "* **Relative position**: “This word is *2 steps after* that one.”\n",
    "\n",
    "In **absolute encoding**, each token gets its coordinates based on this line.\n",
    "In **relative encoding**, the model learns the *distance and direction* between any two tokens — for example, token 3 is +2 away from token 1.\n",
    "\n",
    "That’s why we say relative embeddings tell the model *how far (and in what direction)* one token is from another.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.4. Contrast with semantic embeddings (“king - man + woman = queen”)**\n",
    "\n",
    "That’s a different *embedding space* — the **semantic word embedding space**, not the positional one.\n",
    "\n",
    "**Word embeddings**\n",
    "\n",
    "* Capture **semantic meaning** (king/man/woman/queen)\n",
    "* Are learned from co-occurrence patterns\n",
    "* Encode *analogical structure* like\n",
    "  $$\n",
    "  \\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n",
    "  $$\n",
    "* Each dimension in that space doesn’t mean “position in a sentence,”\n",
    "  it means *semantic feature direction* (e.g., gender, royalty, etc.)\n",
    "\n",
    "**Positional embeddings**\n",
    "\n",
    "* Capture **structural order** (position in sequence)\n",
    "* Encode either:\n",
    "\n",
    "  * Absolute position (index on a line), or\n",
    "  * Relative position (offset between two positions)\n",
    "* Their geometry encodes *sequence order*, not meaning.\n",
    "\n",
    "So:\n",
    "\n",
    "* “King – man + woman = queen” lives in **semantic space**.\n",
    "* “Token 5 is two steps after token 3” lives in **positional space**.\n",
    "\n",
    "Both use vectors and addition, but represent different concepts.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11.5. Why the distinction matters**\n",
    "\n",
    "* The **embedding of the word “king”** tells you *what it means* (semantic).\n",
    "* The **positional embedding of index 5** tells you *where it appears* (syntactic).\n",
    "\n",
    "When we feed a Transformer input, we actually **combine them**:\n",
    "\n",
    "$$\n",
    "x_i = \\text{WordEmbedding}(w_i) + \\text{PositionalEmbedding}(i)\n",
    "$$\n",
    "\n",
    "So each token embedding encodes both:\n",
    "\n",
    "* **content meaning** (from the word embedding)\n",
    "* **position meaning** (from the positional embedding)\n",
    "\n",
    "---\n",
    "\n",
    "#### 11.6. How “relative” modifies this\n",
    "\n",
    "In relative embeddings, the model doesn’t learn where tokens *are*, but how they *relate*:\n",
    "\n",
    "* Instead of “I’m token 5,”\n",
    "  it learns “I’m 2 tokens after token 3.”\n",
    "\n",
    "That makes the model **translation-invariant**:\n",
    "shifting the whole sequence by 2 doesn’t change attention patterns.\n",
    "\n",
    "For example, in music:\n",
    "\n",
    "> A melody pattern repeated later in the song should mean the same thing.\n",
    "> That’s why the **Music Transformer** used relative embeddings —\n",
    "> it doesn’t care *where* the pattern starts, only *how notes relate in time*.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "| Concept                           | Embedding Space | Captures             | “Axis”                                      | Example                         |\n",
    "| --------------------------------- | --------------- | -------------------- | ------------------------------------------- | ------------------------------- |\n",
    "| **Word embedding**                | Semantic        | meaning, analogy     | conceptual dimensions (e.g., gender, tense) | king − man + woman ≈ queen      |\n",
    "| **Absolute positional embedding** | Structural      | order in sequence    | absolute index axis (0,1,2,…)               | “token 3 is at position 3”      |\n",
    "| **Relative positional embedding** | Structural      | distance & direction | offset between positions                    | “token j is 2 ahead of token i” |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251108b-74ac-43bc-8fbc-53e209a4cd98",
   "metadata": {},
   "source": [
    "## **12. How Relative Positional Embeddings Stored/ Used**\n",
    "\n",
    "#### **12.1. What are $ b_r $ ?**\n",
    "\n",
    "In **relative positional embeddings**, $ b_r $ (or sometimes $ r^{(K)}_r, r^{(V)}_r )$ represents the **bias or embedding vector** associated with a **relative distance**:\n",
    "\n",
    "$$\n",
    "r = i - j \\in [-R_{\\max}, R_{\\max}]\n",
    "$$\n",
    "\n",
    "Each $ b_r $ is a **learnable parameter** — just like the absolute `pos_embed`, but indexed by **relative distance** instead of absolute index.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **12.2. What $ R_{\\text{max}} $ is**\n",
    "\n",
    "In **relative positional embeddings**, we index relative offsets $ r = i - j $ —\n",
    "the distance between a query position (i) and a key position (j).\n",
    "\n",
    "But in a sequence of length $L$, $r$ could range from $-L+1$ to $+L-1$.\n",
    "\n",
    "Example for $L=8$:\n",
    "$\n",
    "r \\in {-7, -6, …, 0, …, +6, +7}.\n",
    "$\n",
    "\n",
    "If we stored a separate learnable embedding $b_r$ for every possible $r$,\n",
    "that would mean $2L - 1$ learnable parameters per head — too large when $L$ is big (like 512 or 1024).\n",
    "\n",
    "So we **clip or bucket** distances to a **maximum relative distance** $R_{\\text{max}}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.2. Formal definition**\n",
    "\n",
    "We define:\n",
    "\n",
    "$$\n",
    "r_{\\text{clipped}} = \\text{clip}(i - j, -R_{\\text{max}}, +R_{\\text{max}}),\n",
    "$$\n",
    "\n",
    "and use only $2R_{\\text{max}} + 14$ learnable values.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "b_r = \\text{table}[r_{\\text{clipped}} + R_{\\text{max}}],\n",
    "$$\n",
    "\n",
    "where `table` is a learnable parameter tensor of size `[num_heads, 2R_max + 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.3. Where $ R_{\\text{max}} $ comes from**\n",
    "\n",
    "It’s a **hyperparameter** chosen when building the model, not learned.\n",
    "\n",
    "You decide it based on how long your attention window should “see” relative distances distinctly before saturating.\n",
    "\n",
    "Typical choices:\n",
    "\n",
    "| Model                    | $ R_{\\text{max}} $      | Meaning                                           |\n",
    "| ------------------------ | ----------------------- | ------------------------------------------------- |\n",
    "| Transformer-XL           | 16–64                   | beyond that, all far tokens share same embedding  |\n",
    "| T5                       | 128–512 (bucketed)      | finer bins for small distances, coarser for large |\n",
    "| Swin Transformer         | (2H−1, 2W−1) bias table | derived from window size (e.g. 7×7 → $R_{max}=6$)     |\n",
    "| BERT with Shaw-style RPE | 32–64                   | good compromise for sentence length               |\n",
    "\n",
    "So you typically pick it relative to your **attention span or window size**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.4. Why clipping is needed**\n",
    "\n",
    "Without clipping:\n",
    "\n",
    "* For long sequences, (2L-1) parameters explode (e.g., L=2048 → 4095 entries per head).\n",
    "* For distant tokens, fine-grained distance doesn’t help much — “far” is just “far.”\n",
    "\n",
    "So after some threshold, the model just learns a single “far away” embedding.\n",
    "\n",
    "Formally:\n",
    "$$\n",
    "r' =\n",
    "\\begin{cases}\n",
    "-R_{\\text{max}}, & \\text{if } i-j < -R_{\\text{max}}, \\\\\n",
    "i-j, & \\text{if } |i-j| \\le R_{\\text{max}}, \\\\\n",
    "+R_{\\text{max}}, & \\text{if } i-j > R_{\\text{max}}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.5. In 2D (Vision models)**\n",
    "\n",
    "In vision (like Swin Transformer), you have:\n",
    "\n",
    "* A **window** of size $M\\times M$ (e.g. 7×7 patches).\n",
    "* Relative offsets:\n",
    "  $\\Delta h \\in [-(M-1), +(M-1)]$,\n",
    "  $\\Delta w \\in [-(M-1), +(M-1)]$.\n",
    "\n",
    "So you don’t pick $R_{\\text{max}}$ manually — it’s fixed by the window.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "window_size = 7\n",
    "relative_position_bias_table = nn.Parameter(\n",
    "    torch.zeros((2*window_size-1)*(2*window_size-1), num_heads)\n",
    ")\n",
    "# -> (13*13, num_heads) = (169, num_heads)\n",
    "```\n",
    "\n",
    "Here $R_{\\text{max}} = 6$ because the maximum offset is ±6.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.6. Intuitive picture**\n",
    "\n",
    "Think of $R_{\\text{max}}$ as setting the “radius of spatial awareness”:\n",
    "\n",
    "* Within ±R_max → learn individual relations.\n",
    "* Beyond that → treat all distant tokens as “far away” equivalently.\n",
    "\n",
    "For text: “words more than 128 tokens apart are equally distant.”\n",
    "For vision: “patches more than 6 cells apart are equally distant.”\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "| Symbol             | Meaning                                                                                  | Source                                | Typical value                             |\n",
    "| ------------------ | ---------------------------------------------------------------------------------------- | ------------------------------------- | ----------------------------------------- |\n",
    "| $ R_{\\text{max}} $ | Maximum relative offset                                                                  | Hyperparameter (user-set)             | 16–128 for text, 6 for 7×7 vision windows |\n",
    "| Purpose            | Clip or bucket large distances                                                           | avoids huge tables, captures locality |                                           |\n",
    "| Used in            | $b_{r_{\\text{clipped}}}, \\quad r^{(K)} r_{\\text{clipped}}, \\quad r^{(V)} r_{\\text{clipped}}$ | inside attention score                |                                           |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef19e0b-4ef1-466f-85a4-393e5502b5f7",
   "metadata": {},
   "source": [
    "#### **12.6. Numerical Mini**\n",
    "\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Let’s take a **1D sequence** of **length $L = 7$** (tokens numbered 0–6).\n",
    "\n",
    "We define:\n",
    "$$r = i - j$$\n",
    "\n",
    "as the **relative distance** between query position (i) and key position (j).\n",
    "\n",
    "That means $r \\in [-6, +6]$.\n",
    "\n",
    "Now we choose $R_{\\text{max}} = 2$.\n",
    "So we only keep **five learnable biases** (for −2, −1, 0, +1, +2).\n",
    "All larger distances are **clipped** to ±2.\n",
    "\n",
    "---\n",
    "\n",
    "**Bias lookup table**\n",
    "\n",
    "Let’s assign arbitrary learnable scalar biases:\n",
    "\n",
    "| Relative offset (r) | Bias (b_r) |\n",
    "| ------------------- | ---------- |\n",
    "| −2                  | −0.3       |\n",
    "| −1                  | −0.2       |\n",
    "| 0                   | 0.0        |\n",
    "| +1                  | +0.2       |\n",
    "| +2                  | +0.3       |\n",
    "\n",
    "For any distance (r < -2), use (b_{-2}).\n",
    "For any distance (r > +2), use (b_{+2}).\n",
    "\n",
    "---\n",
    "\n",
    "**Compute $r = i - j$ for all pairs**\n",
    "\n",
    "We’ll build a (7×7) grid of all relative offsets.\n",
    "\n",
    "| i\\j | 0      | 1      | 2  | 3  | 4  | 5  | 6  |\n",
    "| --- | ------ | ------ | -- | -- | -- | -- | -- |\n",
    "| 0   | 0−0=0  | 0−1=−1 | −2 | −3 | −4 | −5 | −6 |\n",
    "| 1   | 1−0=+1 | 0      | −1 | −2 | −3 | −4 | −5 |\n",
    "| 2   | 2−0=+2 | +1     | 0  | −1 | −2 | −3 | −4 |\n",
    "| 3   | 3−0=+3 | +2     | +1 | 0  | −1 | −2 | −3 |\n",
    "| 4   | 4−0=+4 | +3     | +2 | +1 | 0  | −1 | −2 |\n",
    "| 5   | 5−0=+5 | +4     | +3 | +2 | +1 | 0  | −1 |\n",
    "| 6   | 6−0=+6 | +5     | +4 | +3 | +2 | +1 | 0  |\n",
    "\n",
    "---\n",
    "\n",
    "**Apply clipping**\n",
    "\n",
    "Now clip all values to the range ([-2, +2]):\n",
    "\n",
    "| i\\j | 0  | 1  | 2  | 3  | 4  | 5  | 6  |\n",
    "| --- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| 0   | 0  | −1 | −2 | −2 | −2 | −2 | −2 |\n",
    "| 1   | +1 | 0  | −1 | −2 | −2 | −2 | −2 |\n",
    "| 2   | +2 | +1 | 0  | −1 | −2 | −2 | −2 |\n",
    "| 3   | +2 | +2 | +1 | 0  | −1 | −2 | −2 |\n",
    "| 4   | +2 | +2 | +2 | +1 | 0  | −1 | −2 |\n",
    "| 5   | +2 | +2 | +2 | +2 | +1 | 0  | −1 |\n",
    "| 6   | +2 | +2 | +2 | +2 | +2 | +1 | 0  |\n",
    "\n",
    "---\n",
    "\n",
    "**Convert to actual bias values**\n",
    "\n",
    "Replace each relative offset with its bias (b_r):\n",
    "\n",
    "| i\\j | 0    | 1    | 2    | 3    | 4    | 5    | 6    |\n",
    "| --- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| 0   | 0.0  | −0.2 | −0.3 | −0.3 | −0.3 | −0.3 | −0.3 |\n",
    "| 1   | +0.2 | 0.0  | −0.2 | −0.3 | −0.3 | −0.3 | −0.3 |\n",
    "| 2   | +0.3 | +0.2 | 0.0  | −0.2 | −0.3 | −0.3 | −0.3 |\n",
    "| 3   | +0.3 | +0.3 | +0.2 | 0.0  | −0.2 | −0.3 | −0.3 |\n",
    "| 4   | +0.3 | +0.3 | +0.3 | +0.2 | 0.0  | −0.2 | −0.3 |\n",
    "| 5   | +0.3 | +0.3 | +0.3 | +0.3 | +0.2 | 0.0  | −0.2 |\n",
    "| 6   | +0.3 | +0.3 | +0.3 | +0.3 | +0.3 | +0.2 | 0.0  |\n",
    "\n",
    "✅ You can see:\n",
    "\n",
    "* Far distances (|r| ≥ 3) all share the same **clipped bias** (−0.3 or +0.3).\n",
    "* The bias pattern stays constant across rows (translation invariant).\n",
    "\n",
    "---\n",
    "\n",
    "**Visual intuition**\n",
    "\n",
    "If we visualize this bias matrix as a heatmap:\n",
    "\n",
    "* Diagonal (r=0) → 0.0 bias (neutral)\n",
    "* Upper-right triangle (looking backward → negative r) → negative bias (−0.2, −0.3)\n",
    "* Lower-left triangle (looking forward → positive r) → positive bias (+0.2, +0.3)\n",
    "* Beyond ±2 → **flat color** (clipped zone)\n",
    "\n",
    "So the model treats distances ≥2 as \"equally far away.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Generalization**\n",
    "\n",
    "For longer sequences or higher dimensions:\n",
    "\n",
    "* You still pick $R_{\\text{max}}$ to control how finely you distinguish local offsets.\n",
    "* Often you **bucket** distances logarithmically (T5) rather than clip linearly.\n",
    "\n",
    "In T5-style bucketing:\n",
    "\n",
    "* 0–16: one bias per distance\n",
    "* 17–32, 33–64, etc.: grouped together\n",
    "\n",
    "That helps the model cover both local and long-range relations efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "| Symbol                              | Meaning                                                   |\n",
    "| ----------------------------------- | --------------------------------------------------------- |\n",
    "| $R_{\\text{max}}$                    | maximum relative offset we treat distinctly               |\n",
    "| $[-R_{\\text{max}}, R_{\\text{max}}]$ | range of learnable biases                                 |\n",
    "| beyond range                        | clipped or bucketed                                       |\n",
    "| effect                              | makes model translation-invariant but parameter-efficient |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2eaff-a810-4a20-a421-6df957f614f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#### **12.2. How are they stored in the model?**\n",
    "\n",
    "Typically, they are implemented as a small **lookup table**:\n",
    "\n",
    "```python\n",
    "self.relative_bias = nn.Parameter(torch.zeros(2*R_max + 1))\n",
    "```\n",
    "\n",
    "or, for multi-head attention (like T5/Swin):\n",
    "\n",
    "```python\n",
    "self.relative_bias = nn.Parameter(torch.zeros(num_heads, 2*R_max + 1))\n",
    "```\n",
    "\n",
    "The model learns these values during training by backpropagation — exactly like weights and biases of linear layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.3. How they enter the attention computation**\n",
    "\n",
    "Recall the equation:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} + b_{i-j}\n",
    "$$\n",
    "\n",
    "Here (b_{i-j}) is a scalar retrieved from that table:\n",
    "\n",
    "```python\n",
    "r = i - j\n",
    "r_clipped = torch.clamp(r, -R_max, R_max)\n",
    "bias = self.relative_bias[r_clipped + R_max]\n",
    "A = (Q @ K.transpose(-2, -1)) / sqrt(d) + bias\n",
    "```\n",
    "\n",
    "If you’re using **multi-head attention**, each head can have its own bias table (b_{r}^{(h)}).\n",
    "If not, it’s shared across heads.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.4. 2D (vision) case**\n",
    "\n",
    "For 2D windows (like Swin Transformer), we have **separate bias tables** for horizontal and vertical offsets:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}}b^{(h)}{\\Delta h(i,j)} + b^{(w)}{\\Delta w(i,j)}\n",
    "$$\n",
    "\n",
    "\n",
    "In code, this usually becomes one parameter tensor:\n",
    "\n",
    "```python\n",
    "self.relative_position_bias_table = nn.Parameter(\n",
    "    torch.zeros((2*Wh - 1)*(2*Ww - 1), num_heads)\n",
    ")\n",
    "```\n",
    "\n",
    "and an **index map** precomputed to lookup the correct bias per (i,j) pair.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.5. Example visualization**\n",
    "\n",
    "Imagine $R_{\\max}=2$, so we have 5 learnable scalars:\n",
    "\n",
    "```text\n",
    "r ∈ {-2, -1, 0, +1, +2}\n",
    "b_r = [-0.1, 0.2, 0.0, 0.3, -0.2]   # learned after training\n",
    "```\n",
    "\n",
    "During backpropagation, the gradient of the attention loss wrt (A_{ij}) updates the corresponding (b_{i-j}).\n",
    "So, if the model learns that **attending to next tokens** helps prediction, it will increase (b_{+1}) and (b_{+2}).\n",
    "\n",
    "---\n",
    "\n",
    "#### **12.6. Comparison**\n",
    "\n",
    "| Concept                                   | Parameter                    | Indexed by         | Learned? | Typical size         |\n",
    "| ----------------------------------------- | ---------------------------- | ------------------ | -------- | -------------------- |\n",
    "| **Absolute positional embedding**         | `pos_embed[position]`        | absolute index (i) | ✅ yes    | $L \\times d$         |\n",
    "| **Relative positional bias (scalar)**     | `b_rel[offset]`              | offset (i-j)       | ✅ yes    | $2R+1)$ or $H×W$ grid |\n",
    "| **Relative key/value embedding (vector)** | `r_K[offset]`, `r_V[offset]` | offset (i-j)       | ✅ yes    | $(2R+1) \\times d$    |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "* $b_r$ are **learnable parameters**, one per relative distance bucket.\n",
    "* They are optimized through normal backpropagation.\n",
    "* This makes relative embeddings trainable **just like absolute embeddings**, but the model learns *distance-dependent* behavior rather than *position-dependent* behavior.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951b870-1d70-41e9-9146-cacb99fb8b2c",
   "metadata": {},
   "source": [
    "\n",
    "## **13. Main Approaches to Relative Positional Embedding (RPE)**\n",
    "\n",
    "We can group them into **four main families**:\n",
    "\n",
    "| # | Approach                                          | Key idea                                                                     | Example models                              |\n",
    "| - | ------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| 1 | **Additive bias (scalar)**                        | Learn one bias per relative distance bucket; add to attention logits         | T5, DeBERTa, Swin, ViTDet                   |\n",
    "| 2 | **Content-dependent relative key/value (vector)** | Learn relative embedding vectors that interact with queries/values           | Shaw et al. (Transformer-XL, DeBERTa-v1)    |\n",
    "| 3 | **Rotary embedding (RoPE)**                       | Encode relative phase difference by rotating Q,K vectors in complex/2D space | RoFormer, GPT-NeoX, Llama-2/3               |\n",
    "| 4 | **Analytic slope (ALiBi)**                        | No parameters; add fixed linear slope per head for distance bias             | ALiBi (Press et al., 2022), Mistral, Falcon |\n",
    "\n",
    "Let’s unpack each precisely.\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.1. Additive bias (scalar per relative distance)**\n",
    "\n",
    "**Equation**\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} + b_{\\text{rel}}(i-j)\n",
    "$$\n",
    "\n",
    "* (b_{\\text{rel}}(r)\\in\\mathbb{R}) is a **learnable scalar bias**.\n",
    "* Often **bucketed**: nearby distances get fine bins, long ones coarse.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "```python\n",
    "b_rel = nn.Parameter(torch.zeros(num_heads, num_buckets))\n",
    "bias = b_rel[:, bucket(i-j)]\n",
    "A = (Q @ K.transpose(-2,-1))/sqrt(d) + bias\n",
    "```\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Simple, efficient (O(L^2)) bias addition.\n",
    "* Works well for long sequences (T5).\n",
    "* Naturally supports 2D windows (Swin).\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Adds only a scalar per distance — can’t model directional content interaction.\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.2. Content-dependent relative key/value (Shaw et al., 2018)**\n",
    "\n",
    "**Equation**\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j + q_i^\\top r^{(K)}*{i-j}}{\\sqrt{d}}, \\qquad\n",
    "\\text{out}*i = \\sum_j \\alpha*{ij}\\left(v_j + r^{(V)}*{i-j}\\right)\n",
    "$$\n",
    "\n",
    "* $r^{(K)}_r, r^{(V)}_r \\in \\mathbb{R}^d$ are **learnable vectors** for each relative offset (r).\n",
    "* Uses the “**skew trick**” to implement efficiently (avoids full (L^2) tensor).\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Allows direction-aware and content-aware attention (richer signal).\n",
    "* Works well in Transformer-XL, DeBERTa.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Slightly more compute/memory.\n",
    "* Doesn’t scale well to huge (L) (needs clipping or bucketing).\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.3. Rotary Positional Embedding (RoPE)**\n",
    "\n",
    "Instead of adding a bias or vector, **rotate** Q and K in 2-D subspaces depending on position:\n",
    "\n",
    "$$\n",
    "\\tilde{q}_i = R(\\theta_i) q_i,\\qquad\n",
    "\\tilde{k}_j = R(\\theta_j) k_j\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\tilde{q}*i^\\top \\tilde{k}*j\n",
    "= q_i^\\top R(\\theta*{j}-\\theta*{i}) k_j\n",
    "$$\n",
    "\n",
    "The dot-product *implicitly* encodes **relative position (j–i)** via phase difference.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* No new parameters (only deterministic frequencies).\n",
    "* Naturally relative; extrapolates to longer sequences.\n",
    "* Now standard in Llama, GPT-NeoX, etc.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Needs even-dim pairing (applies 2-D rotations).\n",
    "* Harder to combine with learned biases (though “XPos” fixes that).\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.4. ALiBi (Attention with Linear Biases)**\n",
    "\n",
    "Add a *fixed linear penalty* proportional to distance:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} - m_h (i-j)\n",
    "$$\n",
    "\n",
    "* (m_h>0) is a fixed slope per head (non-learned).\n",
    "* Encourages each head to prefer local context but still attend long-range.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Zero extra parameters.\n",
    "* Scales to very long context windows.\n",
    "* Excellent extrapolation (no need to interpolate embeddings).\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Fixed — can’t adapt biases through training.\n",
    "* Directional only (penalizes forward distance).\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.5. Specialization for Vision (2D RPE)**\n",
    "\n",
    "For 2D patches/windows (like Swin, ViTDet):\n",
    "\n",
    "$$ A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} b^{(h)}*{\\Delta h(i,j)} + b^{(w)}{\\Delta w(i,j)} $$\n",
    "\n",
    "- Two small 1-D learnable tables, one for height offsets, one for width.\n",
    "- Enables **translation equivariance** across image space.\n",
    "\n",
    "---\n",
    "\n",
    "#### **13.6. Summary table**\n",
    "\n",
    "| Type                 | Added params      | Equation term                             | Depends on         | Learnable?      | Typical use     |\n",
    "| -------------------- | ----------------- | ----------------------------------------- | ------------------ | --------------- | --------------- |\n",
    "| **Additive bias**    | small (2R+1)      | (+ b_{i-j})                               | distance           | ✅               | T5, Swin        |\n",
    "| **Shaw-style**       | moderate (2R+1)×d | (+ q_i^\\top r^{(K)}_{i-j})                | distance + content | ✅               | Transformer-XL  |\n",
    "| **RoPE**             | none              | implicit via rotation                     | relative angle     | ❌ (fixed)       | Llama, GPT-NeoX |\n",
    "| **ALiBi**            | none              | (- m_h (i-j))                             | distance           | ❌ (fixed slope) | ALiBi, Mistral  |\n",
    "| **2D Bias (vision)** | small grid        | (+ b^{(h)}*{\\Delta h}+b^{(w)}*{\\Delta w}) | Δrow, Δcol         | ✅               | Swin, ViTDet    |\n",
    "\n",
    "---\n",
    "\n",
    "## **13.7. Key intuitions**\n",
    "\n",
    "* **Absolute embeddings** encode “I am token #7”.\n",
    "* **Relative embeddings** encode “token *j* is 3 steps ahead of me”.\n",
    "\n",
    "Relative methods:\n",
    "\n",
    "* improve **translation invariance**,\n",
    "* **generalize** to longer sequences,\n",
    "* and **model directionality** (past vs. future, left vs. right).\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "> There isn’t one “RPE” — there are **several compatible mechanisms**.\n",
    "> Modern Transformers usually combine **RoPE** (for long-range generalization)\n",
    "> with **additive relative bias** (for local inductive bias).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524dbe12-a24e-4510-8b62-17b756a99cc2",
   "metadata": {},
   "source": [
    "## **14. Numerical Example, Comparison of Different Relative Positional Embedding (RPE)**\n",
    "Let’s build a **numerical, side-by-side comparison** of the four major **relative positional embedding (RPE)** variants:\n",
    "\n",
    "1. **Additive bias** (scalar per distance)\n",
    "2. **Shaw-style** (content-dependent relative vectors)\n",
    "3. **Rotary (RoPE)**\n",
    "4. **ALiBi (linear slope)**\n",
    "\n",
    "All on a **tiny 1D sequence** of length (L=3), embedding dim (d=2).\n",
    "We’ll keep the numbers small and interpretable so you can literally check them by hand or in a Jupyter cell.\n",
    "\n",
    "---\n",
    "\n",
    "#### 14.1. Shared setup\n",
    "\n",
    "Tokens $x_i$:\n",
    "\n",
    "| index | vector $x_i$ |\n",
    "| ----- | ------------ |\n",
    "| 0     | [1, 0]       |\n",
    "| 1     | [0, 1]       |\n",
    "| 2     | [1, 1]       |\n",
    "\n",
    "Use $Q=K=X$.\n",
    "$\\sqrt{d}=\\sqrt{2}\\approx1.414$.\n",
    "The base content logits:\n",
    "\n",
    "$$\n",
    "A_{\\text{content}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & 0 & 0.707\\\\\n",
    "0 & 0.707 & 0.707\\\\\n",
    "0.707 & 0.707 & 1.414\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We’ll now modify these logits differently for each RPE variant.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Variant 1 — Additive bias (scalar)\n",
    "\n",
    "Each relative offset (r=i-j) gets a learnable scalar $b_r$:\n",
    "\n",
    "| $r$ | $b_r$ |\n",
    "| --- | ----- |\n",
    "| −2  | −0.3  |\n",
    "| −1  | −0.2  |\n",
    "| 0   | 0.0   |\n",
    "| +1  | +0.2  |\n",
    "| +2  | +0.3  |\n",
    "\n",
    "Add $b_{i-j}$ to logits:\n",
    "\n",
    "$$\n",
    "A_{\\text{bias}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & -0.2 & 0.407\\\\\n",
    "0.200 & 0.707 & 0.507\\\\\n",
    "1.007 & 0.907 & 1.414\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Bias shifts attention toward **future tokens** (positive offsets).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Variant 2 — Shaw-style (content-dependent)\n",
    "\n",
    "Each relative offset has a **vector** $r^{(K)}_r$:\n",
    "\n",
    "| $r$ | $r^{(K)}_r$  |\n",
    "| --- | ------------ |\n",
    "| −2  | [−1, 0]      |\n",
    "| −1  | [−0.5, 0.0]  |\n",
    "| 0   | [ 0, 0 ]     |\n",
    "| +1  | [ 0.5, 0.0 ] |\n",
    "| +2  | [ 1, 0 ]     |\n",
    "\n",
    "Compute\n",
    "$$ A_{ij} = \\frac{q_i^\\top(k_j + r^{(K)}_{i-j})}{\\sqrt{2}}. $$\n",
    "\n",
    "Let’s compute row 0 explicitly:\n",
    "\n",
    "* $j=0$: $r=0\\Rightarrow q_0!\\cdot!(k_0+r^{(K)}_0)=1·1+0·0=1$\n",
    "* $j=1$: $r=-1\\Rightarrow q_0!\\cdot!(k_1+r^{(K)}_{-1})=1·(-0.5)+0·0=-0.5$\n",
    "* $j=2$: $r=-2\\Rightarrow q_0!\\cdot!(k_2+r^{(K)}_{-2})=1·(1-1)+0·0=0$\n",
    "\n",
    "Divide by 1.414 → [0.707→0.707? Wait compute again]. Let's compute properly below.\n",
    "\n",
    "Compute all rows:\n",
    "\n",
    "| i | j | $r=i-j$ | $k_j+r_r^{(K)}$ | $q_i\\cdot(\\cdot)$ | /√2    |\n",
    "| - | - | ------- | --------------- | ----------------- | ------ |\n",
    "| 0 | 0 | 0       | [1,0]           | 1                 | 0.707  |\n",
    "| 0 | 1 | −1      | [−0.5,1]        | 1*(−0.5)+0*1=−0.5 | −0.354 |\n",
    "| 0 | 2 | −2      | [0,1]           | 1*0+0*1=0         | 0      |\n",
    "| 1 | 0 | +1      | [1.5,0]         | 0*1.5+1*0=0       | 0      |\n",
    "| 1 | 1 | 0       | [0,1]           | 0*0+1*1=1         | 0.707  |\n",
    "| 1 | 2 | −1      | [0.5,1]         | 0*0.5+1*1=1       | 0.707  |\n",
    "| 2 | 0 | +2      | [2,0]           | 1*2+1*0=2         | 1.414  |\n",
    "| 2 | 1 | +1      | [0.5,1]         | 1*0.5+1*1=1.5     | 1.061  |\n",
    "| 2 | 2 | 0       | [1,1]           | 1*1+1*1=2         | 1.414  |\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "A_{\\text{Shaw}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & -0.354 & 0.000\\\\\n",
    "0.000 & 0.707 & 0.707\\\\\n",
    "1.414 & 1.061 & 1.414\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "You see the pattern: local distance directly changes the dot-product via learned direction vectors.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Variant 3 — Rotary (RoPE)\n",
    "\n",
    "For (d=2), rotation per position (p):\n",
    "\n",
    "$$\n",
    "R(\\theta_p) =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta_p & -\\sin\\theta_p\\\n",
    "\\sin\\theta_p & \\cos\\theta_p\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Let $\\theta_p = p·30^\\circ = [0°, 30°, 60°]$.\n",
    "\n",
    "Compute rotated Q,K:\n",
    "\n",
    "| p | Qp    | Rp(Qp)                                                      | Kp         | Rp(Kp)        |\n",
    "| - | ----- | ----------------------------------------------------------- | ---------- | ------------- |\n",
    "| 0 | [1,0] | [1,0]                                                       | [1,0]      | [1,0]         |\n",
    "| 1 | [0,1] | [−0.5, 0.866]                                               | [0,1]      | [−0.5, 0.866] |\n",
    "| 2 | [1,1] | rotation 60°→ [cos60°−sin60°, sin60°+cos60°]=[0.366, 1.366] | same for K |               |\n",
    "\n",
    "Now compute $\\tilde A_{ij}=(\\tilde q_i^\\top\\tilde k_j)/\\sqrt{2}$:\n",
    "\n",
    "| i\\j | 0                             | 1                                                  | 2                                                      |\n",
    "| --- | ----------------------------- | -------------------------------------------------- | ------------------------------------------------------ |\n",
    "| 0   | (1·1+0·0)/1.414=0.707         | (1·−0.5+0·0.866)/1.414=−0.354                      | (1·0.366+0·1.366)/1.414=0.259                          |\n",
    "| 1   | (−0.5·1+0.866·0)/1.414=−0.354 | ((−0.5)^2+0.866^2)/1.414=(1.0)/1.414=0.707         | ((−0.5·0.366)+(0.866·1.366))/1.414=(0.999)/1.414=0.707 |\n",
    "| 2   | (0.366·1+1.366·0)/1.414=0.259 | (0.366·−0.5+1.366·0.866)/1.414=(0.999)/1.414=0.707 | (0.366·0.366+1.366·1.366)/1.414=(2.0)/1.414=1.414      |\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "A_{\\text{RoPE}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & -0.354 & 0.259\\\\\n",
    "-0.354 & 0.707 & 0.707\\\\\n",
    "0.259 & 0.707 & 1.414\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Notice how it’s **implicitly relative**: row 1 and row 2 patterns are similar but *shifted*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Variant 4 — ALiBi (linear slope)\n",
    "\n",
    "Add a *fixed* distance-dependent penalty:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{q_i^\\top k_j}{\\sqrt{d}} - m (i-j), \\quad m=0.1.\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "| i\\j | r=i−j | base  | −m·r | total |\n",
    "| --- | ----- | ----- | ---- | ----- |\n",
    "| 0   | 0     | 0.707 | 0    | 0.707 |\n",
    "| 0   | 1     | 0     | +0.1 | 0.1   |\n",
    "| 0   | 2     | 0.707 | +0.2 | 0.907 |\n",
    "| 1   | 0     | 0     | −0.1 | −0.1  |\n",
    "| 1   | 1     | 0.707 | 0    | 0.707 |\n",
    "| 1   | 2     | 0.707 | +0.1 | 0.807 |\n",
    "| 2   | 0     | 0.707 | −0.2 | 0.507 |\n",
    "| 2   | 1     | 0.707 | −0.1 | 0.607 |\n",
    "| 2   | 2     | 1.414 | 0    | 1.414 |\n",
    "\n",
    "$$\n",
    "A_{\\text{ALiBi}} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & 0.100 & 0.907\\\\\n",
    "-0.100 & 0.707 & 0.807\\\\\n",
    "0.507 & 0.607 & 1.414\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Each head could have its own slope (m_h), controlling attention decay with distance.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Side-by-side summary\n",
    "\n",
    "| Variant   | Learned?              | Adds what       | Example numeric pattern (row 0) | Comment                           |\n",
    "| --------- | --------------------- | --------------- | ------------------------------- | --------------------------------- |\n",
    "| **Bias**  | ✅ (b_r) scalars       | +bias to logits | [0.707, −0.2, 0.407]            | simplest learnable RPE            |\n",
    "| **Shaw**  | ✅ (r_r^{(K)}) vectors | +dot$q, r_r$    | [0.707, −0.354, 0]              | richer, directional               |\n",
    "| **RoPE**  | ❌ rotations           | implicit        | [0.707, −0.354, 0.259]          | parameter-free, smooth            |\n",
    "| **ALiBi** | ❌ slope (m)           | −m·(i−j)        | [0.707, 0.1, 0.907]             | fixed bias, long-context friendly |\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Interpretation\n",
    "\n",
    "* **Additive bias:** learns fixed attention preferences by *distance*.\n",
    "* **Shaw:** learns *vector* interactions by distance — more expressive.\n",
    "* **RoPE:** encodes distance via rotational phase — parameter-free, extrapolates best.\n",
    "* **ALiBi:** deterministic linear bias — fastest, no learned params, good for autoregressive models.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key takeaway**\n",
    "\n",
    "> All four inject *relative information* into the attention scores,\n",
    "> but differ in **what is learned**, **how it scales**, and **how interpretable** it is:\n",
    ">\n",
    "> * **Bias & Shaw:** learn tables via backprop.\n",
    "> * **RoPE & ALiBi:** analytic / fixed functions of distance.\n",
    "> * **RoPE** ⇒ used in modern LLMs (Llama, GPT-NeoX).\n",
    "> * **Bias (2-D)** ⇒ used in Swin Transformer (vision).\n",
    "> * **Shaw + skew** ⇒ used in Music Transformer, Transformer-XL.\n",
    "> * **ALiBi** ⇒ used in Mistral, Falcon (long context).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b840e-b68c-45f4-9b82-3460bbb469fb",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementation notes (fast paths)\n",
    "\n",
    "* **Bias-only** (T5/ALiBi): precompute an (L\\times L) bias (or “on-the-fly” with buckets); add to logits before softmax.\n",
    "* **Shaw-style**: compute (Q R^{(K)\\top}) once, then **skew** (a reshape+pad+slice) so offset (r) lines up with column (j).\n",
    "* **RoPE**: apply rotations to (Q,K) once per layer; everything else is standard attention.\n",
    "* **2-D**: store small ( (2H!-!1)\\times(2W!-!1)) tables (or factor into two ((2H!-!1)) and ((2W!-!1)) vectors).\n",
    "\n",
    "---\n",
    "\n",
    "#### When to use what?\n",
    "\n",
    "* **RoPE**: ✅ robust extrapolation to longer context; no extra params; default for many LLMs.\n",
    "* **Additive bias (T5)**: ✅ simple, stable, cheap; works great with encoders/decoders; buckets control capacity.\n",
    "* **ALiBi**: ✅ zero learned params; excellent long-context behavior; causal models.\n",
    "* **Shaw-style**: ✅ highest expressivity (content × position); ❌ slightly heavier; good for tasks sensitive to fine relative geometry (e.g., some NMT or local vision windows).\n",
    "* **2-D bias**: ✅ ideal for images/windows (Swin); small overhead; preserves translational inductive bias.\n",
    "\n",
    "---\n",
    "\n",
    "#### Quick reference equations (copy-ready)\n",
    "\n",
    "**Bias-only (per head):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} + b_{\\text{rel}}(\\operatorname{bucket}(i-j)),\\quad\n",
    "\\alpha_{ij}=\\frac{e^{A_{ij}}}{\\sum_{t} e^{A_{it}}},\\quad\n",
    "\\text{out}*i=\\sum_j \\alpha*{ij} v_j.\n",
    "$$\n",
    "\n",
    "**Shaw et al.:**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j + q_i^\\top r^{(K)}{i-j}}{\\sqrt{d}},\\qquad\n",
    "\\text{out}*i=\\sum_j \\alpha*{ij}\\left(v_j + r^{(V)}*{i-j}\\right).\n",
    "$$\n",
    "\n",
    "**ALiBi (causal):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} - m_h,(i-j),\\quad j\\le i.\n",
    "$$\n",
    "\n",
    "**RoPE:**\n",
    "$$\n",
    "\\widetilde{q}_i=R(\\theta_i)q_i,\\quad \\widetilde{k}*j=R(\\theta_j)k_j,\\quad\n",
    "A*{ij}=\\frac{\\widetilde{q}*i^\\top \\widetilde{k}*j}{\\sqrt{d}}\n",
    "=\\frac{q_i^\\top R(\\theta*{j}-\\theta*{i})k_j}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "**2-D windowed bias (vision):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} + b^{(h)}*{\\Delta h(i,j)} + b^{(w)}*{\\Delta w(i,j)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Key intuitions (why it helps)\n",
    "\n",
    "* **Translation invariance**: the model learns “same-pattern, different place” naturally.\n",
    "* **Length generalization**: depends on *offsets*, not absolute indexes.\n",
    "* **Parameter efficiency**: a small table (or none with RoPE/ALiBi) replaces (O(L)) absolute embeddings.\n",
    "* **Locality bias**: buckets/linear slopes can emphasize nearby tokens—crucial for language syntax and vision neighborhoods.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
