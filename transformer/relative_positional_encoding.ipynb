{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43b840e-b68c-45f4-9b82-3460bbb469fb",
   "metadata": {},
   "source": [
    "# Relative positional embedding in self-attention \n",
    "\n",
    "Self-attention is permutation-invariant, so you must inject order. **Relative** positional embeddings (RPE) tell the model *how far (and in what direction)* two tokens are from each other, instead of where each token is on an absolute axis. This brings better length generalization, translation invariance, and parameter efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Absolute vs. Relative (at the logits level)\n",
    "\n",
    "Vanilla attention (per head) uses:\n",
    "$$\n",
    "\\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}!\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V,,\n",
    "$$\n",
    "where (Q,K,V\\in\\mathbb{R}^{L\\times d}).\n",
    "Absolute positions add a term that depends on token index (i). Relative positions add a term that depends on the **pairwise offset** (r=i-j).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Additive relative **bias** (T5/ALiBi-style)\n",
    "\n",
    "You add a scalar bias to each attention logit based on the relative distance bucket of ((i,j)):\n",
    "$$\n",
    "A_{ij} ;=; \\frac{q_i^\\top k_j}{\\sqrt{d}} ;+; b_{\\text{rel}}!\\big(\\operatorname{bucket}(i-j)\\big),\n",
    "$$\n",
    "$$\n",
    "\\alpha_{ij} ;=; \\mathrm{softmax}*j(A*{ij}),\\qquad \\text{output}*i=\\sum_j \\alpha*{ij} v_j.\n",
    "$$\n",
    "\n",
    "* (b_{\\text{rel}}(\\cdot)\\in\\mathbb{R}) is a learned table (often per head).\n",
    "* (\\operatorname{bucket}(\\cdot)) maps distances to a small set of bins (fine for small (|r|), coarse/log for large (|r|)) to keep parameters (O(\\text{#bins})).\n",
    "\n",
    "**ALiBi** is a special case with a *fixed* linear bias per head:\n",
    "$$\n",
    "A_{ij} ;=; \\frac{q_i^\\top k_j}{\\sqrt{d}} ;-; m_h,(i-j), \\qquad m_h>0.\n",
    "$$\n",
    "No learned table; scales well to long contexts.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Content-aware relative keys/values (Shaw et al.)\n",
    "\n",
    "Here the relative embedding is **vector-valued** and interacts with content:\n",
    "$$\n",
    "A_{ij} ;=; \\frac{q_i^\\top k_j + q_i^\\top r^{(K)}*{i-j}}{\\sqrt{d}},\\qquad\n",
    "\\text{output}*i = \\sum_j \\alpha*{ij}\\left(v_j + r^{(V)}*{i-j}\\right).\n",
    "$$\n",
    "\n",
    "* (r^{(K)}*{r}, r^{(V)}*{r}\\in\\mathbb{R}^d) are learned for each offset (r) (often clipped to (|r|\\le R_{\\max})).\n",
    "* Efficient implementations use the **“skew” trick** to avoid building all pairwise (r=i-j) tensors explicitly (keeps (O(Ld)) memory rather than (O(L^2))).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Rotary positional embeddings (RoPE)\n",
    "\n",
    "RoPE encodes relative positions by **rotating** query/key subvectors in 2D planes. Split (q_i,k_j) into 2-D pairs and apply a rotation whose angle grows with position:\n",
    "$$\n",
    "\\widetilde{q}_i ;=; R(\\theta_i),q_i,\\qquad\n",
    "\\widetilde{k}_j ;=; R(\\theta_j),k_j,\n",
    "$$\n",
    "with (R(\\theta)) block-diagonal 2D rotations and frequencies (\\omega) geometrically spaced (like sinusoidal embeddings). The dot-product becomes\n",
    "$$\n",
    "\\widetilde{q}*i^\\top \\widetilde{k}*j\n",
    ";=;\n",
    "q_i^\\top R(\\theta_i)^\\top R(\\theta_j),k_j\n",
    ";=;\n",
    "q_i^\\top R(\\theta*{j}-\\theta*{i}),k_j,\n",
    "$$\n",
    "which depends only on the **relative** position ((j-i)). No bias tables, great extrapolation, widely used (e.g., Llama-family).\n",
    "\n",
    "A common per-pair formula for a single 2-D plane:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\\tilde q^{(2t)}_i \\ \\tilde q^{(2t+1)}_i\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\\cos\\theta^{(t)}_i & -\\sin\\theta^{(t)}_i \\ \\sin\\theta^{(t)}_i & \\cos\\theta^{(t)}_i\\end{bmatrix}\n",
    "\\begin{bmatrix}q^{(2t)}*i \\ q^{(2t+1)}*i\\end{bmatrix},\\\n",
    "\\theta^{(t)}*i &= \\frac{i}{\\omega_t},\\quad \\omega_t=\\omega*{\\min},(\\omega*{\\max}/\\omega*{\\min})^{t/(d/2-1)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5) 2-D relative positions (vision)\n",
    "\n",
    "For images (e.g., ViT/Swin windows), relative offsets decompose along height/width:\n",
    "$$\n",
    "A_{ij}\n",
    "======\n",
    "\n",
    "\\frac{q_i^\\top k_j}{\\sqrt{d}}\n",
    ";+;\n",
    "b^{(h)}*{\\Delta h(i,j)} ;+; b^{(w)}*{\\Delta w(i,j)},\n",
    "$$\n",
    "where (\\Delta h,\\Delta w) are row/column differences between patches, and (b^{(h)}, b^{(w)}) are learned tables (often small, tied per head or shared).\n",
    "\n",
    "Shaw-style content-aware 2-D also exists by learning (r^{(K)}_{\\Delta h,\\Delta w}) (factored or full).\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Causal masking & clipping\n",
    "\n",
    "For autoregressive models, only (j\\le i) are visible. Relative indices are typically **clipped**:\n",
    "$$\n",
    "r = \\mathrm{clip}(i-j,,-R_{\\max},,R_{\\max}),\n",
    "$$\n",
    "keeping parameter count bounded while still giving strong distance signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Implementation notes (fast paths)\n",
    "\n",
    "* **Bias-only** (T5/ALiBi): precompute an (L\\times L) bias (or “on-the-fly” with buckets); add to logits before softmax.\n",
    "* **Shaw-style**: compute (Q R^{(K)\\top}) once, then **skew** (a reshape+pad+slice) so offset (r) lines up with column (j).\n",
    "* **RoPE**: apply rotations to (Q,K) once per layer; everything else is standard attention.\n",
    "* **2-D**: store small ( (2H!-!1)\\times(2W!-!1)) tables (or factor into two ((2H!-!1)) and ((2W!-!1)) vectors).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) When to use what?\n",
    "\n",
    "* **RoPE**: ✅ robust extrapolation to longer context; no extra params; default for many LLMs.\n",
    "* **Additive bias (T5)**: ✅ simple, stable, cheap; works great with encoders/decoders; buckets control capacity.\n",
    "* **ALiBi**: ✅ zero learned params; excellent long-context behavior; causal models.\n",
    "* **Shaw-style**: ✅ highest expressivity (content × position); ❌ slightly heavier; good for tasks sensitive to fine relative geometry (e.g., some NMT or local vision windows).\n",
    "* **2-D bias**: ✅ ideal for images/windows (Swin); small overhead; preserves translational inductive bias.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Quick reference equations (copy-ready)\n",
    "\n",
    "**Bias-only (per head):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} + b_{\\text{rel}}(\\operatorname{bucket}(i-j)),\\quad\n",
    "\\alpha_{ij}=\\frac{e^{A_{ij}}}{\\sum_{t} e^{A_{it}}},\\quad\n",
    "\\text{out}*i=\\sum_j \\alpha*{ij} v_j.\n",
    "$$\n",
    "\n",
    "**Shaw et al.:**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j + q_i^\\top r^{(K)}*{i-j}}{\\sqrt{d}},\\qquad\n",
    "\\text{out}*i=\\sum_j \\alpha*{ij}\\left(v_j + r^{(V)}*{i-j}\\right).\n",
    "$$\n",
    "\n",
    "**ALiBi (causal):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} - m_h,(i-j),\\quad j\\le i.\n",
    "$$\n",
    "\n",
    "**RoPE:**\n",
    "$$\n",
    "\\widetilde{q}_i=R(\\theta_i)q_i,\\quad \\widetilde{k}*j=R(\\theta_j)k_j,\\quad\n",
    "A*{ij}=\\frac{\\widetilde{q}*i^\\top \\widetilde{k}*j}{\\sqrt{d}}\n",
    "=\\frac{q_i^\\top R(\\theta*{j}-\\theta*{i})k_j}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "**2-D windowed bias (vision):**\n",
    "$$\n",
    "A_{ij}=\\frac{q_i^\\top k_j}{\\sqrt{d}} + b^{(h)}*{\\Delta h(i,j)} + b^{(w)}*{\\Delta w(i,j)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Key intuitions (why it helps)\n",
    "\n",
    "* **Translation invariance**: the model learns “same-pattern, different place” naturally.\n",
    "* **Length generalization**: depends on *offsets*, not absolute indexes.\n",
    "* **Parameter efficiency**: a small table (or none with RoPE/ALiBi) replaces (O(L)) absolute embeddings.\n",
    "* **Locality bias**: buckets/linear slopes can emphasize nearby tokens—crucial for language syntax and vision neighborhoods.\n",
    "\n",
    "If you want, I can add a tiny PyTorch snippet for each variant that you can paste into a Jupyter cell and visualize the induced (L\\times L) bias matrices. ✅\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
