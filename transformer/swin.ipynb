{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3768141f-8f0c-4521-ac46-6c52785add0a",
   "metadata": {},
   "source": [
    "# **Swin Transformer**\n",
    "\n",
    "The **Swin Transformer** (Shifted Window Transformer, Liu et al., 2021) extends the **Vision Transformer (ViT)** to handle **high-resolution** and **dense prediction tasks** (e.g., detection, segmentation) efficiently — without losing its transformer flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Motivation**\n",
    "\n",
    "**ViT** treats an image as a sequence of patches and applies **global self-attention**.\n",
    "While this works well for classification, it faces key limitations:\n",
    "\n",
    "* **Quadratic complexity** in the number of patches.\n",
    "* **No local inductive bias** (poor handling of fine details).\n",
    "* **Fixed spatial resolution**, unsuitable for dense predictions.\n",
    "\n",
    "The **Swin Transformer** solves these problems by:\n",
    "\n",
    "1. Applying **local attention** inside non-overlapping windows (reducing complexity).\n",
    "2. **Shifting windows** between layers to connect across regions.\n",
    "3. Introducing **patch merging** to build a **hierarchical (multi-scale)** representation — similar to CNNs.\n",
    "\n",
    "<img src=\"images/swin_global_vs_local_attention.png\" height=\"60%\" width=\"60%\" />\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Architecture Overview**\n",
    "\n",
    "Swin Transformer follows a **hierarchical pyramid design**, much like ResNet:\n",
    "\n",
    "| Stage   | Input Resolution | Patch Operation | Output Channels | Description      |\n",
    "| ------- | ---------------- | --------------- | --------------- | ---------------- |\n",
    "| Stage 1 | 4×4 patches      | Patch Embedding | 96              | Linear embedding |\n",
    "| Stage 2 | 1/2 spatial size | Patch Merging   | 192             | Downsampling     |\n",
    "| Stage 3 | 1/4 spatial size | Patch Merging   | 384             | Downsampling     |\n",
    "| Stage 4 | 1/8 spatial size | Patch Merging   | 768             | Downsampling     |\n",
    "\n",
    "Each stage contains several **Swin Transformer Blocks**, each block consisting of:\n",
    "\n",
    "1. **W-MSA** (Window-based Multi-Head Self-Attention)\n",
    "2. **SW-MSA** (Shifted-Window Multi-Head Self-Attention)\n",
    "3. **Feed-forward MLP**\n",
    "4. **LayerNorm + Residual connections**\n",
    "\n",
    "\n",
    "<img src=\"images/swin_architecture.png\" height=\"60%\" width=\"60%\" />\n",
    "\n",
    "\n",
    "<img src=\"images/two_successive_swin_transformer_blocks.png\" height=\"30%\" width=\"30%\" />\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Architecture Variants\n",
    "\n",
    "- Swin-T: $C = 96$, $ \\text{layer numbers} =\\{2, 2, 6, 2\\}$\n",
    "- Swin-S: $C = 96$, $ \\text{layer numbers} =\\{2, 2, 18, 2\\}$\n",
    "- Swin-B: $C = 128$, $ \\text{layer numbers}= \\{2, 2, 18, 2\\}$\n",
    "- Swin-L: $C = 192$, $ \\text{layer numbers} =\\{2, 2, 18, 2\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f18ee-cf34-47cc-a7ce-19b027b43ed3",
   "metadata": {},
   "source": [
    "## **3. Step-by-Step Pipeline**\n",
    "\n",
    "Suppose you start with an image of size\n",
    "$$\n",
    "H = W = 224, \\quad C = 3\n",
    "$$\n",
    "so the input tensor is\n",
    "$$\n",
    "X \\in \\mathbb{R}^{B \\times 224 \\times 224 \\times 3}\n",
    "$$\n",
    "where (B) is the batch size.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.1. Patch Partitioning**\n",
    "\n",
    "Swin uses **patch size = 4×4**, meaning each patch covers 4×4 pixels.\n",
    "\n",
    "Number of patches per dimension:\n",
    "$$\n",
    "\\frac{224}{4} = 56\n",
    "$$\n",
    "\n",
    "So after patch partitioning, we have\n",
    "$$\n",
    "N = 56 \\times 56 = 3136 \\text{ patches.}\n",
    "$$\n",
    "\n",
    "Each patch is flattened:\n",
    "$$\n",
    "4 \\times 4 \\times 3 = 48\n",
    "$$\n",
    "values per patch.\n",
    "\n",
    "The shape becomes:\n",
    "$$\n",
    "[B, 3136, 48]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2 Linear Embedding (Projection Layer)**\n",
    "\n",
    "Each 48-dimensional flattened patch vector is projected into a higher-dimensional embedding space of **C=96** (the feature dimension of Stage 1):\n",
    "\n",
    "$$\n",
    "[B, 3136, 48] \\xrightarrow{\\text{Linear(48→96)}} [B, 3136, 96]\n",
    "$$\n",
    "\n",
    "Now each patch token is a **96-dimensional feature vector**.\n",
    "This is analogous to the **stem convolution** in CNNs.\n",
    "\n",
    "You can reshape it back to a 2D feature map:\n",
    "$$\n",
    "[B, 56, 56, 96]\n",
    "$$\n",
    "\n",
    "---\n",
    "#### **Fusing Patch Partitioning and Linear Embedding**\n",
    "In most **Swin Transformer implementations**, the **patch partitioning** and **linear embedding** are fused into **a single convolution operation** at the very beginning.\n",
    "\n",
    "Let’s see why and how.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conceptually**\n",
    "\n",
    "We said:\n",
    "\n",
    "* Partition image into non-overlapping 4×4 patches\n",
    "* Flatten each patch (4×4×3 = 48)\n",
    "* Apply a linear projection (48 → 96)\n",
    "\n",
    "That’s two separate steps conceptually.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implementation Trick**\n",
    "\n",
    "In code (e.g., in `timm`, `swin_transformer.py`), this is implemented as a **Conv2d layer**:\n",
    "\n",
    "```python\n",
    "self.patch_embed = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=96,\n",
    "    kernel_size=4,\n",
    "    stride=4\n",
    ")\n",
    "```\n",
    "\n",
    "This single convolution:\n",
    "\n",
    "* Takes a **4×4 receptive field** (kernel)\n",
    "* Moves by **stride 4** (non-overlapping patches)\n",
    "* Outputs **96 channels**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Resulting Dimensions**\n",
    "\n",
    "This convolution directly produces:\n",
    "\n",
    "$$\n",
    "[B, 96, H/4, W/4]\n",
    "$$\n",
    "\n",
    "For a 224×224 input:\n",
    "$$\n",
    "[B, 96, 56, 56]\n",
    "$$\n",
    "\n",
    "If you then flatten the spatial dimensions:\n",
    "$$\n",
    "[B, 3136, 96]\n",
    "$$\n",
    "\n",
    "— which is exactly what you’d get from explicit patch partitioning + linear embedding.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Advantages of the Convolutional Implementation**\n",
    "\n",
    "* **Efficiency:** No explicit loops or reshaping of patches.\n",
    "* **GPU-optimized:** Convolution is highly optimized for performance.\n",
    "* **Equivalent to linear projection:** Each 4×4 patch is flattened and multiplied by a weight matrix of shape [96, 48], which is exactly what convolution with kernel 4×4 does.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**\n",
    "\n",
    "| Step                               | Conceptual View                    | Implementation                     |\n",
    "| ---------------------------------- | ---------------------------------- | ---------------------------------- |\n",
    "| Patch Partition + Linear Embedding | Flatten 4×4 patches, Linear(48→96) | `Conv2d(3→96, kernel=4, stride=4)` |\n",
    "| Output Shape                       | [B, 56×56, 96]                     | [B, 96, 56, 56] (then flattened)   |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#### **3.3. Stage 1 — Swin Transformer Block(s)**\n",
    "\n",
    "Stage 1 applies **two Swin Transformer Blocks**, each consisting of:\n",
    "\n",
    "1. **W-MSA** (Window Multi-head Self-Attention)\n",
    "2. **SW-MSA** (Shifted-Window MSA)\n",
    "\n",
    "\n",
    "#### **Window-based Multi-Head Self-Attention (W-MSA)**\n",
    "\n",
    "\n",
    "Each feature map is divided into **non-overlapping windows** (e.g., 7×7 patches).\n",
    "Attention is computed **independently** within each window.\n",
    "\n",
    "This reduces computational cost from global $ O((HW)^2) $ to\n",
    "$$ O(M^2HW) $$\n",
    "where $ M $ is the window size (e.g., 7).\n",
    "\n",
    "In this setup,\n",
    "\n",
    "> **All queries within a window share the same key set.**\n",
    "\n",
    "That is, every patch inside a window can only attend to other patches inside **that same window**, not across windows.\n",
    "\n",
    "Formally, for window $ w $:\n",
    "$$\n",
    "Q^{(w)} = X^{(w)}W^Q,\\quad\n",
    "K^{(w)} = X^{(w)}W^K,\\quad\n",
    "V^{(w)} = X^{(w)}W^V\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{Attention}^{(w)} = \\text{Softmax}\\left(\\frac{Q^{(w)}{K^{(w)}}^T}{\\sqrt{d}}\\right)V^{(w)}\n",
    "$$\n",
    "\n",
    "This local attention structure introduces **spatial locality** and scales efficiently with image size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d2a84-90ef-4d4c-b78f-0357462c02c9",
   "metadata": {},
   "source": [
    "#### **Shifted-Window Multi-Head Self-Attention (SW-MSA)**\n",
    "\n",
    "\n",
    "In the **next block**, windows are **shifted by half the window size** (e.g., 3 pixels if ( M=7 )).\n",
    "This shift allows patches that were previously in separate windows to now fall in the same window.\n",
    "\n",
    "Thus, alternating between **W-MSA** and **SW-MSA** layers enables:\n",
    "\n",
    "* Local attention within windows.\n",
    "* Cross-window communication.\n",
    "* Gradual expansion of the receptive field — achieving a global view over multiple layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832e444-a39f-48cb-a64e-82b39126eb3d",
   "metadata": {},
   "source": [
    "\n",
    "Both operate **within local windows**, not globally.\n",
    "\n",
    "#### Window setup:\n",
    "\n",
    "Each window covers **7×7 patches**, so each window has\n",
    "$$\n",
    "49 \\text{ tokens.}\n",
    "$$\n",
    "\n",
    "Since the feature map is (56×56):\n",
    "$$\n",
    "\\frac{56}{7} = 8 \\text{ windows per side} \\Rightarrow 8×8 = 64 \\text{ windows total.}\n",
    "$$\n",
    "\n",
    "So within Stage 1:\n",
    "\n",
    "* Input: [B, 56, 56, 96]\n",
    "* Divide into windows of [7, 7, 96]\n",
    "* Self-attention is computed **inside each 7×7 window**\n",
    "* The output tokens are reassembled back to [B, 56, 56, 96]\n",
    "* The second block uses **shifted windows** (by 3 patches = 7//2) to mix information across boundaries\n",
    "\n",
    "After these two blocks, Stage 1 outputs the same resolution:\n",
    "$$\n",
    "[B, 56, 56, 96]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.4. Patch Merging (between Stage 1 → Stage 2)**\n",
    "\n",
    "Before entering Stage 2, the resolution is halved and channel depth doubles:\n",
    "\n",
    "* Merge each **2×2 patch group**\n",
    "* So new spatial size:\n",
    "  $$\n",
    "  56/2 = 28 \\Rightarrow [B, 28, 28, 192]\n",
    "  $$\n",
    "* (Each merge concatenates 4 neighboring patch vectors → 4×96 = 384 → linear → 192)\n",
    "\n",
    "\n",
    "\n",
    "To reduce spatial resolution and increase semantic richness, Swin introduces **Patch Merging**, analogous to CNN downsampling.\n",
    "\n",
    "Given:\n",
    "$$ X \\in \\mathbb{R}^{H \\times W \\times C} $$\n",
    "\n",
    "1. **Group 2×2 neighboring patches:**\n",
    "   Each group of four patches is concatenated:\n",
    "   $$\n",
    "   [x_{00}, x_{01}, x_{10}, x_{11}] \\in \\mathbb{R}^{4C}\n",
    "   $$\n",
    "\n",
    "\n",
    "<img src=\"images/patch_merging1.png\" height=\"40%\" width=\"40%\" />\n",
    "\n",
    "\n",
    "2. **Linear Projection:**\n",
    "   Reduce dimensionality from $ 4C $ → $ 2C $:\n",
    "   $$\n",
    "   X' = \\text{Linear}(\\text{Concat}_{2\\times2}(X)) \\in \\mathbb{R}^{\\frac{H}{2} \\times \\frac{W}{2} \\times 2C}\n",
    "   $$\n",
    "\n",
    "<img src=\"images/patch_merging2.png\" height=\"40%\" width=\"40%\" />\n",
    "\n",
    "\n",
    "Thus:\n",
    "\n",
    "* Spatial size halves.\n",
    "* Channel dimension doubles.\n",
    "\n",
    "After several patch-merging steps, the model forms a **feature pyramid** where deeper stages capture more abstract semantics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfae8b0-2101-42e2-9672-e6beac50b992",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Inside a Swin Transformer Block\n",
    "\n",
    "Given an input $ X \\in \\mathbb{R}^{H \\times W \\times C} $:\n",
    "\n",
    "1. **LayerNorm:**\n",
    "   $$\n",
    "   \\hat{X} = \\text{LN}(X)\n",
    "   $$\n",
    "2. **(Shifted) Window Attention:**\n",
    "   $$\n",
    "   X' = X + \\text{WindowAttention}(\\hat{X})\n",
    "   $$\n",
    "3. **Feed-forward (MLP) with residual:**\n",
    "   $$\n",
    "   X'' = X' + \\text{MLP}(\\text{LN}(X'))\n",
    "   $$\n",
    "4. **MLP structure:**\n",
    "   $$\n",
    "   \\text{MLP}(x) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(x)))\n",
    "   $$\n",
    "   where hidden dimension = $ 4C $.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Multi-Head Self-Attention Inside a Window\n",
    "\n",
    "Within each window:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}} + B\\right)V\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $ B $ — learnable **relative position bias** for spatial awareness.\n",
    "* $ Q, K, V $ — derived from the same window.\n",
    "* Computation is efficient since windows are small.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Hierarchical Output Example\n",
    "\n",
    "For a 224×224 input image, Swin Transformer produces multi-scale features:\n",
    "\n",
    "| Stage | Resolution | Channels |\n",
    "| ----- | ---------- | -------- |\n",
    "| 1     | 56×56      | 96       |\n",
    "| 2     | 28×28      | 192      |\n",
    "| 3     | 14×14      | 384      |\n",
    "| 4     | 7×7        | 768      |\n",
    "\n",
    "These outputs form a **feature pyramid** — ideal for:\n",
    "\n",
    "* **Classification:** via global average pooling.\n",
    "* **Detection/Segmentation:** as inputs to FPNs (e.g., Mask R-CNN).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Intuitive Summary\n",
    "\n",
    "* **ViT**: global attention, single-scale, quadratic complexity.\n",
    "* **Swin**: local attention, hierarchical, linear complexity.\n",
    "* **Shifted windows**: enable cross-region communication.\n",
    "* **Patch merging**: provides multi-scale features like CNNs.\n",
    "\n",
    " **In one sentence:**\n",
    "\n",
    "> Swin Transformer limits self-attention to local windows (shared key sets), shifts them between layers for global context, and builds a hierarchical multi-scale representation through patch merging — combining the strengths of CNNs and Transformers.\n",
    "\n",
    "---\n",
    "## **Summary Table**\n",
    "\n",
    "| Step                      | Operation          | Output Shape     | Notes                             |\n",
    "| ------------------------- | ------------------ | ---------------- | --------------------------------- |\n",
    "| Input                     | RGB image          | [B, 224, 224, 3] | Raw pixels                        |\n",
    "| Patch Partition           | 4×4 blocks         | [B, 56×56, 48]   | Flatten each 4×4×3 patch          |\n",
    "| Linear Embedding          | Linear(48→96)      | [B, 3136, 96]    | Project to feature dim 96         |\n",
    "| Reshape                   | —                  | [B, 56, 56, 96]  | Spatial 2D form                   |\n",
    "| Stage 1 (2 × Swin Blocks) | W-MSA + SW-MSA     | [B, 56, 56, 96]  | Local attention windows 7×7       |\n",
    "| Patch Merging             | 2×2 merge + Linear | [B, 28, 28, 192] | Halve resolution, double channels |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f736a-4924-41dd-a55a-3fadd471b91e",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=qUSPbHE3OeU), [2](https://www.youtube.com/watch?v=z_8lajPxGQo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
