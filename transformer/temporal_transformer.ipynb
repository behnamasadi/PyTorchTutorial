{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e227fabd-7fc0-4a33-bd05-d3fe06d0982c",
   "metadata": {},
   "source": [
    "# **Temporal Transformer**\n",
    "## 1. Motivation\n",
    "\n",
    "The **Temporal Transformer** extends the **Vision Transformer (ViT)** or **Spatial Transformer** idea into the **time domain**.\n",
    "If ViT handles *spatial* relationships between image patches in a single frame,\n",
    "the Temporal Transformer handles *temporal* relationships between frames in a **video sequence**.\n",
    "\n",
    "In a video, each frame has:\n",
    "\n",
    "* **Spatial features** — what’s inside the frame (objects, shapes, textures)\n",
    "* **Temporal features** — how these change over time (motion, actions)\n",
    "\n",
    "A Temporal Transformer captures **how things evolve over time**, similar to how ViT captures how patches relate within space.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Basic Idea\n",
    "\n",
    "Suppose you extract features from each frame using a CNN or ViT encoder.\n",
    "Then you have a sequence of tokens representing time:\n",
    "\n",
    "$$\n",
    "X = [x_1, x_2, x_3, \\ldots, x_T], \\quad x_t \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "Each $x_t$ encodes the spatial information of frame $t$.\n",
    "\n",
    "The **Temporal Transformer** applies **self-attention over time**, learning how each frame relates to others.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Temporal Self-Attention\n",
    "\n",
    "Just like standard attention, we compute queries, keys, and values:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "Then temporal attention is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Here, the attention matrix is of size $T \\times T$ — each frame attends to all others.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Combining with Spatial Transformers\n",
    "\n",
    "There are **two main architectures** for handling video with transformers:\n",
    "\n",
    "### (a) Factorized (Space + Time separately)\n",
    "\n",
    "* **Step 1:** Apply a **spatial transformer** on each frame independently.\n",
    "  This captures spatial relationships.\n",
    "* **Step 2:** Apply a **temporal transformer** across the resulting frame embeddings.\n",
    "  This captures motion and temporal dynamics.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "X' = \\text{SpatialTransformer}(X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = \\text{TemporalTransformer}(X')\n",
    "$$\n",
    "\n",
    "Example: **TimeSformer** (Bertasius et al., 2021)\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Joint Space–Time Attention\n",
    "\n",
    "The transformer attends jointly to space and time dimensions using 3D tokens.\n",
    "This is more expensive but captures both simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Temporal Positional Encoding\n",
    "\n",
    "Just as ViT adds **positional embeddings** to patches in space,\n",
    "Temporal Transformers add **temporal embeddings** to indicate *when* each frame occurs.\n",
    "\n",
    "$$\n",
    "X_t = X_t + P_t\n",
    "$$\n",
    "\n",
    "where $P_t$ is a learned vector encoding the frame index.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example: TimeSformer (2021)\n",
    "\n",
    "**TimeSformer** uses divided space–time attention:\n",
    "\n",
    "1. Split the video into **T frames**, each into **N patches**.\n",
    "2. Flatten them into tokens of size $(T \\times N, D)$.\n",
    "3. In each block:\n",
    "\n",
    "   * Apply *temporal attention* among the same spatial patch across frames.\n",
    "   * Then apply *spatial attention* within each frame.\n",
    "\n",
    "This factorization reduces computation from quadratic in $(T \\times N)$\n",
    "to linear in $T$ and $N$ separately.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Applications\n",
    "\n",
    "- ✅ **Action recognition**\n",
    "- ✅ **Video classification**\n",
    "- ✅ **Motion understanding**\n",
    "- ✅ **Video captioning**\n",
    "- ✅ **Dynamic NeRFs / Temporal 3D Reconstruction**\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Minimal PyTorch-style Pseudocode\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [T, B, D]\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* $T$: number of frames (time steps)\n",
    "* $B$: batch size\n",
    "* $D$: embedding dimension\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a4675-3307-41d3-9fc1-1f159d937fe8",
   "metadata": {},
   "source": [
    "## Comparing **Spatial**, **Temporal**, and **Spatio-Temporal** \n",
    "\n",
    "\n",
    "A **Temporal Transformer** fits into the bigger picture of **video transformers**.\n",
    "We’ll walk through 3 levels:\n",
    "\n",
    "1. **Spatial Transformer** (within each frame)\n",
    "2. **Temporal Transformer** (across frames)\n",
    "3. **Spatio-Temporal Transformer** (joint space+time attention)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Toy Video Setup\n",
    "\n",
    "Assume a video of **4 frames** (T = 4).\n",
    "Each frame has **2 × 2 patches**, so **N = 4 patches per frame**.\n",
    "Each patch is embedded into a D-dimensional vector (say D = 3 for simplicity).\n",
    "\n",
    "So the total tokens are:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{T \\times N \\times D} = \\mathbb{R}^{4 \\times 4 \\times 3}\n",
    "$$\n",
    "\n",
    "That means:\n",
    "\n",
    "| Frame | Patch 0 | Patch 1 | Patch 2 | Patch 3 |\n",
    "| :---- | :------ | :------ | :------ | :------ |\n",
    "| F₁    | x₁₁     | x₁₂     | x₁₃     | x₁₄     |\n",
    "| F₂    | x₂₁     | x₂₂     | x₂₃     | x₂₄     |\n",
    "| F₃    | x₃₁     | x₃₂     | x₃₃     | x₃₄     |\n",
    "| F₄    | x₄₁     | x₄₂     | x₄₃     | x₄₄     |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Spatial Transformer (per-frame attention)\n",
    "\n",
    "In each frame, we look **within** that frame only:\n",
    "\n",
    "$$\n",
    "\\text{SpatialAttention}(x_t) = \\text{softmax}\\left(\\frac{Q_t K_t^\\top}{\\sqrt{d_k}}\\right)V_t\n",
    "$$\n",
    "\n",
    "where each $x_t \\in \\mathbb{R}^{N \\times D}$.\n",
    "\n",
    "Each frame’s patches attend to one another — e.g., patch 1 of F₁ attends to all 4 patches of F₁.\n",
    "\n",
    "After applying this independently to all frames, we obtain:\n",
    "\n",
    "$$\n",
    "X' = [x'_1, x'_2, x'_3, x'_4]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "```\n",
    "Frame 1: [●───●───●───●]   → Spatial attention among its 4 patches\n",
    "Frame 2: [●───●───●───●]   → Spatial attention\n",
    "Frame 3: [●───●───●───●]\n",
    "Frame 4: [●───●───●───●]\n",
    "```\n",
    "\n",
    "Each row is processed separately.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Temporal Transformer (across frames)\n",
    "\n",
    "Now we look **across frames** for each patch location.\n",
    "For example, all patch-1 tokens across time:\n",
    "\n",
    "$$\n",
    "[x'_1(1), x'_2(1), x'_3(1), x'_4(1)]\n",
    "$$\n",
    "\n",
    "This sequence tells how patch 1 evolves over time (motion, brightness, etc.).\n",
    "\n",
    "Temporal self-attention:\n",
    "\n",
    "$$\n",
    "\\text{TemporalAttention}(x'(i)) =\n",
    "\\text{softmax}\\left(\\frac{Q_i K_i^\\top}{\\sqrt{d_k}}\\right)V_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "```\n",
    "Patch 1 across time: ●───●───●───●  (motion of same spatial location)\n",
    "Patch 2 across time: ●───●───●───●\n",
    "Patch 3 across time: ●───●───●───●\n",
    "Patch 4 across time: ●───●───●───●\n",
    "```\n",
    "\n",
    "Each **column** is processed separately.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Combined Space–Time Pipeline\n",
    "\n",
    "Putting it together (like in **TimeSformer**):\n",
    "\n",
    "```\n",
    "[ Video Frames ]\n",
    "   ↓\n",
    "Patch embedding (T × N × D)\n",
    "   ↓\n",
    "Spatial Transformer  → captures spatial structure\n",
    "   ↓\n",
    "Temporal Transformer → captures motion / dynamics\n",
    "   ↓\n",
    "Classification / Prediction Head\n",
    "```\n",
    "\n",
    "This factorization (space first, then time) reduces cost from\n",
    "$O((T·N)^2)$ to $O(T^2 + N^2)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. PyTorch-like Illustration\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniSpaceTimeTransformer(nn.Module):\n",
    "    def __init__(self, dim=3, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.spatial = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.temporal = nn.MultiheadAttention(dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, N, D]\n",
    "        B, T, N, D = x.shape\n",
    "        # --- Spatial attention (per frame) ---\n",
    "        spatial_out = []\n",
    "        for t in range(T):\n",
    "            xt = x[:, t]            # [B, N, D]\n",
    "            yt, _ = self.spatial(xt, xt, xt)\n",
    "            spatial_out.append(yt)\n",
    "        x_spatial = torch.stack(spatial_out, dim=1)  # [B, T, N, D]\n",
    "\n",
    "        # --- Temporal attention (per patch index) ---\n",
    "        temporal_out = []\n",
    "        for n in range(N):\n",
    "            xn = x_spatial[:, :, n]                 # [B, T, D]\n",
    "            yn, _ = self.temporal(xn, xn, xn)\n",
    "            temporal_out.append(yn)\n",
    "        x_temporal = torch.stack(temporal_out, dim=2)  # [B, T, N, D]\n",
    "        return x_temporal\n",
    "```\n",
    "\n",
    "This shows the same factorization logic:\n",
    "\n",
    "* First loop over frames → spatial attention.\n",
    "* Then loop over patches → temporal attention.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da519ab-a317-4124-ae80-d9c96f3b454c",
   "metadata": {},
   "source": [
    "Let’s dive into **MiniSpaceTimeTransformer** step by step,\n",
    "\n",
    "We’ll go through:\n",
    "\n",
    "1. The **goal** of the model\n",
    "2. **Input–output** shapes\n",
    "3. The **forward pass logic** (why and how each line works)\n",
    "4. How this approximates real **video transformers** like **TimeSformer**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Goal of the Model\n",
    "\n",
    "The goal of this toy model is to show how to process a **video** represented as a sequence of **frames** (each containing patch embeddings) using **two stages**:\n",
    "\n",
    "1. **Spatial attention** — learn relationships *within each frame* (e.g., between patches).\n",
    "2. **Temporal attention** — learn relationships *across frames* (e.g., motion through time).\n",
    "\n",
    "This is the essence of a **Temporal Transformer** combined with **Spatial Transformer**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Input and Output\n",
    "\n",
    "We assume that before entering this transformer, the video has already been divided into patches and projected into embeddings.\n",
    "\n",
    "So the input tensor:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^{B \\times T \\times N \\times D}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* **B** = batch size (number of videos processed at once)\n",
    "* **T** = number of frames per video\n",
    "* **N** = number of patches per frame\n",
    "* **D** = feature dimension (embedding size of each patch)\n",
    "\n",
    "**Output** has the same shape:\n",
    "$$\n",
    "y \\in \\mathbb{R}^{B \\times T \\times N \\times D}\n",
    "$$\n",
    "but now each token (patch embedding) has been refined by both spatial and temporal attention.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Code (with full explanation)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniSpaceTimeTransformer(nn.Module):\n",
    "    def __init__(self, dim=3, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.spatial = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.temporal = nn.MultiheadAttention(dim, num_heads)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* We define a PyTorch module.\n",
    "* It has **two attention blocks**:\n",
    "\n",
    "  * `self.spatial`: handles relationships between patches *within* a frame.\n",
    "  * `self.temporal`: handles relationships between frames *over time*.\n",
    "* `dim` is the token embedding size (e.g., 3 in our toy example).\n",
    "* `num_heads` is the number of attention heads.\n",
    "\n",
    "These attention layers use the standard Transformer **scaled dot-product attention** mechanism:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, N, D]\n",
    "        B, T, N, D = x.shape\n",
    "```\n",
    "\n",
    "This extracts shape dimensions from the input tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Spatial Attention\n",
    "\n",
    "```python\n",
    "        # --- Spatial attention (per frame) ---\n",
    "        spatial_out = []\n",
    "        for t in range(T):\n",
    "            xt = x[:, t]            # [B, N, D]\n",
    "            yt, _ = self.spatial(xt, xt, xt)\n",
    "            spatial_out.append(yt)\n",
    "        x_spatial = torch.stack(spatial_out, dim=1)  # [B, T, N, D]\n",
    "```\n",
    "\n",
    "#### Explanation in detail:\n",
    "\n",
    "1. We iterate over each **frame** (time index `t`).\n",
    "2. Extract all **patches** for that frame:\n",
    "\n",
    "   * `x[:, t]` gives a tensor of shape `[B, N, D]` (for that time step).\n",
    "3. Feed it into `nn.MultiheadAttention`:\n",
    "\n",
    "   * Since `nn.MultiheadAttention` expects `[sequence_length, batch_size, dim]`, in practice you’d often permute dimensions.\n",
    "     Here we simplify for clarity (PyTorch can broadcast automatically if needed).\n",
    "4. The attention computes **how patches within that frame attend to each other**, returning:\n",
    "\n",
    "   * `yt`: the new embedding for each patch (after attending to other patches in the same frame).\n",
    "5. Append this processed frame to a list.\n",
    "6. Stack all processed frames back along the time dimension to form:\n",
    "   $$ X' \\in \\mathbb{R}^{B \\times T \\times N \\times D} $$\n",
    "\n",
    "At this point, each frame’s patches know about their spatial context,\n",
    "but not yet about how frames relate over time.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Temporal Attention\n",
    "\n",
    "```python\n",
    "        # --- Temporal attention (per patch index) ---\n",
    "        temporal_out = []\n",
    "        for n in range(N):\n",
    "            xn = x_spatial[:, :, n]                 # [B, T, D]\n",
    "            yn, _ = self.temporal(xn, xn, xn)\n",
    "            temporal_out.append(yn)\n",
    "        x_temporal = torch.stack(temporal_out, dim=2)  # [B, T, N, D]\n",
    "        return x_temporal\n",
    "```\n",
    "\n",
    "#### Explanation in detail:\n",
    "\n",
    "1. Now we iterate over **patch index** `n`.\n",
    "2. Extract that same patch across all frames:\n",
    "\n",
    "   * `x_spatial[:, :, n]` → `[B, T, D]`\n",
    "     (e.g., patch 0 from frame 1, patch 0 from frame 2, etc.)\n",
    "3. Feed it into the **temporal attention** layer.\n",
    "\n",
    "   * This learns how this patch changes **across time** (motion, brightness, movement).\n",
    "4. Append this processed sequence to a list.\n",
    "5. After processing all patches, stack them again to get:\n",
    "   $$ Y \\in \\mathbb{R}^{B \\times T \\times N \\times D} $$\n",
    "\n",
    "At this stage, every token (patch at frame *t*) has learned:\n",
    "\n",
    "* Spatial context (from within its own frame)\n",
    "* Temporal context (from the same patch location across time)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How it Works Conceptually\n",
    "\n",
    "### Before the model:\n",
    "\n",
    "* Each token only knows its *own value* — no interaction between patches or frames.\n",
    "\n",
    "### After spatial attention:\n",
    "\n",
    "* Each patch embedding knows how it relates to other patches **in the same frame**.\n",
    "  For instance, a patch containing part of a “hand” attends to a nearby “arm” patch.\n",
    "\n",
    "### After temporal attention:\n",
    "\n",
    "* Each patch embedding learns **how it changes over time**.\n",
    "  For example, that same “hand” patch learns that it moves upward between frame 2 and frame 4.\n",
    "\n",
    "Together, the model builds a **rich spatio-temporal representation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Complexity and Relation to Real Models\n",
    "\n",
    "In full models like **TimeSformer** or **ViViT**:\n",
    "\n",
    "* Spatial and temporal blocks are stacked in multiple layers.\n",
    "* Tokens are projected into higher dimensions (e.g., D = 768).\n",
    "* Positional encodings are added for both spatial and temporal positions.\n",
    "* Classification is done by a [CLS] token that aggregates all context.\n",
    "\n",
    "This toy model captures the same *core mechanism*:\n",
    "$$\n",
    "\\text{Video Representation} = f_{\\text{Temporal}}\\big(f_{\\text{Spatial}}(X)\\big)\n",
    "$$\n",
    "\n",
    "but in a minimal, educational way.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "| Step | Transformer Type | Input Shape         | Learns                | Effect                 |\n",
    "| ---- | ---------------- | ------------------- | --------------------- | ---------------------- |\n",
    "| 1    | Spatial          | [B, N, D] per frame | Patch–Patch relations | Scene layout per frame |\n",
    "| 2    | Temporal         | [B, T, D] per patch | Frame–Frame relations | Motion / dynamics      |\n",
    "\n",
    "So each token evolves as:\n",
    "\n",
    "$$\n",
    "x_{t,n}^{(out)} = f_{temp}\\big(f_{spatial}(x_{t,n}^{(in)})\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e33b9c-b717-4471-b872-7d966f8338de",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Spatio-Temporal Transformer (joint attention)\n",
    "\n",
    "If we **don’t** separate space and time, we flatten everything:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{(T \\times N) \\times D}\n",
    "$$\n",
    "\n",
    "and perform attention directly:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Now each patch in each frame can attend to **every other patch in every frame** —\n",
    "richer but more expensive ($O((T·N)^2)$).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary Table\n",
    "\n",
    "| Model Type                | Attention     | Complexity | Example              |\n",
    "| ------------------------- | ------------- | ---------- | -------------------- |\n",
    "| Spatial only              | Within frame  | O(N²)      | ViT                  |\n",
    "| Temporal only             | Across frames | O(T²)      | Temporal Transformer |\n",
    "| Factorized (space → time) | Separate      | O(N² + T²) | TimeSformer          |\n",
    "| Joint space-time          | Global        | O((TN)²)   | ViViT, Video Swin    |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Intuition Recap\n",
    "\n",
    "* **Spatial transformer:** What is happening *in* each frame?\n",
    "* **Temporal transformer:** How does it *change over time*?\n",
    "* **Spatio-temporal:** Combines both in one attention mechanism.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
