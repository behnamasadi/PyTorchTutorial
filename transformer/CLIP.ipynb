{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9373146c-9e01-4607-9eb5-3459d69e4b1f",
   "metadata": {},
   "source": [
    "## **CLIP model (Contrastive Language‚ÄìImage Pretraining)**\n",
    "\n",
    "The **CLIP model (Contrastive Language‚ÄìImage Pretraining)**, introduced by **OpenAI (Radford et al., 2021)**, is a **multimodal neural network** that learns to connect **images and text** through **contrastive learning** ‚Äî essentially teaching the model to *understand* which image matches which caption.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Motivation\n",
    "\n",
    "Before CLIP, most vision models (like ResNet, ViT) were trained **supervised** ‚Äî e.g., ImageNet classification with 1000 fixed labels.\n",
    "But these models could not generalize to *new* concepts or tasks.\n",
    "\n",
    "CLIP instead learns from **natural language supervision** ‚Äî hundreds of millions of (image, text) pairs collected from the web ‚Äî so it learns a *broad* visual-language understanding without task-specific labels.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Idea\n",
    "\n",
    "The idea is simple but powerful:\n",
    "\n",
    "> Learn a joint embedding space where **matching image‚Äìtext pairs** have **high similarity**, and **non-matching pairs** have **low similarity**.\n",
    "\n",
    "That is, if we encode an image and its caption, their embeddings should be close; random combinations should be far apart.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Architecture\n",
    "\n",
    "CLIP consists of **two encoders** trained jointly:\n",
    "\n",
    "| Encoder           | Example Architecture     | Input | Output       |\n",
    "| ----------------- | ------------------------ | ----- | ------------ |\n",
    "| **Image Encoder** | ResNet-50 / ViT-B/32     | Image | 512-D vector |\n",
    "| **Text Encoder**  | Transformer (like GPT-2) | Text  | 512-D vector |\n",
    "\n",
    "Both encoders project to the **same latent space** via learned linear projections.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1. Image Encoder\n",
    "\n",
    "For example, if using ViT-B/32:\n",
    "\n",
    "* The image is divided into 32√ó32 patches.\n",
    "* Each patch becomes a token.\n",
    "* Tokens are passed through the Vision Transformer.\n",
    "* The final [CLS] token gives a **feature vector** of dimension `D=512`.\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = f_{\\text{img}}(I) \\in \\mathbb{R}^{512}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2. Text Encoder\n",
    "\n",
    "The text is tokenized (e.g., with BPE), and a Transformer encodes it.\n",
    "The last token‚Äôs hidden state (or a [EOS] token) gives the text representation:\n",
    "\n",
    "$$\n",
    "\\mathbf{t} = f_{\\text{text}}(T) \\in \\mathbb{R}^{512}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aecc5f-3a64-4aa8-89ac-838950a578c9",
   "metadata": {},
   "source": [
    "Excellent and very insightful question ‚Äî and you‚Äôre absolutely right:\n",
    "in the **standard ResNet-50**, the final ‚Äúhead‚Äù is a **fully connected (FC)** layer that outputs **class logits**, e.g. 1000 classes for ImageNet.\n",
    "\n",
    "But in **CLIP**, they *modify* the ResNet so that instead of predicting fixed labels, it produces a **feature embedding vector** (e.g., 512-D).\n",
    "\n",
    "Let‚Äôs go step-by-step through what happens.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Standard ResNet-50 Recap\n",
    "\n",
    "A normal ResNet-50 has this structure:\n",
    "\n",
    "```\n",
    "Input (224√ó224)\n",
    "‚Üí Conv1 + BN + ReLU + MaxPool\n",
    "‚Üí Conv2_x\n",
    "‚Üí Conv3_x\n",
    "‚Üí Conv4_x\n",
    "‚Üí Conv5_x\n",
    "‚Üí Global Average Pool (GAP)\n",
    "‚Üí Fully Connected layer (1000 classes)\n",
    "```\n",
    "\n",
    "The last two layers are:\n",
    "\n",
    "1. **Global average pooling** over the spatial dimension ‚Üí shape becomes `[B, 2048]`\n",
    "2. **FC layer** ‚Üí `[B, 1000]` logits\n",
    "\n",
    "So the last layer is specific to **classification**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How CLIP Uses ResNet-50\n",
    "\n",
    "CLIP **removes the classification head** and replaces it with a **projection head** that maps features to a shared text‚Äìimage embedding space.\n",
    "\n",
    "The pipeline becomes:\n",
    "\n",
    "$$\n",
    "\\text{Image} \\xrightarrow{f_{\\text{ResNet}}} \\mathbf{h}_{\\text{img}} \\in \\mathbb{R}^{2048} \\xrightarrow{W_p} \\mathbf{v} \\in \\mathbb{R}^{512}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* ( f_{\\text{ResNet}} ) is the convolutional body up to the global pooling layer.\n",
    "* ( W_p \\in \\mathbb{R}^{2048 \\times 512} ) is a **learned linear projection**.\n",
    "* The output ( \\mathbf{v} ) is **L2-normalized**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Architectural Modifications in CLIP-ResNet-50\n",
    "\n",
    "OpenAI didn‚Äôt use the *vanilla* ResNet-50 directly ‚Äî they made a few small but important changes to improve alignment with the text encoder.\n",
    "\n",
    "| Change                                     | Description                                                                                                  | Why                                                                                    |\n",
    "| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------- |\n",
    "| **Replace AvgPool with Attention Pooling** | Instead of global average pooling, CLIP uses a **multi-head attention pooling** layer over spatial features. | This allows the model to **learn spatial weighting** (like a soft ‚ÄúCLS‚Äù token in ViT). |\n",
    "| **Projection layer**                       | A new linear layer projects 2048‚Üí512                                                                         | To match text embedding dimension.                                                     |\n",
    "| **L2 normalization**                       | Normalize embeddings before computing cosine similarity                                                      | Makes the contrastive loss stable.                                                     |\n",
    "| **No classifier head**                     | The ImageNet classifier is removed                                                                           | CLIP doesn‚Äôt predict classes directly.                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. In Pseudocode\n",
    "\n",
    "Here‚Äôs a simplified PyTorch-style pseudocode:\n",
    "\n",
    "```python\n",
    "class CLIPImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet50\", embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet50(pretrained=False)\n",
    "        modules = list(resnet.children())[:-2]  # remove avgpool + fc\n",
    "        self.conv_body = nn.Sequential(*modules)\n",
    "        self.attn_pool = AttentionPool2d(spatial_dim=7, embed_dim=2048, num_heads=32)\n",
    "        self.proj = nn.Linear(2048, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_body(x)         # [B, 2048, 7, 7]\n",
    "        x = self.attn_pool(x)         # [B, 2048]\n",
    "        x = self.proj(x)              # [B, 512]\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        return x\n",
    "```\n",
    "\n",
    "‚úÖ Here, the **attention pooling** layer replaces the average pooling:\n",
    "\n",
    "$$\n",
    "\\text{AttentionPool}(X) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d}}\\right)V\n",
    "$$\n",
    "\n",
    "It learns where to ‚Äúlook‚Äù spatially in the feature map when forming the final embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why Not Use the Classification Head?\n",
    "\n",
    "Because CLIP‚Äôs goal is **cross-modal embedding**, not classification.\n",
    "\n",
    "* The ResNet backbone extracts *visual semantics*.\n",
    "* The projection head aligns them to the **text encoder‚Äôs semantic space**.\n",
    "* During training, gradients from the **contrastive loss** update both the image and text encoders so that ‚Äúa dog‚Äù and its picture get close in the shared embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualization of the CLIP-ResNet Flow\n",
    "\n",
    "```\n",
    "Image (224√ó224)\n",
    "‚Üì\n",
    "ResNet Backbone (conv1‚Äìconv5)\n",
    "‚Üì\n",
    "Feature Map (7√ó7√ó2048)\n",
    "‚Üì\n",
    "Attention Pooling (learned global descriptor)\n",
    "‚Üì\n",
    "Linear Projection (2048‚Üí512)\n",
    "‚Üì\n",
    "L2 Normalization\n",
    "‚Üì\n",
    "Image Embedding (512-D)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Equation Summary\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h}*{\\text{img}} &= f*{\\text{ResNet}}(I) \\\n",
    "\\mathbf{v} &= \\frac{W_p \\mathbf{h}*{\\text{img}}}{| W_p \\mathbf{h}*{\\text{img}} |}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The embedding ( \\mathbf{v} ) is then used in the **contrastive similarity** computation with text embeddings ( \\mathbf{t} ).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also show how the **attention pooling** at the end of CLIP‚Äôs ResNet-50 actually works (mathematically and in PyTorch)? It‚Äôs one of the most elegant differences compared to the vanilla ResNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f22b3f-358b-443f-baa2-733803fea3b6",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Contrastive Training Objective\n",
    "\n",
    "The training set contains batches of **N image‚Äìtext pairs**.\n",
    "\n",
    "For each pair ( (I_i, T_i) ):\n",
    "\n",
    "1. Encode them into embeddings ( \\mathbf{v}_i ) and ( \\mathbf{t}_i ).\n",
    "2. Normalize them to unit length.\n",
    "3. Compute a similarity matrix ( S \\in \\mathbb{R}^{N \\times N} ):\n",
    "\n",
    "$$\n",
    "S_{ij} = \\tau \\cdot (\\mathbf{v}_i^\\top \\mathbf{t}_j)\n",
    "$$\n",
    "\n",
    "where ( \\tau ) is a learnable temperature parameter.\n",
    "\n",
    "4. The model is trained using **symmetric cross-entropy loss**:\n",
    "\n",
    "   * For each image, the correct caption should be most similar.\n",
    "   * For each caption, the correct image should be most similar.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}*{\\text{image}} = -\\frac{1}{N} \\sum_i \\log \\frac{\\exp(S*{ii})}{\\sum_j \\exp(S_{ij})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}*{\\text{text}} = -\\frac{1}{N} \\sum_i \\log \\frac{\\exp(S*{ii})}{\\sum_j \\exp(S_{ji})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2}(\\mathcal{L}*{\\text{image}} + \\mathcal{L}*{\\text{text}})\n",
    "$$\n",
    "\n",
    "‚úÖ This encourages **matching pairs** (diagonal of S) to have high cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Zero-Shot Inference\n",
    "\n",
    "Once trained, CLIP can perform **zero-shot classification** without explicit retraining:\n",
    "\n",
    "1. Define class names (e.g., ‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúcar‚Äù).\n",
    "2. Convert them to prompts:\n",
    "   *‚Äúa photo of a cat‚Äù*, *‚Äúa photo of a dog‚Äù*, ‚Ä¶\n",
    "3. Encode all prompts with the **text encoder**.\n",
    "4. Encode the image with the **image encoder**.\n",
    "5. Compute cosine similarity between the image embedding and all text embeddings.\n",
    "6. Pick the most similar ‚Äî that‚Äôs the predicted label.\n",
    "\n",
    "CLIP effectively transforms **natural language descriptions into classifiers**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary of Training and Usage\n",
    "\n",
    "| Stage       | Data                     | Objective                 | Output                 |\n",
    "| ----------- | ------------------------ | ------------------------- | ---------------------- |\n",
    "| Pretraining | 400M (image, text) pairs | Contrastive loss          | Joint embedding space  |\n",
    "| Inference   | Any image/text           | Similarity-based matching | Zero-shot or retrieval |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Applications\n",
    "\n",
    "* **Zero-shot image classification**\n",
    "* **Image‚Äìtext retrieval**\n",
    "* **Text-guided image generation** (used in **DALL¬∑E**, **Stable Diffusion**, etc.)\n",
    "* **Vision‚Äìlanguage understanding** foundation (used in **BLIP**, **CLIPSeg**, **ALIGN**, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Intuition\n",
    "\n",
    "* Instead of *predicting a label*, CLIP learns *which text describes this image best*.\n",
    "* Language provides a **rich supervision signal** ‚Äî it covers semantics much broader than fixed class labels.\n",
    "* Because it learns a joint embedding space, it generalizes across **modalities**, **tasks**, and **domains**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ec3fd-e8da-4113-b236-d382d94004d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7fb2c96-224f-47f4-ab51-f7f8fb2a1a12",
   "metadata": {},
   "source": [
    "## **Numerical Example**\n",
    "\n",
    "Let‚Äôs walk through a **toy numerical example** that illustrates exactly how CLIP‚Äôs **contrastive loss** works. \n",
    "\n",
    "We‚Äôll use very small vectors and no neural networks ‚Äî just random embeddings ‚Äî to show how the **similarity matrix** and **loss** are computed.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Setup\n",
    "\n",
    "Assume a **batch of N = 3 (image, text)** pairs:\n",
    "\n",
    "| Pair | Image      | Text    |\n",
    "| ---- | ---------- | ------- |\n",
    "| 1    | üñº ‚Äúa cat‚Äù | ‚Äúa cat‚Äù |\n",
    "| 2    | üñº ‚Äúa dog‚Äù | ‚Äúa dog‚Äù |\n",
    "| 3    | üñº ‚Äúa car‚Äù | ‚Äúa car‚Äù |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Encoded Embeddings (already normalized)\n",
    "\n",
    "Let‚Äôs say our encoders output 2-dimensional **unit vectors** (after normalization):\n",
    "\n",
    "$$\n",
    "v_1 = [0.9, 0.1], \\quad t_1 = [0.8, 0.2] \\\n",
    "v_2 = [0.1, 0.9], \\quad t_2 = [0.2, 0.8] \\\n",
    "v_3 = [0.9, -0.1], \\quad t_3 = [1.0, 0.0]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Compute Similarity Matrix ( S = v_i^\\top t_j )\n",
    "\n",
    "Each entry ( S_{ij} ) is the **dot product** between image ( i ) and text ( j ):\n",
    "\n",
    "| Image\\Text | t‚ÇÅ                           | t‚ÇÇ                           | t‚ÇÉ                           |\n",
    "| ---------- | ---------------------------- | ---------------------------- | ---------------------------- |\n",
    "| **v‚ÇÅ**     | 0.9√ó0.8 + 0.1√ó0.2 = **0.74** | 0.9√ó0.2 + 0.1√ó0.8 = 0.26     | 0.9√ó1.0 + 0.1√ó0 = 0.9        |\n",
    "| **v‚ÇÇ**     | 0.1√ó0.8 + 0.9√ó0.2 = 0.26     | 0.1√ó0.2 + 0.9√ó0.8 = **0.74** | 0.1√ó1.0 + 0.9√ó0 = 0.1        |\n",
    "| **v‚ÇÉ**     | 0.9√ó0.8 + (‚àí0.1)√ó0.2 = 0.70  | 0.9√ó0.2 + (‚àí0.1)√ó0.8 = 0.10  | 0.9√ó1.0 + (‚àí0.1)√ó0 = **0.9** |\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "S =\n",
    "\\begin{bmatrix}\n",
    "0.74 & 0.26 & 0.9 \\\n",
    "0.26 & 0.74 & 0.1 \\\n",
    "0.70 & 0.10 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Add Temperature Scaling\n",
    "\n",
    "CLIP uses a learnable **temperature parameter** ( \\tau ) (often ‚âà 1/0.07).\n",
    "Let‚Äôs just take ( \\tau = 1 ) for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Compute Image‚ÜíText Probabilities\n",
    "\n",
    "For each **image** ( i ), we apply **softmax** over the row ( S_{i,:} ):\n",
    "\n",
    "Example for image 1:\n",
    "\n",
    "$$\n",
    "p(t_j|v_1) = \\frac{\\exp(S_{1j})}{\\sum_k \\exp(S_{1k})}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "* exp(0.74) = 2.10\n",
    "* exp(0.26) = 1.30\n",
    "* exp(0.90) = 2.46\n",
    "  Sum = 5.86\n",
    "\n",
    "So probabilities:\n",
    "\n",
    "* p(t‚ÇÅ|v‚ÇÅ) = 2.10 / 5.86 = 0.36\n",
    "* p(t‚ÇÇ|v‚ÇÅ) = 1.30 / 5.86 = 0.22\n",
    "* p(t‚ÇÉ|v‚ÇÅ) = 2.46 / 5.86 = 0.42\n",
    "\n",
    "Correct match is **t‚ÇÅ**, so its log-prob = log(0.36) = ‚àí1.02.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Repeat for other rows\n",
    "\n",
    "| Image | exp(scores)        | Sum  | Correct prob | ‚àílog(prob) |\n",
    "| ----- | ------------------ | ---- | ------------ | ---------- |\n",
    "| v‚ÇÅ    | [2.10, 1.30, 2.46] | 5.86 | 0.36         | **1.02**   |\n",
    "| v‚ÇÇ    | [1.30, 2.10, 1.10] | 4.50 | 0.47         | **0.75**   |\n",
    "| v‚ÇÉ    | [2.01, 1.11, 2.46] | 5.58 | 0.36         | **1.02**   |\n",
    "\n",
    "Average image‚Üítext loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{image}} = \\frac{1}{3}(1.02 + 0.75 + 1.02) = 0.93\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Text‚ÜíImage Loss (symmetric)\n",
    "\n",
    "Do the same for **columns** (softmax over S·µÄ).\n",
    "You‚Äôll get a similar value (‚âà0.93).\n",
    "\n",
    "Final CLIP loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2}(\\mathcal{L}*{\\text{image}} + \\mathcal{L}*{\\text{text}}) \\approx 0.93\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Intuition\n",
    "\n",
    "‚úÖ When matching pairs (diagonal) are **larger** than off-diagonal ones,\n",
    "the model gets **low loss** (better alignment).\n",
    "\n",
    "‚ùå If the similarity matrix has mixed values (e.g., an image looks similar to a wrong text), the loss increases ‚Äî forcing the encoders to *separate* them more.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conceptual Visualization\n",
    "\n",
    "The embeddings are pushed such that:\n",
    "\n",
    "* **Positive pairs**: ( v_i ) and ( t_i ) are close.\n",
    "* **Negative pairs**: ( v_i ) and ( t_j ) (for ( j \\neq i )) are far apart.\n",
    "\n",
    "It‚Äôs a **bidirectional contrastive learning**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7521cc-c157-4938-bb07-e797cfe5e201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dffa5-05f8-462c-ac25-ea1359bf8371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913325ce-239e-4b35-b3ab-add004e1ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dccf0-4ca5-4438-ae06-6edc509ca8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db519437-5ff8-4457-ae3e-1e16e0a38e8b",
   "metadata": {},
   "source": [
    "### CLIP Training set\n",
    "\n",
    "Excellent question ‚Äî and it goes straight to the **core strength** (and a bit of the *mystery*) behind CLIP‚Äôs power.\n",
    "\n",
    "Let‚Äôs unpack this precisely.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Source of CLIP‚Äôs Training Data\n",
    "\n",
    "CLIP was trained on a dataset called **‚ÄúWebImageText‚Äù (WIT)** ‚Äî\n",
    "a **private dataset** created by **OpenAI** that contains **‚âà400 million (image, text)** pairs.\n",
    "\n",
    "This dataset was **collected from the public internet** ‚Äî specifically from sources where **images naturally co-occur with descriptive text**.\n",
    "\n",
    "Typical sources include:\n",
    "\n",
    "| Type                         | Example                                             |\n",
    "| ---------------------------- | --------------------------------------------------- |\n",
    "| **Alt-text from web images** | HTML `<img>` tags with `alt=` descriptions          |\n",
    "| **Image captions**           | Stock photo sites, image-sharing sites, articles    |\n",
    "| **Social media posts**       | Tweets, Reddit, etc. containing both image and text |\n",
    "| **Public web pages**         | Where images appear next to relevant sentences      |\n",
    "| **Metadata & filenames**     | e.g., `cat_on_a_bed.jpg` or `dog_playing_ball.png`  |\n",
    "\n",
    "The OpenAI team used a **web crawler** to gather image URLs and associated surrounding text, then applied **filtering and deduplication** to form the final dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Scale and Diversity\n",
    "\n",
    "* **400 million pairs**\n",
    "* **32,768 GPUs** (distributed across clusters)\n",
    "* Data from **hundreds of thousands of websites**\n",
    "* Roughly **5 billion text tokens**\n",
    "* Huge diversity: objects, scenes, actions, memes, artworks, etc.\n",
    "\n",
    "That diversity gives CLIP its **broad generalization** ‚Äî it learns what a ‚Äúphoto of a cat‚Äù means without ever seeing the word ‚Äúcat‚Äù as a *class label*.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Cleaning and Filtering\n",
    "\n",
    "Raw internet data is noisy ‚Äî so OpenAI applied several steps:\n",
    "\n",
    "1. **Text filtering**\n",
    "\n",
    "   * Remove short, meaningless strings (e.g., ‚ÄúIMG_001.jpg‚Äù)\n",
    "   * Exclude non-English or overly rare languages\n",
    "   * Basic profanity and NSFW filtering\n",
    "\n",
    "2. **Image validation**\n",
    "\n",
    "   * Check that URLs actually lead to valid images\n",
    "   * Ensure diversity (avoid duplicates or corrupted images)\n",
    "\n",
    "3. **Pair quality filtering**\n",
    "\n",
    "   * Compute similarity between preliminary text and image embeddings\n",
    "   * Discard pairs that clearly mismatch (e.g., cat image + random quote)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Comparison to Other Datasets\n",
    "\n",
    "| Dataset        | Size | Type                              | Public?   |\n",
    "| -------------- | ---- | --------------------------------- | --------- |\n",
    "| **OpenAI WIT** | 400M | Web image‚Äìtext pairs              | ‚ùå Private |\n",
    "| **LAION-400M** | 400M | Web crawl based on CLIP filtering | ‚úÖ Public  |\n",
    "| **LAION-5B**   | 5B   | Massive extension, multilingual   | ‚úÖ Public  |\n",
    "| **CC3M**       | 3M   | Curated ‚ÄúConceptual Captions‚Äù     | ‚úÖ Public  |\n",
    "| **CC12M**      | 12M  | Larger CC version                 | ‚úÖ Public  |\n",
    "\n",
    "So, when OpenAI released the CLIP paper, they didn‚Äôt release the original **WIT-400M** dataset.\n",
    "But later, **LAION** (a community effort) **recreated** it by using the *same strategy*:\n",
    "use a web crawler, grab image‚Äìtext pairs, and filter them using CLIP‚Äôs own similarity score.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "| Aspect              | Description                                             |\n",
    "| ------------------- | ------------------------------------------------------- |\n",
    "| Dataset             | WebImageText (WIT)                                      |\n",
    "| Size                | ~400M image‚Äìtext pairs                                  |\n",
    "| Source              | Public internet (HTML, captions, metadata)              |\n",
    "| Publicly released?  | ‚ùå No (OpenAI internal)                                  |\n",
    "| Public alternatives | ‚úÖ LAION-400M, LAION-5B                                  |\n",
    "| Purpose             | Learn cross-modal alignment between vision and language |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Insight\n",
    "\n",
    "CLIP didn‚Äôt need manually labeled data like ‚ÄúImageNet-dog-class.‚Äù\n",
    "Instead, it learned *from how humans naturally describe images online.*\n",
    "\n",
    "That‚Äôs why it can recognize **open-vocabulary** concepts ‚Äî things it‚Äôs never seen in labeled datasets ‚Äî simply by matching the semantics of text and vision.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show how **LAION-400M** (the public version) can be **downloaded and filtered** using the CLIP model for your own experiments (e.g., building your own CLIP-style dataset)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
