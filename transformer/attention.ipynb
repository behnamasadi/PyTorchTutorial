{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab80429e-4953-4f20-8a5d-de0358177e50",
   "metadata": {},
   "source": [
    "## **1. Embedding Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394adc69-d7bf-49dd-b6e7-c6600be0c225",
   "metadata": {},
   "source": [
    "which points words into some high dimensional space:\n",
    "\n",
    "<img src=\"images/words_embedding_vector.png\" width=\"50%\"  height=\"50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "229a3eb5-6d3b-45d0-93ca-68b1e83a0ccb",
   "metadata": {},
   "source": [
    "Direction in this space has a semantic meaning, word with similar concepts map to the same spot\n",
    "\n",
    "<img src=\"images/embedding_closest_to_E(tower).png\" width=\"50%\"  height=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddfa51c-d510-4f86-b883-403c1650eb15",
   "metadata": {},
   "source": [
    "A typical example the difference between man and women is very similar to difference between king and Queen \n",
    "\n",
    "\n",
    "<img src=\"images/man_women_king_queen.png\" width=\"50%\"  height=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9bb89-37e7-4382-9d0d-d3ed243851e5",
   "metadata": {},
   "source": [
    "Chat gpt3 has dimensionality of `12288` and has `50257` token\n",
    "\n",
    "<img src=\"images/gpt3_embedding.png\" width=\"50%\"  height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c3596-e27f-4561-b81c-24d51ed1c082",
   "metadata": {},
   "source": [
    "The meaning of each word can be specialized in the context, by adding more vector to it, at the begging each word get it vector from embedding matrix and has an initial meaning but in the context that vector get more specific direction:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/context.png\" width=\"50%\"  height=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5756a0-2f04-4f5c-954c-af6402f6b089",
   "metadata": {},
   "source": [
    "## **2. Unembedding Matrix**\n",
    "The last layer, we get the last token and want a map it back to token: has one row for each word in the vocabulary, just like embedding matrix but it is swaped\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/unembedding.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bdd46ba-a71e-43b9-a903-e116d172002f",
   "metadata": {},
   "source": [
    "## **3. Transformer**\n",
    "The aim of the transformer is to progressively adjust these embedding so they don't code individual words, so they became much more contextual meaning\n",
    "\n",
    "<img src=\"images/transformer1.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/transformer2.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/transformer3.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/transformer4.gif\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38f7f3-a641-4b9a-8d59-1c1555ea04ba",
   "metadata": {},
   "source": [
    "For instance, at begging, the embedding value of the mole is the same in all these contexts:\n",
    "\n",
    "<img src=\"images/mole_transformer.png\" width=\"50%\"  height=\"50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3cc206-7476-451c-b050-9d4edfe81f18",
   "metadata": {},
   "source": [
    "So imagine the following sentence, at the beginning, the embedding encode the meaning of that particular word and its position with n context\n",
    "\n",
    "\n",
    "<img src=\"images/transformer_blue_creature.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "The goal is to have serious of conversation to have a new refined embedding \n",
    "\n",
    "<img src=\"images/refined_embedding.png\" width=\"50%\"  height=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c65589-8a16-4836-ae03-88955d2158dc",
   "metadata": {},
   "source": [
    "####  What actually happens inside a multi-head block\n",
    "\n",
    "You start with an input of shape:\n",
    "\n",
    "$ X \\in \\mathbb{R}^{n \\times d_\\text{model}}$ $\\text{(n tokens, each d\\_model dimensional)}$\n",
    "\n",
    "For GPT-3 style numbers: $d_\\text{model}=12288$.\n",
    "\n",
    "Then:\n",
    "\n",
    "* For each head (h), you have **three separate weight matrices**:\n",
    "\n",
    "  * $W^Q_h \\in \\mathbb{R}^{d_\\text{model} \\times d_k}$\n",
    "  * $W^K_h \\in \\mathbb{R}^{d_\\text{model} \\times d_k}$\n",
    "  * $W^V_h \\in \\mathbb{R}^{d_\\text{model} \\times d_v}$\n",
    "\n",
    "With $d_k=d_v=\\frac{d_{model}}{num_{heads}}$ \n",
    "\n",
    "In GPT-3, $d_k=128$.\n",
    "\n",
    "So each head projects the **full 12288-dimensional input** down to **128-dimensional Q,K,V** for that head.\n",
    "\n",
    "---\n",
    "\n",
    "#### Shapes step by step\n",
    "\n",
    "* Input: $X$: $(n, 12288)$\n",
    "* Per head projections:\n",
    "\n",
    "  * $Q_h = X W^Q_h \\in (n, 128)$\n",
    "  * $K_h = X W^K_h \\in (n, 128)$\n",
    "  * $V_h = X W^V_h \\in (n, 128)$\n",
    "\n",
    "So yes — **each head projects into a smaller subspace** (128-dim per head in this case).\n",
    "\n",
    "Then compute attention:\n",
    "\n",
    "* $A_h = \\text{softmax}(Q_h K_h^\\top/\\sqrt{128}) \\in (n, n)$\n",
    "* $O_h = A_h V_h \\in (n, 128)$\n",
    "\n",
    "Concatenate all heads:\n",
    "\n",
    "$\n",
    "O_\\text{concat} = \\text{concat}(O_1,\\dots,O_h) \\in (n, 96*128)=(n,12288)\n",
    "$\n",
    "\n",
    "Finally apply an output projection $W^O \\in \\mathbb{R}^{(h\\cdot d_v) \\times d_\\text{model}}$:\n",
    "\n",
    "$\n",
    "O_\\text{final} = O_\\text{concat} W^O \\in (n,12288)\n",
    "$\n",
    "\n",
    "You’re back at the model dimension.\n",
    "\n",
    "---\n",
    "\n",
    "#### The key idea\n",
    "\n",
    "* You **split the full model dimension** across heads.\n",
    "* Each head works on a *slice* of the full dimension (here 128 per head).\n",
    "* After concatenating all heads, you recover the full $d_\\text{model}$.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e04d8c-cc73-417c-9a65-b50239e6c7a3",
   "metadata": {},
   "source": [
    "### **3.1 Query Vector**\n",
    "\n",
    "$ W_Q $ (query projection): $ (128, 12288) $\n",
    "\n",
    "<img src=\"images/query_vector.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c227f9-c0ec-4958-85bd-ed38117a9eed",
   "metadata": {},
   "source": [
    "### **3.2 Key Vector**\n",
    "\n",
    "It answers the query\n",
    "\n",
    "\n",
    "$ W_K $ (key projection): $ (128, 12288) $\n",
    "\n",
    "\n",
    "<img src=\"images/key_vector1.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "<img src=\"images/key_vector2.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02ffe5-e10a-4359-8766-7bb7d646f6cc",
   "metadata": {},
   "source": [
    "To measure how well a key match a query, we calculate the dot product:\n",
    "\n",
    "<img src=\"images/key_dot_query.png\" width=\"50%\"  height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d4a80-c1a3-49ec-a386-de1deb03d7a8",
   "metadata": {},
   "source": [
    "### Attends to\n",
    "\n",
    "Since the key value produced by fluffy and blue aligns with the query of creature and dot product is some positive number, in machine learning it is called the embeddings of fluffy and blue **Attends to** creature\n",
    "\n",
    "<img src=\"images/attends_to.png\" width=\"50%\"  height=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19038f-22b6-4781-bf7f-689f71a1eecb",
   "metadata": {},
   "source": [
    "### Attention Pattern\n",
    "\n",
    "<img src=\"images/attention_pattern.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "Attention matrix with **keys on the rows** and **queries on the columns**.\n",
    "\n",
    "---\n",
    "\n",
    "####  What’s on the top (columns)\n",
    "\n",
    "Across the top you see:\n",
    "\n",
    "```\n",
    "a   fluffy   blue   creature  roamed  the  verdant  forest\n",
    "```\n",
    "\n",
    "with arrows down through $E_i$ then $W^Q$ to $\\vec Q_i$.\n",
    "\n",
    "This means:\n",
    "\n",
    "* **Each column corresponds to a *query token*.**\n",
    "* When you move down a column you’re looking at “for this query token, how much weight do I give to each key token?”\n",
    "\n",
    "So in this drawing:\n",
    "\n",
    "* Column “a” = the query for “a” (Q₁).\n",
    "* Column “fluffy” = the query for “fluffy” (Q₂).\n",
    "* Column “blue” = the query for “blue” (Q₃).\n",
    "* etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### What’s on the left (rows)\n",
    "\n",
    "Down the left side you see:\n",
    "\n",
    "```\n",
    "a → E₁ → K₁\n",
    "fluffy → E₂ → K₂\n",
    "blue → E₃ → K₃\n",
    "...\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* **Each row corresponds to a *key token*.**\n",
    "* When you move across a row you’re seeing “how much each query token attends to this key token.”\n",
    "\n",
    "So row “a” = key for “a” (K₁), row “fluffy” = key for “fluffy” (K₂), etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### The circles inside the grid\n",
    "\n",
    "Each circle at row $j$, column $i$ is the attention weight between:\n",
    "\n",
    "* **Query token i** (from the top)\n",
    "* **Key token j** (from the left)\n",
    "\n",
    "Circle size = magnitude of the attention weight.\n",
    "\n",
    "---\n",
    "\n",
    "####  Where the softmax is applied\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "A_{i,j} = \\text{softmax}_j\\left(\\frac{Q_i\\cdot K_j}{\\sqrt{d_k}}\\right)\n",
    "$\n",
    "\n",
    "* That’s an (n\\times n) matrix in code with **rows = queries** and **columns = keys**.\n",
    "* In this picture it’s drawn **transposed**, so **columns = queries** and **rows = keys**.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "* In **code**, each row sums to 1 (one distribution per query).\n",
    "* In **this picture**, each **column** sums to 1, because the artist swapped rows/columns.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| In your image              | Meaning                                                                              | Normalised?                                                |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------ | ---------------------------------------------------------- |\n",
    "| **Column i** under a token | Query token (i): a probability distribution over all keys (down the column).         | ✔ sums to 1 (because it’s the distribution for one query). |\n",
    "| **Row j** next to a token  | Key token (j): shows how much each query token attends to this key (across the row). | ✖ not normalised.                                          |\n",
    "\n",
    "So to answer you directly:\n",
    "\n",
    "> *“It is a masked attention, like in row the sum should be 1 or in a column?”*\n",
    "\n",
    "In **this picture**, the **columns sum to 1** because each column = one query’s distribution over keys.\n",
    "The rows don’t necessarily sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick read of one example:\n",
    "\n",
    "Column “blue” (third column from left):\n",
    "\n",
    "* Down that column:\n",
    "\n",
    "  * Big circle at row “fluffy”: query “blue” attends strongly to key “fluffy”.\n",
    "  * Medium circle at row “blue”: attends to itself.\n",
    "  * Tiny elsewhere.\n",
    "\n",
    "Row “fluffy” (second row from top):\n",
    "\n",
    "* Across that row you see how much all query tokens attend to key “fluffy”. Not a probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "So:\n",
    "\n",
    "* **Top/columns = queries**\n",
    "* **Left/rows = keys**\n",
    "* **Circles = weights**\n",
    "* **Columns sum to 1** in this drawing (masked attention).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fa722-6c39-4f12-aaaf-717b02ee5bbc",
   "metadata": {},
   "source": [
    "Some notation from original paper:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/paper_1.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "For numerical stability, we divide the denominator by the square root of dimension \n",
    "\n",
    "\n",
    "<img src=\"images/paper_2.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb43aa-cc5f-436c-ae43-27906a9c7672",
   "metadata": {},
   "source": [
    "### Normalized Attention Pattern (Masking)\n",
    "\n",
    "Since we don't want the later word to affect the earlier words during training (otherwise they kind of give away the answer), we set them into  $-\\infty$ before applying softmax.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/effect_the_earlier_words.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "<img src=\"images/normalized_attention_pattern.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa0f3e-03f3-4c5b-b750-68de78f6cd3f",
   "metadata": {},
   "source": [
    "### Context Size\n",
    "The size of the attention table is the squared root of the context size:\n",
    "\n",
    "<img src=\"images/attention_table_size.png\" width=\"50%\"  height=\"50%\" />\n",
    "and can quickly become huge, some solution for scaling it up\n",
    "\n",
    "- **Sparse Attention Mechanism**\n",
    "- **Block wise Attention**\n",
    "- **Linformer**\n",
    "- **Ring Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3377ebf3-d452-4261-b672-fda97f8bd76b",
   "metadata": {},
   "source": [
    "### Value Matrix\n",
    "\n",
    "$ W_V $ (value projection): $ (12288, 12288) $\n",
    "\n",
    "In our example, we have the embedding of fluffy, and we want the value to cause changes in the embedding of creature, to do this we add a value matrix and multiply by encoding of the first embedding,  the result of this is called **value Vector**\n",
    "\n",
    "<img src=\"images/updating_embeddings1.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "<img src=\"images/updating_embeddings2.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "So the way we're actually doing that, is in the attention pattern, we multiply the result **key vector** by **value matrix**, and  we get a **value vector**, then for each column in the digram, we multiply the **value vector** by the softmax, and then we add all of them to  original embedding\n",
    "\n",
    "\n",
    "<img src=\"images/complete_update_embedding.png\" width=\"50%\"  height=\"50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aad20f4-ee74-404b-bc43-3a562fc0510a",
   "metadata": {},
   "source": [
    "### Multi head Attention\n",
    "The following operation is a single head attention:  \n",
    "<img src=\"images/head_of_attention.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "The full attention block in the transformer, you have several layers of attention each with its own key, query and value \n",
    "\n",
    "<img src=\"images/multi_head_attention1.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention2.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention3.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention4.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention5.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention6.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/multi_head_attention7.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3c14b-c06d-41a2-a637-d43fe1068e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da5c57b-99cc-4a5f-adbd-9a314a1021d7",
   "metadata": {},
   "source": [
    "### Mapping of Value Matrix of two Matrices\n",
    "\n",
    "<img src=\"images/value_down.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/value_matrix_of_two_matrices.png\" width=\"50%\"  height=\"50%\" />\n",
    "<img src=\"images/value_up.png\" width=\"50%\"  height=\"50%\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff1f50-ba0d-4dd5-827d-cda8d4343635",
   "metadata": {},
   "source": [
    "## **4. Single Head Attention Implementation**\n",
    "\n",
    "\n",
    "In **self-attention**, we usually have:\n",
    "\n",
    "* Number of tokens, corresponds to the context window size (the length of the sequence) : $T$\n",
    "* Embedding dimension: $d_{\\text{model}}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "* $X$ of shape $[T, d_{\\text{model}}]$\n",
    "* $W_Q, W_K, W_V$ of shape $[d_{\\text{model}}, d_{\\text{model}}]  $\n",
    "* and thus $$Q, K, V \\in  \\mathbb{R}^{T,d_{\\text{model}} }$$ each have shape $[T, d_{\\text{model}}] $\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b394c3-afb5-4022-8c92-a3ff1383cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12288])\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "\n",
    "T = 128             # number of tokens\n",
    "d_model = 12288     # embedding dimension\n",
    "\n",
    "X = torch.randn(T, d_model)  # input embeddings for all tokens\n",
    "\n",
    "W_Q = torch.randn(d_model, d_model)\n",
    "W_K = torch.randn(d_model, d_model)\n",
    "W_V = torch.randn(d_model, d_model)\n",
    "\n",
    "Q = X @ W_Q  # [T, d_model]\n",
    "K = X @ W_K  # [T, d_model]\n",
    "V = X @ W_V  # [T, d_model]\n",
    "\n",
    "# attention weights\n",
    "scores = Q @ K.T / math.sqrt(d_model)  # [T, T]\n",
    "weights = torch.softmax(scores, dim=-1)  # [T, T]\n",
    "output = weights @ V  # [T, d_model]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86464cf3-ade3-426b-bd15-8b84b83c7083",
   "metadata": {},
   "source": [
    "**Meaning of `dim=-1`**\n",
    "\n",
    "In PyTorch, **tensors can have multiple dimensions (axes)**.\n",
    "`dim=-1` means *“apply the operation along the last dimension.”*\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "torch.softmax(tensor, dim=-1)\n",
    "```\n",
    "\n",
    "tells PyTorch to:\n",
    "\n",
    "> Take each slice along the last axis, and normalize the values in that slice so they sum to 1.\n",
    "> \n",
    "\n",
    "`dim=0` means collapse the rows, so after the operation we have only `1` row, so `2x3->1x3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0abdf4-65ab-4a5d-a2de-66deb78144c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([3])\n",
      "tensor([0.1800, 0.4894, 1.3304])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(\n",
    "        [[0.0900, 0.2447, 0.6652],\n",
    "        [0.0900, 0.2447, 0.6652]])\n",
    "\n",
    "print(x.shape)\n",
    "s_row_collapse=x.sum(dim=0)\n",
    "print(s_row_collapse.shape)\n",
    "print(s_row_collapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6db21-0cb3-4b46-93a9-10fd62922b9d",
   "metadata": {},
   "source": [
    "`dim=1` means collapse the cols, so after the operation we have only `1` col, so `2x3->2x1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17975f6e-f975-4b5c-89ce-ddf0a26a3c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([0.9999, 0.9999])\n"
     ]
    }
   ],
   "source": [
    "s_col_collapse=x.sum(dim=1)\n",
    "print(s_col_collapse.shape)\n",
    "print(s_col_collapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c703d81-f32d-48d8-b4b8-de91dc3870f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Why “last dimension”?**\n",
    "\n",
    "After computing\n",
    "\n",
    "$$\n",
    "\\text{scores} = Q K^T\n",
    "$$\n",
    "\n",
    "you get a **square matrix** of shape `[T, T]`.\n",
    "\n",
    "Let’s label it:\n",
    "\n",
    "|            | Key₁ | Key₂ | Key₃ | Key₄ |\n",
    "| ---------- | ---- | ---- | ---- | ---- |\n",
    "| **Query₁** | s₁₁  | s₁₂  | s₁₃  | s₁₄  |\n",
    "| **Query₂** | s₂₁  | s₂₂  | s₂₃  | s₂₄  |\n",
    "| **Query₃** | s₃₁  | s₃₂  | s₃₃  | s₃₄  |\n",
    "| **Query₄** | s₄₁  | s₄₂  | s₄₃  | s₄₄  |\n",
    "\n",
    "* **Rows (axis 0)** → represent **queries**\n",
    "* **Columns (axis 1)** → represent **keys**\n",
    "\n",
    "Each element `s_ij` = similarity between **Queryᵢ** and **Keyⱼ**\n",
    "\n",
    "\n",
    "\n",
    "For **each query**, we want its attention scores over **all keys** to form a probability distribution that sums to 1:\n",
    "\n",
    "$$\n",
    "\\sum_j \\text{softmax}(s_{ij}) = 1\n",
    "$$\n",
    "\n",
    "That means we normalize **across columns**, **within each row**.\n",
    "\n",
    "---\n",
    "\n",
    "So:\n",
    "\n",
    "* `dim=1` → apply softmax **horizontally** across each row.\n",
    "* `dim=0` → apply softmax **vertically** down each column.\n",
    "\n",
    "\n",
    "\n",
    "For a tensor of shape `[T, T]` (as in attention scores):\n",
    "\n",
    "* Axis 0 → rows (queries)\n",
    "* Axis 1 → columns (keys)\n",
    "\n",
    "If you do `torch.softmax(scores, dim=-1)` (same as `dim=1` for a 2D tensor),\n",
    "it means:\n",
    "\n",
    "> For each **query** (row), apply softmax across all **keys** (columns).\n",
    "\n",
    "This makes each row sum to 1 — each query’s attention distribution over all keys.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Why we say “sum across the row”**\n",
    "\n",
    "It’s about **which axis you apply softmax *along*,** not which axis you sum *over*.\n",
    "When you say `dim=1`, you mean: “for each fixed row (axis 0), move **along columns** (axis 1), compute exponentials, normalize by the sum of that row.”\n",
    "\n",
    "So the *normalization sum* happens *across the row* (over columns).\n",
    "That’s why **each row** sums to 1 — not each column.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237cd951-2bcf-4cb4-8ad9-7368813061c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0159, 0.1173, 0.8668]])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "scores = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [2.0, 4.0, 6.0]\n",
    "])\n",
    "\n",
    "weights = torch.softmax(scores, dim=-1)\n",
    "print(weights)\n",
    "print(weights.sum(dim=-1))  # sums to 1 for each row\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf0109-f379-4510-be06-b80aab609689",
   "metadata": {},
   "source": [
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "tensor([[0.0900, 0.2447, 0.6652],\n",
    "        [0.0900, 0.2447, 0.6652]])\n",
    "tensor([1.0000, 1.0000])\n",
    "```\n",
    "\n",
    "So each **row** was normalized independently, producing probabilities across columns.\n",
    "\n",
    "---\n",
    "\n",
    "**In attention context**\n",
    "\n",
    "In attention:\n",
    "\n",
    "$$\n",
    "\\text{weights} = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "* Shape of `scores`: `[T, T]`\n",
    "* Each row corresponds to **one query token**\n",
    "* `dim=-1` → normalize across **all keys**, so each query token’s attention weights over all keys sum to 1.\n",
    "\n",
    "That gives a **valid probability distribution** for how much each token should “attend” to others.\n",
    "\n",
    "---\n",
    "\n",
    " **Visual intuition**\n",
    "\n",
    "| Query token | Keys it attends to | After softmax (dim=-1) |\n",
    "| ----------- | ------------------ | ---------------------- |\n",
    "| Token₁      | [3.2, 1.1, 0.4]    | [0.82, 0.13, 0.05]     |\n",
    "| Token₂      | [0.9, 0.8, 2.0]    | [0.19, 0.17, 0.64]     |\n",
    "\n",
    "Each row (query) becomes a normalized vector of attention weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45482c81-d3e9-4496-a6e8-c23e8b3e232c",
   "metadata": {},
   "source": [
    "## **5. Multi-Head Attention Implementation**\n",
    "\n",
    "#### **5.1 Basics**\n",
    "- Number of heads: $h$ (or $n_{\\text{heads}}$)\n",
    "- Dimension per head: $d_k = d_{\\text{model}} // h$\n",
    "\n",
    "For multi-head attention, each head operates on a slice:\n",
    "$$Q_h, K_h, V_h \\in \\mathbb{R}^{T \\times d_k}$$\n",
    "and attention per head is:\n",
    "$$A_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right)V_h$$\n",
    "\n",
    "| Symbol | Meaning                                                     | Depends on                      | Example |\n",
    "| :----- | :---------------------------------------------------------- | :------------------------------ | :------ |\n",
    "| **T**  | Number of tokens in the input sequence (**context window**) | The model’s max sequence length | 128     |\n",
    "| **dₖ** | Dimension per attention head                                | $$d_k = d_{model} / n_{heads}$$ | 128     |\n",
    "\n",
    "Example: $d_{\\text{model}} = 12288$, $n_{\\text{heads}} = 96$, so $d_k = 128$. T=128 is coincidental, making matrices square but not required.\n",
    "\n",
    "#### **Per-Head Projections**\n",
    "If the input embedding is:\n",
    "$$X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}$$\n",
    "For each head $h$, learn:\n",
    "$$W_h^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad\n",
    "W_h^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\quad\n",
    "W_h^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$$\n",
    "$$Q_h = X W_h^Q, \\quad K_h = X W_h^K, \\quad V_h = X W_h^V$$\n",
    "$$Q_h, K_h, V_h \\in \\mathbb{R}^{T \\times d_k}$$\n",
    "\n",
    "#### **5.2 Efficient Implementation: Combined Projections**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1eeefc8-c67b-4ed4-8812-8c4ef7ae3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12288])\n"
     ]
    }
   ],
   "source": [
    "n_heads = 96\n",
    "d_k = d_model // n_heads # 12288//96=128\n",
    "\n",
    "X = torch.randn(T, d_model)\n",
    "\n",
    "W_Q = torch.randn(d_model, d_model)\n",
    "W_K = torch.randn(d_model, d_model)\n",
    "W_V = torch.randn(d_model, d_model)\n",
    "\n",
    "Q = X @ W_Q  # [T, d_model]\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "# reshape into heads: [T, n_heads, d_k]\n",
    "Q = Q.view(T, n_heads, d_k)\n",
    "K = K.view(T, n_heads, d_k)\n",
    "V = V.view(T, n_heads, d_k)\n",
    "\n",
    "scores = torch.einsum('thd,Thd->htT', Q, K) / math.sqrt(d_k)  # [n_heads, T, T]\n",
    "weights = torch.softmax(scores, dim=-1)\n",
    "out = torch.einsum('htT,Thd->thd', weights, V)  # [T, n_heads, d_k]\n",
    "out = out.reshape(T, d_model)  # concatenate heads\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f36daa-98f6-467e-9856-b89d40e43ffe",
   "metadata": {},
   "source": [
    "Instead of separate matrices, concatenate into one big matrix:\n",
    "$$W^Q = [W_1^Q ; W_2^Q ; \\dots ; W_{n_{\\text{heads}}}^Q] \\in \\mathbb{R}^{d_{\\text{model}} \\times (n_{\\text{heads}} \\cdot d_k)}$$\n",
    "(Same for $W^K, W^V$.) This is stored as `nn.Linear(d_model, n_heads * d_k)`.\n",
    "\n",
    "Compute:\n",
    "```python\n",
    "Q = X @ W_Q  # [T, n_heads * d_k]\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "```\n",
    "\n",
    "**Why Reshape**\n",
    "Reshape to separate into per-head parts:\n",
    "```python\n",
    "Q = Q.view(T, n_heads, d_k)  # [T, n_heads, d_k]\n",
    "K = K.view(T, n_heads, d_k)\n",
    "V = V.view(T, n_heads, d_k)\n",
    "```\n",
    "This simulates separate heads with one large matmul, at zero cost.\n",
    "\n",
    "| Axis    | Meaning              |\n",
    "| ------- | -------------------- |\n",
    "| T       | token index          |\n",
    "| n_heads | head index           |\n",
    "| d_k     | per-head feature dim |\n",
    "\n",
    "| Conceptual view                             | Efficient implementation                 |\n",
    "| ------------------------------------------- | ---------------------------------------- |\n",
    "| Each head has its own $W_h^Q, W_h^K, W_h^V$ | Combine all into one big $W^Q, W^K, W^V$ |\n",
    "| Compute $Q_h = X W_h^Q$ separately          | Compute once: $Q = X W^Q$                |\n",
    "| Q shape per head: [T, d_k]                  | Q shape total: [T, n_heads * d_k]        |\n",
    "| —                                           | Then reshape: `.view(T, n_heads, d_k)`   |\n",
    "\n",
    "| Quantity            | Meaning                 | Shape                    |\n",
    "| ------------------- | ----------------------- | ------------------------ |\n",
    "| Per-head projection | $W_h^Q, W_h^K, W_h^V$   | [d_model, d_k]           |\n",
    "| Combined projection | $W^Q, W^K, W^V$         | [d_model, n_heads × d_k] |\n",
    "| Input               | X                       | [T, d_model]             |\n",
    "| Projected output    | $Q = X @ W^Q$           | [T, n_heads × d_k]       |\n",
    "| Reshaped output     | Q.view(T, n_heads, d_k) | [T, n_heads, d_k]        |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d5c9b-236a-4f3a-ace9-2b7f61b24f0a",
   "metadata": {},
   "source": [
    "#### **5.3 Computing Scores**\n",
    "Single-head baseline: $Q @ K.T / \\sqrt{d_k}$, shape [T, T].\n",
    "\n",
    "For multi-head, tensors are 3D: $Q, K, V \\in \\mathbb{R}^{T \\times n_{\\text{heads}} \\times d_k}$.\n",
    "\n",
    "Loop version:\n",
    "```python\n",
    "for h in range(n_heads):\n",
    "    Q_h = Q[:, h, :]  # [T, d_k]\n",
    "    K_h = K[:, h, :]  # [T, d_k]\n",
    "    scores_h = Q_h @ K_h.T  # [T, T]\n",
    "```\n",
    "\n",
    "Vectorized (batched):\n",
    "```python\n",
    "Qh = Q.permute(1, 0, 2)  # [n_heads, T, d_k]\n",
    "Kh = K.permute(1, 0, 2)  # [n_heads, T, d_k]\n",
    "scores = Qh @ Kh.transpose(-2, -1) / math.sqrt(d_k)  # [n_heads, T, T]\n",
    "```\n",
    "\n",
    "#### **5.4 einsum('thd,Thd->htT', Q, K)**\n",
    "\n",
    "Equivalent einsum:\n",
    "\n",
    "```python\n",
    "scores = torch.einsum('thd,Thd->htT', Q, K) / math.sqrt(d_k)  # [n_heads, T, T]\n",
    "```\n",
    "\n",
    "Index meanings:\n",
    "| Symbol | Meaning                           |\n",
    "| ------ | --------------------------------- |\n",
    "| `t`    | query token index                 |\n",
    "| `T`    | key token index                   |\n",
    "| `h`    | head index                        |\n",
    "| `d`    | feature dimension inside the head |\n",
    "\n",
    "Mathematically:\n",
    "$$\\text{scores}[h, t, T] = \\frac{1}{\\sqrt{d_k}} \\sum_d Q[t,h,d] K[T,h,d]$$\n",
    "\n",
    "Small example $d_k=3, \\text{one head}$:\n",
    "\n",
    "$$\n",
    "Q[t=1,h=0,:]=[2,4,6]\n",
    "$$\n",
    "\n",
    "$$\n",
    "K[T=2,h=0,:]=[1,3,5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "scores[0,1,2] = (2*1 + 4*3 + 6*5)/√d_k = 44/√3.\n",
    "$$\n",
    "\n",
    "Why permutation:\n",
    "| Axis | Before permute (Q shape = [T, n_heads, d_k]) | After permute ([n_heads, T, d_k]) | Meaning |\n",
    "| ---- | -------------------------------------------- | --------------------------------- | ------- |\n",
    "| 0    | token                                        | head                              | head    |\n",
    "| 1    | head                                         | token                             | token   |\n",
    "| 2    | feature                                      | feature                           | feature |\n",
    "\n",
    "| Version     | Operation                             | Shapes                                                | Equivalent            |\n",
    "| ----------- | ------------------------------------- | ----------------------------------------------------- | --------------------- |\n",
    "| Single-head | `Q @ K.T`                             | [T, dₖ] × [dₖ, T] → [T, T]                            | per token             |\n",
    "| Multi-head  | `Q.permute(1,0,2) @ K.permute(1,2,0)` | [n_heads, T, dₖ] × [n_heads, dₖ, T] → [n_heads, T, T] | per head              |\n",
    "| `einsum`    | `'thd,Thd->htT'`                      | same result                                           | clearer index mapping |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Meaning of Each Symbol**\n",
    "\n",
    "* $Q$ : Query tensor of shape $[t, h, d]$\n",
    "\n",
    "  * $t$ → token index in the *query sequence*\n",
    "  * $h$ → head index (which attention head we are in)\n",
    "  * $d$ → feature dimension per head\n",
    "\n",
    "* $K$ : Key tensor of shape $[T, h, d]$\n",
    "\n",
    "  * $T$ → token index in the *key sequence*\n",
    "  * $h$ → same head index\n",
    "  * $d$ → feature dimension per head\n",
    "\n",
    "* $\\text{scores}[h, t, T]$ : The attention score between **query token** $t$ and **key token** $T$ in head $h$.\n",
    "\n",
    "---\n",
    "\n",
    "**Expanded Form of the Summation**\n",
    "\n",
    "The summation\n",
    "$$\n",
    "\\sum_d Q[t, h, d],K[T, h, d]\n",
    "$$\n",
    "is simply a **dot product** between the $d$-dimensional query vector and key vector corresponding to the same head $h$:\n",
    "\n",
    "$$\n",
    "\\text{scores}[h, t, T] = Q[t, h, 0].K[T, h, 0] + Q[t, h, 1].K[T, h, 1] + Q[t, h, 2].K[T, h, 2]\n",
    " \\dots + Q[t, h, d_k - 1].K[T, h, d_k - 1]\n",
    "$$\n",
    "\n",
    "where $d_k$ is the dimension per head.\n",
    "\n",
    "---\n",
    "\n",
    "**Vectorized Form**\n",
    "\n",
    "In vector notation, for each head $h$:\n",
    "\n",
    "$$\n",
    "\\text{scores}[h, t, T] = Q_{t,h} \\cdot K_{T,h}\n",
    "= Q_{t,h}^{\\top} K_{T,h}\n",
    "$$\n",
    "\n",
    "That’s a simple inner product between the query vector and the key vector of the same head.\n",
    "\n",
    "---\n",
    "\n",
    "**Full Matrix Form (for context)**\n",
    "\n",
    "If you drop the explicit indices and compute all tokens at once for a single head:\n",
    "\n",
    "$$\n",
    "\\text{Scores}_h = Q_h K_h^{\\top}\n",
    "$$\n",
    "\n",
    "and for all heads in parallel:\n",
    "\n",
    "$$\n",
    "\\text{Scores} = Q K^{\\top}\n",
    "$$\n",
    "\n",
    "with shapes:\n",
    "$$\n",
    "Q, K \\in \\mathbb{R}^{H \\times T \\times d_k}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\text{Scores} \\in \\mathbb{R}^{H \\times T \\times T}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The sigma\n",
    "$$\n",
    "\\sum_d Q[t,h,d],K[T,h,d]\n",
    "$$\n",
    "means:\n",
    "“for a given head $h$, take the query vector for token $t$ and the key vector for token $T$, multiply their corresponding elements across all $d$ dimensions, and sum those products to get one scalar similarity score.”\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  **Equation meaning**\n",
    "\n",
    "The einsum\n",
    "\n",
    "```python\n",
    "scores = torch.einsum('thd,Thd->htT', Q, K)\n",
    "```\n",
    "\n",
    "says:\n",
    "\n",
    "> For each head `h`, for each query token `t`, and for each key token `T`,\n",
    "> multiply `Q[t,h,d]` by `K[T,h,d]` for all dimensions `d`, then sum those products.\n",
    "\n",
    "So the summation over `d` is an **inner product** between the two vectors\n",
    "— the `d_k`-dimensional query and key vectors for head `h`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Write it as explicit summation**\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "\\text{scores}[h,t,T] =\n",
    "\\sum_{d=0}^{d_k - 1} Q[t,h,d] \\cdot K[T,h,d]\n",
    "$$\n",
    "\n",
    "This is exactly what happens in a dot product.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Small numerical example**\n",
    "\n",
    "Let’s take **one head** (`h = 0`),\n",
    "and `d_k = 3` so we can see all terms.\n",
    "\n",
    "Suppose:\n",
    "\n",
    "| d | Q[t=1,h=0,d] | K[T=2,h=0,d] |\n",
    "| - | ------------ | ------------ |\n",
    "| 0 | 2            | 1            |\n",
    "| 1 | 4            | 3            |\n",
    "| 2 | 6            | 5            |\n",
    "\n",
    "Then for that head and those token indices:\n",
    "\n",
    "$$\n",
    "\\text{scores}[0, 1, 2]\n",
    "= (2×1) + (4×3) + (6×5)\n",
    "= 2 + 12 + 30 = 44\n",
    "$$\n",
    "\n",
    "So the sum over `d` is simply the dot product between\n",
    "the 3-element query vector of token 1 and the 3-element key vector of token 2.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How `einsum` sees it**\n",
    "\n",
    "The pattern `'thd,Thd->htT'` gives a very literal mapping:\n",
    "\n",
    "| Symbol | Role        | Appears in   | Action                       |\n",
    "| ------ | ----------- | ------------ | ---------------------------- |\n",
    "| `t`    | query token | first input  | kept in output               |\n",
    "| `T`    | key token   | second input | kept in output               |\n",
    "| `h`    | head index  | both inputs  | kept (same head in both)     |\n",
    "| `d`    | feature dim | both inputs  | **summed over** (disappears) |\n",
    "\n",
    "Thus `einsum` automatically loops and sums over `d`:\n",
    "\n",
    "```\n",
    "for h in range(n_heads):\n",
    "  for t in range(T):\n",
    "    for T_ in range(T):\n",
    "      scores[h,t,T_] = sum(Q[t,h,d] * K[T_,h,d] for d in range(d_k))\n",
    "```\n",
    "\n",
    "This is what happens under the hood — just **vectorized and fused** in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Connecting to matrix multiplication**\n",
    "\n",
    "If we fix a head `h`, then\n",
    "\n",
    "$$\n",
    "\\text{scores}[h,:,:] = Q_h K_h^{\\top}\n",
    "$$\n",
    "\n",
    "where\n",
    "$Q_h$ = `[T, d_k]`,\n",
    "$K_h$ = `[T, d_k]`.\n",
    "\n",
    "And indeed,\n",
    "$(Q_h K_h^{\\top})[t, T] = \\sum_d Q[t,d] K[T,d]$.\n",
    "\n",
    "The einsum simply performs this for *all heads in parallel*.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Shape recap**\n",
    "\n",
    "| Tensor | Shape             | Meaning                          |\n",
    "| ------ | ----------------- | -------------------------------- |\n",
    "| Q      | [T, n_heads, d_k] | query vectors per token per head |\n",
    "| K      | [T, n_heads, d_k] | key vectors per token per head   |\n",
    "| scores | [n_heads, T, T]   | per-head attention matrices      |\n",
    "\n",
    "Each `[T, T]` matrix contains dot-products between every query and key pair for one head.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finally:\n",
    "\n",
    "```python\n",
    "out = out.reshape(T, d_model)  # [T, d_model]\n",
    "out = out @ W_O  # final projection\n",
    "```\n",
    "\n",
    "| Quantity                       | Shape                                 | Description                               |\n",
    "| ------------------------------ | ------------------------------------- | ----------------------------------------- |\n",
    "| `attn`                         | [n_heads, T, T]                       | attention weights per head                |\n",
    "| `V`                            | [T, n_heads, d_k]                     | value vectors per token & head            |\n",
    "| `out = einsum('htT,Thd->thd')` | [T, n_heads, d_k]                     | weighted sum of values per head per query |\n",
    "| Next                           | reshape to [T, n_heads×d_k] → project | combine heads                             |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5df7cd-121e-4e06-ad92-cea579be856e",
   "metadata": {},
   "source": [
    "#### **5.5 Numerical Example**\n",
    "\n",
    "Small example $T=3, n_{heads}=2, d_k=2$:\n",
    "\n",
    "For $t=0$, $h=0$, $d=0$: sum over $T$ of $attn[0,0,T] * V[T,0,0]$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\text{scores}[h, t, T] = \\sum_d Q[t, h, d] , K[T, h, d]\n",
    "$$\n",
    "and **expand the summation step-by-step** so you can *see* what’s happening inside the big sigma.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Perfect — let’s unpack that summation in detail.\n",
    "\n",
    "You wrote:\n",
    "$$\n",
    "\\text{scores}[h, t, T] = \\sum_d Q[t, h, d] , K[T, h, d]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ddf5d-3fb0-495b-89fa-08e95372b791",
   "metadata": {},
   "source": [
    "#### **Softmax and Output**\n",
    "weights = torch.softmax(scores, dim=-1)  # [n_heads, T, T]\n",
    "\n",
    "Then:\n",
    "```python\n",
    "out = torch.einsum('htT,Thd->thd', weights, V)  # [T, n_heads, d_k]\n",
    "```\n",
    "or attn @ V.permute(1, 0, 2)\n",
    "\n",
    "Mathematically:\n",
    "$$\\text{out}[t,h,d] = \\sum_{T} \\text{weights}[h,t,T] \\times V[T,h,d]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc59c0-c2bc-44a4-88ed-a2c61852c10e",
   "metadata": {},
   "source": [
    "#### **5.6. Contextual Meaning Updates (ΔE)**\n",
    "How the Transformer turns raw attention into **contextual meaning updates (ΔE)**.\n",
    "\n",
    "Let’s connect everything you’ve already built (Q, K, V → Attention → out = attn @ V) to what happens next:\n",
    "**residual addition, normalization, and the MLP (feed-forward network).**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Where we are**\n",
    "\n",
    "After\n",
    "$$\n",
    "\\text{out} = \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right) V,\n",
    "$$\n",
    "you now have a **context-aware vector for each token**:\n",
    "\n",
    "| Token    | out (new context-aware embedding) |\n",
    "| -------- | --------------------------------: |\n",
    "| fluffy   |                      [3.00, 1.00] |\n",
    "| blue     |                      [2.74, 1.53] |\n",
    "| creature |                      [2.71, 1.73] |\n",
    "| forest   |                      [2.62, 1.84] |\n",
    "\n",
    "This `out` is often called the **attention output** or **context vector**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Apply the output projection (W^O)**\n",
    "\n",
    "In multi-head attention we would first concatenate all heads,\n",
    "but since this is single-head, we just apply a learned linear transform:\n",
    "\n",
    "$$\n",
    "\\text{contexted} = \\text{out} W^O\n",
    "$$\n",
    "\n",
    "where\n",
    "$W^O \\in \\mathbb{R}^{d_k \\times d_{\\text{model}}}$.\n",
    "\n",
    "It re-mixes the per-head information back into model space.\n",
    "Shape stays `[T, d_model]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae0c6c-1ebe-47a1-ab05-5bb9710706ad",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **5.7. Transformer Layer with Residual (skip) paths**\n",
    "\n",
    "```\n",
    "is this attention block mechanism drawing is correct: \n",
    "E^(l) ────┬────────────┐\n",
    "          │            │              \n",
    "          │            │  (1) Attention Sub-layer\n",
    "          │            │         \n",
    "          │     LayerNorm(E^(l))\n",
    "          │            │                               \n",
    "          │            ▼                               \n",
    "          │     Multi-Head Attention(Q, K, V)          \n",
    "          │            │                               \n",
    "          │            ▼                               \n",
    "          │     out × W^O = ΔE_attn^(l)    \n",
    "          │            │                               \n",
    "          │            ▼                               \n",
    "          └────────► A^(l) = E^(l) + ΔE_attn^(l) ───────────────┐\n",
    "                       │                                        │\n",
    "                       │                                        │   (2) MLP Sub-layer\n",
    "                       │                                        ▼\n",
    "                       │                                 LayerNorm(A^(l))\n",
    "                       │                                        │\n",
    "                       │                                        ▼\n",
    "                       │                                 ΔE_mlp^(l) = W₂ σ(W₁ A^(l) + b₁) + b₂\n",
    "                       │                                        │\n",
    "                       │                                        ▼\n",
    "                       └────────────────────────────────► E^(l+1) = A^(l) + ΔE_mlp^(l)\n",
    "                                                                │                   \n",
    "                                            \t                ▼ \n",
    "                                                  E^(l+1) → input to next block (l+1)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff57183-5f6f-4d75-89dc-a3c7ed5eebf4",
   "metadata": {},
   "source": [
    "Each Transformer block has two major submodules:\n",
    "\n",
    "1. Multi-Head Attention (MHA)\n",
    "2. Feed-Forward Network (MLP)\n",
    "\n",
    "Each of these has its own residual connection.\n",
    "\n",
    "That is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X' &= X + \\text{MHA}(\\text{LayerNorm}(X)) \\\\\n",
    "Y' &= X' + \\text{MLP}(\\text{LayerNorm}(X'))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Step | Equation                                                         | Meaning                        |\n",
    "| ---- | ---------------------------------------------------------------- | ------------------------------ |\n",
    "| ①    | $ \\text{LayerNorm}(E^{(l)}) $                                    | stabilize inputs               |\n",
    "| ②    | $ \\text{MHA}(Q,K,V) $                                            | compute context between tokens |\n",
    "| ③    | $ \\Delta E_{\\text{attn}}^{(l)} = \\text{out} W^O $                | the attention update           |\n",
    "| ④    | $ A^{(l)} = E^{(l)} + \\Delta E_{\\text{attn}}^{(l)} $             | **first residual add**         |\n",
    "| ⑤    | $ \\text{LayerNorm}(A^{(l)}) $                                    | normalize before MLP           |\n",
    "| ⑥    | $ \\Delta E_{\\text{mlp}}^{(l)} = W_2 σ(W_1 A^{(l)} + b_1) + b_2 $ | nonlinear refinement           |\n",
    "| ⑦    | $ E^{(l+1)} = A^{(l)} + \\Delta E_{\\text{mlp}}^{(l)} $            | **second residual add**        |\n",
    "| ⑧    | Pass $E^{(l+1)}$ to next layer                                   | token embeddings updated       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8bcb0-c1ed-401b-9123-b290dbdc04af",
   "metadata": {},
   "source": [
    "#### Stack N layers\n",
    "\n",
    "\n",
    "```\n",
    " Input Tokens\n",
    "      │\n",
    "      ▼\n",
    " [Embedding + Positional Encoding]  →  X₀ \n",
    "      │\n",
    "      ▼\n",
    " ╔════════════════════════════════════════════════════════════╗\n",
    " ║                 Transformer Block 1                        ║\n",
    " ║   X₁ = X₀ + MHA(LN(X₀))                                    ║ \n",
    " ║   X₁ = X₁ + MLP(LN(X₁))                                    ║\n",
    " ╚════════════════════════════════════════════════════════════╝\n",
    "      │\n",
    "      ▼\n",
    " ╔════════════════════════════════════════════════════════════╗\n",
    " ║                 Transformer Block 2                        ║\n",
    " ║   X₂ = X₁ + MHA(LN(X₁))                                    ║\n",
    " ║   X₂ = X₂ + MLP(LN(X₂))                                    ║\n",
    " ╚════════════════════════════════════════════════════════════╝\n",
    "      │\n",
    "      ▼\n",
    "             ⋮   (repeated N times)\n",
    "      ▼\n",
    " ╔════════════════════════════════════════════════════════════╗\n",
    " ║                 Transformer Block N                        ║\n",
    " ║   X_N = X_{N-1} + MHA(LN(X_{N-1}))                         ║\n",
    " ║   X_N = X_N + MLP(LN(X_N))                                 ║\n",
    " ╚════════════════════════════════════════════════════════════╝\n",
    "      │\n",
    "      ▼\n",
    " [Final Embedding E_final = X_N]\n",
    "      │\n",
    "      ▼\n",
    " Linear Projection to Vocabulary  →  Softmax  →  Next-token probabilities\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789558ac-2629-4711-9b60-f27ccc4033e2",
   "metadata": {},
   "source": [
    "#### Transformer Blocks and Layers\n",
    "Each block: Multi-head attention + MLP + residuals + layer norms.\n",
    "\n",
    "Stack N blocks for depth.\n",
    "\n",
    "Typical counts:\n",
    "| Model                | Layers (blocks) | d_model | n_heads |\n",
    "| -------------------- | --------------- | ------- | ------- |\n",
    "| **ViT-Tiny**         | 12              | 192     | 3       |\n",
    "| **ViT-Base (B/16)**  | 12              | 768     | 12      |\n",
    "| **ViT-Large (L/16)** | 24              | 1024    | 16      |\n",
    "| **GPT-2 Small**      | 12              | 768     | 12      |\n",
    "| **GPT-3 (175B)**     | 96              | 12288   | 96      |\n",
    "| **BERT Base**        | 12              | 768     | 12      |\n",
    "| **BERT Large**       | 24              | 1024    | 16      |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116573f3-6f59-4224-b5c6-167b6e10aa91",
   "metadata": {},
   "source": [
    "Each block takes an input $E^{(l)}$ and outputs the **next hidden state**:\n",
    "\n",
    "$$E^{(l+1)} = E^{(l)} + \\Delta E_{\\text{attn}}^{(l)} + \\Delta E_{\\text{mlp}}^{(l)}$$\n",
    "\n",
    "So the block’s **output is the full updated embedding**, not just the delta.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E^{(1)} &= E^{(0)} + \\Delta E^{(1)} \\\\\n",
    "E^{(2)} &= E^{(1)} + \\Delta E^{(2)} = E^{(0)} + \\Delta E^{(1)} + \\Delta E^{(2)} \\\\\n",
    "&\\vdots \\\\\n",
    "E^{(N)} &= E^{(0)} + \\sum_{l=1}^{N} \\Delta E^{(l)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $$\\Delta E^{(l)} = \\Delta E_{\\text{attn}}^{(l)} + \\Delta E_{\\text{mlp}}^{(l)}$$ is the **increment (residual)** contributed by layer $l$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E^{(N)} = E^{(0)} + \\sum_{l=1}^{N}\n",
    "\\left(\n",
    "\\Delta E_{\\text{attn}}^{(l)} + \\Delta E_{\\text{mlp}}^{(l)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The final contextual embeddings $E^{(N)}$ are rich, high-level representations of every token, built by **accumulating small ΔE updates layer by layer**.\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "* **At runtime:**\n",
    "  The model doesn’t explicitly accumulate deltas — it just passes the full $E^{(l)}$ forward.\n",
    "\n",
    "* **Conceptually (for understanding):**\n",
    "  You can view the final embedding as the **initial embedding plus the sum of all residual updates**, hence:\n",
    "  $$\n",
    "  E_{\\text{final}} = E^{(0)} + \\sum_{l=1}^{N} \\Delta E^{(l)}\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n",
    "✅ **Key takeaway**\n",
    "\n",
    "* $E^{(l)}$ → input embeddings to the layer\n",
    "* $ΔE_{\\text{attn}}^{(l)}$ → contextual change from attention\n",
    "* $ΔE_{\\text{mlp}}^{(l)}$ → per-token change from MLP\n",
    "* Residual paths ensure these deltas are **added**, not overwriting\n",
    "* The result $E^{(l+1)}$ flows into the next attention layer\n",
    "\n",
    "You now have the correct structure exactly as in GPT-style Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7d5ab-c66b-4613-a4d5-20298f48b3f5",
   "metadata": {},
   "source": [
    "## **6. MLP**\n",
    "The **MLP part** (also called the *feed-forward network*, or FFN) is **identical in structure** for **both single-head and multi-head** attention models.\n",
    "Its purpose and shape behavior do **not** depend on how many heads you have.\n",
    "\n",
    "####  **What goes into the MLP**\n",
    "\n",
    "After attention — whether it’s single- or multi-head — the output is:\n",
    "\n",
    "$$\n",
    "Y \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "* In **single-head**, `Y` comes directly from one attention matrix (since $n_{\\text{heads}}=1)$.\n",
    "* In **multi-head**, we first concatenate all heads $n_{heads} × d_k = d_{model}$, then project back to `d_model` using $W^O$, so again `Y` has the same shape.\n",
    "\n",
    "So the input to the MLP is **always `[T, d_model]`.**\n",
    "\n",
    "---\n",
    "\n",
    "####  **Structure of the MLP**\n",
    "\n",
    "It’s applied **independently to each token** (position-wise):\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x) = W_2,\\sigma(W_1,x + b_1) + b_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$\n",
    "* $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$\n",
    "* and $\\sigma$ is usually GELU or ReLU.\n",
    "\n",
    "Typical: $d_{\\text{ff}} = 4 \\times d_{\\text{model}}$\n",
    "\n",
    "So for example, if\n",
    "$d_{\\text{model}} = 12288$,\n",
    "then $d_{\\text{ff}} = 49152$.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Shape behavior**\n",
    "\n",
    "| Stage             | Operation      | Shape           |\n",
    "| ----------------- | -------------- | --------------- |\n",
    "| Input             | `[T, d_model]` | input to MLP    |\n",
    "| Linear 1          | `[T, d_ff]`    | expansion       |\n",
    "| Activation (GELU) | `[T, d_ff]`    | nonlinearity    |\n",
    "| Linear 2          | `[T, d_model]` | projection back |\n",
    "| Output            | `[T, d_model]` | same as input   |\n",
    "\n",
    "So the **output shape matches the input shape**, allowing the residual connection:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{MLP}(Y) + Y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  **Why it’s the same for single or multi-head**\n",
    "\n",
    "The only difference between single-head and multi-head attention lies in how the **attention output** `Y` is computed:\n",
    "\n",
    "| Stage              | Single-head                           | Multi-head                                    |\n",
    "| ------------------ | ------------------------------------- | --------------------------------------------- |\n",
    "| Attention output   | `[T, d_model]` directly from one head | `[T, n_heads, d_k]` → concat → `[T, d_model]` |\n",
    "| Feed-forward (MLP) | `[T, d_model] → [T, d_model]`         | `[T, d_model] → [T, d_model]`                 |\n",
    "\n",
    "Once you reach the MLP, both are *identical*.\n",
    "The Transformer block expects and produces `[T, d_model]` regardless of how many heads are inside the attention.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4175863-bd71-4049-a89b-06a31d4eaa82",
   "metadata": {},
   "source": [
    "## 7. Unembedding\n",
    "\n",
    "\n",
    "The **“unembedding”** step is one of the most conceptually elegant parts of how models like ChatGPT (Transformers in general) turn their final embeddings back into **words**.\n",
    "\n",
    "Let’s go step by step and locate **exactly where and why** the *unembedding* happens.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.1. Big picture**\n",
    "\n",
    "Every Transformer language model has two critical mappings related to words (tokens):\n",
    "\n",
    "1. **Embedding**: turns token IDs → vectors\n",
    "   $$ x_i = W_E[\\text{token}_i] $$\n",
    "   where\n",
    "   $ W_E \\in \\mathbb{R}^{V \\times d_{\\text{model}}} $\n",
    "\n",
    "2. **Unembedding**: turns vectors → vocabulary logits\n",
    "   $$ \\text{logits} = E_{\\text{final}} W_U^\\top $$\n",
    "   where\n",
    "   $ W_U \\in \\mathbb{R}^{V \\times d_{\\text{model}}} $\n",
    "\n",
    "and typically **$W_U = W_E$** — i.e., the *same weights reused*.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.2. Where it happens**\n",
    "\n",
    "If we follow the data flow:\n",
    "\n",
    "```\n",
    "Tokens → Embedding → Transformer Layers → Final Embedding (E_final)\n",
    "                                     ↓\n",
    "                              Unembedding (W_U)\n",
    "                                     ↓\n",
    "                              Softmax over vocabulary\n",
    "                                     ↓\n",
    "                              Next-token prediction\n",
    "```\n",
    "\n",
    "So **unembedding happens right after the final Transformer layer**, *before* softmax.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.3. The operation itself**\n",
    "\n",
    "If you denote the final hidden state (the contextual embedding) as:\n",
    "\n",
    "$$\n",
    "E_{\\text{final}} \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "then the unembedding step computes the **raw logits** for every vocabulary word:\n",
    "\n",
    "$$\n",
    "\\text{logits} = E_{\\text{final}} W_U^\\top\n",
    "$$\n",
    "\n",
    "* $E_{\\text{final}}$: contextual representation per token\n",
    "* $W_U^\\top$: matrix of size $[d_{\\text{model}}, V]$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$\n",
    "\\text{logits} \\in \\mathbb{R}^{T \\times V}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "P(\\text{next token} | \\text{context}) = \\text{softmax}(\\text{logits})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.4. Why it’s called *unembedding***\n",
    "\n",
    "At the beginning, you *embed* discrete tokens into continuous vectors using $W_E$:\n",
    "\n",
    "$$\n",
    "\\text{token index}_i \\to W_E[i] \\in \\mathbb{R}^{d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "At the end, you *reverse* that process — you project a continuous vector back into the discrete **vocabulary space** using $W_U$.\n",
    "\n",
    "Hence: **embedding → unembedding**.\n",
    "\n",
    "---\n",
    "\n",
    "###### **7.5. Weight tying (reusing W_E)**\n",
    "\n",
    "In GPT-like models, the **embedding** and **unembedding** weights are the same:\n",
    "\n",
    "$$\n",
    "W_U = W_E\n",
    "$$\n",
    "\n",
    "That is, the same word vectors used to *encode* tokens are reused to *decode* meaning back to tokens.\n",
    "\n",
    "This trick:\n",
    "\n",
    "* Reduces parameter count,\n",
    "* Improves coherence between embedding and output spaces,\n",
    "* Was first formalized as **“weight tying”** (Press & Wolf, 2017).\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.6. Example in PyTorch terms**\n",
    "\n",
    "```python\n",
    "# Embedding\n",
    "x = self.token_embedding(tokens)  # [T, d_model]\n",
    "\n",
    "# Pass through N transformer blocks\n",
    "for block in self.blocks:\n",
    "    x = block(x)\n",
    "\n",
    "# Unembedding\n",
    "logits = x @ self.token_embedding.weight.T  # [T, vocab_size]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "```\n",
    "\n",
    "Note: `self.token_embedding.weight` is reused for the output layer (weight tying).\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.7. Summary**\n",
    "\n",
    "| Stage             | Operation           | Symbol                       | Shape           | Description               |\n",
    "| ----------------- | ------------------- | ---------------------------- | --------------- | ------------------------- |\n",
    "| Embedding         | Token → Vector      | (x = W_E[\\text{token}])      | [T, d_model]    | input embedding           |\n",
    "| Transformer stack | Context mixing      | (x = f(x))                   | [T, d_model]    | contextual representation |\n",
    "| **Unembedding**   | Vector → Vocabulary | (x W_E^\\top)                 | [T, vocab_size] | output logits             |\n",
    "| Softmax           | Normalize           | (\\text{softmax}(x W_E^\\top)) | [T, vocab_size] | token probabilities       |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short**\n",
    "\n",
    "* **Embedding** happens at the start → converts tokens into continuous vectors.\n",
    "* **Unembedding** happens at the end → converts contextual vectors back into token logits.\n",
    "* **Weight tying** means they’re often the same matrix.\n",
    "* This is where ChatGPT “chooses” the next word — the unembedding converts meaning back into vocabulary space.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706abaeb-f966-4325-975e-07e92579a006",
   "metadata": {},
   "source": [
    "## **8. Transformer Numerical Example**\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Sentence\n",
    "\n",
    "> “a fluffy blue creature roams the verdant forest”\n",
    "\n",
    "Let’s limit ourselves to **4 tokens** to keep it small:\n",
    "\n",
    "```\n",
    "[fluffy, blue, creature, forest]\n",
    "```\n",
    "\n",
    "and assume an **embedding dimension = 2**\n",
    "(so each word is represented by a 2D vector).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Embeddings (input X)\n",
    "\n",
    "We’ll invent simple numbers to represent each token’s embedding:\n",
    "\n",
    "| Word     | Embedding (X) |\n",
    "| -------- | ------------- |\n",
    "| fluffy   | [1.0, 2.0]    |\n",
    "| blue     | [2.0, 0.5]    |\n",
    "| creature | [1.5, 1.5]    |\n",
    "| forest   | [0.5, 2.5]    |\n",
    "\n",
    "Thus\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0\\\\\n",
    "2.0 & 0.5\\\\\n",
    "1.5 & 1.5\\\\\n",
    "0.5 & 2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Shape: [T=4, d_model=2]\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Learned projection matrices\n",
    "\n",
    "We’ll make small random projection matrices\n",
    "to produce Q, K, and V.\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "$$\n",
    "W^Q =\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W^K =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5\\\\\n",
    "0.5 & -0.5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W^V =\n",
    "\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Compute Q, K, V\n",
    "\n",
    "$$Q = X W^Q = X$$\n",
    "(same as input here, since (W^Q) is identity)\n",
    "\n",
    "$$K = X W^K, \\quad V = X W^V$$\n",
    "\n",
    "Compute them:\n",
    "\n",
    "####  K = X W^K\n",
    "\n",
    "```\n",
    "fluffy:  [1.0*0.5 + 2.0*0.5,  1.0*0.5 + 2.0*(-0.5)] = [1.5, -0.5]\n",
    "blue:    [2.0*0.5 + 0.5*0.5,  2.0*0.5 + 0.5*(-0.5)] = [1.25, 0.75]\n",
    "creature:[1.5*0.5 + 1.5*0.5,  1.5*0.5 + 1.5*(-0.5)] = [1.5, 0.0]\n",
    "forest:  [0.5*0.5 + 2.5*0.5,  0.5*0.5 + 2.5*(-0.5)] = [1.5, -1.0]\n",
    "```\n",
    "\n",
    "#### V = X W^V\n",
    "\n",
    "```\n",
    "fluffy:  [1.0*1 + 2.0*1, 1.0*1 + 2.0*0] = [3.0, 1.0]\n",
    "blue:    [2.0*1 + 0.5*1, 2.0*1 + 0.5*0] = [2.5, 2.0]\n",
    "creature:[1.5*1 + 1.5*1, 1.5*1 + 1.5*0] = [3.0, 1.5]\n",
    "forest:  [0.5*1 + 2.5*1, 0.5*1 + 2.5*0] = [3.0, 0.5]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Compute attention scores = Q Kᵀ / √dₖ\n",
    "\n",
    "Since $d_k = 2$, divide by √2 ≈ 1.414.\n",
    "\n",
    "#### Step 1: QKᵀ (dot products)\n",
    "\n",
    "Each entry $s_{t,T} = Q_t \\cdot K_T$.\n",
    "\n",
    "Compute roughly:\n",
    "\n",
    "| Query ↓ / Key → |                    fluffy |                       blue |               creature |                    forest |\n",
    "| --------------- | ------------------------: | -------------------------: | ---------------------: | ------------------------: |\n",
    "| **fluffy**      |      (1·1.5 + 2·-0.5)=0.5 |     (1·1.25 + 2·0.75)=2.75 |    (1·1.5 + 2·0.0)=1.5 |     (1·1.5 + 2·-1.0)=-0.5 |\n",
    "| **blue**        |   (2·1.5 + 0.5·-0.5)=2.75 |   (2·1.25 + 0.5·0.75)=2.88 |    (2·1.5 + 0.5·0)=3.0 |      (2·1.5 + 0.5·-1)=2.5 |\n",
    "| **creature**    |  (1.5·1.5 + 1.5·-0.5)=1.5 |  (1.5·1.25 + 1.5·0.75)=3.0 | (1.5·1.5 + 1.5·0)=2.25 |   (1.5·1.5 + 1.5·-1)=0.75 |\n",
    "| **forest**      | (0.5·1.5 + 2.5·-0.5)=-0.5 | (0.5·1.25 + 2.5·0.75)=2.25 | (0.5·1.5 + 2.5·0)=0.75 | (0.5·1.5 + 2.5·-1)= -1.25 |\n",
    "\n",
    "Now divide all by √2 ≈ 1.414.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Before softmax** — raw scores\n",
    "\n",
    "For your 4-token example (`[fluffy, blue, creature, forest]`) we computed:\n",
    "\n",
    "| Query ↓ / Key → | fluffy | blue | creature | forest |\n",
    "| --------------- | -----: | ---: | -------: | -----: |\n",
    "| **fluffy**      |    0.5 | 2.75 |      1.5 |   –0.5 |\n",
    "| **blue**        |   2.75 | 2.88 |      3.0 |    2.5 |\n",
    "| **creature**    |    1.5 |  3.0 |     2.25 |   0.75 |\n",
    "| **forest**      |   –0.5 | 2.25 |     0.75 |  –1.25 |\n",
    "\n",
    "(we’d divide each by √2 ≈ 1.414 before softmax, but scaling doesn’t change the pattern much.)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **6. Apply the causal mask**\n",
    "\n",
    "Causal mask means:\n",
    "each token can only attend to itself **and earlier tokens**,\n",
    "not to future ones.\n",
    "\n",
    "We replace **future** entries with `–∞` (conceptually; in code we set a large negative number before softmax).\n",
    "\n",
    "Masked table:\n",
    "\n",
    "| Query ↓ / Key → | fluffy | blue | creature | forest |\n",
    "| --------------- | -----: | ---: | -------: | -----: |\n",
    "| **fluffy**      |    0.5 |   –∞ |       –∞ |     –∞ |\n",
    "| **blue**        |   2.75 | 2.88 |       –∞ |     –∞ |\n",
    "| **creature**    |    1.5 |  3.0 |     2.25 |     –∞ |\n",
    "| **forest**      |   –0.5 | 2.25 |     0.75 |  –1.25 |\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Compute softmax row-wise**\n",
    "\n",
    "We’ll softmax each row **over non-masked keys only**.\n",
    "Remember:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 1 — Query = “fluffy”\n",
    "\n",
    "Allowed: [0.5]\n",
    "Softmax → [1.0]\n",
    "\n",
    "| fluffy | blue | creature | forest |\n",
    "| :----: | :--: | :------: | :----: |\n",
    "|  1.00  | 0.00 |   0.00   |  0.00  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 2 — Query = “blue”\n",
    "\n",
    "Allowed: [2.75, 2.88]\n",
    "\n",
    "Exponentials:\n",
    "$$ e^{2.75} = 15.64, ; e^{2.88} = 17.79 $$\n",
    "Sum = 33.43\n",
    "\n",
    "Softmax:\n",
    "\n",
    "* fluffy = 15.64 / 33.43 = 0.47\n",
    "* blue = 17.79 / 33.43 = 0.53\n",
    "\n",
    "| fluffy | blue | creature | forest |\n",
    "| :----: | :--: | :------: | :----: |\n",
    "|  0.47  | 0.53 |   0.00   |  0.00  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 3 — Query = “creature”\n",
    "\n",
    "Allowed: [1.5, 3.0, 2.25]\n",
    "\n",
    "Exponentials:\n",
    "$$ e^{1.5}=4.48, ; e^{3.0}=20.09, ; e^{2.25}=9.49 $$\n",
    "Sum = 34.06\n",
    "\n",
    "Softmax:\n",
    "\n",
    "* fluffy = 4.48 / 34.06 = 0.13\n",
    "* blue = 20.09 / 34.06 = 0.59\n",
    "* creature = 9.49 / 34.06 = 0.28\n",
    "\n",
    "| fluffy | blue | creature | forest |\n",
    "| :----: | :--: | :------: | :----: |\n",
    "|  0.13  | 0.59 |   0.28   |  0.00  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 4 — Query = “forest”\n",
    "\n",
    "Allowed: [–0.5, 2.25, 0.75, –1.25]\n",
    "\n",
    "Exponentials:\n",
    "$$ e^{-0.5}=0.61,; e^{2.25}=9.49,; e^{0.75}=2.12,; e^{-1.25}=0.29 $$\n",
    "Sum = 12.51\n",
    "\n",
    "Softmax:\n",
    "\n",
    "* fluffy = 0.61 / 12.51 = 0.05\n",
    "* blue = 9.49 / 12.51 = 0.76\n",
    "* creature = 2.12 / 12.51 = 0.17\n",
    "* forest = 0.29 / 12.51 = 0.02\n",
    "\n",
    "| fluffy | blue | creature | forest |\n",
    "| :----: | :--: | :------: | :----: |\n",
    "|  0.05  | 0.76 |   0.17   |  0.02  |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Final softmaxed (masked) attention weights**\n",
    "\n",
    "| Query ↓ / Key → | fluffy | blue | creature | forest |\n",
    "| --------------- | -----: | ---: | -------: | -----: |\n",
    "| **fluffy**      |   1.00 | 0.00 |     0.00 |   0.00 |\n",
    "| **blue**        |   0.47 | 0.53 |     0.00 |   0.00 |\n",
    "| **creature**    |   0.13 | 0.59 |     0.28 |   0.00 |\n",
    "| **forest**      |   0.05 | 0.76 |     0.17 |   0.02 |\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. The Value matrix V**\n",
    "\n",
    "Recall from before:\n",
    "\n",
    "| Token    | V vector   |\n",
    "| -------- | ---------- |\n",
    "| fluffy   | [3.0, 1.0] |\n",
    "| blue     | [2.5, 2.0] |\n",
    "| creature | [3.0, 1.5] |\n",
    "| forest   | [3.0, 0.5] |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Compute `out = attn @ V`**\n",
    "\n",
    "This is matrix multiplication `[T×T] @ [T×dₖ] → [T×dₖ]`.\n",
    "\n",
    "Each query row (attention weights) acts as a set of weights applied to all Value vectors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 1 — “fluffy”\n",
    "\n",
    "Weights = [1.00, 0, 0, 0]\n",
    "\n",
    "→ output = 1×V_fluffy = [3.0, 1.0]\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 2 — “blue”\n",
    "\n",
    "Weights = [0.47, 0.53, 0, 0]\n",
    "\n",
    "→ output = 0.47×[3.0,1.0] + 0.53×[2.5,2.0]\n",
    "= [1.41+1.33, 0.47+1.06] = [2.74, 1.53]\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 3 — “creature”\n",
    "\n",
    "Weights = [0.13, 0.59, 0.28, 0]\n",
    "\n",
    "→ output = 0.13×[3.0,1.0] + 0.59×[2.5,2.0] + 0.28×[3.0,1.5]\n",
    "= [0.39+1.48+0.84, 0.13+1.18+0.42]\n",
    "= [2.71, 1.73]\n",
    "\n",
    "---\n",
    "\n",
    "#### Row 4 — “forest”\n",
    "\n",
    "Weights = [0.05, 0.76, 0.17, 0.02]\n",
    "\n",
    "→ output = 0.05×[3.0,1.0] + 0.76×[2.5,2.0] + 0.17×[3.0,1.5] + 0.02×[3.0,0.5]\n",
    "= [0.15+1.90+0.51+0.06, 0.05+1.52+0.26+0.01]\n",
    "= [2.62, 1.84]\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Final output vectors**\n",
    "\n",
    "| Query token | Weighted V output (new embedding) |\n",
    "| ----------- | --------------------------------: |\n",
    "| fluffy      |                      [3.00, 1.00] |\n",
    "| blue        |                      [2.74, 1.53] |\n",
    "| creature    |                      [2.71, 1.73] |\n",
    "| forest      |                      [2.62, 1.84] |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary of the whole computation**\n",
    "\n",
    "| Step | Operation              | Shape   | Meaning                       |\n",
    "| ---- | ---------------------- | ------- | ----------------------------- |\n",
    "| 1    | Compute `QKᵀ / √dₖ`    | [T, T]  | raw similarity                |\n",
    "| 2    | Apply causal mask      | [T, T]  | block future tokens           |\n",
    "| 3    | Softmax over last axis | [T, T]  | normalized attention weights  |\n",
    "| 4    | Multiply by V          | [T, dₖ] | weighted sum of Value vectors |\n",
    "| 5    | Result                 | [T, dₖ] | new contextual embeddings     |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213a23b-9396-49cd-a8f6-b5356064c10b",
   "metadata": {},
   "source": [
    "#### **Output Projection**\n",
    "\n",
    "In multi-head attention this projection combines all heads into model space;\n",
    "we’ll use a simple **output matrix (W^O)**:\n",
    "\n",
    "$$\n",
    "W^O =\n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.2\\\\\n",
    "0.1 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute `contexted = out × W^O`:\n",
    "\n",
    "| Token    | out          | contexted = out × W^O                                                                    |\n",
    "| :------- | :----------- | :--------------------------------------------------------------------------------------- |\n",
    "| fluffy   | [3.00, 1.00] | [3×0.8 + 1×0.1, 3×0.2 + 1×0.9] = [2.4 + 0.1, 0.6 + 0.9] = [2.5, 1.5]                     |\n",
    "| blue     | [2.74, 1.53] | [2.74×0.8 + 1.53×0.1, 2.74×0.2 + 1.53×0.9] = [2.192+0.153, 0.548+1.377] = [2.345, 1.925] |\n",
    "| creature | [2.71, 1.73] | [2.168+0.173, 0.542+1.557] = [2.341, 2.099]                                              |\n",
    "| forest   | [2.62, 1.84] | [2.096+0.184, 0.524+1.656] = [2.280, 2.180]                                              |\n",
    "\n",
    "So:\n",
    "\n",
    "| Token    | ΔE_attn = contexted |\n",
    "| -------- | ------------------: |\n",
    "| fluffy   |          [2.5, 1.5] |\n",
    "| blue     |        [2.35, 1.93] |\n",
    "| creature |        [2.34, 2.10] |\n",
    "| forest   |        [2.28, 2.18] |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e304a-fe50-4312-af48-d70675aa576b",
   "metadata": {},
   "source": [
    "#### **Add the residual (skip connection)**\n",
    "\n",
    "We add these ΔE’s to the **original input embeddings** $E^{(l)} = X$.\n",
    "\n",
    "Recall our original embeddings:\n",
    "\n",
    "| Token    | E^(l) (original) |\n",
    "| -------- | ---------------: |\n",
    "| fluffy   |       [1.0, 2.0] |\n",
    "| blue     |       [2.0, 0.5] |\n",
    "| creature |       [1.5, 1.5] |\n",
    "| forest   |       [0.5, 2.5] |\n",
    "\n",
    "Add them:\n",
    "\n",
    "| Token    |             A^(l) = E^(l) + ΔE_attn |\n",
    "| -------- | ----------------------------------: |\n",
    "| fluffy   |     [1.0+2.5, 2.0+1.5] = [3.5, 3.5] |\n",
    "| blue     | [2.0+2.35, 0.5+1.93] = [4.35, 2.43] |\n",
    "| creature | [1.5+2.34, 1.5+2.10] = [3.84, 3.60] |\n",
    "| forest   | [0.5+2.28, 2.5+2.18] = [2.78, 4.68] |\n",
    "\n",
    "These are the **updated embeddings after the attention sub-layer**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44023844-7e9a-496e-b003-9e9638010b89",
   "metadata": {},
   "source": [
    "#### **Apply the Feed-Forward (MLP) sublayer**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Structure of the MLP (feed-forward network)**\n",
    "\n",
    "Every token vector (for us of size 2) goes through the same small network:\n",
    "\n",
    "$$\n",
    "\\text{MLP}(x)\n",
    "= W_2  \\sigma(W_1 x + b_1) + b_2\n",
    "$$\n",
    "where\n",
    "\n",
    "* $x \\in \\mathbb{R}^{d_{\\text{model}}}$\n",
    "* $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$ (expands the dimension)\n",
    "* $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$ (compresses it back)\n",
    "* $\\sigma$ is a nonlinearity (ReLU or GELU)\n",
    "\n",
    "For our tiny example:\n",
    "$d_{\\text{model}} = 2,; d_{\\text{ff}} = 4$.\n",
    "\n",
    "---\n",
    "\n",
    "**What each matrix does**\n",
    "\n",
    "- (a) $W_1$\n",
    "\n",
    "Takes the 2-D token $[x_1, x_2]$ and maps it to a 4-D “hidden” vector.\n",
    "It’s just a linear projection that learns four features from the two inputs.\n",
    "\n",
    "- (b) Nonlinearity $σ$\n",
    "\n",
    "Applies ReLU or GELU element-wise to inject non-linear behavior.\n",
    "\n",
    "- (c) $W_2$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**What happens next**\n",
    "\n",
    "That MLP output $y = ΔE_{\\text{mlp}}$\n",
    "is then *added* (residual connection) to the previous token embedding:\n",
    "\n",
    "$$\n",
    "E^{(l+1)} = A^{(l)} + ΔE_{\\text{mlp}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Stage             | Input → Output | Role                             |\n",
    "| ----------------- | -------------- | -------------------------------- |\n",
    "| $W_1$             | 2 → 4          | expand feature space             |\n",
    "| $σ$                 | 4 → 4          | apply nonlinearity               |\n",
    "| $W_2$             | 4 → 2          | compress back to model dimension |\n",
    "| $ΔE_{\\text{mlp}}$ | 2-vector       | MLP delta added to embedding     |\n",
    "\n",
    "So the “left” and “right” outputs were just the **two numbers** of the 2-D vector produced by (W_2); they correspond directly to the two coordinates of the model’s embedding for that token after the MLP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We’ll make an extremely simple 2-layer MLP:\n",
    "\n",
    "* $W_1 \\in \\mathbb{R}^{2\\times4}$, $W_2 \\in \\mathbb{R}^{4\\times2}$\n",
    "* We’ll use a ReLU activation.\n",
    "\n",
    "Let’s pick:\n",
    "\n",
    "$$\n",
    "W_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 1\\\\\n",
    "0 & 1 & 1 & 0\n",
    "\\end{bmatrix},\\quad\n",
    "W_2 =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5\\\\\n",
    "0.5 & 0.5\\\\\n",
    "0.2 & 0.8\\\\\n",
    "0.8 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: First linear layer (expand)\n",
    "\n",
    "Compute `hidden = A × W₁`\n",
    "(for each token, `[x1, x2] → [x1, x2, x2, x1]`)\n",
    "\n",
    "| Token    | A^(l)       | hidden (before ReLU)  |\n",
    "| -------- | ----------- | --------------------- |\n",
    "| fluffy   | [3.5,3.5]   | [3.5,3.5,3.5,3.5]     |\n",
    "| blue     | [4.35,2.43] | [4.35,2.43,2.43,4.35] |\n",
    "| creature | [3.84,3.60] | [3.84,3.60,3.60,3.84] |\n",
    "| forest   | [2.78,4.68] | [2.78,4.68,4.68,2.78] |\n",
    "\n",
    "ReLU doesn’t change these (all positive).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Second linear layer (compress)\n",
    "\n",
    "Compute `ΔE_mlp = hidden × W₂`:\n",
    "\n",
    "For example, for “blue”:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta E_{\\text{mlp}} &=\n",
    "\\begin{bmatrix}\n",
    "4.35 & 2.43 & 2.43 & 4.35\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "0.2 & 0.8 \\\\\n",
    "0.8 & 0.2\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "4.35(0.5)+2.43(0.5)+2.43(0.2)+4.35(0.8) \\\\\n",
    "4.35(0.5)+2.43(0.5)+2.43(0.8)+4.35(0.2)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "7.356 \\\\ 6.204\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So “blue” ΔE_mlp ≈ [7.36, 6.20]\n",
    "\n",
    "You can compute others similarly; we’ll summarize approximate results:\n",
    "\n",
    "| Token    |       ΔE_mlp |\n",
    "| -------- | -----------: |\n",
    "| fluffy   |   [7.0, 7.0] |\n",
    "| blue     | [7.36, 6.20] |\n",
    "| creature |   [6.8, 6.5] |\n",
    "| forest   |   [7.2, 6.9] |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Second residual add**\n",
    "\n",
    "Add these ΔE_mlp’s to the previous A^(l):\n",
    "\n",
    "| Token    |               E^(l+1) = A^(l) + ΔE_mlp |\n",
    "| -------- | -------------------------------------: |\n",
    "| fluffy   |      [3.5+7.0, 3.5+7.0] = [10.5, 10.5] |\n",
    "| blue     | [4.35+7.36, 2.43+6.20] = [11.71, 8.63] |\n",
    "| creature |  [3.84+6.8, 3.60+6.5] = [10.64, 10.10] |\n",
    "| forest   |   [2.78+7.2, 4.68+6.9] = [9.98, 11.58] |\n",
    "\n",
    "Now we have **final embeddings after one Transformer layer**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0c502-3a83-45bd-bd9e-127222398efb",
   "metadata": {},
   "source": [
    "## **6. Unembedding (vocabulary projection)**\n",
    "\n",
    "At the end, each token’s embedding is mapped to the vocabulary size (V).\n",
    "Let’s say our vocabulary is just **4 words** (`fluffy`, `blue`, `creature`, `forest`)\n",
    "and we use an **unembedding matrix** (tied to embedding weights):\n",
    "\n",
    "$$\n",
    "W_E =\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0\\\\\n",
    "2.0 & 0.5\\\\\n",
    "1.5 & 1.5\\\\\n",
    "0.5 & 2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Same as our input embedding table!\n",
    "The unembedding uses its **transpose**:\n",
    "\n",
    "$$\n",
    "W_U = W_E^{\\top} =\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5\\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute logits = $E^{(l+1)} W_U$\n",
    "Shape: [4 tokens × 4 vocab].\n",
    "\n",
    "---\n",
    "\n",
    "#### Example for “blue”:\n",
    "\n",
    "$$\n",
    "E_{\\text{blue}} = [11.71, 8.63]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{logits} = [11.71, 8.63]\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5\\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute each:\n",
    "\n",
    "| Vocab word |                        Logit |\n",
    "| ---------- | ---------------------------: |\n",
    "| fluffy     | 11.71×1.0 + 8.63×2.0 = 28.97 |\n",
    "| blue       | 11.71×2.0 + 8.63×0.5 = 28.64 |\n",
    "| creature   | 11.71×1.5 + 8.63×1.5 = 30.51 |\n",
    "| forest     | 11.71×0.5 + 8.63×2.5 = 28.49 |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Softmax (prediction)**\n",
    "\n",
    "Convert logits → probabilities:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{\\text{logit}_i}}{\\sum_j e^{\\text{logit}_j}}\n",
    "$$\n",
    "\n",
    "Here, “creature” has the highest logit ≈ 30.5,\n",
    "so the model predicts the next likely word = **“creature.”**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c98a4-49c2-4d45-a50b-e0c3a0e7bd4d",
   "metadata": {},
   "source": [
    "✅ Yes — that Python code is **entirely correct**, and it exactly implements the **unembedding and softmax** step we discussed.\n",
    "Let’s walk through it carefully and confirm every detail.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Code analysis**\n",
    "\n",
    "```python\n",
    "E_l_plus_1 = torch.tensor([\n",
    "    [10.5, 10.5],\n",
    "    [11.71, 8.63],\n",
    "    [10.64, 10.10],\n",
    "    [9.98, 11.58]\n",
    "])\n",
    "print(E_l_plus_1.shape)\n",
    "```\n",
    "\n",
    "**Shape:** `[4, 2]` → 4 tokens × 2-dim embedding size.\n",
    "Each row = final contextual embedding of one token. ✅\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "W_E = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [2.0, 0.5],\n",
    "    [1.5, 1.5],\n",
    "    [0.5, 2.5]\n",
    "])\n",
    "```\n",
    "\n",
    "This is your **embedding table**, shape `[4, 2]` →\n",
    "4 vocabulary words × 2 embedding dims. ✅\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "W_U = W_E.T\n",
    "print(W_U)\n",
    "```\n",
    "\n",
    "**Transpose:** shape `[2, 4]`, which is the correct **unembedding matrix**.\n",
    "It maps model-space embeddings `[*, 2] → [*, 4]` vocabulary logits. ✅\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "logits = E_l_plus_1 @ W_U\n",
    "print(logits)\n",
    "```\n",
    "\n",
    "Matrix multiplication: `[4, 2] @ [2, 4] = [4, 4]`.\n",
    "\n",
    "Each token now has a row of 4 logits, one for each vocabulary word. ✅\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "soft_values = torch.softmax(logits, dim=1)\n",
    "print(soft_values)\n",
    "```\n",
    "\n",
    "Applies softmax **along the vocabulary dimension** (`dim=1`),\n",
    "so each row sums to 1 — giving probabilities over 4 possible next words. ✅\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Expected shapes**\n",
    "\n",
    "| Variable      | Shape    | Meaning                 |\n",
    "| ------------- | -------- | ----------------------- |\n",
    "| `E_l_plus_1`  | `[4, 2]` | contextual embeddings   |\n",
    "| `W_E`         | `[4, 2]` | embedding matrix        |\n",
    "| `W_U`         | `[2, 4]` | unembedding matrix      |\n",
    "| `logits`      | `[4, 4]` | scores per vocab        |\n",
    "| `soft_values` | `[4, 4]` | probabilities per vocab |\n",
    "\n",
    "Everything lines up correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Optional improvement (dtype)**\n",
    "\n",
    "For real code, always set a floating dtype:\n",
    "\n",
    "```python\n",
    "E_l_plus_1 = torch.tensor([...], dtype=torch.float32)\n",
    "W_E = torch.tensor([...], dtype=torch.float32)\n",
    "```\n",
    "\n",
    "Otherwise, PyTorch may default to `torch.float64` (depending on version).\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Final check**\n",
    "\n",
    "Your code precisely performs:\n",
    "\n",
    "$$\n",
    "\\text{logits} = E^{(l+1)} W_E^{\\top}, \\quad\n",
    "p = \\text{softmax}(\\text{logits}, \\text{dim}=1)\n",
    "$$\n",
    "\n",
    "which is **the correct unembedding + probability prediction** step for a Transformer language model.\n",
    "\n",
    "You can print them cleanly as:\n",
    "\n",
    "```python\n",
    "for token_idx, probs in enumerate(soft_values):\n",
    "    print(f\"Token {token_idx}: probs={probs.tolist()}, sum={probs.sum().item():.4f}\")\n",
    "```\n",
    "\n",
    "Each `sum` should be `1.0`, confirming the softmax normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e815c-6a3a-4fa6-8ec6-b672b2bad03b",
   "metadata": {},
   "source": [
    "Perfect — let’s complete the **unembedding step** explicitly, both **before** and **after** the softmax, using our running numerical example for the 4 tokens:\n",
    "\n",
    "> “fluffy”, “blue”, “creature”, “forest”\n",
    "\n",
    "Each token now has a final embedding $ E^{(l+1)} $ of size 2 after all the residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Final embeddings after MLP residual**\n",
    "\n",
    "| Token    |  $ E^{(l+1)} $ |\n",
    "| :------- | :------------: |\n",
    "| fluffy   |  [10.5, 10.5]  |\n",
    "| blue     |  [11.71, 8.63] |\n",
    "| creature | [10.64, 10.10] |\n",
    "| forest   |  [9.98, 11.58] |\n",
    "\n",
    "Each of these is a 2-dimensional **model-space embedding** for that token.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Vocabulary (embedding) matrix**\n",
    "\n",
    "Let’s reuse the same 4-word vocabulary and embedding table we defined earlier:\n",
    "\n",
    "$$\n",
    "W_E =\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0\\\n",
    "2.0 & 0.5\\\n",
    "1.5 & 1.5\\\n",
    "0.5 & 2.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* Row 1 → “fluffy”\n",
    "* Row 2 → “blue”\n",
    "* Row 3 → “creature”\n",
    "* Row 4 → “forest”\n",
    "\n",
    "and the **unembedding matrix** is its transpose:\n",
    "\n",
    "$$\n",
    "W_U = W_E^{\\top} =\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Compute unembedding logits**\n",
    "\n",
    "We now compute:\n",
    "\n",
    "$$\n",
    "\\text{logits} = E^{(l+1)} W_U\n",
    "$$\n",
    "Shape: [4 tokens × 4 vocabulary].\n",
    "\n",
    "Each token gets one row of 4 logits — one per vocab word.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) For **“fluffy”**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "10.5 & 10.5\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5 \\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "10.5(1.0) + 10.5(2.0) &\n",
    "10.5(2.0) + 10.5(0.5) &\n",
    "10.5(1.5) + 10.5(1.5) &\n",
    "10.5(0.5) + 10.5(2.5)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "31.5 & 26.25 & 31.5 & 31.5\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "| Vocab word | Logit |\n",
    "| :--------- | ----: |\n",
    "| fluffy     |  31.5 |\n",
    "| blue       | 26.25 |\n",
    "| creature   |  31.5 |\n",
    "| forest     |  31.5 |\n",
    "\n",
    "---\n",
    "\n",
    "### (b) For **“blue”**\n",
    "\n",
    "$$\\begin{align*}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "11.71 & 8.63\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5 \\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "11.71(1.0) + 8.63(2.0) &\n",
    "11.71(2.0) + 8.63(0.5) &\n",
    "11.71(1.5) + 8.63(1.5) &\n",
    "11.71(0.5) + 8.63(2.5)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "28.97 & 27.755 & 30.51 & 27.28\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "| Vocab word |     Logit |\n",
    "| :--------- | --------: |\n",
    "| fluffy     |     28.97 |\n",
    "| blue       |     28.64 |\n",
    "| creature   | **30.51** |\n",
    "| forest     |     28.49 |\n",
    "\n",
    "---\n",
    "\n",
    "### (c) For **“creature”**\n",
    "\n",
    "$$\\begin{align*}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "10.64 & 10.10\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5 \\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "10.64(1.0) + 10.10(2.0) &\n",
    "10.64(2.0) + 10.10(0.5) &\n",
    "10.64(1.5) + 10.10(1.5) &\n",
    "10.64(0.5) + 10.10(2.5)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "30.84 & 26.33 & 31.11 & 30.595\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "| Vocab word |     Logit |\n",
    "| :--------- | --------: |\n",
    "| fluffy     |     30.84 |\n",
    "| blue       |     26.80 |\n",
    "| creature   |     31.11 |\n",
    "| forest     | **31.16** |\n",
    "\n",
    "---\n",
    "\n",
    "### (d) For **“forest”**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "9.98 & 11.58\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 1.5 & 0.5 \\\\\n",
    "2.0 & 0.5 & 1.5 & 2.5\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "9.98(1.0) + 11.58(2.0) &\n",
    "9.98(2.0) + 11.58(0.5) &\n",
    "9.98(1.5) + 11.58(1.5) &\n",
    "9.98(0.5) + 11.58(2.5)\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "33.14 & 25.75 & 32.34 & 33.94\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "| Vocab word |     Logit |\n",
    "| :--------- | --------: |\n",
    "| fluffy     |     33.14 |\n",
    "| blue       |     24.65 |\n",
    "| creature   |     31.74 |\n",
    "| forest     | **33.21** |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Apply softmax**\n",
    "\n",
    "The softmax converts each token’s logits into **probabilities** over the 4 vocabulary words:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{\\text{logit}_i}}{\\sum_j e^{\\text{logit}_j}}\n",
    "$$\n",
    "\n",
    "Let’s do this qualitatively (since these numbers are big, we only care about relative differences).\n",
    "\n",
    "For each token:\n",
    "\n",
    "| Token    | Highest logit                            | Predicted next word           | Explanation               |\n",
    "| :------- | :--------------------------------------- | :---------------------------- | :------------------------ |\n",
    "| fluffy   | 31.5 (“fluffy”/“creature”/“forest” tied) | roughly equal for those three | flat probabilities        |\n",
    "| blue     | 30.51 (“creature”)                       | **creature**                  | highest attention to noun |\n",
    "| creature | 31.16 (“forest”)                         | **forest**                    | predicts a location next  |\n",
    "| forest   | 33.21 (“forest”)                         | **forest** (self)             | final token, self-attends |\n",
    "\n",
    "If you wanted to normalize for one row (e.g. “blue”), you could compute softmax numerically:\n",
    "\n",
    "Let’s subtract max = 30.51 for stability:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{logits} - \\text{max} &= [-1.54, -1.87, 0, -2.02] \\\\\n",
    "e^{(\\text{logits} - \\text{max})} &= [0.21, 0.15, 1.00, 0.13] \\\\\n",
    "\\text{sum}(e^{(\\text{logits} - \\text{max})}) &= 1.49 \\\\\n",
    "p &= \\left[ \\frac{0.21}{1.49}, \\frac{0.15}{1.49}, \\frac{1.00}{1.49}, \\frac{0.13}{1.49} \\right] \\\\\n",
    "p &\\approx [0.14, 0.10, 0.67, 0.09]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So for “blue”:\n",
    "\n",
    "| Word     |     Prob |\n",
    "| -------- | -------: |\n",
    "| fluffy   |     0.14 |\n",
    "| blue     |     0.10 |\n",
    "| creature | **0.67** |\n",
    "| forest   |     0.09 |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Final summary**\n",
    "\n",
    "| Stage       | Operation                    | Shape  | Meaning                     |\n",
    "| ----------- | ---------------------------- | ------ | --------------------------- |\n",
    "| Embedding   | (E^{(l+1)})                  | [4, 2] | final contextual embeddings |\n",
    "| Unembedding | (E^{(l+1)} W_E^T)            | [4, 4] | raw logits per vocab word   |\n",
    "| Softmax     | (p_i = e^{x_i}/\\sum e^{x_j}) | [4, 4] | probabilities per word      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "Each token now outputs a **probability distribution** over all possible next tokens.\n",
    "In our example:\n",
    "\n",
    "* “blue” predicts **creature** next (makes sense grammatically).\n",
    "* “creature” predicts **forest** next.\n",
    "* “forest” (last token) predicts itself, as it has no future context.\n",
    "\n",
    "That’s exactly what happens inside a GPT-style model —\n",
    "after all ΔE updates, the final unembedding + softmax produces the **next-token prediction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3179a36-2f51-4722-9a41-7f580d2a9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: probs=[0.3327512741088867, 0.0017461184179410338, 0.3327512741088867, 0.3327512741088867], sum=1.0000\n",
      "Token 1: probs=[0.16207978129386902, 0.047138407826423645, 0.7560350298881531, 0.03474681079387665], sum=1.0000\n",
      "Token 2: probs=[0.32421818375587463, 0.003565906547009945, 0.42471447587013245, 0.24750138819217682], sum=1.0000\n",
      "Token 3: probs=[0.27207237482070923, 0.00016797648277133703, 0.12224962562322617, 0.6055100560188293], sum=1.0000\n",
      "************************************************************\n",
      "tensor([[3.3275e-01, 1.7461e-03, 3.3275e-01, 3.3275e-01],\n",
      "        [1.6208e-01, 4.7138e-02, 7.5604e-01, 3.4747e-02],\n",
      "        [3.2422e-01, 3.5659e-03, 4.2471e-01, 2.4750e-01],\n",
      "        [2.7207e-01, 1.6798e-04, 1.2225e-01, 6.0551e-01]])\n"
     ]
    }
   ],
   "source": [
    "# fluffy\t[3.5+7.0, 3.5+7.0] = [10.5, 10.5]\n",
    "# blue\t[4.35+7.36, 2.43+6.20] = [11.71, 8.63]\n",
    "# creature\t[3.84+6.8, 3.60+6.5] = [10.64, 10.10]\n",
    "# forest\t[2.78+7.2, 4.68+6.9] = [9.98, 11.58]\n",
    "E_l_plus_1=  torch.tensor( [ [10.5, 10.5], [11.71, 8.63] , [10.64, 10.10] ,[9.98, 11.58] ] ,dtype=torch.float32 )\n",
    "\n",
    "# print(E_l_plus_1.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W_E = torch.tensor(\n",
    "[\n",
    "[1.0 , 2.0],\n",
    "[2.0 , 0.5],\n",
    "[1.5 , 1.5],\n",
    "[0.5 , 2.5]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "W_U=W_E.T\n",
    "# print(W_U)\n",
    "\n",
    "\n",
    "logits=E_l_plus_1@W_U\n",
    "# print(logits)\n",
    "soft_values=torch.softmax(logits, dim=1)\n",
    "for token_idx, probs in enumerate(soft_values):\n",
    "    print(f\"Token {token_idx}: probs={probs.tolist()}, sum={probs.sum().item():.4f}\")\n",
    "\n",
    "\n",
    "print(\"*\"*60)\n",
    "probs = torch.softmax(logits,dim=-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68dfc4-70fc-4727-90eb-9fb4c6cab112",
   "metadata": {},
   "source": [
    "Perfect — that’s the **final stage** of a forward pass through your mini-Transformer layer, and those numbers are exactly what a real GPT model produces internally right before sampling or predicting the next token.\n",
    "\n",
    "Let’s interpret what happens *after* you’ve obtained these `softmax` probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What the output means**\n",
    "\n",
    "Each row corresponds to a token position in your input sequence:\n",
    "\n",
    "| Token index | What it represents | Shape | Meaning                                                               |\n",
    "| ----------- | ------------------ | ----- | --------------------------------------------------------------------- |\n",
    "| 0           | “fluffy”           | `[4]` | probability distribution over vocab for the next token after “fluffy” |\n",
    "| 1           | “blue”             | `[4]` | distribution for the token after “blue”                               |\n",
    "| 2           | “creature”         | `[4]` | distribution for the token after “creature”                           |\n",
    "| 3           | “forest”           | `[4]` | distribution for the token after “forest”                             |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Example interpretation**\n",
    "\n",
    "From your output:\n",
    "\n",
    "```\n",
    "Token 1 (“blue”):\n",
    "[0.162, 0.047, 0.756, 0.035]\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "| Vocab word   | Probability |\n",
    "| ------------ | ----------: |\n",
    "| fluffy       |       0.162 |\n",
    "| blue         |       0.047 |\n",
    "| **creature** |   **0.756** |\n",
    "| forest       |       0.035 |\n",
    "\n",
    "So the model believes the word **“creature”** should follow **“blue”** —\n",
    "which makes perfect linguistic sense: *“blue creature.”* ✅\n",
    "\n",
    "---\n",
    "\n",
    "## **3. What to do next**\n",
    "\n",
    "At this stage, you have two main choices — just like GPT:\n",
    "\n",
    "### **A. Deterministic decoding**\n",
    "\n",
    "Pick the token with the **highest probability** for each position.\n",
    "\n",
    "```python\n",
    "pred_ids = torch.argmax(soft_values, dim=1)\n",
    "print(pred_ids)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "tensor([0, 2, 2, 3])\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* after “fluffy” → “fluffy” (tie)\n",
    "* after “blue” → “creature”\n",
    "* after “creature” → “creature” (highest 0.42)\n",
    "* after “forest” → “forest”\n",
    "\n",
    "So the sequence “a fluffy blue creature roams the verdant forest”\n",
    "is internally predicting “blue creature forest forest...”.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Stochastic (sampling) decoding**\n",
    "\n",
    "If you’re *generating* text, you **sample** from these probabilities instead of taking the max:\n",
    "\n",
    "```python\n",
    "for i, probs in enumerate(soft_values):\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    print(f\"Token {i} → next = {next_token.item()}\")\n",
    "```\n",
    "\n",
    "This introduces randomness and diversity in generation (like GPT’s temperature-based sampling).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. If you trained this model**\n",
    "\n",
    "During training, you’d compare these probabilities (or raw logits) against the **true next tokens** using cross-entropy loss:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_t \\log p_t(y_t)$$\n",
    "\n",
    "where $y_t$ is the true next token index at time step (t).\n",
    "\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "loss = torch.nn.functional.cross_entropy(logits, target_ids)\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "This is how the network learns to adjust all the weights $(W^Q, W^K, W^V, W^O, W_1, W_2, W_E)$ so that the correct next word has the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Summary**\n",
    "\n",
    "| Stage                | Purpose       | What happens next         |\n",
    "| -------------------- | ------------- | ------------------------- |\n",
    "| `logits = E @ W_E.T` | raw scores    | before softmax            |\n",
    "| `softmax(logits)`    | probabilities | next-token distribution   |\n",
    "| `argmax` or `sample` | decoding      | choose next token(s)      |\n",
    "| `cross_entropy`      | training loss | used only during training |\n",
    "\n",
    "So your printed `soft_values` are the **final probabilities over the vocabulary**.\n",
    "In a real Transformer, the next step is either:\n",
    "\n",
    "* **generate** the next token (sampling / argmax), or\n",
    "* **train** (compute loss vs. the true next token).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1013f9-487b-4f4a-9b2c-e8d1611ae4df",
   "metadata": {},
   "source": [
    "Excellent — you’ve nailed the essence of how inference works inside ChatGPT!\n",
    "Let’s go step by step with both the **conceptual reasoning** and a **tiny numerical example** that behaves like GPT, showing exactly what happens when we multiply ( E^{(l+1)} @ W_U ).\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Clarify what happens in ChatGPT at inference**\n",
    "\n",
    "After all the attention and MLP layers, you have:\n",
    "\n",
    "$$\n",
    "E^{(l+1)} \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "For ChatGPT:\n",
    "\n",
    "* (T) = context length (e.g. 8192)\n",
    "* (d_{\\text{model}}) = 12288\n",
    "\n",
    "You then multiply by the **unembedding matrix**:\n",
    "\n",
    "$$\n",
    "W_U = W_E^{\\top} \\in \\mathbb{R}^{d_{\\text{model}} \\times |V|}\n",
    "$$\n",
    "\n",
    "where (|V| = 50,000) is the vocabulary size.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\text{logits} = E^{(l+1)} W_U\n",
    "$$\n",
    "\n",
    "→ shape `[T, 50000]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "* Each **row** (index `t`) corresponds to one token position in the input.\n",
    "* Each **column** corresponds to a possible vocabulary token.\n",
    "* The entry `[t, v]` is the *score* for predicting token `v` as the **next token** after position `t`.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "✅ You take the **last row** (the embedding of the last token in your context)\n",
    "and find its **columnwise maximum** (argmax over vocab):\n",
    "\n",
    "```python\n",
    "next_token_id = torch.argmax(logits[-1], dim=-1)\n",
    "```\n",
    "\n",
    "That gives you the ID of the most likely next token.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mini numerical example**\n",
    "\n",
    "Let’s make a dummy “ChatGPT” with:\n",
    "\n",
    "* Context length (T = 3)\n",
    "* Model dim (d_{\\text{model}} = 4)\n",
    "* Vocabulary size (|V| = 6)\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1 — Define tensors**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "T, d_model, V = 3, 4, 6\n",
    "\n",
    "# final layer embeddings (one per token)\n",
    "E = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [0.5, 1.0, 2.0, 1.5],\n",
    "    [2.0, 1.0, 0.5, 3.0]\n",
    "])  # shape [T, d_model]\n",
    "\n",
    "# unembedding matrix (tied to embedding matrix)\n",
    "W_U = torch.randn(d_model, V)  # shape [4, 6]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2 — Compute logits**\n",
    "\n",
    "```python\n",
    "logits = E @ W_U  # shape [T, V]\n",
    "print(logits.shape)\n",
    "```\n",
    "\n",
    "→ `[3, 6]`\n",
    "\n",
    "Now each of the 3 rows (one per token) has 6 numbers —\n",
    "a score for every vocab word.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3 — Pick next token**\n",
    "\n",
    "GPT always looks at the **last token**:\n",
    "\n",
    "```python\n",
    "last_logits = logits[-1]     # shape [6]\n",
    "next_token_id = torch.argmax(last_logits)\n",
    "print(next_token_id)\n",
    "```\n",
    "\n",
    "* `logits[-1]` → scores for 6 words (the “vocab”).\n",
    "* `argmax` → ID of the most likely next word.\n",
    "\n",
    "✅ That’s the next token ChatGPT would generate.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4 — Optional: see probabilities**\n",
    "\n",
    "If you want the probabilities (for sampling or temperature-based decoding):\n",
    "\n",
    "```python\n",
    "probs = torch.softmax(last_logits / 1.0, dim=-1)\n",
    "print(probs)\n",
    "print(\"Sum:\", probs.sum())  # should be 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Shape summary (ChatGPT vs. dummy)**\n",
    "\n",
    "| Symbol               | Meaning             | ChatGPT scale    | Dummy example |\n",
    "| -------------------- | ------------------- | ---------------- | ------------- |\n",
    "| (E^{(l+1)})          | final embeddings    | `[8192, 12288]`  | `[3, 4]`      |\n",
    "| (W_U)                | unembedding         | `[12288, 50000]` | `[4, 6]`      |\n",
    "| `logits = E @ W_U`   | raw vocab scores    | `[8192, 50000]`  | `[3, 6]`      |\n",
    "| `logits[-1]`         | last token’s scores | `[50000]`        | `[6]`         |\n",
    "| `argmax(logits[-1])` | next token id       | scalar           | scalar        |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Bottom line**\n",
    "\n",
    "* We **multiply the final embedding matrix** by the unembedding matrix (W_U).\n",
    "* That gives **logits** (scores) over every vocabulary token.\n",
    "* We **take the last row** (the most recent token)\n",
    "  and **choose the max column** (highest scoring word).\n",
    "* That’s your **next token**.\n",
    "\n",
    "If you’d like, I can expand this dummy example to actually **simulate ChatGPT’s autoregressive loop** — i.e. keep appending the predicted token and generating the next one step-by-step. Would you like that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347b556-3d7b-46e0-81cd-59038b8e24b0",
   "metadata": {},
   "source": [
    "## **Transformer Explainer**\n",
    "\n",
    "\"a fluffy blue creature roamed the verdant forest\"\n",
    "\n",
    "<img src=\"images/transformer-explainer.gif\" />\n",
    "\n",
    "\n",
    "Ref:[1](https://poloclub.github.io/transformer-explainer/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcbb17-e33b-477a-ba51-37ab96364f32",
   "metadata": {},
   "source": [
    "## **LLM Visualization**\n",
    "\n",
    "<img src=\"images/LLM-visualization.gif\" />\n",
    "\n",
    "\n",
    "Ref:[1](https://bbycroft.net/llm)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
