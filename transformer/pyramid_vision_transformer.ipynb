{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20ae3ba-9380-4f20-911b-f2c6238c0e02",
   "metadata": {},
   "source": [
    "# **Pyramid Vision Transformer (PVT)**\n",
    "The **Pyramid Vision Transformer (PVT)** — a fundamental architecture that bridges **ViTs and CNNs** for dense visual tasks like **object detection** and **semantic segmentation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Motivation**\n",
    "\n",
    "The **Vision Transformer (ViT)** introduced powerful global attention but has drawbacks:\n",
    "\n",
    "* Requires **fixed-size inputs** (e.g., 224×224).\n",
    "* Produces **single-scale features**, unsuitable for dense prediction tasks.\n",
    "* Has **quadratic complexity** with respect to the number of patches.\n",
    "* Lacks **local inductive bias** (like translation invariance from CNNs).\n",
    "\n",
    "The **Pyramid Vision Transformer (PVT)** (Wang et al., *ICCV 2021*) fixes these issues by **building a hierarchical (multi-scale) feature pyramid** — like a CNN backbone (e.g., ResNet).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Key Idea**\n",
    "\n",
    "PVT = **Hierarchical Vision Transformer Backbone**\n",
    "\n",
    "It mimics CNNs by generating **multi-resolution feature maps**:\n",
    "\n",
    "| Stage | Resolution                | Channels      | Purpose                 |\n",
    "| :---: | :------------------------ | :------------ | :---------------------- |\n",
    "|   1   | High (e.g., 1/4 of input) | Low           | Capture local features  |\n",
    "|   2   | Medium                    | More channels | Broader receptive field |\n",
    "|   3   | Low                       | More channels | Semantic features       |\n",
    "|   4   | Very low                  | Deep features | Global context          |\n",
    "\n",
    "These outputs can directly feed **FPN**, **Mask R-CNN**, **U-Net**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Architecture Overview**\n",
    "\n",
    "The PVT backbone has 4 stages, each performing:\n",
    "\n",
    "1. **Patch embedding** (patchify + linear projection)\n",
    "2. **Transformer encoder blocks**\n",
    "3. **Spatial reduction attention (SRA)** — reduces tokens before attention\n",
    "4. **Downsampling between stages** (to form a pyramid)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.1. Stage 1: Patch Embedding**\n",
    "\n",
    "The image is split into small patches (like ViT):\n",
    "\n",
    "$$\n",
    "x_0 = \\text{PatchEmbed}(I) \\in \\mathbb{R}^{H_0 \\times W_0 \\times C_0}\n",
    "$$\n",
    "\n",
    "Then flattened into tokens for the first transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2. Transformer Encoder with SRA**\n",
    "\n",
    "Standard self-attention has **O(N²)** cost (N = number of patches).\n",
    "To make it scalable, **PVT** introduces **Spatial Reduction Attention (SRA)**:\n",
    "\n",
    "Instead of using all keys and values, SRA **downsamples** them:\n",
    "\n",
    "$$\n",
    "K' = \\text{Downsample}(K), \\quad V' = \\text{Downsample}(V)\n",
    "$$\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K', V') = \\text{Softmax}\\left( \\frac{Q {K'}^T}{\\sqrt{d}} \\right) V'\n",
    "$$\n",
    "\n",
    "This reduces complexity from **O(N²)** → **O(N × N/s²)** where *s* is the reduction ratio.\n",
    "\n",
    "✅ This keeps **global receptive field** but reduces computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc65db7-df16-4122-af9f-22c36c98f16e",
   "metadata": {},
   "source": [
    "#### **3.3. Downsampling**\n",
    "This is the *core trick* of **Spatial Reduction Attention (SRA)** in **PVT**.\n",
    "\n",
    "The **downsampling** of $ K $ and ( V ) is **not** done by a linear projection (like `nn.Linear`), but by a **2D convolution with stride = s**, followed by normalization.\n",
    "\n",
    "---\n",
    "\n",
    "**Recall the Context**\n",
    "\n",
    "In normal self-attention:\n",
    "$$\n",
    "Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V\n",
    "$$\n",
    "\n",
    "where $$ X \\in \\mathbb{R}^{B \\times N \\times C} , N = H \\times W .$$\n",
    "\n",
    "In **PVT**, we *only* reduce the spatial size of $ K $ and $ V $:\n",
    "before computing attention, we apply a **downsampling operator** to them.\n",
    "\n",
    "---\n",
    "\n",
    "**3.3.1 The Spatial Reduction Step**\n",
    "\n",
    "For each transformer stage with reduction ratio $ s $:\n",
    "\n",
    "1. **Reshape tokens back to 2D feature map**\n",
    "   $$ X \\in \\mathbb{R}^{B \\times N \\times C} \\Rightarrow X_{\\text{map}} \\in \\mathbb{R}^{B \\times C \\times H \\times W} $$\n",
    "\n",
    "2. **Apply 2D convolution with stride = s**\n",
    "   $$ X_{\\text{reduced}} = \\text{Conv2d}(X_{\\text{map}},\\ \\text{stride}=s,\\ \\text{kernel}=s) $$\n",
    "\n",
    "   So if $ s=8 $ and $ X_{\\text{map}} $ is 56×56, we get 7×7 output.\n",
    "\n",
    "3. **Flatten back to tokens**\n",
    "   $$ X_{\\text{reduced}} \\Rightarrow X' \\in \\mathbb{R}^{B \\times N' \\times C} $$\n",
    "   where $ N' = \\frac{H}{s} \\times \\frac{W}{s} $\n",
    "\n",
    "4. **Normalize**\n",
    "   Apply `LayerNorm(C)` before computing $ K', V' $.\n",
    "\n",
    "5. **Linear projections**\n",
    "   $$ K' = X' W_K,\\quad V' = X' W_V $$\n",
    "\n",
    "---\n",
    "\n",
    "**3.3.2 Why Conv2D?**\n",
    "\n",
    " **Conv2D with stride s** naturally performs **spatial averaging / subsampling**,\n",
    "similar to how CNN backbones reduce resolution.\n",
    "\n",
    "It learns *how* to summarize spatial context (via kernel weights) instead of a fixed pooling rule.\n",
    "\n",
    "This gives two benefits:\n",
    "\n",
    "* Learnable spatial reduction (not hardcoded average pooling).\n",
    "* Maintains **translation equivariance** and **spatial coherence**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**3.3.3 Visual Summary**\n",
    "\n",
    "```\n",
    "[Before reduction]\n",
    "X: (B, 3136, 64) → reshape → (B, 64, 56, 56)\n",
    "\n",
    "↓ Conv2d(kernel=8, stride=8)\n",
    "\n",
    "[After reduction]\n",
    "→ (B, 64, 7, 7) → flatten → (B, 49, 64)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* **Conv2d** performs the spatial reduction.\n",
    "* **Linear layers** still compute Q, K′, V′ after reduction.\n",
    "* **LayerNorm** ensures stable distribution after spatial subsampling.\n",
    "\n",
    "---\n",
    "\n",
    "**3.3.4 Summary**\n",
    "\n",
    "- ✅ Downsampling of K, V = **Conv2D(stride=s)**, not Linear.\n",
    "- ✅ This yields $ N' = (H/s)(W/s) $ tokens for K′ and V′.\n",
    "- ✅ Learnable → model decides *how* to aggregate spatial context.\n",
    "- ✅ Efficient → reduces attention cost from $ O(N^2) $ to $ O(N·N′) $.\n",
    "- ✅ Keeps global field of view → Q remains full-size.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f708b0d-60c7-4d49-be40-3f1b99a67068",
   "metadata": {},
   "source": [
    "#### **3.4. Pyramid Hierarchy**\n",
    "\n",
    "After each stage, feature resolution is halved, and channels increase:\n",
    "\n",
    "| Stage | Resolution | Channels | Patch size | Reduction ratio |\n",
    "| :---: | :--------- | :------- | :--------- | :-------------- |\n",
    "|   1   | 1/4        | 64       | 4×4        | 8               |\n",
    "|   2   | 1/8        | 128      | 2×2        | 4               |\n",
    "|   3   | 1/16       | 320      | 2×2        | 2               |\n",
    "|   4   | 1/32       | 512      | 2×2        | 1               |\n",
    "\n",
    "Each stage is a **Transformer encoder** operating on the corresponding feature scale.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Advantages**\n",
    "\n",
    "- ✅ **Hierarchical features** → usable as a CNN backbone (e.g., in Mask R-CNN).\n",
    "- ✅ **Global receptive field** from transformers.\n",
    "- ✅ **Efficient attention** via spatial reduction.\n",
    "- ✅ **Variable input resolution** support.\n",
    "- ✅ **Strong performance on dense tasks** (segmentation, detection).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Comparison with ViT and Swin**\n",
    "\n",
    "| Model | Attention Type       | Hierarchy | Complexity | Windowed? | Suitable for Detection? |\n",
    "| :---- | :------------------- | :-------- | :--------- | :-------- | :---------------------- |\n",
    "| ViT   | Global               | ✖         | O(N²)      | ✖         | ✖                       |\n",
    "| Swin  | Window-based (local) | ✅         | O(N)       | ✅         | ✅                       |\n",
    "| PVT   | Global (SRA-reduced) | ✅         | O(N/s²)    | ✖         | ✅                       |\n",
    "\n",
    "So:\n",
    "\n",
    "* **PVT keeps global attention** but makes it efficient (SRA).\n",
    "* **Swin** uses local window attention and shifting to connect neighborhoods.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Equation Summary**\n",
    "\n",
    "Let’s define for stage *i*:\n",
    "\n",
    "* Input tokens:\n",
    "  $$ X_i \\in \\mathbb{R}^{N_i \\times C_i} $$\n",
    "* Spatial reduction ratio: *r*\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "Q = X_i W_Q, \\quad K = \\text{Down}(X_i W_K), \\quad V = \\text{Down}(X_i W_V)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SRA}(X_i) = \\text{Softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\text{Down}(\\cdot) = \\text{Reshape→Conv2d(stride=r)→Flatten}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Visual Summary**\n",
    "\n",
    "```\n",
    "Input Image\n",
    "   ↓\n",
    "[Stage 1] Patch Embed → Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 2] Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 3] Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 4] Transformer (SRA)\n",
    "   ↓\n",
    "Feature Pyramid Outputs → Detection / Segmentation Head\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Typical Usage**\n",
    "\n",
    "PVT variants:\n",
    "\n",
    "* **PVT-Tiny**, **PVT-Small**, **PVT-Medium**, **PVT-Large**\n",
    "  differ in embedding dims and number of blocks.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* **PVT + FPN → RetinaNet / Mask R-CNN**\n",
    "* **PVT + UPerNet → Semantic Segmentation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499da1d-c0aa-420c-bce7-feef12422b4a",
   "metadata": {},
   "source": [
    "## **Numerical Example**\n",
    "Let’s go through a **numerical example** showing how **Spatial Reduction Attention (SRA)** in **Pyramid Vision Transformer (PVT)** drastically reduces computational cost compared to standard self-attention.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Reminder: Complexity of Self-Attention**\n",
    "\n",
    "For each layer, self-attention requires:\n",
    "\n",
    "$$\n",
    "\\text{Cost} \\sim O(N^2 \\cdot d)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $ N $ = number of tokens (patches)\n",
    "* $ d $ = embedding dimension\n",
    "\n",
    "In ViT, $ N = (H/P) \\times (W/P) $.\n",
    "\n",
    "Example: for 224×224 input and 16×16 patch size:\n",
    "$$\n",
    "N = (224/16)^2 = 14^2 = 196\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eceff-c080-4c29-8ef9-75fc07df3a58",
   "metadata": {},
   "source": [
    "### **Recap of PVT-Tiny Parameters**\n",
    "\n",
    "Let’s include **Stage 1**, so we can see how the **token counts**, **spatial reduction ratio**, and **computational cost** evolve across the full PVT pyramid.\n",
    "\n",
    "We’ll continue with the **PVT-Tiny** configuration (most common example).\n",
    "\n",
    "\n",
    "\n",
    "| Stage | Output Resolution | Channels | Patch Size | SRA Reduction Ratio (s) | #Blocks |\n",
    "| :---: | :---------------: | :------: | :--------: | :---------------------: | :-----: |\n",
    "|   1   |       56×56       |    64    |     4×4    |            8            |    2    |\n",
    "|   2   |       28×28       |    128   |     2×2    |            4            |    2    |\n",
    "|   3   |       14×14       |    320   |     2×2    |            2            |    2    |\n",
    "|   4   |        7×7        |    512   |     2×2    |            1            |    2    |\n",
    "\n",
    "Input is 224×224 RGB image → patchify step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Stage 1: 56×56 Feature Map**\n",
    "\n",
    "#### Token count\n",
    "\n",
    "$$\n",
    "N_1 = 56 \\times 56 = 3136\n",
    "$$\n",
    "Embedding dim: $ d_1 = 64 $\n",
    "\n",
    "#### Standard self-attention cost\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(1)} = N_1^2 \\times d_1 = 3136^2 \\times 64 = 9.8 \\times 10^8\n",
    "$$\n",
    "\n",
    "#### Spatial Reduction Attention (s = 8)\n",
    "\n",
    "Downsample keys/values by factor 8:\n",
    "\n",
    "$$\n",
    "N_1' = \\frac{N_1}{8^2} = \\frac{3136}{64} = 49\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(1)} = N_1 \\times N_1' \\times d_1 = 3136 \\times 49 \\times 64 = 9.8 \\times 10^6\n",
    "$$\n",
    "\n",
    "Assume the mini-batch size is **B = 1** for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Symbol        | Meaning                             | Shape         |\n",
    "| :------------ | :---------------------------------- | :------------ |\n",
    "| Input         | Flattened tokens                    | (1, 3136, 64) |\n",
    "| Q             | Query projection of all tokens      | (1, 3136, 64) |\n",
    "| K′            | Keys after spatial reduction by 8   | (1, 49, 64)   |\n",
    "| V′            | Values after spatial reduction by 8 | (1, 49, 64)   |\n",
    "| Attention map | Q × K′ᵀ                             | (3136, 49)    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ **≈ 100× reduction** already at the first stage.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Stage 2: 28×28 Feature Map**\n",
    "\n",
    "From Stage 1 → Stage 2 we downsample 2×.\n",
    "\n",
    "$$\n",
    "N_2 = 28 \\times 28 = 784, \\quad d_2 = 128, \\quad s = 4\n",
    "$$\n",
    "\n",
    "Standard:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(2)} = 784^2 \\times 128 = 7.9 \\times 10^7\n",
    "$$\n",
    "\n",
    "Reduced:\n",
    "\n",
    "$$\n",
    "N_2' = \\frac{784}{4^2} = 49\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(2)} = 784 \\times 49 \\times 128 = 4.9 \\times 10^6\n",
    "$$\n",
    "\n",
    "| Symbol        | Meaning                              | Shape |\n",
    "| :------------ | :----------------------------------- | :---- |\n",
    "| Input         | (1, 784, 128)                        |       |\n",
    "| Q             | (1, 784, 128)                        |       |\n",
    "| K′            | (1, 49, 128) (because 784 / 4² = 49) |       |\n",
    "| V′            | (1, 49, 128)                         |       |\n",
    "| Attention map | (784, 49)                            |       |\n",
    "\n",
    "✅ ~16× less compute.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Stage 3: 14×14 Feature Map**\n",
    "\n",
    "$$\n",
    "N_3 = 196, \\quad d_3 = 320, \\quad s = 2\n",
    "$$\n",
    "\n",
    "Standard:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(3)} = 196^2 \\times 320 = 1.2 \\times 10^7\n",
    "$$\n",
    "\n",
    "Reduced:\n",
    "\n",
    "$$\n",
    "N_3' = \\frac{196}{2^2} = 49\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(3)} = 196 \\times 49 \\times 320 = 3.1 \\times 10^6\n",
    "$$\n",
    "\n",
    "\n",
    "| Symbol        | Meaning                              | Shape |\n",
    "| :------------ | :----------------------------------- | :---- |\n",
    "| Input         | (1, 196, 320)                        |       |\n",
    "| Q             | (1, 196, 320)                        |       |\n",
    "| K′            | (1, 49, 320) (because 196 / 2² = 49) |       |\n",
    "| V′            | (1, 49, 320)                         |       |\n",
    "| Attention map | (196, 49)                            |       |\n",
    "\n",
    "✅ ~4× less compute.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Stage 4: 7×7 Feature Map**\n",
    "\n",
    "$$\n",
    "N_4 = 49, \\quad d_4 = 512, \\quad s = 1\n",
    "$$\n",
    "\n",
    "No reduction (since it’s already small).\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(4)} = N_4^2 \\times d_4 = 49^2 \\times 512 = 1.2 \\times 10^6\n",
    "$$\n",
    "\n",
    "SRA = same (s = 1).\n",
    "\n",
    "\n",
    "| Symbol        | Meaning                            | Shape |\n",
    "| :------------ | :--------------------------------- | :---- |\n",
    "| Input         | (1, 49, 512)                       |       |\n",
    "| Q             | (1, 49, 512)                       |       |\n",
    "| K′            | (1, 49, 512) (no reduction, s = 1) |       |\n",
    "| V′            | (1, 49, 512)                       |       |\n",
    "| Attention map | (49, 49)                           |       |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Full Pyramid Comparison**\n",
    "\n",
    "| Stage | Resolution |   N  |  d  |  s  | Standard SA |    SRA    | Reduction |\n",
    "| :---: | :--------- | :--: | :-: | :-: | :---------: | :-------: | :-------: |\n",
    "|   1   | 56×56      | 3136 |  64 |  8  |  9.8 × 10⁸  | 9.8 × 10⁶ |  **100×** |\n",
    "|   2   | 28×28      |  784 | 128 |  4  |  7.9 × 10⁷  | 4.9 × 10⁶ |  **16×**  |\n",
    "|   3   | 14×14      |  196 | 320 |  2  |  1.2 × 10⁷  | 3.1 × 10⁶ |   **4×**  |\n",
    "|   4   | 7×7        |  49  | 512 |  1  |  1.2 × 10⁶  | 1.2 × 10⁶ |     —     |\n",
    "\n",
    "\n",
    "| Stage | Resolution | Tokens (N) | Channels (C) |  s  |    Q Shape    |  K′/V′ Shape | Attention Map |\n",
    "| :---: | :--------- | :--------: | :----------: | :-: | :-----------: | :----------: | :-----------: |\n",
    "|   1   | 56×56      |    3136    |      64      |  8  | (1, 3136, 64) |  (1, 49, 64) |   (3136, 49)  |\n",
    "|   2   | 28×28      |     784    |      128     |  4  | (1, 784, 128) | (1, 49, 128) |   (784, 49)   |\n",
    "|   3   | 14×14      |     196    |      320     |  2  | (1, 196, 320) | (1, 49, 320) |   (196, 49)   |\n",
    "|   4   | 7×7        |     49     |      512     |  1  |  (1, 49, 512) | (1, 49, 512) |    (49, 49)   |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Observations**\n",
    "\n",
    "* The largest savings occur in **early high-resolution stages**.\n",
    "* Later stages are already small, so reduction is less critical.\n",
    "* Total FLOPs ≈ **10× less** overall vs. pure global attention backbone.\n",
    "* **Q** keeps full spatial resolution — each token still “looks” globally.\n",
    "* **K′, V′** are spatially reduced to a **coarse grid of 7×7 = 49 tokens**, fixed across stages 1–3.\n",
    "* Thus, the **attention map size** shrinks from millions (3136²) to only tens of thousands (3136×49).\n",
    "\n",
    "The model preserves **global context** but cuts the computational cost by roughly **100×** in early stages.\n",
    "\n",
    "\n",
    "This design choice is why **PVT can scale to high-res images** and still serve as a **drop-in backbone** for detection or segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf1a7b-4ce6-4a96-860f-d28d3f37af80",
   "metadata": {},
   "source": [
    "## **Python Code**\n",
    "\n",
    "```python\n",
    "class SpatialReductionAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, sr_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of attention heads (same as standard multi-head attention)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Scaling factor used in the attention softmax\n",
    "        # scale = 1 / sqrt(head_dim)\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        # Linear layers to create Q and [K,V]\n",
    "        # Standard ViT-style projection layers\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "\n",
    "        # Spatial reduction ratio (how much to downsample K,V)\n",
    "        # e.g. 8, 4, 2, 1 for PVT-Tiny stages\n",
    "        self.sr_ratio = sr_ratio\n",
    "\n",
    "        # Only create Conv2D if we actually reduce spatial resolution\n",
    "        if sr_ratio > 1:\n",
    "            # Conv2d performs learnable downsampling\n",
    "            # (stride = sr_ratio) → spatially reduces tokens\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "\n",
    "            # Normalization layer after reduction\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c193b-e861-42ec-b721-c3e4cb85e2ef",
   "metadata": {},
   "source": [
    "#### Scaling factor\n",
    "Scaling factor is one of those subtle but *essential* details in every attention mechanism.\n",
    "\n",
    "**1. Where it comes from**\n",
    "\n",
    "In all Transformer-style attention mechanisms, the core operation is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{Q K^{T}}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $ Q \\in \\mathbb{R}^{N \\times d_k} $: queries\n",
    "* $ K \\in \\mathbb{R}^{N' \\times d_k} $: keys\n",
    "* $ V \\in \\mathbb{R}^{N' \\times d_v} $: values\n",
    "* $ d_k $: dimension of each head (after splitting channels into heads).\n",
    "\n",
    "That denominator **$ \\sqrt{d_k} $** is precisely the **scaling factor** implemented as:\n",
    "\n",
    "```python\n",
    "self.scale = (dim // num_heads) ** -0.5\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\text{scale} = \\frac{1}{\\sqrt{d_k}} = (d_k)^{-1/2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why do we need scaling?**\n",
    "\n",
    "Without scaling, the dot-product $ QK^{T} $ can have very large values when $ d_k $ is large.\n",
    "This leads to **unstable gradients** and **softmax saturation**.\n",
    "\n",
    "Let’s see why.\n",
    "\n",
    "Each entry of $ QK^T $ is the dot product between two vectors of length $ d_k $:\n",
    "\n",
    "$$\n",
    "(QK^T)_{ij} = \\sum_{t=1}^{d_k} Q_{it} K_{jt}\n",
    "$$\n",
    "\n",
    "If the components of Q and K are zero-mean with variance 1, then the variance of the dot product grows linearly with ( d_k ):\n",
    "\n",
    "$$\n",
    "\\text{Var}(QK^T) \\propto d_k\n",
    "$$\n",
    "\n",
    "So as $ d_k $ increases (e.g., 64, 128, 256), the logits become large,\n",
    "and **$softmax(QK^T)$** becomes extremely peaked → gradients vanish for most entries.\n",
    "\n",
    "---\n",
    "\n",
    "**3. The fix: scale by $ 1/\\sqrt{d_k} $**\n",
    "\n",
    "Dividing by $ \\sqrt{d_k} $ normalizes the variance of the dot products, so the values stay roughly in a stable range (e.g., around -1 to 1), which keeps the softmax smooth and gradients well-behaved.\n",
    "\n",
    "So we compute:\n",
    "\n",
    "$$\n",
    "\\text{Attention weights} = \\text{Softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "\n",
    "* Stable softmax range\n",
    "* Balanced gradient flow\n",
    "* Faster and smoother convergence during training\n",
    "\n",
    "---\n",
    "\n",
    "**4. In the PVT Code**\n",
    "\n",
    "In the code:\n",
    "\n",
    "```python\n",
    "self.scale = (dim // num_heads) ** -0.5\n",
    "```\n",
    "\n",
    "* `dim` = total embedding dimension $ C $\n",
    "* `num_heads` = number of attention heads $ h $\n",
    "* `dim // num_heads` = per-head dimension $ d_k $\n",
    "* `** -0.5` = take inverse square root\n",
    "\n",
    "So if $\\text{dim} = 64$ and $$\\text{num\\_heads}=4:$$\n",
    "\n",
    "$$\n",
    "d_k = 64 / 4 = 16 \\Rightarrow \\text{scale} = 1 / \\sqrt{16} = 0.25\n",
    "$$\n",
    "\n",
    "and inside `forward()`:\n",
    "\n",
    "```python\n",
    "attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "```\n",
    "\n",
    "means we are computing:\n",
    "\n",
    "$$\n",
    "A = \\frac{Q K^{T}}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**5. Numerical example**\n",
    "\n",
    "Suppose\n",
    "$ Q, K \\in \\mathbb{R}^{3 \\times 4} $,\n",
    "so $ d_k = 4 $.\n",
    "\n",
    "If we skip scaling, the dot products might look like:\n",
    "\n",
    "$$\n",
    "QK^T =\n",
    "\\begin{bmatrix}\n",
    "10 & 12 & 9 \\\n",
    "7 & 11 & 13 \\\n",
    "12 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Softmax of these large values → almost one-hot vectors (e.g., [0.99, 0.005, 0.005]),\n",
    "and gradients vanish for most entries.\n",
    "\n",
    "After scaling by $ 1/\\sqrt{4} = 0.5 $:\n",
    "\n",
    "$$\n",
    "(QK^T)/\\sqrt{d_k} =\n",
    "\\begin{bmatrix}\n",
    "5 & 6 & 4.5 \\\n",
    "3.5 & 5.5 & 6.5 \\\n",
    "6 & 4 & 4.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Softmax becomes more balanced, e.g., [0.65, 0.20, 0.15],\n",
    "leading to healthier gradient updates.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Analogy**\n",
    "\n",
    "Think of the scale factor as a **temperature control** for the softmax:\n",
    "\n",
    "| Effect          | Formula                    | Behavior                             |\n",
    "| :-------------- | :------------------------- | :----------------------------------- |\n",
    "| No scaling      | Softmax(QKᵀ)               | Overconfident, sharp, poor gradients |\n",
    "| Proper scaling  | Softmax(QKᵀ / √dₖ)         | Smooth, stable gradients             |\n",
    "| Too small scale | Softmax(QKᵀ / large value) | Too flat, underconfident             |\n",
    "\n",
    "---\n",
    "\n",
    "**7. Summary**\n",
    "\n",
    "- ✅ `self.scale = (dim // num_heads) ** -0.5` implements $ 1 / \\sqrt{d_k} $\n",
    "- ✅ Prevents softmax saturation and stabilizes training\n",
    "- ✅ Keeps attention values in a numerically safe range\n",
    "- ✅ Essential for all transformer-based attention (ViT, Swin, PVT, etc.)\n",
    "- ✅ Even though PVT adds *Spatial Reduction Attention*, the attention core remains the same — and scaling is just as necessary.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8369ed-d855-49f4-a8e7-6e8905a267c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In **standard PyTorch Transformers** (and frameworks built on top of them, like `torch.nn.MultiheadAttention`, `timm`, or `transformers`), the scaling factor\n",
    "$$ \\frac{1}{\\sqrt{d_k}} $$\n",
    "is **already built-in** — so we *don’t* manually multiply by `self.scale`.\n",
    "\n",
    "However, when implementing **custom attention modules** (like in PVT, Swin, ViT-from-scratch), we need to include it explicitly.\n",
    "\n",
    "Let’s go through this in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. PyTorch’s Built-in Attention Layer**\n",
    "\n",
    "PyTorch provides this module:\n",
    "\n",
    "```python\n",
    "torch.nn.MultiheadAttention(embed_dim, num_heads)\n",
    "```\n",
    "\n",
    "When you call it:\n",
    "\n",
    "```python\n",
    "attn_output, attn_weights = self_attention(x, x, x)\n",
    "```\n",
    "\n",
    "internally it performs:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{Softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "The scaling factor is *automatically applied* inside the source code.\n",
    "\n",
    "If you look at the PyTorch implementation (simplified):\n",
    "\n",
    "```python\n",
    "attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "attn_output_weights = attn_output_weights / math.sqrt(head_dim)\n",
    "attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "```\n",
    "\n",
    "So yes — it’s **already handled for you**.\n",
    "\n",
    "That’s why, when you use something like `nn.TransformerEncoder` or `nn.MultiheadAttention`,\n",
    "you don’t explicitly see or need to define `self.scale`.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why do we define `self.scale` manually in custom architectures?**\n",
    "\n",
    "In research code (ViT, Swin, PVT), we often **re-implement attention** manually, because we:\n",
    "\n",
    "* Customize **token layout** (windows, shifted windows, reduced tokens)\n",
    "* Add **spatial convolutions**, **relative position biases**, etc.\n",
    "* Split **Q, K, V** differently (like PVT does with Conv2D for K,V)\n",
    "\n",
    "Thus we can’t rely on PyTorch’s built-in layer — and we need to handle every step explicitly:\n",
    "\n",
    "1. Project Q, K, V\n",
    "2. Split heads\n",
    "3. Compute attention weights\n",
    "4. Apply scaling\n",
    "5. Softmax\n",
    "6. Aggregate V\n",
    "\n",
    "That’s why lines like this appear:\n",
    "\n",
    "```python\n",
    "self.scale = (dim // num_heads) ** -0.5\n",
    "attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "attn = attn.softmax(dim=-1)\n",
    "```\n",
    "\n",
    "They replicate what PyTorch’s internal module does,\n",
    "but now we have the freedom to modify the attention logic.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example Comparison**\n",
    "\n",
    "### (A) Using **PyTorch built-in**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "attn = nn.MultiheadAttention(embed_dim=64, num_heads=4)\n",
    "out, weights = attn(x, x, x)\n",
    "```\n",
    "\n",
    "✅ PyTorch handles:\n",
    "\n",
    "* Linear projection to Q,K,V\n",
    "* Scaling\n",
    "* Softmax\n",
    "* Dropout\n",
    "* Output projection\n",
    "\n",
    "---\n",
    "\n",
    "### (B) Custom attention (ViT/Swin/PVT style)\n",
    "\n",
    "```python\n",
    "q = self.q(x)\n",
    "k = self.k(x)\n",
    "v = self.v(x)\n",
    "\n",
    "attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "attn = attn.softmax(dim=-1)\n",
    "out = (attn @ v)\n",
    "```\n",
    "\n",
    "✅ You explicitly:\n",
    "\n",
    "* Create Q,K,V\n",
    "* Define `self.scale`\n",
    "* Apply it manually\n",
    "* Can add spatial reduction, window masking, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why this distinction matters**\n",
    "\n",
    "| Case                                | Scaling done automatically?         | Typical Usage                    |\n",
    "| :---------------------------------- | :---------------------------------- | :------------------------------- |\n",
    "| `nn.MultiheadAttention`             | ✅ Yes (inside PyTorch)              | Standard NLP / small ViT         |\n",
    "| Custom Attention (PVT, Swin, ViT)   | ❌ No, must do manually              | Research or custom vision models |\n",
    "| HuggingFace models (`transformers`) | ✅ Yes (inside their implementation) | High-level APIs                  |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary**\n",
    "\n",
    "✅ The scaling factor ( 1/\\sqrt{d_k} ) is *always used* — it’s fundamental to all attention.\n",
    "✅ In **PyTorch’s built-in layers**, it’s already applied internally.\n",
    "✅ In **custom attention implementations** (ViT, Swin, PVT, etc.), we must explicitly define and apply it ourselves (`self.scale`).\n",
    "✅ The underlying math is **identical** in all cases.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show the actual internal snippet from `torch.nn.MultiheadAttention` (the part where PyTorch applies the scaling)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e62360-b16e-45d8-815e-3e9dc93565a0",
   "metadata": {},
   "source": [
    "### **Now the forward()**\n",
    "\n",
    "```python\n",
    "    def forward(self, x, H, W):\n",
    "        # Input:\n",
    "        # x: [B, N, C] where N = H*W tokens\n",
    "        B, N, C = x.shape\n",
    "```\n",
    "\n",
    "At this point,\n",
    "\n",
    "* `x` = flattened spatial tokens (from a transformer block input)\n",
    "* Each token has `C` channels (the embedding dimension).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        # Compute Q as usual (no reduction)\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "```\n",
    "\n",
    "✅ **Q** keeps *full spatial resolution* — every token has its own query vector.\n",
    "Shape:\n",
    "$$\n",
    "Q \\in \\mathbb{R}^{B \\times N \\times h \\times (C/h)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Spatial Reduction for K, V**\n",
    "\n",
    "```python\n",
    "        if self.sr_ratio > 1:\n",
    "            # Reshape flattened tokens back into 2D feature maps\n",
    "            # from [B, N, C] → [B, C, H, W]\n",
    "            x_ = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "\n",
    "            # Apply learnable Conv2D with stride = sr_ratio\n",
    "            # Reduces H, W → H/sr_ratio, W/sr_ratio\n",
    "            x_ = self.sr(x_)  # [B, C, H/s, W/s]\n",
    "\n",
    "            # Flatten spatial map back into token sequence\n",
    "            # [B, C, H/s, W/s] → [B, N', C]\n",
    "            x_ = x_.reshape(B, C, -1).transpose(1, 2)\n",
    "\n",
    "            # Normalize the reduced tokens before projecting K, V\n",
    "            x_ = self.norm(x_)\n",
    "        else:\n",
    "            # No reduction for small feature maps (e.g., stage 4)\n",
    "            x_ = x\n",
    "```\n",
    "\n",
    "✅ This is where **Spatial Reduction Attention** happens.\n",
    "\n",
    "* The **Conv2D layer** (`self.sr`) is **learnable**, acting like a *trainable pooling* operator.\n",
    "* `sr_ratio` controls how much to downsample:\n",
    "\n",
    "  * stage 1 → stride 8 (56×56 → 7×7)\n",
    "  * stage 2 → stride 4 (28×28 → 7×7)\n",
    "  * stage 3 → stride 2 (14×14 → 7×7)\n",
    "  * stage 4 → stride 1 (7×7 → 7×7)\n",
    "\n",
    "So K′, V′ are *always* roughly 7×7 spatial tokens = 49 tokens per head.\n",
    "\n",
    "---\n",
    "\n",
    "### **Create K′ and V′**\n",
    "\n",
    "```python\n",
    "        kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads)\n",
    "        k, v = kv[:, :, 0], kv[:, :, 1]\n",
    "```\n",
    "\n",
    "✅ Here we generate **K′** and **V′** from the reduced sequence `x_`.\n",
    "\n",
    "Shapes:\n",
    "\n",
    "* K′, V′ ∈ [B, N′, h, C/h], with N′ ≈ 49\n",
    "\n",
    "That matches what we discussed:\n",
    "\n",
    "| Stage |   N  | N′ (after reduction) |\n",
    "| :---: | :--: | :------------------: |\n",
    "|   1   | 3136 |          49          |\n",
    "|   2   |  784 |          49          |\n",
    "|   3   |  196 |          49          |\n",
    "|   4   |  49  |          49          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Compute attention and output**\n",
    "\n",
    "```python\n",
    "        # Compute scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Weighted sum of V′\n",
    "        out = (attn @ v).reshape(B, N, C)\n",
    "        return out\n",
    "```\n",
    "\n",
    "✅ Standard transformer attention, but with **reduced K′, V′**.\n",
    "\n",
    "* The attention matrix has shape [N, N′] instead of [N, N].\n",
    "* This makes the computation **O(N × N′)** instead of **O(N²)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Shape Flow Example (Stage 1, 56×56)**\n",
    "\n",
    "| Tensor    | Operation          | Shape         |\n",
    "| :-------- | :----------------- | :------------ |\n",
    "| Input `x` | —                  | [1, 3136, 64] |\n",
    "| Q         | Linear             | [1, 3136, 64] |\n",
    "| x_        | Conv2D(stride = 8) | [1, 49, 64]   |\n",
    "| K′, V′    | Linear             | [1, 49, 64]   |\n",
    "| Attention | Q × K′ᵀ            | [3136, 49]    |\n",
    "| Output    | Weighted sum       | [1, 3136, 64] |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Relation to What We Discussed**\n",
    "\n",
    "| Concept                           | Implementation in Code        | Explanation                                |\n",
    "| :-------------------------------- | :---------------------------- | :----------------------------------------- |\n",
    "| **Learnable spatial reduction**   | `self.sr = nn.Conv2d(...)`    | Learns how to downsample spatial features. |\n",
    "| **Q keeps full resolution**       | `q = self.q(x)`               | Every patch still queries globally.        |\n",
    "| **Reduced K, V**                  | `x_ = self.sr(x_map)`         | Keys/values summarize local context.       |\n",
    "| **Normalization after reduction** | `self.norm(x_)`               | Keeps feature statistics stable.           |\n",
    "| **Efficiency**                    | `attn = (q @ kᵀ)` → O(N × N′) | Computation cost drastically reduced.      |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Key Takeaways**\n",
    "\n",
    "✅ The **Conv2D layer is learnable**, not fixed pooling.\n",
    "✅ It performs **spatial reduction** → fewer K, V tokens.\n",
    "✅ Q remains full → still **global attention**.\n",
    "✅ Complexity:\n",
    "$$\n",
    "O(N^2) \\to O(N \\cdot N') = O\\left(N \\cdot \\frac{N}{s^2}\\right)\n",
    "$$\n",
    "✅ SRA gives PVT its **pyramid hierarchy** and **efficiency**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedbda5-edc1-457c-9324-669c92fdf6ed",
   "metadata": {},
   "source": [
    "## self.kv\n",
    "Excellent — this is one of the key *implementation tricks* used in Transformers (including PVT) for **efficiency** and **cleaner code**.\n",
    "Let’s go step by step through why we write\n",
    "\n",
    "```python\n",
    "self.kv = nn.Linear(dim, dim * 2)\n",
    "```\n",
    "\n",
    "instead of two separate projections (`self.k` and `self.v`).\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Recall the purpose of Q, K, V**\n",
    "\n",
    "In self-attention, we project the input tokens ( X ) into three spaces:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "where each ( W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{head}}} ).\n",
    "\n",
    "Each of these is just a **linear transformation** — conceptually independent.\n",
    "\n",
    "So one could write in code:\n",
    "\n",
    "```python\n",
    "self.q = nn.Linear(dim, dim)\n",
    "self.k = nn.Linear(dim, dim)\n",
    "self.v = nn.Linear(dim, dim)\n",
    "```\n",
    "\n",
    "and later compute:\n",
    "\n",
    "```python\n",
    "q = self.q(x)\n",
    "k = self.k(x)\n",
    "v = self.v(x)\n",
    "```\n",
    "\n",
    "But in practice, this is **inefficient** (three separate matrix multiplications and memory reads).\n",
    "So we combine **K** and **V** into a single linear layer.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Combined linear layer:**\n",
    "\n",
    "```python\n",
    "self.kv = nn.Linear(dim, dim * 2)\n",
    "```\n",
    "\n",
    "This means one linear layer outputs **twice as many channels** — the first half for K, second half for V.\n",
    "\n",
    "When we run:\n",
    "\n",
    "```python\n",
    "kv = self.kv(x_)\n",
    "```\n",
    "\n",
    "the result has shape `[B, N', 2*dim]`.\n",
    "Then we split it into **two parts**:\n",
    "\n",
    "```python\n",
    "kv = kv.reshape(B, -1, 2, self.num_heads, C // self.num_heads)\n",
    "k, v = kv[:, :, 0], kv[:, :, 1]\n",
    "```\n",
    "\n",
    "So effectively:\n",
    "\n",
    "$$\n",
    "[K ; V] = X W_{KV}, \\quad \\text{where } W_{KV} = [W_K, W_V]\n",
    "$$\n",
    "\n",
    "and we just slice it into two pieces.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Why this is equivalent**\n",
    "\n",
    "A linear layer does:\n",
    "\n",
    "$$\n",
    "Y = X W + b\n",
    "$$\n",
    "\n",
    "If ( W = [W_K, W_V] ), then:\n",
    "\n",
    "$$\n",
    "Y = X [W_K, W_V] + [b_K, b_V] = [XW_K + b_K, ; XW_V + b_V]\n",
    "$$\n",
    "\n",
    "So the output naturally splits into two independent parts — exactly what we want for K and V.\n",
    "\n",
    "✅ Mathematically equivalent\n",
    "✅ Computationally more efficient\n",
    "✅ Simpler code\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why not combine Q as well?**\n",
    "\n",
    "We *could* combine all three (Q, K, V) into a single `nn.Linear(dim, dim * 3)` —\n",
    "and in many Transformer implementations (e.g., PyTorch’s `nn.MultiheadAttention` or ViT), that’s exactly what’s done.\n",
    "\n",
    "However, in **PVT** (and other hierarchical Transformers), they **separate Q** because:\n",
    "\n",
    "* ( Q ) always uses the **full-resolution tokens** (shape ( N )),\n",
    "* while ( K ) and ( V ) use **spatially reduced tokens** (shape ( N' )).\n",
    "\n",
    "So ( Q ) and ( [K, V] ) are computed from **different feature maps** (`x` vs `x_`).\n",
    "\n",
    "Thus:\n",
    "\n",
    "* Q: `x` (no reduction)\n",
    "* K, V: `x_` (after Conv2d downsampling)\n",
    "\n",
    "That’s why we can’t combine Q, K, and V into a single linear layer in PVT.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary Table**\n",
    "\n",
    "| Projection | Computed from              | Linear layer              | Output shape   | Purpose                    |\n",
    "| :--------- | :------------------------- | :------------------------ | :------------- | :------------------------- |\n",
    "| **Q**      | Full-resolution tokens `x` | `nn.Linear(dim, dim)`     | [B, N, dim]    | Queries (global view)      |\n",
    "| **K′, V′** | Downsampled tokens `x_`    | `nn.Linear(dim, dim * 2)` | [B, N′, 2*dim] | Keys & values (summarized) |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary Explanation**\n",
    "\n",
    "✅ `self.kv = nn.Linear(dim, dim * 2)` combines **K and V projections** into a single operation.\n",
    "✅ After passing through it, the tensor is split into K and V halves.\n",
    "✅ Saves computation and memory vs. two separate layers.\n",
    "✅ Q is kept separate because in **Spatial Reduction Attention**, it uses the **original** resolution while K and V use the **downsampled** one.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa96b6-9877-4432-be0e-928d556771d8",
   "metadata": {},
   "source": [
    "Perfect — let’s make this **very concrete** with a **toy PyTorch example** that shows:\n",
    "\n",
    "1. How `self.kv = nn.Linear(dim, dim * 2)` works internally.\n",
    "2. How the tensor shapes look **before and after** the projection.\n",
    "3. How we split the result into **K** and **V**.\n",
    "\n",
    "We’ll use small, human-readable numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**\n",
    "\n",
    "We’ll simulate:\n",
    "\n",
    "* Batch size ( B = 1 )\n",
    "* Number of tokens ( N = 4 )\n",
    "* Embedding dimension ( \\text{dim} = 8 )\n",
    "* 2 attention heads (( h = 2 ))\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Small example\n",
    "B, N, dim = 1, 4, 8\n",
    "num_heads = 2\n",
    "\n",
    "# Input tensor (4 tokens, each of 8 features)\n",
    "x = torch.arange(B * N * dim, dtype=torch.float32).reshape(B, N, dim)\n",
    "print(\"Input x shape:\", x.shape)\n",
    "print(x)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Input x shape: torch.Size([1, 4, 8])\n",
    "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
    "         [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n",
    "         [16., 17., 18., 19., 20., 21., 22., 23.],\n",
    "         [24., 25., 26., 27., 28., 29., 30., 31.]]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Define the `kv` Linear Layer**\n",
    "\n",
    "```python\n",
    "kv_layer = nn.Linear(dim, dim * 2, bias=False)\n",
    "print(\"Weight shape:\", kv_layer.weight.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Weight shape: torch.Size([16, 8])\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Input: 8\n",
    "* Output: 16 (= 2 × 8)\n",
    "* So this layer outputs both K and V concatenated along the last dimension.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Forward Pass**\n",
    "\n",
    "```python\n",
    "kv = kv_layer(x)\n",
    "print(\"After kv projection:\", kv.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "After kv projection: torch.Size([1, 4, 16])\n",
    "```\n",
    "\n",
    "So for each token (length 8), the linear layer produces **16 outputs** —\n",
    "the first 8 correspond to **K**, and the next 8 correspond to **V**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Split into K and V**\n",
    "\n",
    "```python\n",
    "k, v = kv.chunk(2, dim=-1)\n",
    "print(\"K shape:\", k.shape)\n",
    "print(\"V shape:\", v.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "K shape: torch.Size([1, 4, 8])\n",
    "V shape: torch.Size([1, 4, 8])\n",
    "```\n",
    "\n",
    "✅ Now you see:\n",
    "\n",
    "* One linear layer → produces both K and V.\n",
    "* We just split the last dimension in half.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. With multiple heads (optional)**\n",
    "\n",
    "Let’s reshape into multi-head format:\n",
    "\n",
    "```python\n",
    "head_dim = dim // num_heads\n",
    "k = k.reshape(B, N, num_heads, head_dim)\n",
    "v = v.reshape(B, N, num_heads, head_dim)\n",
    "print(\"K per head:\", k.shape)\n",
    "print(\"V per head:\", v.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "K per head: torch.Size([1, 4, 2, 4])\n",
    "V per head: torch.Size([1, 4, 2, 4])\n",
    "```\n",
    "\n",
    "✅ Each token now has 2 heads, each of 4 dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Visual Summary**\n",
    "\n",
    "| Step               | Operation                                | Shape          | Comment                  |\n",
    "| :----------------- | :--------------------------------------- | :------------- | :----------------------- |\n",
    "| Input              | `x`                                      | [1, 4, 8]      | 4 tokens, 8 channels     |\n",
    "| Linear projection  | `self.kv(x)`                             | [1, 4, 16]     | outputs K+V concatenated |\n",
    "| Split              | `chunk(2, dim=-1)`                       | [1, 4, 8] each | separates K and V        |\n",
    "| Multi-head reshape | reshape to `[B, N, num_heads, head_dim]` | [1, 4, 2, 4]   | per-head views           |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Takeaways**\n",
    "\n",
    "✅ `self.kv = nn.Linear(dim, dim * 2)` projects input tokens into a **combined (K,V)** space.\n",
    "✅ It’s equivalent to having two independent linear layers (`W_K`, `W_V`) concatenated.\n",
    "✅ The first half of the output corresponds to **K**, the second half to **V**.\n",
    "✅ It saves both computation and memory bandwidth — one matrix multiply instead of two.\n",
    "✅ Later, we reshape for multi-head attention.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25938c72-e2f5-410c-8f24-13993d39c6c9",
   "metadata": {},
   "source": [
    "## **PVT outputs**\n",
    "\n",
    "\n",
    "## **1. PVT produces a feature hierarchy (multi-scale outputs)**\n",
    "\n",
    "The **Pyramid Vision Transformer (PVT)** is built to behave like a CNN backbone (e.g. ResNet).\n",
    "Instead of giving only one global feature, it produces **4 feature maps at different resolutions**:\n",
    "\n",
    "| Stage | Symbol | Resolution (for 224×224 input) | Channels | Type of Information     |\n",
    "| :---: | :----- | :----------------------------: | :------: | :---------------------- |\n",
    "|   1   | **C1** |              56×56             |    64    | Local textures, edges   |\n",
    "|   2   | **C2** |              28×28             |    128   | Small object parts      |\n",
    "|   3   | **C3** |              14×14             |    320   | Large object regions    |\n",
    "|   4   | **C4** |               7×7              |    512   | Global semantic context |\n",
    "\n",
    "Each `Cᵢ` is the output of a stage containing several **Transformer blocks with Spatial Reduction Attention (SRA)**.\n",
    "\n",
    "So when you call in PyTorch:\n",
    "\n",
    "```python\n",
    "features = backbone(x)   # e.g., PVT from timm with features_only=True\n",
    "```\n",
    "\n",
    "you get:\n",
    "\n",
    "```python\n",
    "C1 = features[0]  # [B, 64, 56, 56]\n",
    "C2 = features[1]  # [B,128, 28, 28]\n",
    "C3 = features[2]  # [B,320, 14, 14]\n",
    "C4 = features[3]  # [B,512,  7,  7]\n",
    "```\n",
    "\n",
    "These are **multi-resolution, multi-semantic** features — perfect inputs for FPN.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339209d5-bd38-485d-a213-05d141174e51",
   "metadata": {},
   "source": [
    "Excellent question — and a very practical one. ✅\n",
    "\n",
    "Yes, you **can absolutely use the Pyramid Vision Transformer (PVT)** directly via the **[timm](https://github.com/huggingface/pytorch-image-models)** library.\n",
    "The **timm** package (by Ross Wightman) includes official and community-backed implementations of **PVT, PVTv2**, and many of their variants.\n",
    "\n",
    "Let’s go through:\n",
    "\n",
    "1. **Installation**\n",
    "2. **Available PVT models in timm**\n",
    "3. **Example usage (feature extraction, forward pass, and visualization)**\n",
    "4. **Integration notes for detection/segmentation**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Install timm**\n",
    "\n",
    "```bash\n",
    "pip install timm\n",
    "```\n",
    "\n",
    "To check your version:\n",
    "\n",
    "```bash\n",
    "python -c \"import timm; print(timm.__version__)\"\n",
    "```\n",
    "\n",
    "You should ideally have **timm ≥ 0.9.10** (contains PVTv2 and updated models).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. List available PVT models**\n",
    "\n",
    "You can see all models containing “pvt”:\n",
    "\n",
    "```python\n",
    "import timm\n",
    "models = timm.list_models(\"*pvt*\")\n",
    "for m in models:\n",
    "    print(m)\n",
    "```\n",
    "\n",
    "Typical output includes:\n",
    "\n",
    "```\n",
    "pvt_tiny\n",
    "pvt_small\n",
    "pvt_medium\n",
    "pvt_large\n",
    "pvt_v2_b0\n",
    "pvt_v2_b1\n",
    "pvt_v2_b2\n",
    "pvt_v2_b3\n",
    "pvt_v2_b4\n",
    "pvt_v2_b5\n",
    "```\n",
    "\n",
    "✅ The “v2” series are improved versions with:\n",
    "\n",
    "* Linear complexity attention\n",
    "* Improved positional encoding\n",
    "* Better pretrained weights\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Load a PVT model**\n",
    "\n",
    "### **Classification example**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "# Create model\n",
    "model = timm.create_model('pvt_v2_b2', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Random input (B=1, C=3, H=224, W=224)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Output shape: torch.Size([1, 1000])\n",
    "```\n",
    "\n",
    "✅ This is ImageNet-1k classification output.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Extract intermediate feature maps (for detection or segmentation)**\n",
    "\n",
    "If you want to use **PVT as a backbone**, not for classification, you can set:\n",
    "\n",
    "```python\n",
    "model = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "```\n",
    "\n",
    "Now it returns **pyramid feature maps** from multiple stages:\n",
    "\n",
    "```python\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    features = model(x)\n",
    "\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"Stage {i+1} feature:\", f.shape)\n",
    "```\n",
    "\n",
    "**Typical output:**\n",
    "\n",
    "```\n",
    "Stage 1 feature: torch.Size([1, 64, 56, 56])\n",
    "Stage 2 feature: torch.Size([1, 128, 28, 28])\n",
    "Stage 3 feature: torch.Size([1, 320, 14, 14])\n",
    "Stage 4 feature: torch.Size([1, 512, 7, 7])\n",
    "```\n",
    "\n",
    "✅ Exactly matches the hierarchical pyramid structure we discussed earlier.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualize the hierarchy**\n",
    "\n",
    "You can see how the channels and resolutions evolve:\n",
    "\n",
    "| Stage | Output Shape     | Channels   | Description        |\n",
    "| :---: | :--------------- | :--------- | :----------------- |\n",
    "|   1   | [B, 64, 56, 56]  | Low-level  | local texture      |\n",
    "|   2   | [B, 128, 28, 28] | mid-level  | edge/shape         |\n",
    "|   3   | [B, 320, 14, 14] | high-level | semantics          |\n",
    "|   4   | [B, 512, 7, 7]   | global     | full-image context |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Using as a backbone for downstream tasks**\n",
    "\n",
    "You can plug these outputs into:\n",
    "\n",
    "* **FPN / UPerNet** for segmentation\n",
    "* **RetinaNet / Mask R-CNN** for detection\n",
    "* **Custom encoder-decoder architectures (e.g., U-Net)**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from timm import create_model\n",
    "from torch import nn\n",
    "\n",
    "# PVT backbone\n",
    "backbone = create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "\n",
    "# Example decoder\n",
    "class SimpleSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x[-1])  # use last stage feature\n",
    "\n",
    "model = SimpleSegmentationHead(512, num_classes=21)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Example end-to-end flow**\n",
    "\n",
    "```python\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = backbone(x)\n",
    "\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"Stage {i+1}: {f.shape}\")\n",
    "\n",
    "out = model(features)\n",
    "print(\"Segmentation map:\", out.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Stage 1: torch.Size([1, 64, 56, 56])\n",
    "Stage 2: torch.Size([1, 128, 28, 28])\n",
    "Stage 3: torch.Size([1, 320, 14, 14])\n",
    "Stage 4: torch.Size([1, 512, 7, 7])\n",
    "Segmentation map: torch.Size([1, 21, 7, 7])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary**\n",
    "\n",
    "✅ **timm** provides ready-to-use **PVT and PVTv2** models.\n",
    "✅ You can use `pretrained=True` for ImageNet weights.\n",
    "✅ Use `features_only=True` to get multi-scale backbone outputs.\n",
    "✅ Perfect for **detection**, **segmentation**, **pose estimation**, etc.\n",
    "✅ All models include correct scaling, attention, and spatial reduction internally.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **complete minimal segmentation example** using `pvt_v2_b2` from timm + a small decoder (like a U-Net or FPN-style head)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
