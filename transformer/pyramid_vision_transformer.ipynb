{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20ae3ba-9380-4f20-911b-f2c6238c0e02",
   "metadata": {},
   "source": [
    "Let's go step by step through the **Pyramid Vision Transformer (PVT)** — a fundamental architecture that bridges **ViTs and CNNs** for dense visual tasks like **object detection** and **semantic segmentation**.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Motivation**\n",
    "\n",
    "The **Vision Transformer (ViT)** introduced powerful global attention but has drawbacks:\n",
    "\n",
    "* Requires **fixed-size inputs** (e.g., 224×224).\n",
    "* Produces **single-scale features**, unsuitable for dense prediction tasks.\n",
    "* Has **quadratic complexity** with respect to the number of patches.\n",
    "* Lacks **local inductive bias** (like translation invariance from CNNs).\n",
    "\n",
    "The **Pyramid Vision Transformer (PVT)** (Wang et al., *ICCV 2021*) fixes these issues by **building a hierarchical (multi-scale) feature pyramid** — like a CNN backbone (e.g., ResNet).\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Key Idea**\n",
    "\n",
    "PVT = **Hierarchical Vision Transformer Backbone**\n",
    "\n",
    "It mimics CNNs by generating **multi-resolution feature maps**:\n",
    "\n",
    "| Stage | Resolution                | Channels      | Purpose                 |\n",
    "| :---: | :------------------------ | :------------ | :---------------------- |\n",
    "|   1   | High (e.g., 1/4 of input) | Low           | Capture local features  |\n",
    "|   2   | Medium                    | More channels | Broader receptive field |\n",
    "|   3   | Low                       | More channels | Semantic features       |\n",
    "|   4   | Very low                  | Deep features | Global context          |\n",
    "\n",
    "These outputs can directly feed **FPN**, **Mask R-CNN**, **U-Net**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Architecture Overview**\n",
    "\n",
    "The PVT backbone has 4 stages, each performing:\n",
    "\n",
    "1. **Patch embedding** (patchify + linear projection)\n",
    "2. **Transformer encoder blocks**\n",
    "3. **Spatial reduction attention (SRA)** — reduces tokens before attention\n",
    "4. **Downsampling between stages** (to form a pyramid)\n",
    "\n",
    "---\n",
    "\n",
    "## **3.1. Stage 1: Patch Embedding**\n",
    "\n",
    "The image is split into small patches (like ViT):\n",
    "\n",
    "$$\n",
    "x_0 = \\text{PatchEmbed}(I) \\in \\mathbb{R}^{H_0 \\times W_0 \\times C_0}\n",
    "$$\n",
    "\n",
    "Then flattened into tokens for the first transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.2. Transformer Encoder with SRA**\n",
    "\n",
    "Standard self-attention has **O(N²)** cost (N = number of patches).\n",
    "To make it scalable, **PVT** introduces **Spatial Reduction Attention (SRA)**:\n",
    "\n",
    "Instead of using all keys and values, SRA **downsamples** them:\n",
    "\n",
    "$$\n",
    "K' = \\text{Downsample}(K), \\quad V' = \\text{Downsample}(V)\n",
    "$$\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K', V') = \\text{Softmax}\\left( \\frac{Q {K'}^T}{\\sqrt{d}} \\right) V'\n",
    "$$\n",
    "\n",
    "This reduces complexity from **O(N²)** → **O(N × N/s²)** where *s* is the reduction ratio.\n",
    "\n",
    "✅ This keeps **global receptive field** but reduces computation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.3. Pyramid Hierarchy**\n",
    "\n",
    "After each stage, feature resolution is halved, and channels increase:\n",
    "\n",
    "| Stage | Resolution | Channels | Patch size | Reduction ratio |\n",
    "| :---: | :--------- | :------- | :--------- | :-------------- |\n",
    "|   1   | 1/4        | 64       | 4×4        | 8               |\n",
    "|   2   | 1/8        | 128      | 2×2        | 4               |\n",
    "|   3   | 1/16       | 320      | 2×2        | 2               |\n",
    "|   4   | 1/32       | 512      | 2×2        | 1               |\n",
    "\n",
    "Each stage is a **Transformer encoder** operating on the corresponding feature scale.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Advantages**\n",
    "\n",
    "✅ **Hierarchical features** → usable as a CNN backbone (e.g., in Mask R-CNN).\n",
    "✅ **Global receptive field** from transformers.\n",
    "✅ **Efficient attention** via spatial reduction.\n",
    "✅ **Variable input resolution** support.\n",
    "✅ **Strong performance on dense tasks** (segmentation, detection).\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Comparison with ViT and Swin**\n",
    "\n",
    "| Model | Attention Type       | Hierarchy | Complexity | Windowed? | Suitable for Detection? |\n",
    "| :---- | :------------------- | :-------- | :--------- | :-------- | :---------------------- |\n",
    "| ViT   | Global               | ✖         | O(N²)      | ✖         | ✖                       |\n",
    "| Swin  | Window-based (local) | ✅         | O(N)       | ✅         | ✅                       |\n",
    "| PVT   | Global (SRA-reduced) | ✅         | O(N/s²)    | ✖         | ✅                       |\n",
    "\n",
    "So:\n",
    "\n",
    "* **PVT keeps global attention** but makes it efficient (SRA).\n",
    "* **Swin** uses local window attention and shifting to connect neighborhoods.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Equation Summary**\n",
    "\n",
    "Let’s define for stage *i*:\n",
    "\n",
    "* Input tokens:\n",
    "  $$ X_i \\in \\mathbb{R}^{N_i \\times C_i} $$\n",
    "* Spatial reduction ratio: *r*\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "Q = X_i W_Q, \\quad K = \\text{Down}(X_i W_K), \\quad V = \\text{Down}(X_i W_V)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SRA}(X_i) = \\text{Softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\text{Down}(\\cdot) = \\text{Reshape→Conv2d(stride=r)→Flatten}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Visual Summary**\n",
    "\n",
    "```\n",
    "Input Image\n",
    "   ↓\n",
    "[Stage 1] Patch Embed → Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 2] Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 3] Transformer (SRA) → Downsample\n",
    "   ↓\n",
    "[Stage 4] Transformer (SRA)\n",
    "   ↓\n",
    "Feature Pyramid Outputs → Detection / Segmentation Head\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **8. Typical Usage**\n",
    "\n",
    "PVT variants:\n",
    "\n",
    "* **PVT-Tiny**, **PVT-Small**, **PVT-Medium**, **PVT-Large**\n",
    "  differ in embedding dims and number of blocks.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* **PVT + FPN → RetinaNet / Mask R-CNN**\n",
    "* **PVT + UPerNet → Semantic Segmentation**\n",
    "\n",
    "---\n",
    "\n",
    "# **9. Example PyTorch Skeleton**\n",
    "\n",
    "```python\n",
    "class SpatialReductionAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, sr_ratio):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "        \n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).transpose(1, 2)\n",
    "            x_ = self.norm(x_)\n",
    "        else:\n",
    "            x_ = x\n",
    "        \n",
    "        kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads)\n",
    "        k, v = kv[:, :, 0], kv[:, :, 1]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).reshape(B, N, C)\n",
    "        return out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **10. Summary**\n",
    "\n",
    "✅ **Concept:** Build a **multi-scale feature pyramid** with transformers.\n",
    "✅ **Trick:** **Spatial Reduction Attention (SRA)** → efficient global attention.\n",
    "✅ **Benefit:** Works as a **drop-in CNN backbone** for detection/segmentation.\n",
    "✅ **Key papers:**\n",
    "\n",
    "* *Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions* (Wang et al., ICCV 2021)\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499da1d-c0aa-420c-bce7-feef12422b4a",
   "metadata": {},
   "source": [
    "## **numerical example**\n",
    "Perfect — let’s go through a **numerical example** showing how **Spatial Reduction Attention (SRA)** in **Pyramid Vision Transformer (PVT)** drastically reduces computational cost compared to standard self-attention.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Reminder: Complexity of Self-Attention**\n",
    "\n",
    "For each layer, self-attention requires:\n",
    "\n",
    "$$\n",
    "\\text{Cost} \\sim O(N^2 \\cdot d)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* ( N ) = number of tokens (patches)\n",
    "* ( d ) = embedding dimension\n",
    "\n",
    "In ViT, ( N = (H/P) \\times (W/P) ).\n",
    "\n",
    "Example: for 224×224 input and 16×16 patch size:\n",
    "$$\n",
    "N = (224/16)^2 = 14^2 = 196\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Example Setup**\n",
    "\n",
    "Let’s consider a **PVT stage** operating on feature maps of size 56×56 (like Stage 2 in PVT-Tiny).\n",
    "\n",
    "| Parameter               | Symbol                     | Value |\n",
    "| ----------------------- | -------------------------- | ----- |\n",
    "| Feature map size        | ( H \\times W )             | 56×56 |\n",
    "| Tokens                  | ( N = 3136 )               |       |\n",
    "| Embedding dim           | ( d = 64 )                 |       |\n",
    "| Heads                   | ( h = 1 ) (for simplicity) |       |\n",
    "| Spatial reduction ratio | ( s = 8 )                  |       |\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Cost of Standard Self-Attention**\n",
    "\n",
    "In vanilla ViT-style attention:\n",
    "\n",
    "$$\n",
    "Q, K, V \\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "Attention matrix has size ( N \\times N ).\n",
    "\n",
    "Total multiply-adds per head:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}} = N^2 \\cdot d\n",
    "$$\n",
    "\n",
    "Plug in the numbers:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}} = (3136)^2 \\times 64 = 9.8 \\times 10^8\n",
    "$$\n",
    "\n",
    "Almost **1 billion operations per head per layer** — extremely heavy!\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Cost of SRA (Spatial Reduction Attention)**\n",
    "\n",
    "Now, in SRA we **downsample K and V** by ratio ( s ).\n",
    "This means:\n",
    "\n",
    "$$\n",
    "N' = \\frac{N}{s^2}\n",
    "$$\n",
    "\n",
    "So for ( s = 8 ):\n",
    "\n",
    "$$\n",
    "N' = \\frac{3136}{8^2} = \\frac{3136}{64} = 49\n",
    "$$\n",
    "\n",
    "Now, K and V each have 49 tokens instead of 3136.\n",
    "\n",
    "The attention matrix now has size ( N \\times N' = 3136 \\times 49 ).\n",
    "\n",
    "Cost becomes:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}} = N \\times N' \\times d = 3136 \\times 49 \\times 64 = 9.8 \\times 10^6\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Comparison**\n",
    "\n",
    "| Method      | Tokens in K/V | Attention Matrix | Operations | Reduction       |\n",
    "| :---------- | :------------ | :--------------- | :--------- | :-------------- |\n",
    "| Standard SA | 3136          | 3136×3136        | 9.8×10⁸    | —               |\n",
    "| SRA (s=8)   | 49            | 3136×49          | 9.8×10⁶    | **100× faster** |\n",
    "\n",
    "✅ The SRA reduces computational cost by roughly **100×** at this stage.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Why It Still Works**\n",
    "\n",
    "* **Queries (Q)** remain full-resolution (one per token),\n",
    "  so each patch still \"attends\" globally.\n",
    "* **Keys/Values** are **downsampled**, so we summarize context efficiently.\n",
    "* The global context is preserved, but at a **lower spatial resolution**.\n",
    "\n",
    "This is similar to how **FPN** or **feature maps in CNNs** summarize spatial information at different scales.\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Visual Intuition**\n",
    "\n",
    "```\n",
    "Standard Attention\n",
    "------------------\n",
    "Q : 3136 tokens (56×56)\n",
    "K : 3136 tokens\n",
    "V : 3136 tokens\n",
    "→ Attention matrix: 3136 × 3136\n",
    "→ Full resolution, costly!\n",
    "\n",
    "SRA (s = 8)\n",
    "------------\n",
    "Q : 3136 tokens (56×56)\n",
    "K : 49 tokens (7×7 after downsampling)\n",
    "V : 49 tokens\n",
    "→ Attention matrix: 3136 × 49\n",
    "→ Much smaller, same global reach\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **8. Formula Recap**\n",
    "\n",
    "Standard attention:\n",
    "\n",
    "$$\n",
    "\\text{SA}(Q,K,V) = \\text{Softmax}\\left( \\frac{QK^T}{\\sqrt{d}} \\right)V\n",
    "$$\n",
    "\n",
    "Spatial Reduction Attention:\n",
    "\n",
    "$$\n",
    "\\text{SRA}(Q,K',V') = \\text{Softmax}\\left( \\frac{Q{K'}^T}{\\sqrt{d}} \\right)V'\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "K',V' = \\text{Conv2D_Downsample}(K,V, \\text{stride}=s)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **9. Summary**\n",
    "\n",
    "✅ **Spatial Reduction Attention (SRA)** dramatically lowers attention cost\n",
    "✅ **Global context preserved**, since Q still covers all spatial tokens\n",
    "✅ **Multi-scale hierarchy** gives CNN-like feature pyramids\n",
    "✅ **Ideal backbone** for detection and segmentation\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to extend this with a **small PyTorch example** showing the actual tensor shapes before and after applying SRA (e.g., 56×56 → 7×7 reduction)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
