{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20ae3ba-9380-4f20-911b-f2c6238c0e02",
   "metadata": {},
   "source": [
    "# **Pyramid Vision Transformer (PVT)**\n",
    "The **Pyramid Vision Transformer (PVT)** — a fundamental architecture that bridges **ViTs and CNNs** for dense visual tasks like **object detection** and **semantic segmentation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Motivation**\n",
    "\n",
    "The **Vision Transformer (ViT)** introduced powerful global attention but has drawbacks:\n",
    "\n",
    "* Requires **fixed-size inputs** (e.g., 224×224).\n",
    "* Produces **single-scale features**, unsuitable for dense prediction tasks.\n",
    "* Has **quadratic complexity** with respect to the number of patches.\n",
    "* Lacks **local inductive bias** (like translation invariance from CNNs).\n",
    "\n",
    "The **Pyramid Vision Transformer (PVT)** (Wang et al., *ICCV 2021*) fixes these issues by **building a hierarchical (multi-scale) feature pyramid** — like a CNN backbone (e.g., ResNet).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Key Idea**\n",
    "\n",
    "PVT = **Hierarchical Vision Transformer Backbone**\n",
    "\n",
    "It mimics CNNs by generating **multi-resolution feature maps**:\n",
    "\n",
    "| Stage | Resolution                | Channels      | Purpose                 |\n",
    "| :---: | :------------------------ | :------------ | :---------------------- |\n",
    "|   1   | High (e.g., 1/4 of input) | Low           | Capture local features  |\n",
    "|   2   | Medium                    | More channels | Broader receptive field |\n",
    "|   3   | Low                       | More channels | Semantic features       |\n",
    "|   4   | Very low                  | Deep features | Global context          |\n",
    "\n",
    "These outputs can directly feed **FPN**, **Mask R-CNN**, **U-Net**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Architecture Overview**\n",
    "\n",
    "The PVT backbone has 4 stages, each performing:\n",
    "\n",
    "1. **Patch embedding** (patchify + linear projection)\n",
    "2. **Transformer encoder blocks**\n",
    "3. **Spatial reduction attention (SRA)** — reduces tokens before attention\n",
    "4. **Downsampling between stages** (to form a pyramid)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.1. Stage 1: Patch Embedding**\n",
    "\n",
    "The image is split into small patches (like ViT):\n",
    "\n",
    "$$\n",
    "x_0 = \\text{PatchEmbed}(I) \\in \\mathbb{R}^{H_0 \\times W_0 \\times C_0}\n",
    "$$\n",
    "\n",
    "Then flattened into tokens for the first transformer block.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2. Transformer Encoder with SRA**\n",
    "\n",
    "Standard self-attention has **O(N²)** cost (N = number of patches).\n",
    "To make it scalable, **PVT** introduces **Spatial Reduction Attention (SRA)**:\n",
    "\n",
    "Instead of using all keys and values, SRA **downsamples** them:\n",
    "\n",
    "$$\n",
    "K' = \\text{Downsample}(K), \\quad V' = \\text{Downsample}(V)\n",
    "$$\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K', V') = \\text{Softmax}\\left( \\frac{Q {K'}^T}{\\sqrt{d}} \\right) V'\n",
    "$$\n",
    "\n",
    "This reduces complexity from **O(N²)** → **O(N × N/s²)** where *s* is the reduction ratio.\n",
    "\n",
    "✅ This keeps **global receptive field** but reduces computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc65db7-df16-4122-af9f-22c36c98f16e",
   "metadata": {},
   "source": [
    "#### **3.3. Downsampling**\n",
    "\n",
    "\n",
    "**PVT performs two different kinds of downsampling**:\n",
    "\n",
    "1. **Downsampling the input tokens between stages** (via Conv2d with stride)\n",
    "2. **Downsampling the K and V tokens inside attention** (via Spatial Reduction Attention, SRA)\n",
    "\n",
    "These two are separate mechanisms.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a3c61-4bcb-467c-bfea-65b0121ab784",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Downsampling of the **input** (Patch Embedding)**\n",
    "\n",
    "This is the **true spatial downsampling** of feature maps.\n",
    "\n",
    "At each stage:\n",
    "\n",
    "* Stage 1: stride 4\n",
    "* Stage 2: stride 2\n",
    "* Stage 3: stride 2\n",
    "* Stage 4: stride 2\n",
    "\n",
    "This reduces the resolution:\n",
    "\n",
    "$$\n",
    "224 \\to 56 \\to 28 \\to 14 \\to 7\n",
    "$$\n",
    "\n",
    "This is identical to CNN backbones (ResNet, EfficientNet, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.1 How Conv2d Performs Downsampling**\n",
    "\n",
    "If your input has spatial size\n",
    "$$\n",
    "H_{\\text{in}} \\times W_{\\text{in}},\n",
    "$$\n",
    "a convolution with:\n",
    "\n",
    "* kernel size: $ k $\n",
    "* stride: $ s $\n",
    "* padding: $ p $\n",
    "* dilation: $ d $\n",
    "\n",
    "produces an output of size\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - d (k-1) - 1}{s} + 1 \\right\\rfloor,\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2p - d (k-1) - 1}{s} + 1 \\right\\rfloor.\n",
    "$$\n",
    "\n",
    "\n",
    "$d=1, k=2, s=2$\n",
    "\n",
    "<img src=\"../conv/images/no_padding_strides.gif\" />\n",
    "\n",
    "\n",
    "\n",
    "$d=2, k=3, s=1$\n",
    "\n",
    "<img src=\"../conv/images/dilation.gif\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#### **4.2 Kernel Size, Stride, and Padding**  \n",
    "\n",
    "If you set:\n",
    "\n",
    "* stride = 2\n",
    "* kernel = 3\n",
    "* padding = 1\n",
    "\n",
    "Plugging into the general equation (with d = 1)\n",
    "\n",
    "General form:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - d(k-1) - 1}{s} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "Set\n",
    "$d = 1,\\ p = 1,\\ k = 3,\\ s = 2$:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}}\n",
    "= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} + 2 - (3 - 1) - 1}{2} + 1\n",
    "\\right\\rfloor\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}}\n",
    "= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} - 1}{2} + 1\n",
    "\\right\\rfloor\n",
    "$$\n",
    "\n",
    "Combine:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}}\n",
    "= \\left\\lfloor \\frac{H_{\\text{in}} + 1}{2} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "For even sizes:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\frac{H_{\\text{in}}}{2}\n",
    "$$\n",
    "\n",
    "This is exactly PVT's downsampling behavior.\n",
    "\n",
    "\n",
    "This is the famous **“same” downsampling pattern** used in ResNet, EfficientNet, etc.\n",
    "\n",
    "But that requires padding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "```\n",
    "\n",
    "This **halves** height and width:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\frac{H_{\\text{in}}}{2},\\quad W_{\\text{out}} = \\frac{W_{\\text{in}}}{2}.\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b27cb-d57e-4865-9907-10e48d5ab64f",
   "metadata": {},
   "source": [
    "## **5. Spatial Reduction Attention(SRA) Downsampling**\n",
    "\n",
    "This is the *core trick* of **Spatial Reduction Attention (SRA)** in **PVT**.\n",
    "\n",
    "The **downsampling** of $ K $ and $ V $ is **not** done by a linear projection (like `nn.Linear`), but by a **2D convolution with stride = s**, followed by normalization.\n",
    "\n",
    "\n",
    "#### **5.1 Downsampling of **K** and **V** only (inside attention)**\n",
    "\n",
    "Inside each Transformer block, PVT introduces **Spatial Reduction Attention (SRA)**:\n",
    "\n",
    "* **Q** is NOT downsampled\n",
    "* **K** and **V** are downsampled using a linear projection with reduction ratio (r)\n",
    "\n",
    "**Reduction ratios:**\n",
    "\n",
    "| Stage | Reduction (r) |\n",
    "| ----- | ------------- |\n",
    "| 1     | 8             |\n",
    "| 2     | 4             |\n",
    "| 3     | 2             |\n",
    "| 4     | 1             |\n",
    "\n",
    "Thus:\n",
    "\n",
    "* **K and V tokens become fewer**\n",
    "* **Q keeps the full resolution**\n",
    "\n",
    "This reduces the cost of attention.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3. Why downsample only K and V?**\n",
    "\n",
    "Standard attention complexity is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(N^2)\n",
    "$$\n",
    "\n",
    "If you reduce only K and V by a factor $r$:\n",
    "\n",
    "* Q dimension: $N$\n",
    "* K, V dimension: $N/r$\n",
    "\n",
    "Then attention cost becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(N \\cdot N/r) = \\mathcal{O}(N^2/r)\n",
    "$$\n",
    "\n",
    "This keeps performance high while reducing runtime and memory.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5.4. Visual summary**\n",
    "\n",
    "**True downsampling (Conv):**\n",
    "\n",
    "```\n",
    "Input 224x224\n",
    "    ↓ stride 4\n",
    "56x56 tokens\n",
    "    ↓ stride 2\n",
    "28x28 tokens\n",
    "    ↓ stride 2\n",
    "14x14 tokens\n",
    "    ↓ stride 2\n",
    "7x7 tokens\n",
    "```\n",
    "\n",
    "**Inside each Transformer block (SRA):**\n",
    "\n",
    "```\n",
    "Q: full resolution\n",
    "K: reduced with factor r\n",
    "V: reduced with factor r\n",
    "```\n",
    "\n",
    "Example in stage 2:\n",
    "\n",
    "* Q = 28×28 = 784 tokens\n",
    "* K,V reduced by r = 4\n",
    "\n",
    "So K,V = 784 / 4 = 196 tokens.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff21612-5f2f-41b6-9d34-55535d29bd6f",
   "metadata": {},
   "source": [
    "## **6. PVT-v1 and PVT-v2 Downsampling Parameters**\n",
    "\n",
    "Below is the **precise list of kernel sizes (k)** and **strides (s)** used by **PVT-v1** and **PVT-v2** at **every stage**, for **Patch Embedding** and **Spatial-Reduction Attention (SRA)**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **6.1. PVT-v1 (original PVT)**\n",
    "\n",
    "#### **6.1.1Patch Embedding Layers (downsampling)**\n",
    "\n",
    "PVT-v1 downsamples the image using **Conv2d with kernel=7 or 3** and **stride=4 / 2 / 2 / 2**.\n",
    "\n",
    "| Stage   | Input → Output Resolution | Conv2d kernel $k$ | stride $s$ | padding | Channels |\n",
    "| ------- | ------------------------- | ----------------- | ---------- | ------- | -------- |\n",
    "| Stage 1 | $224 \\to 56$              | $k = 7$           | $s = 4$    | $p = 3$ | 64       |\n",
    "| Stage 2 | $56 \\to 28$               | $k = 3$           | $s = 2$    | $p = 1$ | 128      |\n",
    "| Stage 3 | $28 \\to 14$               | $k = 3$           | $s = 2$    | $p = 1$ | 320      |\n",
    "| Stage 4 | $14 \\to 7$                | $k = 3$           | $s = 2$    | $p = 1$ | 512      |\n",
    "\n",
    "**How they downsample:**\n",
    "\n",
    "Using the standard equation:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - k}{s} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "Example: Stage 1\n",
    "$H_{\\text{in}} = 224,\\ k = 7,\\ s = 4,\\ p = 3$\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\frac{224 + 6 - 7}{4} + 1 = 56.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.1.2 SRA: Spatial-Reduction Attention**\n",
    "\n",
    "Inside each transformer block, the **key/value** tokens are reduced using a **linear projection with stride (r)**, not a convolution.\n",
    "\n",
    "Reduction ratios (r):\n",
    "\n",
    "| Stage | Reduction (r) |\n",
    "| ----- | ------------- |\n",
    "| 1     | 8             |\n",
    "| 2     | 4             |\n",
    "| 3     | 2             |\n",
    "| 4     | 1             |\n",
    "\n",
    "This affects **attention**, not spatial downsampling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6da38-ff6f-4d68-a000-9b6558b8dd5b",
   "metadata": {},
   "source": [
    "### **6.2. PVT-v2 (improved version)**\n",
    "\n",
    "PVT-v2 replaces the large kernel=7 with kernel=3 everywhere, but still keeps the same downsampling ratio.\n",
    "\n",
    "#### **6.2.1 Patch Embedding Layers**\n",
    "\n",
    "| Stage   | Input → Output | Conv2d kernel $k$ | stride $s$ | padding | Channels |\n",
    "| ------- | -------------- | ----------------- | ---------- | ------- | -------- |\n",
    "| Stage 1 | $224 \\to 56$   | $k = 3$           | $s = 4$    | $p = 1$ | 64       |\n",
    "| Stage 2 | $56 \\to 28$    | $k = 3$           | $s = 2$    | $p = 1$ | 128      |\n",
    "| Stage 3 | $28 \\to 14$    | $k = 3$           | $s = 2$    | $p = 1$ | 320      |\n",
    "| Stage 4 | $14 \\to 7$     | $k = 3$           | $s = 2$    | $p = 1$ | 512      |\n",
    "\n",
    "**Key change:**\n",
    "\n",
    "* kernel=7 → kernel=3 in Stage 1\n",
    "* exact same spatial resolutions as PVT-v1\n",
    "\n",
    "---\n",
    "\n",
    "#### **6.2.2 SRA Reduction Ratios in PVT-v2**\n",
    "\n",
    "Same idea, but slightly different values depending on variant (Tiny/Small/Medium/Large):\n",
    "\n",
    "Typical:\n",
    "\n",
    "| Stage | Reduction (r) |\n",
    "| ----- | ------------- |\n",
    "| 1     | 8             |\n",
    "| 2     | 4             |\n",
    "| 3     | 2             |\n",
    "| 4     | 1             |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf47c1-1b4b-4556-a85e-f990107d552b",
   "metadata": {},
   "source": [
    "## **7. Summary Table for PVT-v1 and PVT-v2**\n",
    "\n",
    "#### **7.1 (k and s only)**\n",
    "\n",
    "| Stage   | PVT-v1 $k,s$ | PVT-v2 $k,s$ | Output Res   |\n",
    "| ------- | ------------ | ------------ | ------------ |\n",
    "| Stage 1 | $k=7, s=4$   | $k=3, s=4$   | $224 \\to 56$ |\n",
    "| Stage 2 | $k=3, s=2$   | $k=3, s=2$   | $56 \\to 28$  |\n",
    "| Stage 3 | $k=3, s=2$   | $k=3, s=2$   | $28 \\to 14$  |\n",
    "| Stage 4 | $k=3, s=2$   | $k=3, s=2$   | $14 \\to 7$   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **7.2  Why these choices**\n",
    "\n",
    "* Stage 1 must reduce resolution strongly (224 → 56).\n",
    "  That is why stride=4 is used.\n",
    "* Later stages use stride=2, like a CNN backbone (ResNet).\n",
    "* kernel=3 with padding=1 maintains a stable “same-like” downsampling behavior.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f708b0d-60c7-4d49-be40-3f1b99a67068",
   "metadata": {},
   "source": [
    "## **8. Advantages**\n",
    "\n",
    "- ✅ **Hierarchical features** → usable as a CNN backbone (e.g., in Mask R-CNN).\n",
    "- ✅ **Global receptive field** from transformers.\n",
    "- ✅ **Efficient attention** via spatial reduction.\n",
    "- ✅ **Variable input resolution** support.\n",
    "- ✅ **Strong performance on dense tasks** (segmentation, detection).\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Comparison with ViT and Swin**\n",
    "\n",
    "| Model | Attention Type       | Hierarchy | Complexity | Windowed? | Suitable for Detection? |\n",
    "| :---- | :------------------- | :-------- | :--------- | :-------- | :---------------------- |\n",
    "| ViT   | Global               | ✖         | O(N²)      | ✖         | ✖                       |\n",
    "| Swin  | Window-based (local) | ✅         | O(N)       | ✅         | ✅                       |\n",
    "| PVT   | Global (SRA-reduced) | ✅         | O(N/s²)    | ✖         | ✅                       |\n",
    "\n",
    "So:\n",
    "\n",
    "* **PVT keeps global attention** but makes it efficient (SRA).\n",
    "* **Swin** uses local window attention and shifting to connect neighborhoods.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Equation Summary**\n",
    "\n",
    "Let’s define for stage *i*:\n",
    "\n",
    "* Input tokens:\n",
    "  $$ X_i \\in \\mathbb{R}^{N_i \\times C_i} $$\n",
    "* Spatial reduction ratio: *r*\n",
    "\n",
    "Then attention becomes:\n",
    "\n",
    "$$\n",
    "Q = X_i W_Q, \\quad K = \\text{Down}(X_i W_K), \\quad V = \\text{Down}(X_i W_V)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SRA}(X_i) = \\text{Softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\text{Down}(\\cdot) = \\text{Reshape→Conv2d(stride=r)→Flatten}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Typical Usage**\n",
    "\n",
    "PVT variants:\n",
    "\n",
    "* **PVT-Tiny**, **PVT-Small**, **PVT-Medium**, **PVT-Large**\n",
    "  differ in embedding dims and number of blocks.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* **PVT + FPN → RetinaNet / Mask R-CNN**\n",
    "* **PVT + UPerNet → Semantic Segmentation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eceff-c080-4c29-8ef9-75fc07df3a58",
   "metadata": {},
   "source": [
    "## **12. Numerical Example**\n",
    "\n",
    "\n",
    "#### **12.1. Input → Stage 1 (Patch Embedding)**\n",
    "\n",
    "Start with a $224 \\times 224$ RGB image.\n",
    "\n",
    "PVT-Tiny uses a Conv2d patch embedding with **stride 4**, which gives an **effective patch size of $4 \\times 4$**.\n",
    "\n",
    "Take PVT-v2 style:\n",
    "\n",
    "* kernel $k = 3$\n",
    "* stride $s = 4$\n",
    "* padding $p = 1$\n",
    "* dilation $d = 1$\n",
    "\n",
    "Output size per dimension:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\text{out}} &= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} + 2p - d(k-1) - 1}{s} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{224 + 2(1) - 1(3-1) - 1}{4} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{224 + 2 - 2 - 1}{4} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{223}{4} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\lfloor 55.75 + 1 \\rfloor = \\lfloor 56.75 \\rfloor \\\\\n",
    "&= 56\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- Resolution: $56 \\times 56$\n",
    "\n",
    "- Tokens:\n",
    "\n",
    "  $$\n",
    "  N_1 = 56 \\times 56 = 3136\n",
    "  $$\n",
    "\n",
    "* Channels: $d_1 = 64$\n",
    "\n",
    "Now we apply **SRA inside Stage 1**.\n",
    "\n",
    "#### Stage 1 SRA reduction ratio $s_{\\text{SRA}} = 8$\n",
    "\n",
    "Standard self-attention cost:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(1)} = N_1^2 \\times d_1\n",
    "= 3136^2 \\times 64\n",
    "\\approx 9.8 \\times 10^8.\n",
    "$$\n",
    "\n",
    "SRA downsamples K,V by spatial factor $8$:\n",
    "\n",
    "* Spatially: $56 \\times 56 \\to 7 \\times 7$\n",
    "* Tokens for K,V:\n",
    "\n",
    "  $$\n",
    "  N_1' = \\frac{N_1}{8^2}\n",
    "  = \\frac{3136}{64}\n",
    "  = 49\n",
    "  $$\n",
    "\n",
    "Cost:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(1)} = N_1 \\times N_1' \\times d_1\n",
    "= 3136 \\times 49 \\times 64\n",
    "\\approx 9.8 \\times 10^6.\n",
    "$$\n",
    "\n",
    "Shapes (batch (B=1)):\n",
    "\n",
    "* Input / tokens: $(1, 3136, 64)$\n",
    "* $Q$: $(1, 3136, 64)$\n",
    "* $K', V'$: $(1, 49, 64)$\n",
    "* Attention map: $(3136, 49)$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Stage 1 → Stage 2 (Conv downsampling)\n",
    "\n",
    "Between Stage 1 and 2, PVT uses another Conv2d with:\n",
    "\n",
    "* $k = 3$, $s = 2$, $p = 1$, $d = 1$\n",
    "\n",
    "Input resolution: $56 \\times 56$.\n",
    "\n",
    "Using\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\text{out}} &= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} + 2p - d(k-1) - 1}{s} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{56 + 2(1) - 1(3-1) - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{56 + 2 - 2 - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{55}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\lfloor 27.5 + 1 \\rfloor = \\lfloor 28.5 \\rfloor \\\\\n",
    "&= 28\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Resolution: $28 \\times 28$\n",
    "\n",
    "* Tokens:\n",
    "\n",
    "  $$\n",
    "  N_2 = 28 \\times 28 = 784\n",
    "  $$\n",
    "\n",
    "* Channels: $d_2 = 128$\n",
    "\n",
    "#### Stage 2 SRA $(s_{\\text{SRA}} = 4)$\n",
    "\n",
    "Standard SA cost:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(2)} = N_2^2 \\times d_2\n",
    "= 784^2 \\times 128\n",
    "\\approx 7.9 \\times 10^7.\n",
    "$$\n",
    "\n",
    "SRA:\n",
    "\n",
    "* Spatial reduction by (4): $28 \\times 28 \\to 7 \\times 7$\n",
    "* Tokens:\n",
    "\n",
    "  $$\n",
    "  N_2' = \\frac{N_2}{4^2}\n",
    "  = \\frac{784}{16}\n",
    "  = 49\n",
    "  $$\n",
    "\n",
    "Cost:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(2)} = N_2 \\times N_2' \\times d_2\n",
    "= 784 \\times 49 \\times 128\n",
    "\\approx 4.9 \\times 10^6.\n",
    "$$\n",
    "\n",
    "Shapes:\n",
    "\n",
    "* Input: $(1, 784, 128)$\n",
    "* $Q$: $(1, 784, 128)$\n",
    "* $K', V'$: $(1, 49, 128)$\n",
    "* Attention map: $(784, 49)$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Stage 2 → Stage 3 (Conv downsampling)\n",
    "\n",
    "Same Conv pattern:\n",
    "\n",
    "* $k = 3$, $s = 2$, $p = 1$, $d = 1$\n",
    "\n",
    "Input: $28 \\times 28$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\text{out}} &= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} + 2p - d(k-1) - 1}{s} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{28 + 2(1) - 1(3-1) - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{28 + 2 - 2 - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{27}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\lfloor 13.5 + 1 \\rfloor = \\lfloor 14.5 \\rfloor \\\\\n",
    "&= 14\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Resolution: $14 \\times 14$\n",
    "\n",
    "* Tokens:\n",
    "\n",
    "  $$\n",
    "  N_3 = 14 \\times 14 = 196\n",
    "  $$\n",
    "\n",
    "* Channels: $d_3 = 320$\n",
    "\n",
    "#### Stage 3 SRA $(s_{\\text{SRA}} = 2)$\n",
    "\n",
    "Standard SA:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(3)} = N_3^2 \\times d_3\n",
    "= 196^2 \\times 320\n",
    "\\approx 1.2 \\times 10^7.\n",
    "$$\n",
    "\n",
    "SRA:\n",
    "\n",
    "* Spatial reduction by (2): (14 \\times 14 \\to 7 \\times 7)\n",
    "* Tokens:\n",
    "\n",
    "  $$\n",
    "  N_3' = \\frac{N_3}{2^2}\n",
    "  = \\frac{196}{4}\n",
    "  = 49\n",
    "  $$\n",
    "\n",
    "Cost:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{SRA}}^{(3)} = N_3 \\times N_3' \\times d_3\n",
    "= 196 \\times 49 \\times 320\n",
    "\\approx 3.1 \\times 10^6.\n",
    "$$\n",
    "\n",
    "Shapes:\n",
    "\n",
    "* Input: $(1, 196, 320)$\n",
    "* $Q$: ((1, 196, 320))\n",
    "* $K', V'$: $(1, 49, 320)$\n",
    "* Attention map: $(196, 49)$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Stage 3 → Stage 4 (Conv downsampling)\n",
    "\n",
    "Again Conv with $k=3, s=2, p=1, d=1$:\n",
    "\n",
    "Input: $14 \\times 14$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{\\text{out}} &= \\left\\lfloor\n",
    "\\frac{H_{\\text{in}} + 2p - d(k-1) - 1}{s} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{14 + 2(1) - 1(3-1) - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{14 + 2 - 2 - 1}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\left\\lfloor\n",
    "\\frac{13}{2} + 1\n",
    "\\right\\rfloor \\\\\n",
    "&= \\lfloor 6.5 + 1 \\rfloor = \\lfloor 7.5 \\rfloor \\\\\n",
    "&= 7\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Resolution: $7 \\times 7$\n",
    "\n",
    "* Tokens:\n",
    "\n",
    "  $$\n",
    "  N_4 = 7 \\times 7 = 49\n",
    "  $$\n",
    "\n",
    "* Channels: $d_4 = 512$\n",
    "\n",
    "#### Stage 4 SRA ($s_{\\text{SRA}} = 1$)\n",
    "\n",
    "Now the feature map is already small, so SRA does **no extra reduction**:\n",
    "\n",
    "* $N_4' = N_4 = 49$\n",
    "\n",
    "Standard SA and SRA costs are identical:\n",
    "\n",
    "$$\n",
    "\\text{Cost}_{\\text{standard}}^{(4)}\n",
    "= \\text{Cost}_{\\text{SRA}}^{(4)}\n",
    "= N_4^2 \\times d_4\n",
    "= 49^2 \\times 512\n",
    "\\approx 1.2 \\times 10^6.\n",
    "$$\n",
    "\n",
    "Shapes:\n",
    "\n",
    "* Input: $(1, 49, 512)$\n",
    "* $Q$: $(1, 49, 512)$\n",
    "* $K', V'$: $(1, 49, 512)$\n",
    "* Attention map: $(49, 49)$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Combined View: Conv Downsampling + SRA\n",
    "\n",
    "Here is everything together:\n",
    "\n",
    "\n",
    "| Stage | Conv Downsampling $in → out$      | Output Res | Tokens $N$ | Channels $d$ | SRA ratio $s_{\\text{SRA}}$ | $N'$ for K,V |    Q shape    |  K′/V′ shape |  Attn map  |\n",
    "| :---: | :-------------------------------- | :--------: | :--------: | :----------: | :------------------------: | :----------: | :-----------: | :----------: | :--------: |\n",
    "|   1   | $224^2 \\xrightarrow{k=3,s=4,p=1}$ |    56×56   |    3136    |      64      |              8             |      49      | $1, 3136, 64$ |  $1, 49, 64$ | $3136, 49$ |\n",
    "|   2   | $56^2 \\xrightarrow{k=3,s=2,p=1}$  |    28×28   |     784    |      128     |              4             |      49      | $1, 784, 128$ | $1, 49, 128$ |  $784, 49$ |\n",
    "|   3   | $28^2 \\xrightarrow{k=3,s=2,p=1}$  |    14×14   |     196    |      320     |              2             |      49      | $1, 196, 320$ | $1, 49, 320$ |  $196, 49$ |\n",
    "|   4   | $14^2 \\xrightarrow{k=3,s=2,p=1}$  |     7×7    |     49     |      512     |              1             |      49      |  $1, 49, 512$ | $1, 49, 512$ |  $49, 49$  |\n",
    "\n",
    "\n",
    "So now you can clearly see:\n",
    "\n",
    "* **Conv downsampling between stages**:\n",
    "  $224 \\to 56 \\to 28 \\to 14 \\to 7$\n",
    "* **SRA downsampling inside stages**:\n",
    "  keeps Q at full resolution in that stage, but reduces K,V to always $7 \\times 7 = 49$ tokens in the first three stages.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339209d5-bd38-485d-a213-05d141174e51",
   "metadata": {},
   "source": [
    "## **Python Example**\n",
    "\n",
    "\n",
    "You **can absolutely use the Pyramid Vision Transformer (PVT)** directly via the **[timm](https://github.com/huggingface/pytorch-image-models)** for both **PVT, PVTv2**, and many of their variants.\n",
    "\n",
    "---\n",
    "\n",
    "#### **List available PVT models**\n",
    "\n",
    "You can see all models containing “pvt”:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7650134a-e909-4acb-a38e-fe5c3ffabd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvt_v2_b0\n",
      "pvt_v2_b1\n",
      "pvt_v2_b2\n",
      "pvt_v2_b2_li\n",
      "pvt_v2_b3\n",
      "pvt_v2_b4\n",
      "pvt_v2_b5\n",
      "twins_pcpvt_base\n",
      "twins_pcpvt_large\n",
      "twins_pcpvt_small\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm \n",
    "from timm import create_model\n",
    "\n",
    "# fmt: on\n",
    "\n",
    "import timm\n",
    "models = timm.list_models(\"*pvt*\")\n",
    "for m in models:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d593d251-f0df-42e6-b8e7-746b6a5cff43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backbone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbackbone\u001b[49m.named_children():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(module).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'backbone' is not defined"
     ]
    }
   ],
   "source": [
    "for name, module in backbone.named_children():\n",
    "    print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecb768-8d41-4656-a82c-a3e47452ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pvt_v2_b2\"\n",
    "\n",
    "backbone = timm.create_model(model_name, pretrained=True, features_only=True)\n",
    "\n",
    "print(\"-\"*60)\n",
    "name, module = list(backbone.named_children())[0]\n",
    "print(module)\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eee13-bad9-4d30-8228-892804cce8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "name, module = list(backbone.named_children())[1]\n",
    "print(module)\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d80e4b-4020-4764-b295-03b02647bd4c",
   "metadata": {},
   "source": [
    "✅ The “v2” series are improved versions with:\n",
    "\n",
    "* Linear complexity attention\n",
    "* Improved positional encoding\n",
    "* Better pretrained weights\n",
    "---\n",
    "\n",
    "#### **Classification Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cf2b9-beff-4d55-a97c-0240c41440b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# Create model\n",
    "model = timm.create_model('pvt_v2_b2', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Random input (B=1, C=3, H=224, W=224)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4de999-27ee-48bb-b422-43f5e262fb51",
   "metadata": {},
   "source": [
    "✅ This is ImageNet-1k classification output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Extract intermediate feature maps (for detection or segmentation)**\n",
    "\n",
    "If you want to use **PVT as a backbone**, not for classification, you can set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf5882-8876-4b04-ab4c-9d04c6a73aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a156571-47de-4aab-b6c2-62b315cde16f",
   "metadata": {},
   "source": [
    "Now it returns **pyramid feature maps** from multiple stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441777f-9fd8-4881-ae3b-ef35abc4f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    features = model(x)\n",
    "\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"Stage {i+1} feature:\", f.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12001f97-d0a1-4085-96d9-a55de7255e7b",
   "metadata": {},
   "source": [
    "✅ Exactly matches the hierarchical pyramid structure we discussed earlier.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Using as a backbone for downstream tasks**\n",
    "\n",
    "You can plug these outputs into:\n",
    "\n",
    "* **FPN / UPerNet** for segmentation\n",
    "* **RetinaNet / Mask R-CNN** for detection\n",
    "* **Custom encoder-decoder architectures (e.g., U-Net)**\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634027e-3ba2-45bc-a13e-943362321a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# PVT backbone\n",
    "backbone = timm.create_model('pvt_v2_b2', pretrained=True, features_only=True)\n",
    "\n",
    "# Example decoder\n",
    "class SimpleSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x[-1])  # use last stage feature\n",
    "\n",
    "model = SimpleSegmentationHead(512, num_classes=21)\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = backbone(x)\n",
    "\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"Stage {i+1}: {f.shape}\")\n",
    "\n",
    "out = model(features)\n",
    "print(\"Segmentation map:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df4636-d144-4f93-84c0-5f8813b1a1b1",
   "metadata": {},
   "source": [
    "#### `features_only=True`\n",
    "\n",
    "This changes the forward pass so that the model **returns the feature maps of each stage**, not the classification head.\n",
    "\n",
    "PVT-v2-B2 has **4 stages**:\n",
    "\n",
    "| Stage   | Resolution shrink | Output Channels |\n",
    "| ------- | ----------------- | --------------- |\n",
    "| Stage 1 | 4× ↓              | 64              |\n",
    "| Stage 2 | 8× ↓              | 128             |\n",
    "| Stage 3 | 16× ↓             | 320             |\n",
    "| Stage 4 | 32× ↓             | 512             |\n",
    "\n",
    "So for an input image of size\n",
    "$$H \\times W$$\n",
    "\n",
    "the outputs are:\n",
    "\n",
    "| Stage | Tensor shape                           |\n",
    "| ----- | -------------------------------------- |\n",
    "| p1    | $$B, 64, \\frac{H}{4}, \\frac{W}{4}$$    |\n",
    "| p2    | $$B, 128, \\frac{H}{8}, \\frac{W}{8}$$   |\n",
    "| p3    | $$B, 320, \\frac{H}{16}, \\frac{W}{16}$$ |\n",
    "| p4    | $$B, 512, \\frac{H}{32}, \\frac{W}{32}$$ |\n",
    "\n",
    "This is what you get from:\n",
    "\n",
    "```python\n",
    "features = backbone(x)\n",
    "```\n",
    "\n",
    "**`features` is a Python list:**\n",
    "\n",
    "```python\n",
    "features = [p1, p2, p3, p4]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Forward pass inside PVT-v2-B2 (conceptual)\n",
    "\n",
    "Each stage performs:\n",
    "\n",
    "#### (1) Patch embedding\n",
    "\n",
    "For stage 1:\n",
    "\n",
    "$$X_1 = \\text{Conv2d}(X_0)$$\n",
    "This converts RGB image to 64 channels and downsamples by 4×.\n",
    "\n",
    "For later stages:\n",
    "\n",
    "$$X_{i} = \\text{PatchMerge}(X_{i-1})$$\n",
    "Downsamples spatial resolution again.\n",
    "\n",
    "---\n",
    "\n",
    "#### (2) Spatial Reduction Attention (SRA)\n",
    "\n",
    "Each Transformer block inside PVT uses:\n",
    "\n",
    "$$\n",
    "Q = XW_Q, \\quad K = X_{\\text{reduced}}W_K, \\quad V = X_{\\text{reduced}}W_V\n",
    "$$\n",
    "\n",
    "where\n",
    "$$X_{\\text{reduced}} = \\text{Downsample}(X, r)$$\n",
    "\n",
    "with reduction ratios\n",
    "$$r = {8,4,2,1}$$\n",
    "for the four stages.\n",
    "\n",
    "The attention score:\n",
    "\n",
    "$$\n",
    "A = \\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "The output:\n",
    "\n",
    "$$\n",
    "Y = A V\n",
    "$$\n",
    "\n",
    "Then residuals + MLP, repeated.\n",
    "\n",
    "Each stage has multiple blocks.\n",
    "\n",
    "---\n",
    "\n",
    "#### So the final forward of the backbone returns:\n",
    "\n",
    "```python\n",
    "return [stage1_out, stage2_out, stage3_out, stage4_out]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Parameters of PVT-v2-B2\n",
    "\n",
    "#### Patch embeddings (conv layers)\n",
    "\n",
    "Each stage has a convolution that changes channels and reduces resolution.\n",
    "\n",
    "#### Transformer layers\n",
    "\n",
    "Every layer has:\n",
    "\n",
    "* $$W_Q, W_K, W_V \\in \\mathbb{R}^{C \\times C}$$\n",
    "* MLP weights\n",
    "* LayerNorm parameters\n",
    "* SRA reduction projections\n",
    "\n",
    "Total parameters ≈ **35M** (depending on version).\n",
    "\n",
    "---\n",
    "\n",
    "#### Your SimpleSegmentationHead\n",
    "\n",
    "```python\n",
    "class SimpleSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x[-1])\n",
    "```\n",
    "\n",
    "#### Forward logic\n",
    "\n",
    "You receive a feature list:\n",
    "\n",
    "```python\n",
    "[p1, p2, p3, p4]\n",
    "```\n",
    "\n",
    "`x[-1]` = `p4`\n",
    "\n",
    "Shape of `p4`:\n",
    "\n",
    "$$\n",
    "B, 512, \\frac{H}{32}, \\frac{W}{32}\n",
    "$$\n",
    "\n",
    "#### The segmentation head does:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{Conv2d}_{1\\times1}(p4)\n",
    "$$\n",
    "\n",
    "This is linear projection per pixel:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{b,c,i,j} = \\sum_{k=1}^{C_{in}} W_{c,k}  p4_{b,k,i,j} + b_c\n",
    "$$\n",
    "\n",
    "So you get logits:\n",
    "\n",
    "\n",
    "$$\n",
    "\\left( B, \\text{num\\_classes}, \\frac{H}{32}, \\frac{W}{32} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Parameters of the head:\n",
    "\n",
    "A 1×1 convolution has:\n",
    "\n",
    "$$\n",
    "\\text{Params} = C_{\\text{in}} \\cdot C_{\\text{out}} + C_{\\text{out}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $C_{\\text{in}} = 512$ (from PVT stage 4)\n",
    "* $C_{\\text{out}} = \\text{num\\_classes}$\n",
    "\n",
    "Example: for 21 classes (PASCAL VOC):\n",
    "\n",
    "$$\n",
    "512 \\times 21 + 21 = 10,773\n",
    "$$\n",
    "\n",
    "Very small.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Backbone output\n",
    "\n",
    "You get:\n",
    "\n",
    "```python\n",
    "[\n",
    " B, 64,  H/4,  W/4,\n",
    " B,128,  H/8,  W/8,\n",
    " B,320, H/16, W/16,\n",
    " B,512, H/32, W/32\n",
    "]\n",
    "```\n",
    "\n",
    "### Segmentation head\n",
    "\n",
    "Projects the **last** feature map to class logits using a 1×1 convolution.\n",
    "\n",
    "#### Result\n",
    "\n",
    "Low-resolution segmentation map at 1/32 spatial resolution.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c768c-cbc2-4ba3-ad37-03f4824c4ca3",
   "metadata": {},
   "source": [
    "#### Freeze all layers except the head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415af0cc-e374-4d3a-9c9e-8616f2924d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in pvt_model.named_parameters():\n",
    "    if 'head' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18c913-70ca-4c2d-b3be-2d7194237d6c",
   "metadata": {},
   "source": [
    "#### Trainable Parameter Count\n",
    "\n",
    "when you froze everything except the head, since only the final linear layer stays trainable, you end up with roughly `num_features × num_classes + num_classes` parameters (e.g. `256 * 10 + 10 = 2,570`). Total model size is ≈3.4 M, so `2,570 / 3,412,330 ≈ 0.08%` is absolutely expected. That means freezing worked correctly.\n",
    "\n",
    "If you need more layers to fine-tune, selectively unfreeze later stages (e.g., keep `stage3` and `stage4` trainable) by checking `name.startswith('stages.2')`.\n",
    "\n",
    "#### Dropout Before the Head\n",
    "\n",
    "PVT includes a `head_drop` module (Dropout) before the classifier head. When freezing the backbone and only training the head, you usually keep that dropout enabled—especially if data is limited—since it regularizes the linear head weights. Options:\n",
    "\n",
    "  1. **Keep default** (recommended): leave `pvt_model.head_drop` intact; it only runs during training.\n",
    "  2. **Reduce or disable** if you’re seeing underfitting: set `pvt_model.head_drop.p = 0` or wrap the head as `nn.Sequential(nn.Dropout(new_p), nn.Linear(...))`.\n",
    "  3. **Custom head**: replace `pvt_model.head = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(...))` to control dropout explicitly.\n",
    "\n",
    "Just remember that if you disable dropout, you should monitor validation metrics closely to ensure you’re not overfitting the small trainable portion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1a31e-1e62-4f9d-8a73-a4dafc021191",
   "metadata": {},
   "source": [
    "#### input size\n",
    "You can read the required input size straight from the model’s config instead of guessing. Every timm model exposes a default (or pretrained) configuration dictionary with fields like `input_size`, `mean`, `std`, etc.\n",
    "Add something like this right after you create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49409f5-8fc9-4e30-a6d6-fa61d5da86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvt_model = timm.create_model(\"pvt_v2_b0\", pretrained=True)\n",
    "cfg = pvt_model.pretrained_cfg  # (older timm versions call it default_cfg)\n",
    "print(\"Required input size:\", cfg[\"input_size\"])  # e.g., (3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc23992-8124-4972-95eb-cebca8a5664d",
   "metadata": {},
   "source": [
    "This gives you the exact `(channels, height, width)` expected by the pretrained weights. If you’re on a newer timm release, use the helper instead:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a300de8-8142-4910-88c3-0fb78d9e8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_model_data_config\n",
    "cfg = resolve_model_data_config(pvt_model)\n",
    "print(cfg[\"input_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33478873-aa89-4e74-8943-b00ce00ac369",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Where PVT Works the Best**\n",
    "\n",
    "\n",
    "Here are the clearest, most practical cases where **PVT (Pyramid Vision Transformer)** is one of the strongest choices — often *better than ViT/DeiT, better than Swin for certain setups, and much better than CNNs for multi-scale tasks*.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Dense Prediction Tasks with Many Scales**\n",
    "\n",
    "*(Semantic segmentation, instance segmentation, panoptic segmentation)*\n",
    "\n",
    "PVT was designed to **replace ResNet backbones** inside segmentation/detection heads like FPN, UPerNet, Mask2Former, Detectron2 models.\n",
    "\n",
    "### **Why PVT shines here**\n",
    "\n",
    "* It has a **true pyramid** (4 stages: 1/4, 1/8, 1/16, 1/32) just like CNNs.\n",
    "* Attention uses **spatial-reduction attention (SRA)** so it scales to high resolutions.\n",
    "* Therefore models like UPerNet, FPN, DeepLab work naturally with it.\n",
    "\n",
    "### **Typical best-case examples**\n",
    "\n",
    "* **Medical segmentation** (multi-scale lesions, polyps, tumors)\n",
    "* **Remote sensing segmentation** (buildings, roads, agricultural fields)\n",
    "* **Autonomous driving segmentation** (Cityscapes, ADE20K)\n",
    "* **Indoor scene parsing**\n",
    "* **Industrial defect segmentation** (scratches, cracks at multiple scales)\n",
    "\n",
    "### **Real benchmark evidence**\n",
    "\n",
    "PVT-v2-B2 + UPerNet **outperforms ResNet-50 + UPerNet** on ADE20K with fewer FLOPs.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Object Detection With Many Object Sizes**\n",
    "\n",
    "*(especially small + large objects in the same scene)*\n",
    "\n",
    "PVT is extremely strong when **object scale varies**.\n",
    "\n",
    "### **Why?**\n",
    "\n",
    "It provides:\n",
    "\n",
    "* Strong global modeling\n",
    "* Pyramid features\n",
    "* Works with RetinaNet, Faster R-CNN, Mask R-CNN directly\n",
    "\n",
    "### **Best-case domains**\n",
    "\n",
    "* **Drone detection** (small pedestrians, vehicles from above)\n",
    "* **Wildlife detection** (animals at multiple scales)\n",
    "* **COCO-like multi-object scenes**\n",
    "* **Industrial inspection** (tiny screws + large components)\n",
    "\n",
    "### **Example**\n",
    "\n",
    "RetinaNet + PVT-v2 **beats ResNet backbones of same complexity**.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Any Vision Task Where You Want Transformer Power but Need a Pyramid Backbone**\n",
    "\n",
    "*(i.e., transformer + CNN-like architecture)*\n",
    "\n",
    "ViT/DeiT are **single-scale**, giving only a 7×7 token map after patch embedding.\n",
    "PVT gives multi-scale features natively.\n",
    "\n",
    "### **Excellent fits**\n",
    "\n",
    "* **Feature extractors for SLAM, SfM, VIO**\n",
    "  (you want multi-scale features; PVT performs better than DeiT here)\n",
    "* **Depth estimation** (MDE)\n",
    "* **Optical flow**\n",
    "* **Super-resolution** (multi-scale helps)\n",
    "* **3D reconstruction / NeRF auxiliary encoders**\n",
    "\n",
    "This is why **MAE-style PVT encoders** have been explored for reconstruction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. When Memory Is Limited but Resolution Is High**\n",
    "\n",
    "*(e.g. 1024×1024+) ⇒ DeiT/ViT would break)*\n",
    "\n",
    "PVT’s **spatial reduction attention** reduces keys/values by a factor (like 4×, 8×).\n",
    "\n",
    "### **Meaning:**\n",
    "\n",
    "* High-resolution images stay feasible\n",
    "* Training on large GPUs becomes manageable\n",
    "* Still benefits from global attention\n",
    "\n",
    "### **Best-case roles**\n",
    "\n",
    "* **Microscopy images**\n",
    "* **Satellite SIGINT datasets**\n",
    "* **High-resolution industrial inspection**\n",
    "* **Medical scans** (pathology slides)\n",
    "\n",
    "In these cases, DeiT/ViT is unusable without strong downsampling.\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Applications That Need Both Global and Local Context**\n",
    "\n",
    "PVT mixes global context (transformers) with local processing (pyramid).\n",
    "This makes PVT ideal for tasks requiring:\n",
    "\n",
    "* fine-grained local detail\n",
    "* long-range interaction\n",
    "\n",
    "Examples:\n",
    "\n",
    "* **Agricultural plant disease detection** (field images + leaf closeups)\n",
    "* **Robotics scene understanding** (room-wide + small objects)\n",
    "* **Document understanding** (global layout + small text regions)\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary Table (Best Use Cases for PVT)**\n",
    "\n",
    "| Domain / Task                 | Why PVT Excels                              | Competes Against |\n",
    "| ----------------------------- | ------------------------------------------- | ---------------- |\n",
    "| Semantic Segmentation         | Multi-scale transformer features            | Swin, ResNet+FPN |\n",
    "| Panoptic Segmentation         | Strong global + local modeling              | Swin, ConvNeXt   |\n",
    "| Instance Segmentation         | Pyramid attention backbone                  | ResNet+FPN       |\n",
    "| Object Detection              | Good at small+large objects in same frame   | Swin, ResNet     |\n",
    "| Medical Imaging               | Handles large resolutions cheaply           | ViT, DeiT        |\n",
    "| Remote Sensing                | Long-range transformer context              | ViT, Swin        |\n",
    "| SLAM / SfM feature extraction | Multi-scale + attention                     | ResNet, DeiT     |\n",
    "| Industrial Defect Detection   | Requires high-res + local detail            | EfficientNet     |\n",
    "| Autonomous Driving            | Multi-scale segmentation/detection backbone | ConvNeXt         |\n",
    "\n",
    "---\n",
    "\n",
    "# **When NOT to Use PVT**\n",
    "\n",
    "PVT is **not** ideal when:\n",
    "\n",
    "* Training data is **very small** (< 1000 samples)\n",
    "* You need extreme speed on mobile devices (CNNs win)\n",
    "* A pure global attention model like ViT is enough for classification\n",
    "* Strong inductive bias is desired (Swin may do better)\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also provide:\n",
    "\n",
    "✅ a **comparison table: PVT vs DeiT vs ViT vs Swin**\n",
    "✅ recommended **PVT variant (B0–B5) for each dataset/task**\n",
    "✅ minimal code for **PVT-v2 segmentation, detection, or classification**\n",
    "\n",
    "Just tell me which one you want.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
