{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eee36d0-c26f-49f8-8a19-aa06a2742b7c",
   "metadata": {},
   "source": [
    "# **DeiT (Data-Efficient Image Transformer)**\n",
    "\n",
    "**Paper:** *Training data-efficient image transformers & distillation through attention* (ICML 2021, Facebook Research, 2020)\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "**Vision Transformer (ViT)** achieved strong results but required **huge datasets** (ImageNet-21k, JFT-300M) and **massive compute**.\n",
    "**DeiT** made ViT trainable **on ImageNet-1k (1.3 M images)** — *no extra data* — through **data-efficient augmentation** and **distillation through attention**.\n",
    "\n",
    "---\n",
    "\n",
    "## ViT Recap\n",
    "\n",
    "1. **Patch embedding**\n",
    "\n",
    "   Split an image of size $H\\times W\\times C$ into patches $P\\times P$; number of patches:\n",
    "   $$\n",
    "   N=\\frac{H\\times W}{P^2}\n",
    "   $$\n",
    "   Each patch → vector → $\\mathbf X_p\\in\\mathbb R^{N\\times D}$.\n",
    "\n",
    "2. **Add tokens and positional embeddings**\n",
    "   $$\n",
    "   Z_0=[x_{\\text{cls}},x_p^1,\\dots,x_p^N]+E_{\\text{pos}}\n",
    "   $$\n",
    "\n",
    "3. **Transformer encoder**\n",
    "\n",
    "   $L$ blocks of MSA + FFN + residuals + layer norm.\n",
    "\n",
    "4. **Classification head** uses final `[CLS]`.\n",
    "\n",
    "---\n",
    "\n",
    "## DeiT Architecture\n",
    "\n",
    "Adds one more token — **[DIST]**:\n",
    "\n",
    "$$\n",
    "Z_0=[x_{\\text{cls}},x_{\\text{dist}},x_p^1,\\dots,x_p^N]+E_{\\text{pos}}\n",
    "$$\n",
    "\n",
    "Both `[CLS]` and `[DIST]` participate equally in attention through all layers.\n",
    "\n",
    "### Output heads\n",
    "\n",
    "After the final block:\n",
    "$$\n",
    "Z_L=[z_{\\text{cls}},z_{\\text{dist}},z_p^1,\\dots,z_p^N]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d66ab4-9c7f-47da-8c81-6148ef9ea43c",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/deit.png\" height=\"30%\" width=\"30%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a5b6f-51f6-4297-8bf3-903f2ba9f1d4",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DeiT}} = (1-\\lambda)\\mathcal{L}_{\\text{CE}}\\big(\\sigma(Z_{\\text{cls}}), y\\big) + \\lambda\\tau^2\\mathrm{KL}\\left( \\sigma\\left(\\frac{Z_{\\text{dist}}}{\\tau}\\right), \\sigma\\left(\\frac{Z_t}{\\tau}\\right) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Symbol               | Meaning                                                         |\n",
    "| :------------------- | :-------------------------------------------------------------- |\n",
    "| $ Z_{\\text{cls}} $   | logits from the **student’s `[CLS]` head**.                     |\n",
    "| $ Z_{\\text{dist}} $  | logits from the **student’s `[DIST]` head**.                    |\n",
    "| $ Z_t $              | Teacher logits (from pretrained CNN like RegNetY)               |\n",
    "| $ y $                | Ground-truth class label                                        |\n",
    "| $ \\sigma(\\cdot) $    | Softmax function                                                |\n",
    "| $ \\mathcal{L}_{CE} $ | Cross-entropy loss with ground truth                            |\n",
    "| $ KL(\\cdot,\\cdot) $  | Kullback–Leibler divergence (between probability distributions) |\n",
    "| $ \\tau $             | **Temperature** to soften the logits                            |\n",
    "| $ \\lambda $          | Balancing factor between supervised and distillation losses     |\n",
    "\n",
    "\n",
    "So overall DeiT loss becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DeiT}} =\n",
    "\\mathcal{L}_{CE}^{[\\text{CLS}]} +\n",
    "\\mathcal{L}_{\\text{student}}^{[\\text{DIST}]}\n",
    "$$\n",
    "\n",
    "---\n",
    "#### Intuitive Meaning\n",
    "\n",
    "The student (DeiT) is trained to satisfy **two goals simultaneously**:\n",
    "\n",
    "1. **Match the true labels**\n",
    "   → via the standard cross-entropy term\n",
    "\n",
    "   $$\n",
    "   (1-\\lambda)\\mathcal{L}_{\\text{CE}}\\big(\\sigma(Z_{\\text{cls}}), y\\big)\n",
    "   $$\n",
    "\n",
    "3. **Mimic the teacher’s “dark knowledge”** (soft class probabilities)\n",
    "   → via the KL divergence term\n",
    "   $$\n",
    "   \\lambda\\tau^2\\mathrm{KL}\\left( \\sigma\\left(\\frac{Z_{\\text{dist}}}{\\tau}\\right), \\sigma\\left(\\frac{Z_t}{\\tau}\\right) \\right)\n",
    "   $$\n",
    "\n",
    "The **teacher’s predictions** (even for incorrect classes) contain valuable information about class similarity — e.g., a cat image might get 0.7 cat, 0.2 dog, 0.1 fox.\n",
    "These *soft targets* help the student generalize better than one-hot labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Role of **Temperature $ \\tau $**\n",
    "\n",
    "* When $ \\tau > 1 $, the softmax becomes **softer** — it spreads probability mass across classes.\n",
    "* This reveals **relative similarities** between classes.\n",
    "\n",
    "$$\n",
    "\\sigma_i(Z / \\tau) = \\frac{e^{Z_i / \\tau}}{\\sum_j e^{Z_j / \\tau}}\n",
    "$$\n",
    "\n",
    "Typical values: $ \\tau \\in [2, 5] $\n",
    "\n",
    "During training:\n",
    "\n",
    "* Compute both soft teacher and soft student distributions at temperature $ \\tau $.\n",
    "* Multiply the KL term by $ \\tau^2 $ (to keep gradient magnitudes consistent).\n",
    "\n",
    "---\n",
    "\n",
    "#### Role of **Balancing factor $ \\lambda $**\n",
    "\n",
    "Controls the tradeoff between:\n",
    "\n",
    "* Fitting to **ground truth** (hard labels)\n",
    "* Mimicking the **teacher** (soft labels)\n",
    "\n",
    "Typical choice: $ \\lambda = 0.5 $\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### PyTorch Implementation Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    \"\"\"DeiT-style KD loss\"\"\"\n",
    "    # Hard-label loss\n",
    "    ce_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Soft-label loss (teacher guidance)\n",
    "    p_s = F.log_softmax(student_logits / T, dim=1)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    kd_loss = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "\n",
    "    # Weighted sum\n",
    "    return (1 - alpha) * ce_loss + alpha * kd_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b72e5-bcf6-48a7-9d90-6c7eabded308",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Use either\n",
    "$$\n",
    "\\hat y=p_{\\text{cls}}\\quad\\text{or}\\quad\n",
    "\\hat y=\\tfrac12(p_{\\text{cls}}+p_{\\text{dist}})\n",
    "$$\n",
    "Averaging usually gives slightly higher accuracy.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Textual Architecture Diagram\n",
    "\n",
    "```\n",
    "Image (224×224×3)\n",
    "   ↓\n",
    "Patch split (16×16) → 196 patches\n",
    "   ↓\n",
    "Linear projection → Patch embeddings (196×D)\n",
    "   ↓\n",
    "Add [CLS] and [DIST] tokens\n",
    "   ↓\n",
    "Add positional embeddings\n",
    "   ↓\n",
    "Transformer encoder (12 layers)\n",
    "   ↓\n",
    "Outputs:\n",
    "   [CLS] → Classification head → CE loss\n",
    "   [DIST] → Distillation head → KD loss\n",
    "   ↓\n",
    "Combined loss → update weights\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ecfe2-c965-455c-bffb-3d9e00701241",
   "metadata": {},
   "source": [
    "## **Choosing Architectures For Teacher and Student**\n",
    "\n",
    "\n",
    "DeiT’s teacher–student setup is designed as follows:\n",
    "\n",
    "| Role        | Model Type                   | Example Architecture         | Training                                     |\n",
    "| ----------- | ---------------------------- | ---------------------------- | -------------------------------------------- |\n",
    "| **Teacher** | **CNN**                      | **RegNetY-16GF**             | Pretrained on ImageNet-1k                    |\n",
    "| **Student** | **Vision Transformer (ViT)** | **DeiT-Tiny / Small / Base** | Trained from scratch on ImageNet-1k using KD |\n",
    "\n",
    "So DeiT uses a **CNN teacher** and a **Transformer student**.\n",
    "\n",
    "That’s important because it transfers **CNN inductive biases** (local spatial patterns, hierarchical representations) into the ViT-like student, which otherwise lacks them.\n",
    "\n",
    "---\n",
    "\n",
    "#### Teacher Architecture — **RegNetY**\n",
    "\n",
    "**RegNetY** (from Facebook AI, 2020) is a family of efficient convolutional networks.\n",
    "\n",
    "**Why RegNetY?**\n",
    "\n",
    "* Stable training on ImageNet.\n",
    "* Scalable family of models (RegNetY-4GF, 8GF, 16GF, etc.).\n",
    "* Excellent accuracy–efficiency tradeoff.\n",
    "* Provides good *soft targets* (probability distributions) for distillation.\n",
    "\n",
    "**Teacher setup in DeiT paper:**\n",
    "\n",
    "* RegNetY-16GF trained on ImageNet-1k.\n",
    "* ImageNet-1k: involved `1,000` classes, contains `1,281,167` training images, `50,000` validation images and `100,000` test images\n",
    "* Used only to produce logits $ Z_t $ for distillation — not updated during DeiT training.\n",
    "\n",
    "---\n",
    "\n",
    "#### Student Architecture — **Vision Transformer (ViT-like)**\n",
    "\n",
    "DeiT’s student is **exactly a ViT**, with minimal modifications:\n",
    "\n",
    "Common structure:\n",
    "\n",
    "| Stage                   | Description                                                 | DeiT implementation |\n",
    "| ----------------------- | ----------------------------------------------------------- | ------------------- |\n",
    "| **Patch embedding**     | Split image into $16 \\times 16$ patches → linear projection | same as ViT         |\n",
    "| **Tokens**              | Add `[CLS]` + `[DIST]` + patch tokens                       | DeiT modification   |\n",
    "| **Positional encoding** | Learnable 1D positional embedding                           | same as ViT         |\n",
    "| **Encoder**             | 12 Transformer blocks (MHSA + MLP + LayerNorm)              | same as ViT         |\n",
    "| **Heads**               | Two linear heads: classification and distillation           | DeiT modification   |\n",
    "\n",
    "---\n",
    "\n",
    "#### DeiT Model Variants\n",
    "\n",
    "| Model      | Layers | Hidden dim (D) | MLP dim | Heads | Params | Patch size |\n",
    "| ---------- | ------ | -------------- | ------- | ----- | ------ | ---------- |\n",
    "| DeiT-Tiny  | 12     | 192            | 768     | 3     | 5M     | 16         |\n",
    "| DeiT-Small | 12     | 384            | 1536    | 6     | 22M    | 16         |\n",
    "| DeiT-Base  | 12     | 768            | 3072    | 12    | 86M    | 16         |\n",
    "\n",
    "All trained **from scratch** on ImageNet-1k.\n",
    "\n",
    "---\n",
    "\n",
    "#### Teacher–Student Interaction Diagram\n",
    "\n",
    "```\n",
    "             ┌─────────────────────────┐\n",
    "             │   Teacher: RegNetY-16GF │\n",
    "             └──────────┬──────────────┘\n",
    "                        │\n",
    "                 soft targets (Z_t)\n",
    "                        │\n",
    "────────────────────────────────────────────\n",
    "                        │\n",
    "             ┌─────────────────────────────┐\n",
    "             │  Student: DeiT Transformer  │\n",
    "             │  (with [CLS] and [DIST])    │\n",
    "             └─────────────────────────────┘\n",
    "                        │\n",
    "           ┌────────────┴────────────┐\n",
    "           │                         │\n",
    "   [CLS] Head (CrossEntropy)   [DIST] Head (KD Loss with Z_t)\n",
    "           │                         │\n",
    "           └────────────┬────────────┘\n",
    "                        ↓\n",
    "                 Total Loss = CE + KD\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Combination Works\n",
    "\n",
    "| Aspect                          | CNN (Teacher)                                    | Transformer (Student)                   |\n",
    "| ------------------------------- | ------------------------------------------------ | --------------------------------------- |\n",
    "| **Inductive bias**              | Strong (local filters, translation equivariance) | Weak (global attention)                 |\n",
    "| **Data efficiency**             | High                                             | Low                                     |\n",
    "| **Training stability**          | Very stable                                      | Needs large data                        |\n",
    "| **Knowledge distillation role** | Provides local, structured soft targets          | Learns global features more efficiently |\n",
    "\n",
    "So the CNN acts as a **structural prior** for the Transformer, teaching it to focus on *semantically meaningful local regions* even with limited data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7268a4-ffb0-4fce-a777-85f4b01984fd",
   "metadata": {},
   "source": [
    "#### Load Teacher (CNN) and Student (DeiT)\n",
    "\n",
    "Teacher: pretrained CNN (frozen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "teacher = create_model('regnety_160.pycls_in1k', pretrained=True)\n",
    "```\n",
    "\n",
    "Alternative Models:\n",
    "\n",
    "If you want different capacity levels, you can use:\n",
    "\n",
    "| Model Name | FLOPs | Parameters |\n",
    "|------------|-------|------------|\n",
    "| `regnety_016.pycls_in1k` | 1.6 GF | ~11M |\n",
    "| `regnety_032.pycls_in1k` | 3.2 GF | ~20M |\n",
    "| `regnety_080.pycls_in1k` | 8.0 GF | ~39M |\n",
    "| `regnety_160.pycls_in1k` | **16 GF** | **84M** ← **You're using this** |\n",
    "| `regnety_320.pycls_in1k` | 32 GF | 145M |\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# Student: Vision Transformer with [CLS] + [DIST] tokens\n",
    "student = create_model(\n",
    "    'deit_base_distilled_patch16_224.fb_in1k', pretrained=False)\n",
    "# Enable distillation mode to return (cls_logits, dist_logits)\n",
    "student.distilled_training = True\n",
    "student.train()  # Must be in training mode to get both cls and dist logits\n",
    "\n",
    "# Example input (batch of 4 images, 3×224×224)\n",
    "x = torch.randn(4, 3, 224, 224)\n",
    "# random ground-truth labels, since we have 1k classes in ImageNet, classes are between 0-1000 and the shape should be (4,)\n",
    "y = torch.randint(0, 1000, (4,))\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2. Forward Pass\n",
    "# --------------------------------------------\n",
    "with torch.no_grad():\n",
    "    teacher_logits = teacher(x)   # [B, num_classes]\n",
    "\n",
    "print(teacher_logits)\n",
    "\n",
    "# Student forward:\n",
    "# timm's DeiT returns:\n",
    "#   - if distilled: a tuple (cls_logits, dist_logits)\n",
    "#   - if not distilled: a single tensor\n",
    "out = student(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ca4d2-dd38-47ba-b8bf-383f90f5bf1c",
   "metadata": {},
   "source": [
    "This line is a **fallback mechanism** to handle both distilled and non-distilled models with the same code. \n",
    "\n",
    "```python\n",
    "    cls_logits = dist_logits = out  # fallback (non-distilled)\n",
    "```\n",
    "\n",
    "#### Why This Line Exists?\n",
    "\n",
    "DeiT Model Outputs:\n",
    "\n",
    "**Distilled DeiT models** (like `deit_base_distilled_patch16_224`) have two special tokens:\n",
    "- **[CLS] token**: Standard classification token → produces `cls_logits`\n",
    "- **[DIST] token**: Distillation token → produces `dist_logits`\n",
    "\n",
    "These models return a **tuple**: `(cls_logits, dist_logits)`\n",
    "\n",
    "**Non-distilled models** (like regular `deit_base_patch16_224`) only have:\n",
    "- **[CLS] token**: Standard classification token\n",
    "\n",
    "These models return a **single tensor**\n",
    "\n",
    "**The Problem**\n",
    "\n",
    "The loss function on line 66 expects BOTH arguments:\n",
    "\n",
    "```python\n",
    "loss = deit_distillation_loss(cls_logits, dist_logits, teacher_logits, y)\n",
    "```\n",
    "\n",
    "**The Solution**\n",
    "\n",
    "The code handles both cases:\n",
    "\n",
    "```python\n",
    "if isinstance(out, tuple):\n",
    "    cls_logits, dist_logits = out\n",
    "else:\n",
    "    cls_logits = dist_logits = out  # fallback (non-distilled)\n",
    "```\n",
    "\n",
    "so:\n",
    "- `cls_logits` = the model's output\n",
    "- `dist_logits` = the model's output (same reference)\n",
    "\n",
    "This way, if you accidentally use a non-distilled model, the code still works. Both losses (CE loss and KD loss) will be computed using the same logits.\n",
    "\n",
    "```python\n",
    "if isinstance(out, tuple):\n",
    "    print(\"distilled\")\n",
    "    cls_logits, dist_logits = out\n",
    "else:\n",
    "    print(\"non-distilled\")\n",
    "    cls_logits = dist_logits = out  # fallback (non-distilled)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa281509-5ffc-4266-bd1f-817aab8ab2c3",
   "metadata": {},
   "source": [
    "```python\n",
    "# --------------------------------------------\n",
    "# 3. Define Distillation Loss\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "def deit_distillation_loss(cls_logits, dist_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    DeiT-style distillation loss\n",
    "    \"\"\"\n",
    "    # Cross-entropy with ground truth (for [CLS] token)\n",
    "    ce_loss = F.cross_entropy(cls_logits, labels)\n",
    "\n",
    "    # KL divergence with teacher soft targets (for [DIST] token)\n",
    "    p_s = F.log_softmax(dist_logits / T, dim=1)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    kd_loss = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "\n",
    "    # Weighted combination\n",
    "    return (1 - alpha) * ce_loss + alpha * kd_loss\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4. Compute total loss\n",
    "# --------------------------------------------\n",
    "loss = deit_distillation_loss(cls_logits, dist_logits, teacher_logits, y)\n",
    "print(f\"Total training loss: {loss.item():.4f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. Backpropagation\n",
    "# --------------------------------------------\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
