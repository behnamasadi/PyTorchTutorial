{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eee36d0-c26f-49f8-8a19-aa06a2742b7c",
   "metadata": {},
   "source": [
    "# **DeiT (Data-Efficient Image Transformer)**\n",
    "\n",
    "**Paper:** *Training data-efficient image transformers & distillation through attention* (ICML 2021, Facebook Research, 2020)\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "**Vision Transformer (ViT)** achieved strong results but required **huge datasets** (ImageNet-21k, JFT-300M) and **massive compute**.\n",
    "**DeiT** made ViT trainable **on ImageNet-1k (1.3 M images)** — *no extra data* — through **data-efficient augmentation** and **distillation through attention**.\n",
    "\n",
    "---\n",
    "\n",
    "## ViT Recap\n",
    "\n",
    "1. **Patch embedding**\n",
    "\n",
    "   Split an image of size $H\\times W\\times C$ into patches $P\\times P$; number of patches:\n",
    "   $$\n",
    "   N=\\frac{H\\times W}{P^2}\n",
    "   $$\n",
    "   Each patch → vector → $\\mathbf X_p\\in\\mathbb R^{N\\times D}$.\n",
    "\n",
    "2. **Add tokens and positional embeddings**\n",
    "   $$\n",
    "   Z_0=[x_{\\text{cls}},x_p^1,\\dots,x_p^N]+E_{\\text{pos}}\n",
    "   $$\n",
    "\n",
    "3. **Transformer encoder**\n",
    "\n",
    "   $L$ blocks of MSA + FFN + residuals + layer norm.\n",
    "\n",
    "4. **Classification head** uses final `[CLS]`.\n",
    "\n",
    "---\n",
    "\n",
    "## DeiT Architecture\n",
    "\n",
    "Adds one more token — **[DIST]**:\n",
    "\n",
    "$$\n",
    "Z_0=[x_{\\text{cls}},x_{\\text{dist}},x_p^1,\\dots,x_p^N]+E_{\\text{pos}}\n",
    "$$\n",
    "\n",
    "Both `[CLS]` and `[DIST]` participate equally in attention through all layers.\n",
    "\n",
    "### Output heads\n",
    "\n",
    "After the final block:\n",
    "$$\n",
    "Z_L=[z_{\\text{cls}},z_{\\text{dist}},z_p^1,\\dots,z_p^N]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d66ab4-9c7f-47da-8c81-6148ef9ea43c",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/deit.png\" height=\"30%\" width=\"30%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a5b6f-51f6-4297-8bf3-903f2ba9f1d4",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DeiT}} = (1-\\lambda)\\mathcal{L}_{\\text{CE}}\\big(\\sigma(Z_{\\text{cls}}), y\\big) + \\lambda\\tau^2\\mathrm{KL}\\left( \\sigma\\left(\\frac{Z_{\\text{dist}}}{\\tau}\\right), \\sigma\\left(\\frac{Z_t}{\\tau}\\right) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Symbol               | Meaning                                                         |\n",
    "| :------------------- | :-------------------------------------------------------------- |\n",
    "| $ Z_{\\text{cls}} $   | logits from the **student’s `[CLS]` head**.                     |\n",
    "| $ Z_{\\text{dist}} $  | logits from the **student’s `[DIST]` head**.                    |\n",
    "| $ Z_t $              | Teacher logits (from pretrained CNN like RegNetY)               |\n",
    "| $ y $                | Ground-truth class label                                        |\n",
    "| $ \\sigma(\\cdot) $    | Softmax function                                                |\n",
    "| $ \\mathcal{L}_{CE} $ | Cross-entropy loss with ground truth                            |\n",
    "| $ KL(\\cdot,\\cdot) $  | Kullback–Leibler divergence (between probability distributions) |\n",
    "| $ \\tau $             | **Temperature** to soften the logits                            |\n",
    "| $ \\lambda $          | Balancing factor between supervised and distillation losses     |\n",
    "\n",
    "\n",
    "So overall DeiT loss becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DeiT}} =\n",
    "\\mathcal{L}_{CE}^{[\\text{CLS}]} +\n",
    "\\mathcal{L}_{\\text{student}}^{[\\text{DIST}]}\n",
    "$$\n",
    "\n",
    "---\n",
    "#### Intuitive Meaning\n",
    "\n",
    "The student (DeiT) is trained to satisfy **two goals simultaneously**:\n",
    "\n",
    "1. **Match the true labels**\n",
    "   → via the standard cross-entropy term\n",
    "\n",
    "   $$\n",
    "   (1-\\lambda)\\mathcal{L}_{\\text{CE}}\\big(\\sigma(Z_{\\text{cls}}), y\\big)\n",
    "   $$\n",
    "\n",
    "3. **Mimic the teacher’s “dark knowledge”** (soft class probabilities)\n",
    "   → via the KL divergence term\n",
    "   $$\n",
    "   \\lambda\\tau^2\\mathrm{KL}\\left( \\sigma\\left(\\frac{Z_{\\text{dist}}}{\\tau}\\right), \\sigma\\left(\\frac{Z_t}{\\tau}\\right) \\right)\n",
    "   $$\n",
    "\n",
    "The **teacher’s predictions** (even for incorrect classes) contain valuable information about class similarity — e.g., a cat image might get 0.7 cat, 0.2 dog, 0.1 fox.\n",
    "These *soft targets* help the student generalize better than one-hot labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Role of **Temperature $ \\tau $**\n",
    "\n",
    "* When $ \\tau > 1 $, the softmax becomes **softer** — it spreads probability mass across classes.\n",
    "* This reveals **relative similarities** between classes.\n",
    "\n",
    "$$\n",
    "\\sigma_i(Z / \\tau) = \\frac{e^{Z_i / \\tau}}{\\sum_j e^{Z_j / \\tau}}\n",
    "$$\n",
    "\n",
    "Typical values: $ \\tau \\in [2, 5] $\n",
    "\n",
    "During training:\n",
    "\n",
    "* Compute both soft teacher and soft student distributions at temperature $ \\tau $.\n",
    "* Multiply the KL term by $ \\tau^2 $ (to keep gradient magnitudes consistent).\n",
    "\n",
    "---\n",
    "\n",
    "#### Role of **Balancing factor $ \\lambda $**\n",
    "\n",
    "Controls the tradeoff between:\n",
    "\n",
    "* Fitting to **ground truth** (hard labels)\n",
    "* Mimicking the **teacher** (soft labels)\n",
    "\n",
    "Typical choice: $ \\lambda = 0.5 $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8fd199-ffe4-49f6-9504-a9b0118d33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit3_base_patch16_224\n",
      "deit3_base_patch16_384\n",
      "deit3_huge_patch14_224\n",
      "deit3_large_patch16_224\n",
      "deit3_large_patch16_384\n",
      "deit3_medium_patch16_224\n",
      "deit3_small_patch16_224\n",
      "deit3_small_patch16_384\n",
      "deit_base_distilled_patch16_224\n",
      "deit_base_distilled_patch16_384\n",
      "deit_base_patch16_224\n",
      "deit_base_patch16_384\n",
      "deit_small_distilled_patch16_224\n",
      "deit_small_patch16_224\n",
      "deit_tiny_distilled_patch16_224\n",
      "deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "# isort: skip_file\n",
    "# DO NOT reorganize imports - warnings filter must be FIRST!\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import timm\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "all_deit = timm.list_models(\"*deit*\")\n",
    "for m in all_deit:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6680f-c90a-456c-ae3f-0412d2d1023c",
   "metadata": {},
   "source": [
    "## **Student/Teacher Architectures For Every DeiT and DeiT-III** \n",
    "\n",
    "---\n",
    "\n",
    "# 1. Original DeiT (2020)\n",
    "\n",
    "Paper: **Training data-efficient image transformers & distillation through attention**\n",
    "\n",
    "DeiT introduced two things:\n",
    "\n",
    "1. A **distillation token**\n",
    "2. A **CNN teacher network** (RegNetY-16GF)\n",
    "\n",
    "Only **distilled variants** have a teacher.\n",
    "Non-distilled variants are trained normally.\n",
    "\n",
    "---\n",
    "\n",
    "# Complete mapping for original DeiT\n",
    "\n",
    "| Model name                           | Student architecture           | Teacher used? | Teacher architecture |\n",
    "| ------------------------------------ | ------------------------------ | ------------- | -------------------- |\n",
    "| **deit_tiny_patch16_224**            | ViT-Tiny                       | No            | None                 |\n",
    "| **deit_tiny_distilled_patch16_224**  | ViT-Tiny + distillation token  | Yes           | **RegNetY-16GF**     |\n",
    "| **deit_small_patch16_224**           | ViT-Small                      | No            | None                 |\n",
    "| **deit_small_distilled_patch16_224** | ViT-Small + distillation token | Yes           | **RegNetY-16GF**     |\n",
    "| **deit_base_patch16_224**            | ViT-Base                       | No            | None                 |\n",
    "| **deit_base_patch16_384**            | ViT-Base                       | No            | None                 |\n",
    "| **deit_base_distilled_patch16_224**  | ViT-Base + distillation token  | Yes           | **RegNetY-16GF**     |\n",
    "| **deit_base_distilled_patch16_384**  | ViT-Base + distillation token  | Yes           | **RegNetY-16GF**     |\n",
    "\n",
    "**Important:** In DeiT, all *distilled* models use the **same teacher**:\n",
    "$$\n",
    "\\text{Teacher} = \\text{RegNetY-16GF}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. DeiT-III (2022)\n",
    "\n",
    "Paper: **DeiT III: Revenge of the ViT**\n",
    "\n",
    "DeiT-III completely removes:\n",
    "\n",
    "* teacher network\n",
    "* distillation token\n",
    "\n",
    "They only use:\n",
    "\n",
    "* stronger data augmentation\n",
    "* regularization\n",
    "* training for more epochs\n",
    "* better optimization settings\n",
    "\n",
    "Therefore:\n",
    "\n",
    "### **All DeiT-III models have NO teacher.**\n",
    "\n",
    "---\n",
    "\n",
    "# Complete mapping for DeiT-III\n",
    "\n",
    "| Model name                   | Student architecture         | Teacher? | Teacher architecture |\n",
    "| ---------------------------- | ---------------------------- | -------- | -------------------- |\n",
    "| **deit3_tiny_patch16_224**   | ViT-Tiny (improved training) | No       | None                 |\n",
    "| **deit3_small_patch16_224**  | ViT-Small                    | No       | None                 |\n",
    "| **deit3_medium_patch16_224** | ViT-Medium                   | No       | None                 |\n",
    "| **deit3_base_patch16_224**   | ViT-Base                     | No       | None                 |\n",
    "| **deit3_base_patch16_384**   | ViT-Base                     | No       | None                 |\n",
    "| **deit3_large_patch16_224**  | ViT-Large                    | No       | None                 |\n",
    "| **deit3_large_patch16_384**  | ViT-Large                    | No       | None                 |\n",
    "| **deit3_huge_patch14_224**   | ViT-Huge (patch 14)          | No       | None                 |\n",
    "\n",
    "---\n",
    "\n",
    "# Summary (short answer)\n",
    "\n",
    "### Original DeiT (2020)\n",
    "\n",
    "* Distilled variants use the teacher **RegNetY-16GF**.\n",
    "* Non-distilled variants have **no teacher**.\n",
    "\n",
    "### DeiT-III (2022)\n",
    "\n",
    "* **No teacher at all.**\n",
    "* Entire family is trained without distillation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac4e8e-544e-4d72-8259-e2dbcf452437",
   "metadata": {},
   "source": [
    "# Widely used DeiT models (ranked)\n",
    "\n",
    "The community usage depends on the **task**, **dataset size**, and **training budget**, but overall there is a clear pattern.\n",
    "\n",
    "Below is the **real-world ranking** of DeiT and DeiT-III models, from **most used → least used**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Most commonly used (top tier)\n",
    "\n",
    "### **1. deit_base_patch16_224**\n",
    "\n",
    "This is by far the most widely used DeiT variant.\n",
    "\n",
    "Reason:\n",
    "\n",
    "* It is the **original DeiT** baseline used in papers and experiments.\n",
    "* Works well on medium-sized datasets.\n",
    "* Does not need a teacher (no distillation token).\n",
    "* Good speed and accuracy.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Classification (ImageNet fine-tuning)\n",
    "* Transfer learning on medical images, plant disease, remote sensing\n",
    "* Research baselines for Vision Transformers\n",
    "\n",
    "---\n",
    "\n",
    "### **2. deit_base_distilled_patch16_224**\n",
    "\n",
    "Also extremely popular.\n",
    "\n",
    "Reason:\n",
    "\n",
    "* This is the **teacher-distilled version** from the original DeiT paper.\n",
    "* Best accuracy for the base model.\n",
    "* Still efficient.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Classification where you want a stronger pretrained backbone\n",
    "* As a backbone for segmentation/detection (sometimes)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. deit_small_patch16_224**\n",
    "\n",
    "Very widely used in datasets < 50k images.\n",
    "\n",
    "Reason:\n",
    "\n",
    "* Lower compute\n",
    "* Still significantly better than CNNs of similar size\n",
    "* Ideal for small dataset fine-tuning (medical, industrial)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. deit_tiny_patch16_224**\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Dataset is very small\n",
    "* Training resources are very limited\n",
    "* Need a fast model on CPU or edge device\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Moderately used (middle tier)\n",
    "\n",
    "### **5. deit3_base_patch16_224**\n",
    "\n",
    "DeiT-III is newer, stronger, and used increasingly often.\n",
    "\n",
    "Reasons:\n",
    "\n",
    "* No teacher required\n",
    "* More stable training\n",
    "* Better regularization and data augmentation strategy\n",
    "\n",
    "But:\n",
    "\n",
    "* Still less common than the original DeiT-base because many repos did not update their baselines.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. deit3_small_patch16_224**\n",
    "\n",
    "Common in smaller training setups, good for:\n",
    "\n",
    "* Kaggle\n",
    "* Prototyping\n",
    "* Edge usage\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Rarely used (low tier)\n",
    "\n",
    "### **7. deit3_large_patch16_224**\n",
    "\n",
    "### **8. deit3_large_patch16_384**\n",
    "\n",
    "### **9. deit3_huge_patch14_224**\n",
    "\n",
    "These are rarely used because:\n",
    "\n",
    "* Very expensive to train\n",
    "* Need heavy regularization\n",
    "* Require advanced data pipelines\n",
    "* Compete with better alternatives like ViT-Large, Swin-Large, or PVT-Large\n",
    "\n",
    "Only seen in:\n",
    "\n",
    "* Large research labs\n",
    "* Benchmark papers\n",
    "* OpenAI-style pretraining setups\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Very rare or rarely needed\n",
    "\n",
    "### **10. 384-resolution versions**\n",
    "\n",
    "* deit_base_patch16_384\n",
    "* deit_base_distilled_patch16_384\n",
    "* deit3_base_patch16_384\n",
    "* deit3_large_patch16_384\n",
    "* etc.\n",
    "\n",
    "These are used only for:\n",
    "\n",
    "* High-accuracy ImageNet validation\n",
    "* Vision benchmarks\n",
    "\n",
    "But almost never in real projects because:\n",
    "\n",
    "* Quadrupled compute and memory\n",
    "* Minor accuracy gain\n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "| Model                               | Community usage   | Why                                    |\n",
    "| ----------------------------------- | ----------------- | -------------------------------------- |\n",
    "| **deit_base_patch16_224**           | Widely used (top) | Standard DeiT baseline, stable, strong |\n",
    "| **deit_base_distilled_patch16_224** | Widely used (top) | Highest accuracy among base DeiT       |\n",
    "| **deit_small_patch16_224**          | Widely used       | Good for small datasets                |\n",
    "| **deit_tiny_patch16_224**           | Common            | Fast, efficient                        |\n",
    "| **deit3_base_patch16_224**          | Moderately used   | Improved training, newer               |\n",
    "| **deit3_small_patch16_224**         | Moderately used   | Efficient                              |\n",
    "| deit3_large/huges                   | Rare              | Very expensive                         |\n",
    "| 384-resolution versions             | Rare              | High compute                           |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb95ff-ce52-4f6b-b6fc-b65b240d2bb4",
   "metadata": {},
   "source": [
    "## Name decoding of `deit_base_patch16_224` and Parameters \n",
    "\n",
    "Roughly speaking, each part of `deit_base_patch16_224` is a compact code for the architecture and input resolution.\n",
    "\n",
    "Let’s decode it piece by piece:\n",
    "\n",
    "---\n",
    "\n",
    "### `deit`\n",
    "\n",
    "Short for **DeiT = Data-efficient image Transformers**\n",
    "This is the family name of the model, from the paper *“Training data-efficient image transformers & distillation through attention”*.\n",
    "\n",
    "It means:\n",
    "\n",
    "* Vision Transformer-style backbone (ViT)\n",
    "* Trained with the DeiT recipe (strong augmentation, regularization, possibly distillation for some variants)\n",
    "\n",
    "---\n",
    "\n",
    "### `base`\n",
    "\n",
    "This is the **model size** (like small/medium/large in ResNet):\n",
    "\n",
    "* `tiny`  → ViT-Tiny (very small)\n",
    "* `small` → ViT-Small\n",
    "* `base`  → **ViT-Base**\n",
    "\n",
    "For `base`, the typical ViT-Base config is:\n",
    "\n",
    "* Number of transformer layers:\n",
    "  $$ L = 12 $$\n",
    "* Embedding dimension:\n",
    "  $$ D = 768 $$\n",
    "* Number of heads:\n",
    "  $$ H = 12 $$\n",
    "* MLP hidden dim:\n",
    "  $$ 4D = 3072 $$ (roughly)\n",
    "\n",
    "So `base` ≈ medium-sized ViT.\n",
    "\n",
    "---\n",
    "\n",
    "### `patch16`\n",
    "\n",
    "This says how the image is split into patches:\n",
    "\n",
    "* Patch size =\n",
    "  $$ 16 \\times 16 \\text{ pixels} $$\n",
    "\n",
    "For a (224 \\times 224) image, that gives:\n",
    "\n",
    "* Number of patches per side:\n",
    "  $$ 224 / 16 = 14 $$\n",
    "* Total number of patches (tokens before adding [CLS]):\n",
    "  $$ 14 \\times 14 = 196 $$\n",
    "\n",
    "So the sequence length (tokens) is 196 image tokens + 1 class token.\n",
    "\n",
    "---\n",
    "\n",
    "### `224`\n",
    "\n",
    "This is the **default input resolution**:\n",
    "\n",
    "* Image size:\n",
    "  $$ 224 \\times 224 \\text{ pixels} $$\n",
    "\n",
    "So:\n",
    "\n",
    "* Input: $224 \\times 224$,\n",
    "* Split into $16 \\times 16$ patches → $14 \\times 14 = 196$ tokens,\n",
    "* Processed by a ViT-Base transformer, trained with the DeiT recipe.\n",
    "\n",
    "---\n",
    "\n",
    "### Put together\n",
    "\n",
    "`deit_base_patch16_224` means:\n",
    "\n",
    "> “A DeiT model with a ViT-Base backbone, using 16×16 image patches, trained on 224×224 input images.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813b6cc-07da-4be0-8f04-0b24fbaa9708",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Input and Patch Embeddings\n",
    "\n",
    "### Image\n",
    "\n",
    "Resolution:\n",
    "$$224 \\times 224 \\times 3$$\n",
    "\n",
    "### Patch Size\n",
    "\n",
    "$$16 \\times 16$$\n",
    "\n",
    "This gives:\n",
    "$$\\frac{224}{16}=14$$\n",
    "So the number of patches:\n",
    "$$14 \\times 14 = 196$$\n",
    "\n",
    "Each patch is flattened:\n",
    "$$16 \\times 16 \\times 3 = 768\\text{ dimensions}$$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Tokenization\n",
    "\n",
    "### Patch tokens\n",
    "\n",
    "Shape:\n",
    "$$196 \\times 768$$\n",
    "\n",
    "### CLS token\n",
    "\n",
    "A learnable vector:\n",
    "$$1 \\times 768$$\n",
    "\n",
    "Total sequence length:\n",
    "$$196 + 1 = 197\\text{ tokens}$$\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Positional Embedding\n",
    "\n",
    "A learnable embedding:\n",
    "$$197 \\times 768$$\n",
    "\n",
    "Added to the token embeddings:\n",
    "$$X_0 = E_{\\text{patches}} + E_{\\text{pos}}$$\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Transformer Encoder Structure\n",
    "\n",
    "There are **12 transformer encoder blocks**.\n",
    "\n",
    "Each block has:\n",
    "\n",
    "### (1) LayerNorm\n",
    "\n",
    "### (2) Multi-Head Self-Attention (MSA)\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* Embedding dimension:\n",
    "  $$D = 768$$\n",
    "\n",
    "* Number of heads:\n",
    "  $$H = 12$$\n",
    "\n",
    "So dimension per head:\n",
    "$$d_k = \\frac{768}{12} = 64$$\n",
    "\n",
    "Attention uses:\n",
    "\n",
    "* Query:\n",
    "  $$Q = X W_Q$$\n",
    "* Key:\n",
    "  $$K = X W_K$$\n",
    "* Value:\n",
    "  $$V = X W_V$$\n",
    "\n",
    "Where each projection matrix has shape:\n",
    "$$W_Q, W_K, W_V \\in \\mathbb{R}^{768 \\times 768}$$\n",
    "\n",
    "Attention output uses:\n",
    "$$W_O \\in \\mathbb{R}^{768 \\times 768}$$\n",
    "\n",
    "### (3) Skip connection\n",
    "\n",
    "### (4) LayerNorm\n",
    "\n",
    "### (5) MLP block\n",
    "\n",
    "MLP has:\n",
    "\n",
    "Hidden size:\n",
    "$$3072 = 4 \\times 768$$\n",
    "\n",
    "Layers:\n",
    "\n",
    "* Linear:\n",
    "  $$768 \\rightarrow 3072$$\n",
    "* GELU\n",
    "* Linear:\n",
    "  $$3072 \\rightarrow 768$$\n",
    "\n",
    "### (6) Skip connection\n",
    "\n",
    "This repeats for **12 layers**.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Output\n",
    "\n",
    "We take the final representation of the **CLS token**:\n",
    "\n",
    "$$z_{\\text{CLS}} \\in \\mathbb{R}^{768}$$\n",
    "\n",
    "Pass it to a linear head:\n",
    "$$y = z_{\\text{CLS}} W_{\\text{head}}$$\n",
    "\n",
    "For ImageNet:\n",
    "$$W_{\\text{head}} \\in \\mathbb{R}^{768 \\times 1000}$$\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Summary Table\n",
    "\n",
    "| Component            | Configuration     |\n",
    "| -------------------- | ----------------- |\n",
    "| Image size           | 224×224           |\n",
    "| Patch size           | 16×16             |\n",
    "| Number of patches    | 196               |\n",
    "| Patch embedding dim  | 768               |\n",
    "| Sequence length      | 197 (196 + CLS)   |\n",
    "| Positional embedding | 197×768 (learned) |\n",
    "| Encoder blocks       | 12                |\n",
    "| MSA heads            | 12                |\n",
    "| Dim per head         | 64                |\n",
    "| MLP hidden dim       | 3072              |\n",
    "| Final head           | 768 → 1000        |\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Important note\n",
    "\n",
    "**Architecture = ViT-B/16.\n",
    "What makes it DEiT is not architecture but the training recipe.**\n",
    "\n",
    "DEiT training includes:\n",
    "\n",
    "* Mixup\n",
    "* CutMix\n",
    "* RandAugment\n",
    "* Stochastic depth\n",
    "* EMA\n",
    "* Label smoothing\n",
    "* Optional distillation (for distilled variants)\n",
    "\n",
    "But the architecture stays **pure ViT Base Patch16**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
