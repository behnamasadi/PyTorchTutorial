{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moderate-diversity",
   "metadata": {},
   "source": [
    "# Variational Autoencoders \n",
    "Variational Autoencoders (VAEs) are a type of generative model that learn to encode data into a compressed representation and then decode this representation back into data. They are particularly known for their ability to generate new data samples that are similar to the training data. The core idea behind VAEs involves understanding the underlying probability distribution of the data, and they use principles from probability theory and statistics to achieve this.\n",
    "\n",
    "Here's a high-level explanation of the key components and equations that define a VAE:\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder part of a VAE takes input data $x$ and transforms it into a distribution over the latent space, which is typically a Gaussian distribution. The encoder is defined by two functions: $ \\mu(x) $ and $ \\sigma^2(x) $, which represent the mean and variance of the Gaussian distribution. These functions are usually implemented as neural networks.\n",
    "\n",
    "### Latent Space\n",
    "\n",
    "The latent space is a lower-dimensional space into which the input data is encoded. Each point in the latent space represents a potential generation of the decoder. The crucial aspect of VAEs is that they learn a distribution over this latent space rather than deterministic points.\n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "To enable backpropagation through the network, VAEs use the \"reparameterization trick.\" Instead of sampling $z$ directly from the Gaussian distribution defined by $ \\mu $ and $ \\sigma^2 $, we sample $ \\epsilon $ from a standard normal distribution and compute $ z = \\mu + \\sigma \\odot \\epsilon $. This trick allows the gradient to bypass the random sampling step, making the training process differentiable.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder part of the VAE takes a point $z$ from the latent space and transforms it back into the data space, attempting to reconstruct the original input $x$. The output of the decoder is typically parameters of a probability distribution of the reconstructed data. For binary data, the output could be parameters of a Bernoulli distribution, and for continuous data, it could be parameters of a Gaussian distribution.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function of a VAE has two main components:\n",
    "\n",
    "1. **Reconstruction Loss**: This part of the loss measures how well the reconstructed data matches the original data. For different types of data, different probability distributions are used to model the reconstruction loss.\n",
    "\n",
    "2. **KL Divergence**: This term acts as a regularizer that measures how much the learned distribution in the latent space deviates from a prior distribution (typically a standard normal distribution). It is given by:\n",
    "   $ D_{KL}[q_\\phi(z|x) || p(z)] $\n",
    "   where $q_\\phi(z|x)$ is the distribution defined by the encoder, and $p(z)$ is the prior distribution over the latent variables (usually a standard normal distribution).\n",
    "\n",
    "Putting it all together, the loss function of a VAE can be summarized as:\n",
    "$ L(x) = -E_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + D_{KL}[q_\\phi(z|x) || p(z)] $\n",
    "where the first term is the negative log-likelihood of the reconstructed data (reconstruction loss), and the second term is the KL divergence between the encoder's distribution and the prior distribution in the latent space.\n",
    "\n",
    "This framework allows VAEs to both learn a compressed representation of the data and generate new data samples by sampling from the latent space and decoding those samples.\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=9zKuYvjFFS8&ab_channel=ArxivInsights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-sunglasses",
   "metadata": {},
   "source": [
    "# Reparameterisation Trick\n",
    "\n",
    "\n",
    "The reparameterization trick is a fundamental technique in the training of Variational Autoencoders (VAEs) that enables the backpropagation of gradients through a stochastic process. This trick is crucial for learning in VAEs because it allows for the optimization of the model parameters even though part of the model involves randomness. Let's dive into the details of how this trick works and why it's important.\n",
    "\n",
    "### Problem in Stochastic Gradient Descent\n",
    "\n",
    "In a VAE, during the encoding process, an input $x$ is mapped to a distribution over latent variables $z$, typically a Gaussian distribution with mean $\\mu(x)$ and variance $\\sigma^2(x)$. Ideally, we would sample $z$ from this distribution and pass it through the decoder to generate a reconstruction of $x$, and then compute the loss to update the encoder and decoder parameters via gradient descent. However, directly sampling $z = \\mu + \\sigma \\epsilon$, where $\\epsilon$ is a sample from a standard normal distribution $N(0,1)$, introduces a random step that is not differentiable, preventing the gradient from being backpropagated through the sampling process.\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "The reparameterization trick addresses this problem by reformulating the sampling process. Instead of sampling $z$ directly from its distribution, the trick involves sampling an auxiliary variable $\\epsilon$ from a standard normal distribution (which does not depend on the parameters of the encoder) and then deterministically transforming $\\epsilon$ to obtain the sample $z$. The transformation uses the parameters $\\mu(x)$ and $\\sigma(x)$ output by the encoder:\n",
    "\n",
    "$ z = \\mu(x) + \\sigma(x) \\odot \\epsilon $\n",
    "\n",
    "where $\\epsilon \\sim N(0, I)$ (a standard Gaussian distribution), and the operation $\\odot$ denotes element-wise multiplication. This transformation ensures that $z$ is a sample from the Gaussian distribution with mean $\\mu(x)$ and variance $\\sigma^2(x)$, but crucially, it allows the gradient of the loss with respect to $\\mu$ and $\\sigma$ to be computed since the sampling of $\\epsilon$ is independent of the model parameters.\n",
    "\n",
    "### Why It's Important\n",
    "\n",
    "The reparameterization trick allows the use of gradient-based optimization methods, such as stochastic gradient descent, for training VAEs. By making the sampling process differentiable, it enables the gradient of the loss function to flow back through the stochastic node of $z$, thus allowing for the efficient training of the encoder parameters that define the mean and variance of the latent distribution.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In essence, the reparameterization trick:\n",
    "- Makes the training of VAEs feasible by allowing gradients to be backpropagated through random sampling steps.\n",
    "- Maintains the stochasticity required for the model by sampling $\\epsilon$ from a standard normal distribution.\n",
    "- Facilitates the learning of complex distributions of data by enabling the optimization of parameters that define the latent space distribution.\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
