{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "The standard (unit) softmax function $\\sigma :\\mathbb {R} ^{K}\\to \\mathbb {R} ^{K}$ is defined by the formula\n",
    "\n",
    "${\\displaystyle a_{i}=\\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}}$\n",
    "\n",
    "In words: we apply the standard exponential function to each element ${\\displaystyle z_{i}}$ of the input vector ${\\displaystyle \\mathbf {z} }$ and normalize these values by dividing by the sum of all these exponentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This normalization ensures that the sum of the components of the output vector ${\\displaystyle \\sigma (\\mathbf {z} )}$ is 1. \n",
    "For all neurons in the output layer:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\sum_j a^L_j & = & \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Output from the softmax layer can be thought of as a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Softmax derivativ:\n",
    "[1](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
