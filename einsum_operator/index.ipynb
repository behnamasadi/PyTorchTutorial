{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8423e8-e5b1-4415-85f0-2ca904bc0e1d",
   "metadata": {},
   "source": [
    "## Einsum Operator\n",
    "`torch.einsum` is a **powerful and compact way to express tensor operations** (like matrix multiplication, dot products, outer products, transposes, or reductions) using **Einstein summation notation**.\n",
    "\n",
    "It allows you to **specify exactly how indices should align, reduce, or broadcast** — without writing loops or calling multiple PyTorch ops.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Einstein summation rule**\n",
    "\n",
    "If an index appears **twice**, it’s **summed over**.\n",
    "If it appears **once**, it’s **kept**.\n",
    "\n",
    "Example of notation:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_k A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "is written in `einsum` as:\n",
    "\n",
    "```python\n",
    "torch.einsum('ik,kj->ij', A, B)\n",
    "```\n",
    "\n",
    "which is standard **matrix multiplication**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Simple numerical examples**\n",
    "\n",
    "#### **Example 1 — Dot product**\n",
    "\n",
    "$$\n",
    "c = \\sum_i a_i b_i\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613b78a6-d96b-46a3-bd92-73cd6c45d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Dot product\n",
    "c = torch.einsum('i,i->', a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbfcb0-d4d7-4fee-a7d1-66ee060eb9c5",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "$$\n",
    "1×4 + 2×5 + 3×6 = 32\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc32372-974f-4cea-b280-dd2efd60bb31",
   "metadata": {},
   "source": [
    "### **Example 2: Sum of all Pairwise Products**\n",
    "\n",
    "`torch.einsum('i,j->', a, b)`\n",
    "\n",
    "Now the indices are **different** (`i` and `j`).\n",
    "\n",
    "Each of them appears **only once**, and both **disappear** in the output (no index after `->`),\n",
    "so both are **summed over**.\n",
    "\n",
    "That means:\n",
    "\n",
    "$$\n",
    "c = \\sum_i \\sum_j a_i b_j\n",
    "$$\n",
    "\n",
    "This is the **sum of all pairwise products** between elements of `a` and `b`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6ccffa-7647-4694-a0ca-3aec6ad56039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.einsum('i,j->', a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d8de-07de-4e1c-9e66-49a15baa2abf",
   "metadata": {},
   "source": [
    "Manual calculation:\n",
    "\n",
    "$$\n",
    "(1×4 + 1×5 + 1×6) + (2×4 + 2×5 + 2×6) + (3×4 + 3×5 + 3×6) = 90\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Expression  | Meaning                      | Formula                  | Result Type |\n",
    "| ----------- | ---------------------------- | ------------------------ | ----------- |\n",
    "| `'i,i->'`   | Dot product                  | $$\\sum_i a_i b_i$$       | Scalar      |\n",
    "| `'i,j->ij'` | Outer product                | $$a_i b_j$$              | Matrix      |\n",
    "| `'i,j->'`   | Sum of all pairwise products | $$\\sum_i\\sum_j a_i b_j$$ | Scalar      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26812464-72d1-4693-a74a-ef6cd3213fa7",
   "metadata": {},
   "source": [
    "#### **Example 3 — Outer product**\n",
    "\n",
    "$$\n",
    "C_{ij} = a_i b_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b8ae70-5c57-47fa-bd5c-3647ec920069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6],\n",
      "        [ 8, 10, 12],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c=torch.einsum('i,j->ij', a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b198-a8a9-4885-abce-28128e192034",
   "metadata": {},
   "source": [
    "No summation here — each pair is multiplied directly.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example 4 — Matrix multiplication**\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_k A_{ik} B_{kj}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be89085-fd75-48a4-839f-fcb2336ea9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "B = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "\n",
    "C = torch.einsum('ik,kj->ij', A, B)\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8fd76-1577-4983-8b2f-99b80b443191",
   "metadata": {},
   "source": [
    "Same as `A @ B`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example 5 — Sum over specific axes**\n",
    "\n",
    "You can sum over an axis without writing `torch.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef4ca8d7-0cbd-447d-b826-23cbd9c08568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# Sum along rows (i.e., keep columns)\n",
    "torch.einsum('ij->j', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fc745-751f-468c-b85f-dd42aeae73b3",
   "metadata": {},
   "source": [
    "Equivalent to `x.sum(dim=0)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Why we need `einsum`**\n",
    "\n",
    "- ✅ **Expressive and compact**: One line replaces multiple `matmul`, `transpose`, and `sum` operations.\n",
    "- ✅ **Clear intent**: The subscript notation shows how indices interact.\n",
    "- ✅ **Efficient**: PyTorch optimizes many `einsum` patterns to use fast BLAS or cuBLAS kernels.\n",
    "- ✅ **Flexible**: Works for batched operations, attention mechanisms, and tensor contractions in transformers.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
