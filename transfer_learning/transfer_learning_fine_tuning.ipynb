{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. List of Available Pretrained Models**\n",
    "\n",
    "#### **1.1 Classification Models**\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "\n",
    "# List all classification models\n",
    "print(dir(models))\n",
    "```\n",
    "\n",
    "**Common pretrained classification models**:\n",
    "\n",
    "| Model Family | Model Names                                                  |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| VGG          | `vgg11`, `vgg13`, `vgg16`, `vgg19`, and their `_bn` variants |\n",
    "| ResNet       | `resnet18`, `resnet34`, `resnet50`, `resnet101`, `resnet152` |\n",
    "| DenseNet     | `densenet121`, `densenet161`, `densenet169`, `densenet201`   |\n",
    "| MobileNet    | `mobilenet_v2`, `mobilenet_v3_large`, `mobilenet_v3_small`   |\n",
    "| EfficientNet | `efficientnet_b0` to `efficientnet_b7`                       |\n",
    "| ViT          | `vit_b_16`, `vit_b_32`, `vit_l_16`, etc.                     |\n",
    "| ConvNeXt     | `convnext_tiny`, `convnext_base`, etc.                       |\n",
    "| RegNet       | `regnet_y_400mf`, `regnet_y_1_6gf`, etc.                     |\n",
    "| SqueezeNet   | `squeezenet1_0`, `squeezenet1_1`                             |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.2 Segmentation Models**\n",
    "\n",
    "Available under `torchvision.models.segmentation`:\n",
    "\n",
    "```python\n",
    "from torchvision.models import segmentation\n",
    "print(dir(segmentation))\n",
    "```\n",
    "\n",
    "**Popular segmentation models**:\n",
    "\n",
    "* `fcn_resnet50`\n",
    "* `fcn_resnet101`\n",
    "* `deeplabv3_resnet50`\n",
    "* `deeplabv3_resnet101`\n",
    "* `lraspp_mobilenet_v3_large`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2.Structured of Network in and Modular Components**\n",
    "Models are often structured in modular components referred to as **Backbone**, **Neck**, and **Head**. These components organize how features are extracted, refined, and used for predictions.\n",
    "\n",
    "Here's a breakdown of what each part typically does, along with related components:\n",
    "\n",
    "---\n",
    "\n",
    "####  2.1. **Backbone** ‚Äì Feature Extractor\n",
    "\n",
    "**What it is**:\n",
    "The **backbone** is the main feature extractor. It takes the raw input (e.g., an image) and outputs high-level features.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* **ResNet**, **VGG**, **EfficientNet**, **ViT** (Vision Transformer), **ConvNeXt**\n",
    "* Trained on datasets like ImageNet for classification\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* A feature map with reduced spatial resolution but rich semantic content (e.g., shape `[B, C, H/32, W/32]`)\n",
    "\n",
    "**Usage**:\n",
    "\n",
    "* Used across tasks: classification, detection, segmentation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "####  2.2. **Neck** ‚Äì Feature Refinement / Aggregation\n",
    "\n",
    "**What it is**:\n",
    "The **neck** connects the backbone to the head. It processes and refines feature maps‚Äîoften enhancing multi-scale features or fusing spatial information.\n",
    "\n",
    "**Common types**:\n",
    "\n",
    "* **FPN (Feature Pyramid Network)**: Combines features at different resolutions\n",
    "* **BiFPN (EfficientDet)**: Bidirectional FPN\n",
    "* **PANet**: For better path aggregation\n",
    "* **Transformer Encoders**: As necks in hybrid models\n",
    "\n",
    "**Why use it**:\n",
    "\n",
    "* Helps the model detect objects of different sizes\n",
    "* Improves information flow between layers\n",
    "\n",
    "---\n",
    "\n",
    "####  2.3. **Head** ‚Äì Task-Specific Prediction\n",
    "\n",
    "**What it is**:\n",
    "The **head** converts features into outputs (e.g., class labels, bounding boxes, masks).\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* **Classification head**: `Linear ‚Üí Softmax`\n",
    "* **Detection head** (e.g., YOLO): Predicts classes, bounding boxes, objectness score\n",
    "* **Segmentation head**: Upsamples and predicts pixel-wise labels\n",
    "* **Pose estimation head**: Keypoints or coordinates\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* Final predictions shaped for the task (e.g., `[B, num_classes]` for classification)\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ‚Üì\n",
    "[Preprocessor]\n",
    "    ‚Üì\n",
    "Backbone ‚Üí (Feature Maps)\n",
    "    ‚Üì\n",
    "Neck     ‚Üí (Enhanced Features)\n",
    "    ‚Üì\n",
    "Head     ‚Üí (Predictions: class/box/mask/etc.)\n",
    "    ‚Üì\n",
    "[Post-processing]\n",
    "    ‚Üì\n",
    "Final Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Determining the required input size**\n",
    "\n",
    "\n",
    "#### 3.1. **Use TorchVision Documentation or Model Summary**\n",
    "\n",
    "The [official PyTorch documentation](https://pytorch.org/vision/stable/models.html) lists **default input sizes** for each pretrained model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  3.2 **Inspect the Model Internals**\n",
    "\n",
    "For most models, like `resnet18`, you can inspect how many times the input is halved due to pooling/stride:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "from torchinfo import summary  # pip install torchinfo\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####  3.3 **Render the Model Diagram (Visualization)**\n",
    "\n",
    "```python\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Input Image Size Different From The Pretrained Model Input**\n",
    "\n",
    "If your input image size is **different** from what the pretrained model expects, you have **two main options**, depending on your task:\n",
    "\n",
    "\n",
    "#### **4.1 Resize your input image to match the model**\n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match model's expected input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "#### **4.2 Adapt the model to your image size**\n",
    "\n",
    "**Advanced ‚Äî use only if resizing hurts performance or semantics.**\n",
    "\n",
    "- Use **adaptive pooling** in place of fixed `AvgPool2d` or `Linear` assumptions (e.g., in custom CNNs):\n",
    "\n",
    "```python\n",
    "nn.AdaptiveAvgPool2d((1, 1))  # Allows any input size\n",
    "```\n",
    "\n",
    "- **Replace classifier layers** if needed:\n",
    "\n",
    "If your model fails because of mismatched `in_features` in `Linear`, do:\n",
    "\n",
    "```python\n",
    "# Forward pass dummy input to find flattened size\n",
    "dummy_input = torch.randn(1, 3, your_H, your_W)\n",
    "features = model.features(dummy_input)  # or model.backbone for ResNet\n",
    "flattened_size = features.view(1, -1).shape[1]\n",
    "\n",
    "# Replace classifier accordingly\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(flattened_size, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, num_classes),\n",
    ")\n",
    "```\n",
    "\n",
    "When to use:\n",
    "\n",
    "* You're training **from scratch** or fine-tuning a model deeply.\n",
    "* Your data has **very different resolution** (e.g. medical images 512x512).\n",
    "* You want to **preserve spatial details** for segmentation/localization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Fine-tuning of a Pretrained Network Classifier**\n",
    "#### 5.1  Determining Parameters Network,\n",
    "First you have to know the parameters of your network, for instance:\n",
    "\n",
    "```python\n",
    "model_vgg19_bn = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "print(model_vgg19_bn)\n",
    "```\n",
    "\n",
    "This will give you the entire model features (covnet layer) + fully connected layer:\n",
    "\n",
    "```bash\n",
    "(features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU(inplace=True)\n",
    "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \n",
    "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (51): ReLU(inplace=True)\n",
    "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "(classifier): Sequential(\n",
    "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Dropout(p=0.5, inplace=False)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): Dropout(p=0.5, inplace=False)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "```python\n",
    "for param in model_vgg19_bn.features.parameters():\n",
    "    print(param.shape)\n",
    "```\n",
    "\n",
    "gives you features (covnet layer):\n",
    "\n",
    "```bash\n",
    "torch.Size([64, 3, 3, 3])\n",
    "torch.Size([64])\n",
    ".\n",
    ".\n",
    ".\n",
    "torch.Size([512, 512, 3, 3])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "```\n",
    "\n",
    "\n",
    "For **ResNet18**, we have only covnet layer and 1 fully connected layer, input is 512 and output 1000 classes:\n",
    "\n",
    "```python\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \", resnet18.fc.out_features)\n",
    "```\n",
    "\n",
    "```bash\n",
    "resnet18 input size:  512\n",
    "resnet18 output size:  1000\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Freeze **all feature extractor layers**\n",
    "\n",
    "```python\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "```\n",
    "\n",
    "This makes everything frozen (conv + bn + fc). Usually you then unfreeze the head.\n",
    "\n",
    "#### 5.3 Freeze only the convolutional backbone (leave `fc` trainable)\n",
    "\n",
    "```python\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True   # classifier\n",
    "for p in model.layer4.parameters():\n",
    "    p.requires_grad = False  # example: freeze last block\n",
    "```\n",
    "\n",
    "#### 5.4 Unfreeze some block (e.g. `layer4`)\n",
    "\n",
    "```python\n",
    "for p in model.layer4.parameters():\n",
    "    p.requires_grad = True\n",
    "```\n",
    "\n",
    "ResNet is organized like this:\n",
    "\n",
    "```\n",
    "model.conv1 -> model.bn1 -> model.layer1 -> model.layer2 -> model.layer3 -> model.layer4 -> model.fc\n",
    "```\n",
    "\n",
    "so you can target any block.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5 Safely replace the head\n",
    "\n",
    "ResNet18‚Äôs final FC (`model.fc`) outputs **1000 classes** (ImageNet).\n",
    "You replace it with your own classifier:\n",
    "\n",
    "```python\n",
    "num_features = model.fc.in_features   # 512 for resnet18\n",
    "num_classes = 5                       # example\n",
    "\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "```\n",
    "\n",
    "This is the cleanest and most common way. The rest of the model stays intact.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 5.6 Replace the Final Classifier\n",
    "\n",
    "\n",
    "```python\n",
    "num_classes = 3  # your problem\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(in_features, num_classes)  # new classifier layer\n",
    "```\n",
    "---\n",
    "                        \n",
    "#### 5.7 Optimizer setup (important!)\n",
    "\n",
    "If you froze parameters, make sure your optimizer only updates trainable ones:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad),\n",
    "    lr=1e-3\n",
    ")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. When to  Learn Feature maps**\n",
    "\n",
    "---\n",
    "\n",
    "**Transfer Learning Modes**\n",
    "\n",
    "| Mode                               | Freeze Feature Layers? | Fine-Tune Feature Layers?                     | Train Classifier? | When to Use                                                                                      |\n",
    "| ---------------------------------- | ---------------------- | --------------------------------------------- | ----------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **1. Feature Extraction (Frozen)** | ‚úÖ Yes                  | ‚ùå No                                          | ‚úÖ Yes             | When dataset is **small** and **similar** to ImageNet                                            |\n",
    "| **2. Fine-Tuning Last Block**      | üö´ No (partial)        | ‚úÖ Last layers only (e.g., `layer4` in ResNet) | ‚úÖ Yes             | When dataset is **moderate in size** and **domain-shifted**                                      |\n",
    "| **3. Full Fine-Tuning**            | ‚ùå No                   | ‚úÖ All conv layers                             | ‚úÖ Yes             | When dataset is **large** or **significantly different** from ImageNet (e.g. medical, satellite) |\n",
    "| **4. Training from Scratch**       | ‚ùå N/A                  | ‚úÖ All layers randomly initialized             | ‚úÖ Yes             | When you have a **huge custom dataset** and **no pretraining** is applicable                     |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. `torch.nn.Identity`**\n",
    "`torch.nn.Identity` is a simple module in PyTorch that **does nothing to its input** ‚Äî it just returns it unchanged. It's often used as a **placeholder** when you want to **remove or skip a layer** in a model (e.g., when doing ablation studies, or when modifying pretrained models).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "identity = nn.Identity()\n",
    "output = identity(input)\n",
    "```\n",
    "\n",
    "Here, `output` will be **exactly the same** as `input`.\n",
    "\n",
    "\n",
    "When do we need `nn.Identity`?\n",
    "\n",
    "####  7.1 **Ablation studies / removing layers**\n",
    "\n",
    "If you're testing the effect of removing a layer:\n",
    "\n",
    "```python\n",
    "self.dropout = nn.Dropout(p=0.5) if use_dropout else nn.Identity()\n",
    "```\n",
    "\n",
    "####  7.2 **Replace classifier head**\n",
    "\n",
    "If you load a pretrained model and want to keep everything except the final classification layer:\n",
    "\n",
    "```python\n",
    "model.fc = nn.Identity()  # For example in ResNet\n",
    "```\n",
    "\n",
    "####  7.3 **Skip connections or conditional architectures**\n",
    "\n",
    "If you want to optionally add a layer, but still keep the same forward pass logic:\n",
    "\n",
    "```python\n",
    "self.extra = nn.BatchNorm1d(256) if use_bn else nn.Identity()\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Why Replacing Classifier (Head) of a Pre-trained With a Shallow 2-layer FC**\n",
    "In many deep learning tasks, when they use a pretrained model, they only replace the last layer of the classifier with a sequential linear model of the size of `num_class`, or just replace the entire head (classifier) with a simple `2-layer` FC, Why don't we use a better model (deeper) in the head like using dropout, etc.\n",
    "\n",
    "\n",
    "Below is the **real reason** researchers rarely make the classification head deeper or more complex when fine-tuning pretrained models. This applies to ResNet, EfficientNet, ViT, DeiT, Swin, PVT, CLIP, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.1. The pretrained backbone already extracted almost all useful features**\n",
    "\n",
    "A backbone like ResNet50, ViT-B/16, Swin-T, etc. produces a feature vector\n",
    "\n",
    "$$z \\in \\mathbb{R}^d,$$\n",
    "\n",
    "which is already **high-level, linearly separable** for most tasks.\n",
    "\n",
    "Empirically:\n",
    "\n",
    "$$\\text{Linear}(z) ;\\text{often achieves the same accuracy as a deep MLP on top of } z.$$\n",
    "\n",
    "This is why linear probing gives very strong performance:\n",
    "the backbone has done the hard work; the head only needs to map features to classes.\n",
    "\n",
    "A deeper head adds little gain but increases:\n",
    "\n",
    "* parameters,\n",
    "* training time,\n",
    "* overfitting risk.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.2. Small heads prevent overfitting (especially with small datasets)**\n",
    "\n",
    "For tasks with limited images (medical imaging, low-data regimes):\n",
    "\n",
    "A deep head with many FC layers produces many parameters:\n",
    "\n",
    "$$\\text{Parameters} \\sim O(d^2)$$\n",
    "\n",
    "which massively increases overfitting on small datasets.\n",
    "\n",
    "A linear head has far fewer parameters:\n",
    "\n",
    "$$\\text{Parameters} = d \\times C + C.$$\n",
    "\n",
    "So for medical tasks, a simple linear head is **more generalizable**, not less.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.3. The backbone and head must be trained together**\n",
    "\n",
    "If you put a **large multi-layer head**, gradients will take longer to propagate to the backbone.\n",
    "\n",
    "That means:\n",
    "\n",
    "* slower training,\n",
    "* worse fine-tuning with small batch sizes,\n",
    "* unstable optimization.\n",
    "\n",
    "A shallow head (1‚Äì2 layers) keeps the network easy to train.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.4. Empirically validated by thousands of papers**\n",
    "\n",
    "Papers like:\n",
    "\n",
    "* ViT\n",
    "* DeiT\n",
    "* Swin Transformer\n",
    "* ResNet/ConvNext\n",
    "* EfficientNet\n",
    "* CLIP\n",
    "* PVT\n",
    "* RegNet\n",
    "* Mask R-CNN / DETR / YOLO heads\n",
    "\n",
    "all find that:\n",
    "\n",
    "**A simple linear or small MLP head is almost always optimal.**\n",
    "\n",
    "More depth rarely improves accuracy.\n",
    "\n",
    "Even CLIP uses only a **2-layer** projection head.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.5. The head is not the bottleneck ‚Äî the backbone is**\n",
    "\n",
    "Accuracy ‚â† limited by the head.\n",
    "Accuracy is limited by:\n",
    "\n",
    "* feature quality,\n",
    "* representation size,\n",
    "* backbone pretraining duration.\n",
    "\n",
    "This is why improvements focus on:\n",
    "\n",
    "* better backbones,\n",
    "* better pretraining,\n",
    "* larger datasets,\n",
    "* better augmentations,\n",
    "* self-supervised learning.\n",
    "\n",
    "Changing the head won‚Äôt fix weak representations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.6. Deep heads usually harm transfer learning**\n",
    "\n",
    "If you make a deep head:\n",
    "\n",
    "$$\\text{Head} = \\text{FC} \\to \\text{Dropout} \\to \\text{FC} \\to \\text{Norm} \\to \\text{Activation} \\to \\text{FC}$$\n",
    "\n",
    "then:\n",
    "\n",
    "* the head learns task-specific patterns,\n",
    "* the backbone updates less,\n",
    "* generalization drops.\n",
    "\n",
    "Transfer learning works best when:\n",
    "\n",
    "**Backbone is flexible,\n",
    "Head is simple.**\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.7. Simple heads = reproducibility, simplicity, cleaner signal**\n",
    "\n",
    "A simple head ensures:\n",
    "\n",
    "* fewer hyperparameters,\n",
    "* fewer training instabilities,\n",
    "* easier reproducibility across labs,\n",
    "* clearer attribution of improvements.\n",
    "\n",
    "If heads become complex, performance becomes sensitive to hyperparameters, dropout rates, and optimizer choices.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.8. Dropout on the head is usually useless**\n",
    "\n",
    "Dropout helps when the model is *huge* relative to the data.\n",
    "\n",
    "But pretrained features are already robust.\n",
    "\n",
    "A dropout layer:\n",
    "\n",
    "* slows convergence,\n",
    "* reduces useful feature flow,\n",
    "* rarely improves accuracy in transfer learning.\n",
    "\n",
    "This is why dropout is mostly used inside the backbone (e.g., Transformer‚Äôs MLP block) but not in the classifier head.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.9. A deeper head violates the principle of linear probing**\n",
    "\n",
    "The backbone was pretrained to output a feature that is linearly separable.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\text{Linear classifier is enough.}$$\n",
    "\n",
    "Adding depth violates that assumption and reduces the benefit of pretraining.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8.10. When do we *actually* use deeper heads?**\n",
    "\n",
    "There are only **three cases** where deeper heads are used:\n",
    "\n",
    "### 1. Text encoders (CLIP, BERT head)\n",
    "\n",
    "Need a projection head to match embedding spaces.\n",
    "\n",
    "### 2. Segmentation and detection heads\n",
    "\n",
    "Fully convolutional or transformer-based decoders:\n",
    "\n",
    "* FPN\n",
    "* U-Net decoders\n",
    "* DETR heads\n",
    "* Mask heads\n",
    "\n",
    "Because they operate on **multi-scale spatial features**, not one feature vector.\n",
    "\n",
    "### 3. Non-linear projection in contrastive learning\n",
    "\n",
    "SimCLR:\n",
    "\n",
    "$$\\text{Encoder} \\rightarrow \\text{MLP projection head} \\rightarrow \\text{Contrastive loss}.$$\n",
    "\n",
    "Projection head is used only during training, then thrown away.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary: Why heads remain simple\n",
    "\n",
    "Reasons **not** to use big heads:\n",
    "\n",
    "* no improvement in accuracy\n",
    "* greater overfitting\n",
    "* slower training\n",
    "* harder fine-tuning\n",
    "* gradients flow worse\n",
    "* backbone already extracts linearly separable features\n",
    "* literature overwhelmingly validates simple heads\n",
    "\n",
    "### So the best head is usually:\n",
    "\n",
    "* **Linear**\n",
    "* or **small MLP with 1 hidden layer**\n",
    "\n",
    "Nothing deeper.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. When to Train backbone or Have a Slightly Deeper Head**\n",
    "\n",
    "\n",
    "\n",
    "#### **9.1. When the domain shift is *big*, the classifier head can help rewrite the features**\n",
    "\n",
    "ImageNet ‚Üí medical grayscale is a **massive domain shift**.\n",
    "\n",
    "The features that ImageNet learned (edges, textures, color-selective filters, object parts) are not fully optimal for:\n",
    "\n",
    "* grayscale ultrasound\n",
    "* CT/MRI slices\n",
    "* dermoscopy images\n",
    "* x-rays\n",
    "* histopathology slides\n",
    "\n",
    "In these cases:\n",
    "\n",
    "**A small MLP head helps the model gradually adapt from ImageNet color features to medical features.**\n",
    "\n",
    "Your example:\n",
    "\n",
    "* Linear(1280‚Üí128)\n",
    "* ReLU\n",
    "* Linear(128‚Üí3)\n",
    "* Dropout layers\n",
    "\n",
    "acts as a **task-specific adapter layer**.\n",
    "\n",
    "This is different from the typical classification head used for natural images.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.2. When you train the backbone + the head end-to-end, the extra head helps optimization**\n",
    "\n",
    "If you fine-tune both backbone and head:\n",
    "\n",
    "**A deeper head gives the gradients more flexibility to reshape the earlier layers.**\n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\text{features}_{\\text{ImageNet}} \\longrightarrow \\text{MLP head} \\longrightarrow \\text{medical classes}\n",
    "$$\n",
    "\n",
    "This MLP acts as a **transition function** while the backbone slowly moves from ImageNet space to medical-image space.\n",
    "\n",
    "Without this transition, fine-tuning could be unstable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.3. Medical datasets are small ‚Üí dropout helps in the head**\n",
    "\n",
    "Medical datasets are tiny:\n",
    "\n",
    "* 300 x-rays\n",
    "* 500 CT slices\n",
    "* 200‚Äì1000 dermoscopy images\n",
    "* 800 ultrasound clips\n",
    "\n",
    "So the danger is **overfitting**.\n",
    "\n",
    "Dropout in the head:\n",
    "\n",
    "* does *not* hurt performance much,\n",
    "* regularizes the discriminative layers,\n",
    "* prevents the classifier from memorizing noise.\n",
    "\n",
    "Backbone dropout is dangerous; head dropout is safe.\n",
    "\n",
    "So the design:\n",
    "\n",
    "Dropout ‚Üí Linear ‚Üí ReLU ‚Üí Dropout ‚Üí Linear\n",
    "\n",
    "is literally **standard medical deep learning practice**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.4. For medical grayscale images, the first convolution still benefits from RGB pretraining**\n",
    "\n",
    "Even though your input is grayscale, you can feed it as 3-channel replicated:\n",
    "\n",
    "$$\\text{gray} \\rightarrow [\\text{gray}, \\text{gray}, \\text{gray}].$$\n",
    "\n",
    "The CNN still leverages pretrained filters (edges, blobs, corners) because those early filters detect structure, not color.\n",
    "\n",
    "Color-specific filters (like ‚Äúred vs green‚Äù) will be re-learned during fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.5. Why you *don‚Äôt* want a very deep head**\n",
    "\n",
    "Even in medical imaging:\n",
    "\n",
    "* If head is *too deep*, you reduce the influence of pretrained layers.\n",
    "* Overfitting increases dramatically.\n",
    "* Optimization becomes harder.\n",
    "* Gradients may get stuck in the head and propagate less to the backbone.\n",
    "\n",
    "So you want a **light** MLP head, not a deep one.\n",
    "\n",
    "Typical optimal medical head:\n",
    "\n",
    "$$\n",
    "1280 \\rightarrow 512 \\rightarrow 128 \\rightarrow C\n",
    "$$\n",
    "\n",
    "or smaller.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.6. Why Xception/EfficientNet-style two-layer heads are very popular in medical papers**\n",
    "\n",
    "Because they combine:\n",
    "\n",
    "* pretrained backbone (generalize well),\n",
    "* shallow MLP head (task-specific adaptation),\n",
    "* dropout (regularization).\n",
    "\n",
    "This is especially useful when extracting subtle patterns like:\n",
    "\n",
    "* lung opacities\n",
    "* microcalcifications\n",
    "* lesions\n",
    "* tumor boundaries\n",
    "\n",
    "A single linear classifier is sometimes too weak for these.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.7. Summary: When deeper heads make sense**\n",
    "\n",
    "### Deeper head **not useful**\n",
    "\n",
    "Natural images ‚Üí natural images\n",
    "(small dataset or large dataset)\n",
    "\n",
    "### Deeper head **useful**\n",
    "\n",
    "When **domain shift is large**, e.g.:\n",
    "\n",
    "* grayscale medical\n",
    "* satellite ‚Üí natural\n",
    "* thermal ‚Üí RGB\n",
    "* microscopy ‚Üí natural\n",
    "* radiographs ‚Üí ImageNet\n",
    "\n",
    "because the head acts like a **nonlinear adapter** that helps the backbone transition from ImageNet features to the new domain.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9.8. Your Xception-inspired design makes complete sense**\n",
    "\n",
    "Your architecture:\n",
    "\n",
    "* EfficientNet-B0 backbone ( pretrained ‚Üí gives strong generic features )\n",
    "* Linear(1280‚Üí128) + ReLU\n",
    "* 2√ó Dropout\n",
    "* Linear(128‚Üí3)\n",
    "* Full fine-tuning\n",
    "\n",
    "is exactly how **most medical classification models** are built.\n",
    "\n",
    "It's a small, safe, regularized MLP head that acts as an adapter while the backbone is fully tuned.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
