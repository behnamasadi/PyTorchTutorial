{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. List of Available Pretrained Models**\n",
    "\n",
    "#### **1.1 Classification Models**\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "\n",
    "# List all classification models\n",
    "print(dir(models))\n",
    "```\n",
    "\n",
    "**Common pretrained classification models**:\n",
    "\n",
    "| Model Family | Model Names                                                  |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| VGG          | `vgg11`, `vgg13`, `vgg16`, `vgg19`, and their `_bn` variants |\n",
    "| ResNet       | `resnet18`, `resnet34`, `resnet50`, `resnet101`, `resnet152` |\n",
    "| DenseNet     | `densenet121`, `densenet161`, `densenet169`, `densenet201`   |\n",
    "| MobileNet    | `mobilenet_v2`, `mobilenet_v3_large`, `mobilenet_v3_small`   |\n",
    "| EfficientNet | `efficientnet_b0` to `efficientnet_b7`                       |\n",
    "| ViT          | `vit_b_16`, `vit_b_32`, `vit_l_16`, etc.                     |\n",
    "| ConvNeXt     | `convnext_tiny`, `convnext_base`, etc.                       |\n",
    "| RegNet       | `regnet_y_400mf`, `regnet_y_1_6gf`, etc.                     |\n",
    "| SqueezeNet   | `squeezenet1_0`, `squeezenet1_1`                             |\n",
    "\n",
    "---\n",
    "\n",
    "#### **1.2 Segmentation Models**\n",
    "\n",
    "Available under `torchvision.models.segmentation`:\n",
    "\n",
    "```python\n",
    "from torchvision.models import segmentation\n",
    "print(dir(segmentation))\n",
    "```\n",
    "\n",
    "**Popular segmentation models**:\n",
    "\n",
    "* `fcn_resnet50`\n",
    "* `fcn_resnet101`\n",
    "* `deeplabv3_resnet50`\n",
    "* `deeplabv3_resnet101`\n",
    "* `lraspp_mobilenet_v3_large`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2.Structured of Network in and Modular Components**\n",
    "Models are often structured in modular components referred to as **Backbone**, **Neck**, and **Head**. These components organize how features are extracted, refined, and used for predictions.\n",
    "\n",
    "Here's a breakdown of what each part typically does, along with related components:\n",
    "\n",
    "---\n",
    "\n",
    "####  2.1. **Backbone** ‚Äì Feature Extractor\n",
    "\n",
    "**What it is**:\n",
    "The **backbone** is the main feature extractor. It takes the raw input (e.g., an image) and outputs high-level features.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* **ResNet**, **VGG**, **EfficientNet**, **ViT** (Vision Transformer), **ConvNeXt**\n",
    "* Trained on datasets like ImageNet for classification\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* A feature map with reduced spatial resolution but rich semantic content (e.g., shape `[B, C, H/32, W/32]`)\n",
    "\n",
    "**Usage**:\n",
    "\n",
    "* Used across tasks: classification, detection, segmentation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "####  2.2. **Neck** ‚Äì Feature Refinement / Aggregation\n",
    "\n",
    "**What it is**:\n",
    "The **neck** connects the backbone to the head. It processes and refines feature maps‚Äîoften enhancing multi-scale features or fusing spatial information.\n",
    "\n",
    "**Common types**:\n",
    "\n",
    "* **FPN (Feature Pyramid Network)**: Combines features at different resolutions\n",
    "* **BiFPN (EfficientDet)**: Bidirectional FPN\n",
    "* **PANet**: For better path aggregation\n",
    "* **Transformer Encoders**: As necks in hybrid models\n",
    "\n",
    "**Why use it**:\n",
    "\n",
    "* Helps the model detect objects of different sizes\n",
    "* Improves information flow between layers\n",
    "\n",
    "---\n",
    "\n",
    "####  2.3. **Head** ‚Äì Task-Specific Prediction\n",
    "\n",
    "**What it is**:\n",
    "The **head** converts features into outputs (e.g., class labels, bounding boxes, masks).\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* **Classification head**: `Linear ‚Üí Softmax`\n",
    "* **Detection head** (e.g., YOLO): Predicts classes, bounding boxes, objectness score\n",
    "* **Segmentation head**: Upsamples and predicts pixel-wise labels\n",
    "* **Pose estimation head**: Keypoints or coordinates\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* Final predictions shaped for the task (e.g., `[B, num_classes]` for classification)\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ‚Üì\n",
    "[Preprocessor]\n",
    "    ‚Üì\n",
    "Backbone ‚Üí (Feature Maps)\n",
    "    ‚Üì\n",
    "Neck     ‚Üí (Enhanced Features)\n",
    "    ‚Üì\n",
    "Head     ‚Üí (Predictions: class/box/mask/etc.)\n",
    "    ‚Üì\n",
    "[Post-processing]\n",
    "    ‚Üì\n",
    "Final Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Determining the required input size**\n",
    "\n",
    "\n",
    "#### 3.1. **Use TorchVision Documentation or Model Summary**\n",
    "\n",
    "The [official PyTorch documentation](https://pytorch.org/vision/stable/models.html) lists **default input sizes** for each pretrained model.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  3.2 **Inspect the Model Internals**\n",
    "\n",
    "For most models, like `resnet18`, you can inspect how many times the input is halved due to pooling/stride:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "from torchinfo import summary  # pip install torchinfo\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####  3.3 **Render the Model Diagram (Visualization)**\n",
    "\n",
    "```python\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Input Image Size Different From The Pretrained Model Input**\n",
    "\n",
    "If your input image size is **different** from what the pretrained model expects, you have **two main options**, depending on your task:\n",
    "\n",
    "\n",
    "#### **4.1 Resize your input image to match the model**\n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match model's expected input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "#### **4.2 Adapt the model to your image size**\n",
    "\n",
    "**Advanced ‚Äî use only if resizing hurts performance or semantics.**\n",
    "\n",
    "- Use **adaptive pooling** in place of fixed `AvgPool2d` or `Linear` assumptions (e.g., in custom CNNs):\n",
    "\n",
    "```python\n",
    "nn.AdaptiveAvgPool2d((1, 1))  # Allows any input size\n",
    "```\n",
    "\n",
    "- **Replace classifier layers** if needed:\n",
    "\n",
    "If your model fails because of mismatched `in_features` in `Linear`, do:\n",
    "\n",
    "```python\n",
    "# Forward pass dummy input to find flattened size\n",
    "dummy_input = torch.randn(1, 3, your_H, your_W)\n",
    "features = model.features(dummy_input)  # or model.backbone for ResNet\n",
    "flattened_size = features.view(1, -1).shape[1]\n",
    "\n",
    "# Replace classifier accordingly\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(flattened_size, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, num_classes),\n",
    ")\n",
    "```\n",
    "\n",
    "When to use:\n",
    "\n",
    "* You're training **from scratch** or fine-tuning a model deeply.\n",
    "* Your data has **very different resolution** (e.g. medical images 512x512).\n",
    "* You want to **preserve spatial details** for segmentation/localization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Fine-tuning of a Pretrained Network Classifier**\n",
    "#### 5.1  Determining Parameters Network,\n",
    "First you have to know the parameters of your network, for instance:\n",
    "\n",
    "```python\n",
    "model_vgg19_bn = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "print(model_vgg19_bn)\n",
    "```\n",
    "\n",
    "This will give you the entire model features (covnet layer) + fully connected layer:\n",
    "\n",
    "```bash\n",
    "(features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU(inplace=True)\n",
    "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \n",
    "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (51): ReLU(inplace=True)\n",
    "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "(classifier): Sequential(\n",
    "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Dropout(p=0.5, inplace=False)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): Dropout(p=0.5, inplace=False)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "```python\n",
    "for param in model_vgg19_bn.features.parameters():\n",
    "    print(param.shape)\n",
    "```\n",
    "\n",
    "gives you features (covnet layer):\n",
    "\n",
    "```bash\n",
    "torch.Size([64, 3, 3, 3])\n",
    "torch.Size([64])\n",
    ".\n",
    ".\n",
    ".\n",
    "torch.Size([512, 512, 3, 3])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "```\n",
    "\n",
    "\n",
    "For **ResNet18**, we have only covnet layer and 1 fully connected layer, input is 512 and output 1000 classes:\n",
    "\n",
    "```python\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \", resnet18.fc.out_features)\n",
    "```\n",
    "\n",
    "```bash\n",
    "resnet18 input size:  512\n",
    "resnet18 output size:  1000\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Freeze **all feature extractor layers**\n",
    "\n",
    "```python\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "```\n",
    "\n",
    "This makes everything frozen (conv + bn + fc). Usually you then unfreeze the head.\n",
    "\n",
    "#### 5.3 Freeze only the convolutional backbone (leave `fc` trainable)\n",
    "\n",
    "```python\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True   # classifier\n",
    "for p in model.layer4.parameters():\n",
    "    p.requires_grad = False  # example: freeze last block\n",
    "```\n",
    "\n",
    "#### 5.4 Unfreeze some block (e.g. `layer4`)\n",
    "\n",
    "```python\n",
    "for p in model.layer4.parameters():\n",
    "    p.requires_grad = True\n",
    "```\n",
    "\n",
    "ResNet is organized like this:\n",
    "\n",
    "```\n",
    "model.conv1 -> model.bn1 -> model.layer1 -> model.layer2 -> model.layer3 -> model.layer4 -> model.fc\n",
    "```\n",
    "\n",
    "so you can target any block.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5 Safely replace the head\n",
    "\n",
    "ResNet18‚Äôs final FC (`model.fc`) outputs **1000 classes** (ImageNet).\n",
    "You replace it with your own classifier:\n",
    "\n",
    "```python\n",
    "num_features = model.fc.in_features   # 512 for resnet18\n",
    "num_classes = 5                       # example\n",
    "\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "```\n",
    "\n",
    "This is the cleanest and most common way. The rest of the model stays intact.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 5.6 Replace the Final Classifier\n",
    "\n",
    "\n",
    "```python\n",
    "num_classes = 3  # your problem\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(in_features, num_classes)  # new classifier layer\n",
    "```\n",
    "---\n",
    "                        \n",
    "#### 5.7 Optimizer setup (important!)\n",
    "\n",
    "If you froze parameters, make sure your optimizer only updates trainable ones:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad),\n",
    "    lr=1e-3\n",
    ")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. When to  Learn Feature maps**\n",
    "\n",
    "---\n",
    "\n",
    "**Transfer Learning Modes**\n",
    "\n",
    "| Mode                               | Freeze Feature Layers? | Fine-Tune Feature Layers?                     | Train Classifier? | When to Use                                                                                      |\n",
    "| ---------------------------------- | ---------------------- | --------------------------------------------- | ----------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **1. Feature Extraction (Frozen)** | ‚úÖ Yes                  | ‚ùå No                                          | ‚úÖ Yes             | When dataset is **small** and **similar** to ImageNet                                            |\n",
    "| **2. Fine-Tuning Last Block**      | üö´ No (partial)        | ‚úÖ Last layers only (e.g., `layer4` in ResNet) | ‚úÖ Yes             | When dataset is **moderate in size** and **domain-shifted**                                      |\n",
    "| **3. Full Fine-Tuning**            | ‚ùå No                   | ‚úÖ All conv layers                             | ‚úÖ Yes             | When dataset is **large** or **significantly different** from ImageNet (e.g. medical, satellite) |\n",
    "| **4. Training from Scratch**       | ‚ùå N/A                  | ‚úÖ All layers randomly initialized             | ‚úÖ Yes             | When you have a **huge custom dataset** and **no pretraining** is applicable                     |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. `torch.nn.Identity`**\n",
    "`torch.nn.Identity` is a simple module in PyTorch that **does nothing to its input** ‚Äî it just returns it unchanged. It's often used as a **placeholder** when you want to **remove or skip a layer** in a model (e.g., when doing ablation studies, or when modifying pretrained models).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "identity = nn.Identity()\n",
    "output = identity(input)\n",
    "```\n",
    "\n",
    "Here, `output` will be **exactly the same** as `input`.\n",
    "\n",
    "\n",
    "When do we need `nn.Identity`?\n",
    "\n",
    "####  7.1 **Ablation studies / removing layers**\n",
    "\n",
    "If you're testing the effect of removing a layer:\n",
    "\n",
    "```python\n",
    "self.dropout = nn.Dropout(p=0.5) if use_dropout else nn.Identity()\n",
    "```\n",
    "\n",
    "####  7.2 **Replace classifier head**\n",
    "\n",
    "If you load a pretrained model and want to keep everything except the final classification layer:\n",
    "\n",
    "```python\n",
    "model.fc = nn.Identity()  # For example in ResNet\n",
    "```\n",
    "\n",
    "####  7.3 **Skip connections or conditional architectures**\n",
    "\n",
    "If you want to optionally add a layer, but still keep the same forward pass logic:\n",
    "\n",
    "```python\n",
    "self.extra = nn.BatchNorm1d(256) if use_bn else nn.Identity()\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
