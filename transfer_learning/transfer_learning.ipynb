{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. List of Available Pretrained Models**\n",
    "\n",
    "\n",
    "#### **Classification Models**\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "\n",
    "# List all classification models\n",
    "print(dir(models))\n",
    "```\n",
    "\n",
    "**Common pretrained classification models**:\n",
    "\n",
    "| Model Family | Model Names                                                  |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| VGG          | `vgg11`, `vgg13`, `vgg16`, `vgg19`, and their `_bn` variants |\n",
    "| ResNet       | `resnet18`, `resnet34`, `resnet50`, `resnet101`, `resnet152` |\n",
    "| DenseNet     | `densenet121`, `densenet161`, `densenet169`, `densenet201`   |\n",
    "| MobileNet    | `mobilenet_v2`, `mobilenet_v3_large`, `mobilenet_v3_small`   |\n",
    "| EfficientNet | `efficientnet_b0` to `efficientnet_b7`                       |\n",
    "| ViT          | `vit_b_16`, `vit_b_32`, `vit_l_16`, etc.                     |\n",
    "| ConvNeXt     | `convnext_tiny`, `convnext_base`, etc.                       |\n",
    "| RegNet       | `regnet_y_400mf`, `regnet_y_1_6gf`, etc.                     |\n",
    "| SqueezeNet   | `squeezenet1_0`, `squeezenet1_1`                             |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Segmentation Models**\n",
    "\n",
    "Available under `torchvision.models.segmentation`:\n",
    "\n",
    "```python\n",
    "from torchvision.models import segmentation\n",
    "print(dir(segmentation))\n",
    "```\n",
    "\n",
    "**Popular segmentation models**:\n",
    "\n",
    "* `fcn_resnet50`\n",
    "* `fcn_resnet101`\n",
    "* `deeplabv3_resnet50`\n",
    "* `deeplabv3_resnet101`\n",
    "* `lraspp_mobilenet_v3_large`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. **Input and Output Size of Pretrained Network**\n",
    "\n",
    "Most models expect input images of size **`3 x H x W`**, where:\n",
    "\n",
    "* **3** is the number of color channels (RGB).\n",
    "* **H, W** must be at least **224 x 224** for classification models.\n",
    "* Some segmentation models require specific input sizes to avoid dimensionality issues due to downsampling.\n",
    "\n",
    "Here‚Äôs how you can check the **minimum input size** programmatically:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "model = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Dummy input to test\n",
    "x = torch.randn(1, 3, 224, 224)  # Batch size 1\n",
    "out = model(x)\n",
    "print(out.shape)  # Output: [1, 1000] -> 1000 classes\n",
    "```\n",
    "\n",
    "**General Input Size Guide**:\n",
    "\n",
    "| Model        | Recommended Input Size                         |\n",
    "| ------------ | ---------------------------------------------- |\n",
    "| VGG          | 224 √ó 224                                      |\n",
    "| ResNet       | 224 √ó 224                                      |\n",
    "| DenseNet     | 224 √ó 224                                      |\n",
    "| EfficientNet | Varies per version (e.g., B0: 224, B7: 600)    |\n",
    "| ViT          | Typically 224 √ó 224 (patch size varies)        |\n",
    "| DeepLabV3    | 513 √ó 513 (can vary, should be divisible by 8) |\n",
    "| FCN          | 224 √ó 224 or larger                            |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3. Determining the required input size**\n",
    "---\n",
    "\n",
    "#### 3.1. **Use TorchVision Documentation or Model Summary**\n",
    "\n",
    "The [official PyTorch documentation](https://pytorch.org/vision/stable/models.html) lists **default input sizes** for each pretrained model.\n",
    "\n",
    "---\n",
    "\n",
    "####  3.2 **Inspect the Model Internals**\n",
    "\n",
    "For most models, like `resnet18`, you can inspect how many times the input is halved due to pooling/stride:\n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "from torchinfo import summary  # pip install torchinfo\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "####  3.3 **Render the Model Diagram (Visualization)**\n",
    "\n",
    "```python\n",
    "input=torch.randn(size=[1,3,128,128])\n",
    "\n",
    "resnet18_graph=torchviz.make_dot(resnet18(input) ,dict(resnet18.named_parameters()))\n",
    "resnet18_graph.format='svg'\n",
    "resnet18_graph.save('images/resnet18_graph')\n",
    "resnet18_graph.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Input Image Size Different From The Pretrained Model Input**\n",
    "\n",
    "If your input image size is **different** from what the pretrained model expects, you have **two main options**, depending on your task:\n",
    "\n",
    "\n",
    "#### **4.1 Resize your input image to match the model**\n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match model's expected input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "#### **4.2 Adapt the model to your image size**\n",
    "\n",
    "**Advanced ‚Äî use only if resizing hurts performance or semantics.**\n",
    "\n",
    "- Use **adaptive pooling** in place of fixed `AvgPool2d` or `Linear` assumptions (e.g., in custom CNNs):\n",
    "\n",
    "```python\n",
    "nn.AdaptiveAvgPool2d((1, 1))  # Allows any input size\n",
    "```\n",
    "\n",
    "- **Replace classifier layers** if needed:\n",
    "\n",
    "If your model fails because of mismatched `in_features` in `Linear`, do:\n",
    "\n",
    "```python\n",
    "# Forward pass dummy input to find flattened size\n",
    "dummy_input = torch.randn(1, 3, your_H, your_W)\n",
    "features = model.features(dummy_input)  # or model.backbone for ResNet\n",
    "flattened_size = features.view(1, -1).shape[1]\n",
    "\n",
    "# Replace classifier accordingly\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(flattened_size, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, num_classes),\n",
    ")\n",
    "```\n",
    "\n",
    "When to use:\n",
    "\n",
    "* You're training **from scratch** or fine-tuning a model deeply.\n",
    "* Your data has **very different resolution** (e.g. medical images 512x512).\n",
    "* You want to **preserve spatial details** for segmentation/localization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Customizing of a Pretrained Network Classifier**\n",
    "\n",
    "First you have to know the parameters of your network, for instance:\n",
    "\n",
    "```python\n",
    "model_vgg19_bn = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "print(model_vgg19_bn)\n",
    "```\n",
    "\n",
    "This will give you the entire model features (covnet layer) + fully connected layer:\n",
    "\n",
    "```bash\n",
    "(features): Sequential(\n",
    "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU(inplace=True)\n",
    "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \n",
    "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (51): ReLU(inplace=True)\n",
    "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "(classifier): Sequential(\n",
    "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
    "    (1): ReLU(inplace=True)\n",
    "    (2): Dropout(p=0.5, inplace=False)\n",
    "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "    (4): ReLU(inplace=True)\n",
    "    (5): Dropout(p=0.5, inplace=False)\n",
    "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "```python\n",
    "for param in model_vgg19_bn.features.parameters():\n",
    "    print(param.shape)\n",
    "```\n",
    "\n",
    "gives you features (covnet layer):\n",
    "\n",
    "```bash\n",
    "torch.Size([64, 3, 3, 3])\n",
    "torch.Size([64])\n",
    ".\n",
    ".\n",
    ".\n",
    "torch.Size([512, 512, 3, 3])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "torch.Size([512])\n",
    "```\n",
    "\n",
    "\n",
    "For **ResNet18**, we have only covnet layer and 1 fully connected layer, input is 512 and output 1000 classes:\n",
    "\n",
    "```python\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "print(\"resnet18 input size: \", resnet18.fc.in_features)\n",
    "print(\"resnet18 output size: \", resnet18.fc.out_features)\n",
    "```\n",
    "\n",
    "```bash\n",
    "resnet18 input size:  512\n",
    "resnet18 output size:  1000\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet18\n",
    "\n",
    "Let‚Äôs say your dataset has **3 classes**, and you want to reuse a pretrained model for **image classification**.\n",
    "\n",
    "---\n",
    "\n",
    "**Freezing Feature Extractor**\n",
    "\n",
    "```python\n",
    "# Freeze all layers so we don't train them\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Replace the Final Classifier**\n",
    "\n",
    "\n",
    "```python\n",
    "num_classes = 3  # your problem\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(in_features, num_classes)  # new classifier layer\n",
    "```\n",
    "\n",
    "\n",
    "**Optimizer for parameters**\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(resnet18.fc.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### VGG19\n",
    "\n",
    "If you're using **VGG19**, it would look like this instead:\n",
    "\n",
    "```python\n",
    "model_vgg19_bn = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1)\n",
    "for param in model_vgg19_bn.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "```\n",
    "\n",
    "**Optimizer for parameters**\n",
    "\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "OR if you only want to train the last layer (as in transfer learning):\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.classifier[6].parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "- `model.classifier` is a Sequential block of layers.\n",
    "- `model.classifier[6]` is the last Linear layer you replaced.  (preferred)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Train the Model**\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    for images, labels in train_loader:  # assume you have train_loader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optional: Fine-Tune Some Layers**\n",
    "\n",
    "Instead of freezing all feature layers, you can unfreeze a few:\n",
    "\n",
    "```python\n",
    "# Unfreeze last few layers of ResNet\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name:  # last block of ResNet\n",
    "        param.requires_grad = True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tips for Transfer Learning\n",
    "\n",
    "1. **Classification**\n",
    "\n",
    "   * Replace the last linear layer (`model.fc` for ResNet, `model.classifier` for VGG) with:\n",
    "\n",
    "     ```python\n",
    "     model.fc = nn.Linear(in_features, num_classes)\n",
    "     ```\n",
    "   * Freeze earlier layers:\n",
    "\n",
    "     ```python\n",
    "     for param in model.parameters():\n",
    "         param.requires_grad = False\n",
    "     ```\n",
    "\n",
    "2. **Segmentation**\n",
    "\n",
    "   * Output has shape `[B, C, H, W]`, where `C = num_classes`\n",
    "   * You can replace the classifier head:\n",
    "\n",
    "     ```python\n",
    "     model.classifier[4] = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "     ```\n",
    "\n",
    "Would you like an example of transfer learning for classification and segmentation with a specific model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "1. **Feature extraction** = freeze everything, train new classifier head.\n",
    "2. **Fine-tuning** = unfreeze some or all layers, retrain using your tumor dataset to adapt its internal filters to your domain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When to  Learn Feature maps**\n",
    "\n",
    "---\n",
    "\n",
    "**Transfer Learning Modes**\n",
    "\n",
    "| Mode                               | Freeze Feature Layers? | Fine-Tune Feature Layers?                     | Train Classifier? | When to Use                                                                                      |\n",
    "| ---------------------------------- | ---------------------- | --------------------------------------------- | ----------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **1. Feature Extraction (Frozen)** | ‚úÖ Yes                  | ‚ùå No                                          | ‚úÖ Yes             | When dataset is **small** and **similar** to ImageNet                                            |\n",
    "| **2. Fine-Tuning Last Block**      | üö´ No (partial)        | ‚úÖ Last layers only (e.g., `layer4` in ResNet) | ‚úÖ Yes             | When dataset is **moderate in size** and **domain-shifted**                                      |\n",
    "| **3. Full Fine-Tuning**            | ‚ùå No                   | ‚úÖ All conv layers                             | ‚úÖ Yes             | When dataset is **large** or **significantly different** from ImageNet (e.g. medical, satellite) |\n",
    "| **4. Training from Scratch**       | ‚ùå N/A                  | ‚úÖ All layers randomly initialized             | ‚úÖ Yes             | When you have a **huge custom dataset** and **no pretraining** is applicable                     |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß How to Control It in Code\n",
    "\n",
    "### ‚úÖ 1. **Feature Extraction Only**:\n",
    "\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace and train classifier only\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **Fine-Tune Last Feature Block Only (ResNet)**\n",
    "\n",
    "```python\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# optimizer with fine-tuned + classifier params\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **Full Fine-Tuning**\n",
    "\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Best Practices\n",
    "\n",
    "* ‚úÖ **Small dataset?** ‚Üí Feature extraction only\n",
    "* ‚úÖ **Same domain?** ‚Üí Freeze early layers, fine-tune late ones\n",
    "* ‚úÖ **Different domain or high resolution?** ‚Üí Fine-tune more (or all)\n",
    "* ‚úÖ **Training slow?** ‚Üí Freeze more layers, reduce learning rate\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a utility function like `set_trainable_layers(model, mode='feature', last_block='layer4')` to automate this logic for ResNet or VGG?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
