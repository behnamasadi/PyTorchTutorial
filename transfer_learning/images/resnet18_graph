digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	134503198493744 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	134504159814624 [label=AddmmBackward0]
	134504160027568 -> 134504159814624
	134503198503744 [label="fc.bias
 (1000)" fillcolor=lightblue]
	134503198503744 -> 134504160027568
	134504160027568 [label=AccumulateGrad]
	134504159809776 -> 134504159814624
	134504159809776 [label=ViewBackward0]
	134507860163264 -> 134504159809776
	134507860163264 [label=MeanBackward1]
	134504160031216 -> 134507860163264
	134504160031216 [label=ReluBackward0]
	134507860151312 -> 134504160031216
	134507860151312 [label=AddBackward0]
	134507860162160 -> 134507860151312
	134507860162160 [label=NativeBatchNormBackward0]
	134504159639536 -> 134507860162160
	134504159639536 [label=ConvolutionBackward0]
	134504159632096 -> 134504159639536
	134504159632096 [label=ReluBackward0]
	134504159636800 -> 134504159632096
	134504159636800 [label=NativeBatchNormBackward0]
	134504159322144 -> 134504159636800
	134504159322144 [label=ConvolutionBackward0]
	134504164606784 -> 134504159322144
	134504164606784 [label=ReluBackward0]
	134503202241600 -> 134504164606784
	134503202241600 [label=AddBackward0]
	134503202234736 -> 134503202241600
	134503202234736 [label=NativeBatchNormBackward0]
	134503202235120 -> 134503202234736
	134503202235120 [label=ConvolutionBackward0]
	134503202234688 -> 134503202235120
	134503202234688 [label=ReluBackward0]
	134503202237328 -> 134503202234688
	134503202237328 [label=NativeBatchNormBackward0]
	134503202242224 -> 134503202237328
	134503202242224 [label=ConvolutionBackward0]
	134503202242176 -> 134503202242224
	134503202242176 [label=ReluBackward0]
	134503202235504 -> 134503202242176
	134503202235504 [label=AddBackward0]
	134503202235024 -> 134503202235504
	134503202235024 [label=NativeBatchNormBackward0]
	134503202239680 -> 134503202235024
	134503202239680 [label=ConvolutionBackward0]
	134503202158000 -> 134503202239680
	134503202158000 [label=ReluBackward0]
	134503202157616 -> 134503202158000
	134503202157616 [label=NativeBatchNormBackward0]
	134503202158336 -> 134503202157616
	134503202158336 [label=ConvolutionBackward0]
	134503202241648 -> 134503202158336
	134503202241648 [label=ReluBackward0]
	134503202159392 -> 134503202241648
	134503202159392 [label=AddBackward0]
	134503202157904 -> 134503202159392
	134503202157904 [label=NativeBatchNormBackward0]
	134503202152816 -> 134503202157904
	134503202152816 [label=ConvolutionBackward0]
	134503202152624 -> 134503202152816
	134503202152624 [label=ReluBackward0]
	134503202160496 -> 134503202152624
	134503202160496 [label=NativeBatchNormBackward0]
	134503202150656 -> 134503202160496
	134503202150656 [label=ConvolutionBackward0]
	134503202487696 -> 134503202150656
	134503202487696 [label=ReluBackward0]
	134503202487792 -> 134503202487696
	134503202487792 [label=AddBackward0]
	134503202487984 -> 134503202487792
	134503202487984 [label=NativeBatchNormBackward0]
	134503202487888 -> 134503202487984
	134503202487888 [label=ConvolutionBackward0]
	134503202487024 -> 134503202487888
	134503202487024 [label=ReluBackward0]
	134503202486592 -> 134503202487024
	134503202486592 [label=NativeBatchNormBackward0]
	134503202486400 -> 134503202486592
	134503202486400 [label=ConvolutionBackward0]
	134503202487216 -> 134503202486400
	134503202487216 [label=ReluBackward0]
	134503202484432 -> 134503202487216
	134503202484432 [label=AddBackward0]
	134503202485296 -> 134503202484432
	134503202485296 [label=NativeBatchNormBackward0]
	134503202481408 -> 134503202485296
	134503202481408 [label=ConvolutionBackward0]
	134503202484768 -> 134503202481408
	134503202484768 [label=ReluBackward0]
	134503202484192 -> 134503202484768
	134503202484192 [label=NativeBatchNormBackward0]
	134503202482032 -> 134503202484192
	134503202482032 [label=ConvolutionBackward0]
	134503202483664 -> 134503202482032
	134503202483664 [label=ReluBackward0]
	134503202482176 -> 134503202483664
	134503202482176 [label=AddBackward0]
	134503202481744 -> 134503202482176
	134503202481744 [label=NativeBatchNormBackward0]
	134503202480976 -> 134503202481744
	134503202480976 [label=ConvolutionBackward0]
	134503202479248 -> 134503202480976
	134503202479248 [label=ReluBackward0]
	134503202480448 -> 134503202479248
	134503202480448 [label=NativeBatchNormBackward0]
	134503202479104 -> 134503202480448
	134503202479104 [label=ConvolutionBackward0]
	134503202478336 -> 134503202479104
	134503202478336 [label=ReluBackward0]
	134503202480112 -> 134503202478336
	134503202480112 [label=AddBackward0]
	134503202479008 -> 134503202480112
	134503202479008 [label=NativeBatchNormBackward0]
	134503202478960 -> 134503202479008
	134503202478960 [label=ConvolutionBackward0]
	134503202480016 -> 134503202478960
	134503202480016 [label=ReluBackward0]
	134503202490784 -> 134503202480016
	134503202490784 [label=NativeBatchNormBackward0]
	134503202493232 -> 134503202490784
	134503202493232 [label=ConvolutionBackward0]
	134503202480160 -> 134503202493232
	134503202480160 [label=MaxPool2DWithIndicesBackward0]
	134503202494000 -> 134503202480160
	134503202494000 [label=ReluBackward0]
	134503202492704 -> 134503202494000
	134503202492704 [label=NativeBatchNormBackward0]
	134503202491648 -> 134503202492704
	134503202491648 [label=ConvolutionBackward0]
	134503202493952 -> 134503202491648
	134503196665056 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	134503196665056 -> 134503202493952
	134503202493952 [label=AccumulateGrad]
	134503202492320 -> 134503202492704
	134503196664976 [label="bn1.weight
 (64)" fillcolor=lightblue]
	134503196664976 -> 134503202492320
	134503202492320 [label=AccumulateGrad]
	134503202493184 -> 134503202492704
	134503196663856 [label="bn1.bias
 (64)" fillcolor=lightblue]
	134503196663856 -> 134503202493184
	134503202493184 [label=AccumulateGrad]
	134503202492992 -> 134503202493232
	134503198579664 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	134503198579664 -> 134503202492992
	134503202492992 [label=AccumulateGrad]
	134503202489344 -> 134503202490784
	134503198579504 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	134503198579504 -> 134503202489344
	134503202489344 [label=AccumulateGrad]
	134503202494288 -> 134503202490784
	134503198579744 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	134503198579744 -> 134503202494288
	134503202494288 [label=AccumulateGrad]
	134503202478624 -> 134503202478960
	134503198491104 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	134503198491104 -> 134503202478624
	134503202478624 [label=AccumulateGrad]
	134503202477232 -> 134503202479008
	134503198491184 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	134503198491184 -> 134503202477232
	134503202477232 [label=AccumulateGrad]
	134503202478816 -> 134503202479008
	134503198491584 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	134503198491584 -> 134503202478816
	134503202478816 [label=AccumulateGrad]
	134503202480160 -> 134503202480112
	134503202480304 -> 134503202479104
	134503198492224 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	134503198492224 -> 134503202480304
	134503202480304 [label=AccumulateGrad]
	134503202480496 -> 134503202480448
	134503198492144 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	134503198492144 -> 134503202480496
	134503202480496 [label=AccumulateGrad]
	134503202480400 -> 134503202480448
	134503198492304 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	134503198492304 -> 134503202480400
	134503202480400 [label=AccumulateGrad]
	134503202480592 -> 134503202480976
	134503198495264 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	134503198495264 -> 134503202480592
	134503202480592 [label=AccumulateGrad]
	134503202479776 -> 134503202481744
	134503198495024 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	134503198495024 -> 134503202479776
	134503202479776 [label=AccumulateGrad]
	134503202479344 -> 134503202481744
	134503198495344 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	134503198495344 -> 134503202479344
	134503202479344 [label=AccumulateGrad]
	134503202478336 -> 134503202482176
	134503202479680 -> 134503202482032
	134503198506304 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	134503198506304 -> 134503202479680
	134503202479680 [label=AccumulateGrad]
	134503202479488 -> 134503202484192
	134503198506224 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	134503198506224 -> 134503202479488
	134503202479488 [label=AccumulateGrad]
	134503202483088 -> 134503202484192
	134503198506544 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	134503198506544 -> 134503202483088
	134503202483088 [label=AccumulateGrad]
	134503202482944 -> 134503202481408
	134503198491264 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	134503198491264 -> 134503202482944
	134503202482944 [label=AccumulateGrad]
	134503202483328 -> 134503202485296
	134503198490944 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	134503198490944 -> 134503202483328
	134503202483328 [label=AccumulateGrad]
	134503202482800 -> 134503202485296
	134503198491344 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	134503198491344 -> 134503202482800
	134503202482800 [label=AccumulateGrad]
	134503202484912 -> 134503202484432
	134503202484912 [label=NativeBatchNormBackward0]
	134503202483472 -> 134503202484912
	134503202483472 [label=ConvolutionBackward0]
	134503202483664 -> 134503202483472
	134503202481984 -> 134503202483472
	134503198493984 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	134503198493984 -> 134503202481984
	134503202481984 [label=AccumulateGrad]
	134503202482608 -> 134503202484912
	134503198495824 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	134503198495824 -> 134503202482608
	134503202482608 [label=AccumulateGrad]
	134503202483232 -> 134503202484912
	134503198495904 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	134503198495904 -> 134503202483232
	134503202483232 [label=AccumulateGrad]
	134503202485968 -> 134503202486400
	134503198492624 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	134503198492624 -> 134503202485968
	134503202485968 [label=AccumulateGrad]
	134503202485920 -> 134503202486592
	134503198492384 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	134503198492384 -> 134503202485920
	134503202485920 [label=AccumulateGrad]
	134503202487072 -> 134503202486592
	134503198492704 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	134503198492704 -> 134503202487072
	134503202487072 [label=AccumulateGrad]
	134503202486784 -> 134503202487888
	134503198493504 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	134503198493504 -> 134503202486784
	134503202486784 [label=AccumulateGrad]
	134503202486496 -> 134503202487984
	134503198493424 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	134503198493424 -> 134503202486496
	134503202486496 [label=AccumulateGrad]
	134503202488128 -> 134503202487984
	134503198493584 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	134503198493584 -> 134503202488128
	134503202488128 [label=AccumulateGrad]
	134503202487216 -> 134503202487792
	134503202480640 -> 134503202150656
	134503198495664 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	134503198495664 -> 134503202480640
	134503202480640 [label=AccumulateGrad]
	134503202153152 -> 134503202160496
	134503198495584 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	134503198495584 -> 134503202153152
	134503202153152 [label=AccumulateGrad]
	134503202150080 -> 134503202160496
	134503198495744 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	134503198495744 -> 134503202150080
	134503202150080 [label=AccumulateGrad]
	134503202152720 -> 134503202152816
	134503198499024 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	134503198499024 -> 134503202152720
	134503202152720 [label=AccumulateGrad]
	134503202158432 -> 134503202157904
	134503198498944 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	134503198498944 -> 134503202158432
	134503202158432 [label=AccumulateGrad]
	134503202158096 -> 134503202157904
	134503198499344 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	134503198499344 -> 134503202158096
	134503202158096 [label=AccumulateGrad]
	134503202155168 -> 134503202159392
	134503202155168 [label=NativeBatchNormBackward0]
	134503202149120 -> 134503202155168
	134503202149120 [label=ConvolutionBackward0]
	134503202487696 -> 134503202149120
	134503202486928 -> 134503202149120
	134503198492464 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	134503198492464 -> 134503202486928
	134503202486928 [label=AccumulateGrad]
	134503202157184 -> 134503202155168
	134503198491904 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	134503198491904 -> 134503202157184
	134503202157184 [label=AccumulateGrad]
	134503202157088 -> 134503202155168
	134503198492544 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	134503198492544 -> 134503202157088
	134503202157088 [label=AccumulateGrad]
	134503202159200 -> 134503202158336
	134503198506064 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	134503198506064 -> 134503202159200
	134503202159200 [label=AccumulateGrad]
	134503202157664 -> 134503202157616
	134503198505984 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	134503198505984 -> 134503202157664
	134503202157664 [label=AccumulateGrad]
	134503202157952 -> 134503202157616
	134503198506384 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	134503198506384 -> 134503202157952
	134503202157952 [label=AccumulateGrad]
	134503202157424 -> 134503202239680
	134503198501504 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	134503198501504 -> 134503202157424
	134503202157424 [label=AccumulateGrad]
	134503202232096 -> 134503202235024
	134503198501264 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	134503198501264 -> 134503202232096
	134503202232096 [label=AccumulateGrad]
	134503202241792 -> 134503202235024
	134503198501744 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	134503198501744 -> 134503202241792
	134503202241792 [label=AccumulateGrad]
	134503202241648 -> 134503202235504
	134503202235648 -> 134503202242224
	134503198504624 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	134503198504624 -> 134503202235648
	134503202235648 [label=AccumulateGrad]
	134503202235312 -> 134503202237328
	134503198504544 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	134503198504544 -> 134503202235312
	134503202235312 [label=AccumulateGrad]
	134503202237376 -> 134503202237328
	134503198504704 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	134503198504704 -> 134503202237376
	134503202237376 [label=AccumulateGrad]
	134503202234928 -> 134503202235120
	134503198505264 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	134503198505264 -> 134503202234928
	134503202234928 [label=AccumulateGrad]
	134503202235792 -> 134503202234736
	134503198505184 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	134503198505184 -> 134503202235792
	134503202235792 [label=AccumulateGrad]
	134503202233632 -> 134503202234736
	134503198505344 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	134503198505344 -> 134503202233632
	134503202233632 [label=AccumulateGrad]
	134503202234592 -> 134503202241600
	134503202234592 [label=NativeBatchNormBackward0]
	134503202235408 -> 134503202234592
	134503202235408 [label=ConvolutionBackward0]
	134503202242176 -> 134503202235408
	134503202234496 -> 134503202235408
	134503198502384 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	134503198502384 -> 134503202234496
	134503202234496 [label=AccumulateGrad]
	134503202234544 -> 134503202234592
	134503198502464 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	134503198502464 -> 134503202234544
	134503202234544 [label=AccumulateGrad]
	134503202234832 -> 134503202234592
	134503198502544 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	134503198502544 -> 134503202234832
	134503202234832 [label=AccumulateGrad]
	134504159329680 -> 134504159322144
	134503198504224 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	134503198504224 -> 134504159329680
	134504159329680 [label=AccumulateGrad]
	134504159327184 -> 134504159636800
	134503198504144 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	134503198504144 -> 134504159327184
	134504159327184 [label=AccumulateGrad]
	134504159329344 -> 134504159636800
	134503198503344 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	134503198503344 -> 134504159329344
	134504159329344 [label=AccumulateGrad]
	134504159632144 -> 134504159639536
	134503198502704 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	134503198502704 -> 134504159632144
	134504159632144 [label=AccumulateGrad]
	134504159638336 -> 134507860162160
	134503198503984 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	134503198503984 -> 134504159638336
	134504159638336 [label=AccumulateGrad]
	134504159629504 -> 134507860162160
	134503198502784 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	134503198502784 -> 134504159629504
	134504159629504 [label=AccumulateGrad]
	134504164606784 -> 134507860151312
	134504159821296 -> 134504159814624
	134504159821296 [label=TBackward0]
	134507860159904 -> 134504159821296
	134503198503264 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	134503198503264 -> 134507860159904
	134507860159904 [label=AccumulateGrad]
	134504159814624 -> 134503198493744
}
