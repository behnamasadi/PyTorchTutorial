digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	137689255394624 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	137689264102016 [label=AddmmBackward0]
	137689435337872 -> 137689264102016
	137689253829824 [label="fc.bias
 (1000)" fillcolor=lightblue]
	137689253829824 -> 137689435337872
	137689435337872 [label=AccumulateGrad]
	137689264100864 -> 137689264102016
	137689264100864 [label=ViewBackward0]
	137689264101248 -> 137689264100864
	137689264101248 [label=MeanBackward1]
	137689264102352 -> 137689264101248
	137689264102352 [label=ReluBackward0]
	137689298205040 -> 137689264102352
	137689298205040 [label=AddBackward0]
	137689298198128 -> 137689298205040
	137689298198128 [label=NativeBatchNormBackward0]
	137689298199856 -> 137689298198128
	137689298199856 [label=ConvolutionBackward0]
	137689298198944 -> 137689298199856
	137689298198944 [label=ReluBackward0]
	137689298193136 -> 137689298198944
	137689298193136 [label=NativeBatchNormBackward0]
	137689298204128 -> 137689298193136
	137689298204128 [label=ConvolutionBackward0]
	137689298193856 -> 137689298204128
	137689298193856 [label=ReluBackward0]
	137689298200432 -> 137689298193856
	137689298200432 [label=AddBackward0]
	137689298201104 -> 137689298200432
	137689298201104 [label=NativeBatchNormBackward0]
	137689298204896 -> 137689298201104
	137689298204896 [label=ConvolutionBackward0]
	137689298198608 -> 137689298204896
	137689298198608 [label=ReluBackward0]
	137689298201584 -> 137689298198608
	137689298201584 [label=NativeBatchNormBackward0]
	137689298200576 -> 137689298201584
	137689298200576 [label=ConvolutionBackward0]
	137689298204464 -> 137689298200576
	137689298204464 [label=ReluBackward0]
	137689447101632 -> 137689298204464
	137689447101632 [label=AddBackward0]
	137689298974944 -> 137689447101632
	137689298974944 [label=NativeBatchNormBackward0]
	137689298970816 -> 137689298974944
	137689298970816 [label=ConvolutionBackward0]
	137689298967024 -> 137689298970816
	137689298967024 [label=ReluBackward0]
	137689298976048 -> 137689298967024
	137689298976048 [label=NativeBatchNormBackward0]
	137689298976624 -> 137689298976048
	137689298976624 [label=ConvolutionBackward0]
	137689298971296 -> 137689298976624
	137689298971296 [label=ReluBackward0]
	137689298973168 -> 137689298971296
	137689298973168 [label=AddBackward0]
	137689298972064 -> 137689298973168
	137689298972064 [label=NativeBatchNormBackward0]
	137689298974656 -> 137689298972064
	137689298974656 [label=ConvolutionBackward0]
	137689298963760 -> 137689298974656
	137689298963760 [label=ReluBackward0]
	137689298965536 -> 137689298963760
	137689298965536 [label=NativeBatchNormBackward0]
	137689298965056 -> 137689298965536
	137689298965056 [label=ConvolutionBackward0]
	137689298971200 -> 137689298965056
	137689298971200 [label=ReluBackward0]
	137689298974464 -> 137689298971200
	137689298974464 [label=AddBackward0]
	137689298962128 -> 137689298974464
	137689298962128 [label=NativeBatchNormBackward0]
	137689298884768 -> 137689298962128
	137689298884768 [label=ConvolutionBackward0]
	137689298885488 -> 137689298884768
	137689298885488 [label=ReluBackward0]
	137689298890384 -> 137689298885488
	137689298890384 [label=NativeBatchNormBackward0]
	137689298890864 -> 137689298890384
	137689298890864 [label=ConvolutionBackward0]
	137689298885392 -> 137689298890864
	137689298885392 [label=ReluBackward0]
	137689298878576 -> 137689298885392
	137689298878576 [label=AddBackward0]
	137689298887024 -> 137689298878576
	137689298887024 [label=NativeBatchNormBackward0]
	137689298879200 -> 137689298887024
	137689298879200 [label=ConvolutionBackward0]
	137689298880016 -> 137689298879200
	137689298880016 [label=ReluBackward0]
	137689298880592 -> 137689298880016
	137689298880592 [label=NativeBatchNormBackward0]
	137689298881360 -> 137689298880592
	137689298881360 [label=ConvolutionBackward0]
	137689298890912 -> 137689298881360
	137689298890912 [label=ReluBackward0]
	137689298884192 -> 137689298890912
	137689298884192 [label=AddBackward0]
	137689298885536 -> 137689298884192
	137689298885536 [label=NativeBatchNormBackward0]
	137689298886592 -> 137689298885536
	137689298886592 [label=ConvolutionBackward0]
	137689298886832 -> 137689298886592
	137689298886832 [label=ReluBackward0]
	137689298878864 -> 137689298886832
	137689298878864 [label=NativeBatchNormBackward0]
	137689298882704 -> 137689298878864
	137689298882704 [label=ConvolutionBackward0]
	137689298884960 -> 137689298882704
	137689298884960 [label=ReluBackward0]
	137689298879104 -> 137689298884960
	137689298879104 [label=AddBackward0]
	137689298884480 -> 137689298879104
	137689298884480 [label=NativeBatchNormBackward0]
	137689298884240 -> 137689298884480
	137689298884240 [label=ConvolutionBackward0]
	137689298883616 -> 137689298884240
	137689298883616 [label=ReluBackward0]
	137689298891632 -> 137689298883616
	137689298891632 [label=NativeBatchNormBackward0]
	137689298883328 -> 137689298891632
	137689298883328 [label=ConvolutionBackward0]
	137689298879056 -> 137689298883328
	137689298879056 [label=MaxPool2DWithIndicesBackward0]
	137689298882848 -> 137689298879056
	137689298882848 [label=ReluBackward0]
	137689298890720 -> 137689298882848
	137689298890720 [label=NativeBatchNormBackward0]
	137689298882464 -> 137689298890720
	137689298882464 [label=ConvolutionBackward0]
	137689298889760 -> 137689298882464
	137689254661488 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	137689254661488 -> 137689298889760
	137689298889760 [label=AccumulateGrad]
	137689298882512 -> 137689298890720
	137689254674288 [label="bn1.weight
 (64)" fillcolor=lightblue]
	137689254674288 -> 137689298882512
	137689298882512 [label=AccumulateGrad]
	137689298883232 -> 137689298890720
	137689254673808 [label="bn1.bias
 (64)" fillcolor=lightblue]
	137689254673808 -> 137689298883232
	137689298883232 [label=AccumulateGrad]
	137689298883040 -> 137689298883328
	137689254670128 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137689254670128 -> 137689298883040
	137689298883040 [label=AccumulateGrad]
	137689298883424 -> 137689298891632
	137689254669248 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	137689254669248 -> 137689298883424
	137689298883424 [label=AccumulateGrad]
	137689298891776 -> 137689298891632
	137689254670768 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	137689254670768 -> 137689298891776
	137689298891776 [label=AccumulateGrad]
	137689298883712 -> 137689298884240
	137689255393744 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137689255393744 -> 137689298883712
	137689298883712 [label=AccumulateGrad]
	137689298884384 -> 137689298884480
	137689254670048 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	137689254670048 -> 137689298884384
	137689298884384 [label=AccumulateGrad]
	137689298884432 -> 137689298884480
	137689255386384 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	137689255386384 -> 137689298884432
	137689298884432 [label=AccumulateGrad]
	137689298879056 -> 137689298879104
	137689298878912 -> 137689298882704
	137689255393184 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137689255393184 -> 137689298878912
	137689298878912 [label=AccumulateGrad]
	137689298883376 -> 137689298878864
	137689255392144 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	137689255392144 -> 137689298883376
	137689298883376 [label=AccumulateGrad]
	137689298878528 -> 137689298878864
	137689255388544 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	137689255388544 -> 137689298878528
	137689298878528 [label=AccumulateGrad]
	137689298891536 -> 137689298886592
	137689255386624 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	137689255386624 -> 137689298891536
	137689298891536 [label=AccumulateGrad]
	137689298886496 -> 137689298885536
	137689255391744 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	137689255391744 -> 137689298886496
	137689298886496 [label=AccumulateGrad]
	137689298886160 -> 137689298885536
	137689255387584 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	137689255387584 -> 137689298886160
	137689298886160 [label=AccumulateGrad]
	137689298884960 -> 137689298884192
	137689298882416 -> 137689298881360
	137689253823024 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	137689253823024 -> 137689298882416
	137689298882416 [label=AccumulateGrad]
	137689298880880 -> 137689298880592
	137689253822704 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	137689253822704 -> 137689298880880
	137689298880880 [label=AccumulateGrad]
	137689298880256 -> 137689298880592
	137689253823184 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	137689253823184 -> 137689298880256
	137689298880256 [label=AccumulateGrad]
	137689298879824 -> 137689298879200
	137689253825024 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137689253825024 -> 137689298879824
	137689298879824 [label=AccumulateGrad]
	137689298887312 -> 137689298887024
	137689253824944 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	137689253824944 -> 137689298887312
	137689298887312 [label=AccumulateGrad]
	137689298887216 -> 137689298887024
	137689253824624 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	137689253824624 -> 137689298887216
	137689298887216 [label=AccumulateGrad]
	137689298878816 -> 137689298878576
	137689298878816 [label=NativeBatchNormBackward0]
	137689298966976 -> 137689298878816
	137689298966976 [label=ConvolutionBackward0]
	137689298890912 -> 137689298966976
	137689298890144 -> 137689298966976
	137689255392704 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	137689255392704 -> 137689298890144
	137689298890144 [label=AccumulateGrad]
	137689264101968 -> 137689298878816
	137689255388384 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	137689255388384 -> 137689264101968
	137689264101968 [label=AccumulateGrad]
	137689298887504 -> 137689298878816
	137689255381984 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	137689255381984 -> 137689298887504
	137689298887504 [label=AccumulateGrad]
	137689298887120 -> 137689298890864
	137689253823664 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137689253823664 -> 137689298887120
	137689298887120 [label=AccumulateGrad]
	137689298885776 -> 137689298890384
	137689253823344 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	137689253823344 -> 137689298885776
	137689298885776 [label=AccumulateGrad]
	137689298886304 -> 137689298890384
	137689253825424 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	137689253825424 -> 137689298886304
	137689298886304 [label=AccumulateGrad]
	137689298885440 -> 137689298884768
	137689253825584 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	137689253825584 -> 137689298885440
	137689298885440 [label=AccumulateGrad]
	137689298885200 -> 137689298962128
	137689253825824 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	137689253825824 -> 137689298885200
	137689298885200 [label=AccumulateGrad]
	137689298890624 -> 137689298962128
	137689253836224 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	137689253836224 -> 137689298890624
	137689298890624 [label=AccumulateGrad]
	137689298885392 -> 137689298974464
	137689298976576 -> 137689298965056
	137689253837984 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	137689253837984 -> 137689298976576
	137689298976576 [label=AccumulateGrad]
	137689298971104 -> 137689298965536
	137689253837824 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	137689253837824 -> 137689298971104
	137689298971104 [label=AccumulateGrad]
	137689298965200 -> 137689298965536
	137689253838064 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	137689253838064 -> 137689298965200
	137689298965200 [label=AccumulateGrad]
	137689298964192 -> 137689298974656
	137689253826624 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137689253826624 -> 137689298964192
	137689298964192 [label=AccumulateGrad]
	137689298971584 -> 137689298972064
	137689253838784 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	137689253838784 -> 137689298971584
	137689298971584 [label=AccumulateGrad]
	137689298972256 -> 137689298972064
	137689253826224 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	137689253826224 -> 137689298972256
	137689298972256 [label=AccumulateGrad]
	137689298961360 -> 137689298973168
	137689298961360 [label=NativeBatchNormBackward0]
	137689298971008 -> 137689298961360
	137689298971008 [label=ConvolutionBackward0]
	137689298971200 -> 137689298971008
	137689298973312 -> 137689298971008
	137689253836864 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	137689253836864 -> 137689298973312
	137689298973312 [label=AccumulateGrad]
	137689298975136 -> 137689298961360
	137689253837104 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	137689253837104 -> 137689298975136
	137689298975136 [label=AccumulateGrad]
	137689298973120 -> 137689298961360
	137689253837184 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	137689253837184 -> 137689298973120
	137689298973120 [label=AccumulateGrad]
	137689298970672 -> 137689298976624
	137689253834944 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137689253834944 -> 137689298970672
	137689298970672 [label=AccumulateGrad]
	137689298975904 -> 137689298976048
	137689253837504 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	137689253837504 -> 137689298975904
	137689298975904 [label=AccumulateGrad]
	137689298971776 -> 137689298976048
	137689253835104 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	137689253835104 -> 137689298971776
	137689298971776 [label=AccumulateGrad]
	137689298964624 -> 137689298970816
	137689253827024 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	137689253827024 -> 137689298964624
	137689298964624 [label=AccumulateGrad]
	137689298976672 -> 137689298974944
	137689253837664 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	137689253837664 -> 137689298976672
	137689298976672 [label=AccumulateGrad]
	137689298962368 -> 137689298974944
	137689253837904 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	137689253837904 -> 137689298962368
	137689298962368 [label=AccumulateGrad]
	137689298971296 -> 137689447101632
	137689298192320 -> 137689298200576
	137689253830224 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	137689253830224 -> 137689298192320
	137689298192320 [label=AccumulateGrad]
	137689298197888 -> 137689298201584
	137689253831424 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	137689253831424 -> 137689298197888
	137689298197888 [label=AccumulateGrad]
	137689298202448 -> 137689298201584
	137689253829264 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	137689253829264 -> 137689298202448
	137689298202448 [label=AccumulateGrad]
	137689298192464 -> 137689298204896
	137689253827344 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137689253827344 -> 137689298192464
	137689298192464 [label=AccumulateGrad]
	137689298198992 -> 137689298201104
	137689253829024 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	137689253829024 -> 137689298198992
	137689298198992 [label=AccumulateGrad]
	137689298204416 -> 137689298201104
	137689253827424 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	137689253827424 -> 137689298204416
	137689298204416 [label=AccumulateGrad]
	137689298201056 -> 137689298200432
	137689298201056 [label=NativeBatchNormBackward0]
	137689298192416 -> 137689298201056
	137689298192416 [label=ConvolutionBackward0]
	137689298204464 -> 137689298192416
	137689298200336 -> 137689298192416
	137689253836144 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	137689253836144 -> 137689298200336
	137689298200336 [label=AccumulateGrad]
	137689298199616 -> 137689298201056
	137689253834304 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	137689253834304 -> 137689298199616
	137689298199616 [label=AccumulateGrad]
	137689298202160 -> 137689298201056
	137689253834064 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	137689253834064 -> 137689298202160
	137689298202160 [label=AccumulateGrad]
	137689298198896 -> 137689298204128
	137689253827984 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137689253827984 -> 137689298198896
	137689298198896 [label=AccumulateGrad]
	137689298194624 -> 137689298193136
	137689253827664 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	137689253827664 -> 137689298194624
	137689298194624 [label=AccumulateGrad]
	137689298198848 -> 137689298193136
	137689253828384 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	137689253828384 -> 137689298198848
	137689298198848 [label=AccumulateGrad]
	137689298198080 -> 137689298199856
	137689253829184 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	137689253829184 -> 137689298198080
	137689298198080 [label=AccumulateGrad]
	137689298200528 -> 137689298198128
	137689253828864 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	137689253828864 -> 137689298200528
	137689298200528 [label=AccumulateGrad]
	137689298192080 -> 137689298198128
	137689253829104 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	137689253829104 -> 137689298192080
	137689298192080 [label=AccumulateGrad]
	137689298193856 -> 137689298205040
	137689264101824 -> 137689264102016
	137689264101824 [label=TBackward0]
	137689264110896 -> 137689264101824
	137689253829744 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	137689253829744 -> 137689264110896
	137689264110896 [label=AccumulateGrad]
	137689264102016 -> 137689255394624
}
