{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Neural Network Optimizers\n",
    " \n",
    " The cost function that we want to minimize in neural networks is diffeerence between the output of neural networks and expected output. This should be near zero. We have a set of training data and their corresponding output from a function that we don't have (an ideal image classifer for example). Just like regression that we might use linear regression or polynominal to approximate the underlying function that we try to estimate. \n",
    "\n",
    "Since computational graphs  can calculate any function, we use them to estimate the function that we dont have but we have some samples of that.  The parameters of the neural networks are $W$, $b$ but the parameters of the cost function are $X,W,b$ and outpput is $y_{nn}-y_{lable}$\n",
    "\n",
    "Iagine your network is only consist of only one weight and your input has only 1 dimension. $x$. cost function might look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../nn/images/cost_function.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This surface will show you if you choose a specific item from training set, $x=-0.5$ and you choose different $w$ what would be the value of error function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../nn/images/error_function_fixed_x.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This surface will show you if you set $w=-1.0$ for your model, what would be the value of error function for all $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='../nn/images/error_function_fixed_w.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we have a few samples from $x$ and we are looking for right $w$ to minimize the lost. If we have more than one dimension for $w$  (i.e. 2) we have to iterate over all of them (brute force) to find a right one which in practice is impossible. Also we don't have all values for $x$ and we have only few samples from that (imagine $x$ in higher dimension which could be photos of a dog, only know for some certain values in $x$ the output could be a dog) so in practice we have something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='../nn/images/few_x_cost_function.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, for this fixed $w$, for some of inputs, the output error is small and for others it is big, we need to find a new $w$ such that for all $x$ the loss function is small.\n",
    "\n",
    "So inorder to estimate the actuall unknown function that we have some sample from that in our training set, We select different number of hidden layer and size forthem. In our simple case we use some polynomial, so we will have a function $f(w,x)=error$\n",
    "\n",
    "We compute the derivative $f(w,x)$ w.r.t $w$, $\\frac{df}{dw}$. We set some random value for $w$, and plug all $x$ from our  training set into this derivative function: $\\frac{df}{dw}$. Then we make average over all values and update the $w$ based on gradient descent function:\n",
    "\n",
    "$w_{n+1}=w_{n} +\\text{learning_rate}* \\frac{1}{m}\\Sigma_{m}  \\frac{df(x)}{dw} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bad condition number ratio of between largest and smallest singular value of hessian\n",
    "\n",
    "high condition number\n",
    "\n",
    "\n",
    "math equations shapes of taco shell ill condition\n",
    "\n",
    "saddle point problem is more issue on high dmientional problem rader than local minima\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle - 2 \\left(4 - 2 x\\right) e^{- \\left(x - 2\\right)^{2} - \\left(y - 2\\right)^{2}} - 4 \\left(8 - 2 x\\right) e^{- \\left(x - 4\\right)^{2} - \\left(y - 4\\right)^{2}}$"
      ],
      "text/plain": [
       "                        2          2                         2          2\n",
       "               - (x - 2)  - (y - 2)                 - (x - 4)  - (y - 4) \n",
       "- 2⋅(4 - 2⋅x)⋅ℯ                      - 4⋅(8 - 2⋅x)⋅ℯ                     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "init_printing(use_latex=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "x,y,z=symbols('x,y,z')\n",
    "z=-( 4*exp(-(x-4)**2 - (y-4)**2)+2*exp(-(x-2)**2 - (y-2)**2) )\n",
    "z_x=diff(z,x)\n",
    "z_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle - 2 \\left(4 - 2 y\\right) e^{- \\left(x - 2\\right)^{2} - \\left(y - 2\\right)^{2}} - 4 \\left(8 - 2 y\\right) e^{- \\left(x - 4\\right)^{2} - \\left(y - 4\\right)^{2}}$"
      ],
      "text/plain": [
       "                        2          2                         2          2\n",
       "               - (x - 2)  - (y - 2)                 - (x - 4)  - (y - 4) \n",
       "- 2⋅(4 - 2⋅y)⋅ℯ                      - 4⋅(8 - 2⋅y)⋅ℯ                     "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_y=diff(z,y)\n",
    "z_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ z= -( 4 \\times e^{- ( (x-4)^2 +(y-4)^2 ) }+ 2 \\times e^{- ( (x-2)^2 +(y-2)^2 ) } )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian optimization\n",
    "Reminder: Taylor's theorem\n",
    "\n",
    "Let  $f: \\mathbb {R}^{n}  → \\mathbb {R}$\n",
    "\n",
    " \n",
    "\n",
    "$x=\\begin{bmatrix}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots \n",
    "\\\\x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\begin{align*}\n",
    "  f(\\textbf{x})= f(x_1,x_2, \\ldots, x_n).\n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "$\\begin{align*}\n",
    "  f(\\textbf{x}) \\approx f(\\textbf{a}) + Df(\\textbf{a})_{1\\times n} (\\textbf{x}-\\textbf{a})_{n\\times1}+\n",
    "  \\frac{1}{2} (\\textbf{x}-\\textbf{a})_{1\\times n}^T Hf(\\textbf{a})_{n\\times n} (\\textbf{x}-\\textbf{a})_{n\\times1}.\n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "\n",
    "The cost function can be approximated near a point $w$ by:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  C(w+\\Delta w) & = & C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\n",
    "  \\nonumber  & & + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\n",
    "    \\partial w_k} \\Delta w_k + \\ldots\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  C(w+\\Delta w) = C(w) + \\nabla C \\cdot \\Delta w +\n",
    "  \\frac{1}{2} \\Delta w^T H \\Delta w + \\ldots,\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$H$ is Hessian matrix, whose $jk$th is $\\partial^2\n",
    "C / \\partial w_j \\partial w_k$\n",
    "\n",
    "By taking derivative and set it to zero we would have:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\Delta w = -H^{-1} \\nabla C.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "\n",
    "$w_{n+1} = w_{n} -\\eta H^{-1} \\nabla C$\n",
    "\n",
    "\n",
    "Pros: There are theoretical and empirical results showing that Hessian methods converge on a minimum in fewer steps than standard gradient descent.\n",
    "\n",
    "Cons:  it's very difficult to apply in practice. Suppose you have a neural network with $10^7$ weights and biases. Then the corresponding Hessian matrix will contain $10^7\n",
    "\\times 10^7 = 10^{14}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum-based gradient descent\n",
    "Exponential moving average\n",
    "\n",
    "$\\begin{eqnarray}  & v_{n+1} = \\mu v_{n} - \\eta \\nabla C \\end{eqnarray}$\n",
    "  \n",
    "$\\begin{eqnarray}  w_{n+1} = w_{n}+v_{n+1}.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "Refs: [1](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average), [2](https://github.com/utkarshchawla/GradientDescentVariants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Learning rate optimizers\n",
    "They are able to learn more along one direction than another. Suppose in your data $x,y,z$, $y$ is chaning alot but $x$ and $z$ are not.\n",
    "Adagrad, Adadelta, rmsprop\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=mdKjMPmcWjY), [2](https://www.youtube.com/watch?v=GSmW59dM0-o), [3](https://www.youtube.com/watch?v=EGt-UOIIdDk), [4](https://www.youtube.com/watch?v=EGt-UOIIdDk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "parameters that we are learning should be updated with different learning rates. In the general case, when we don't know exactly the terms of our cost function, one conservative rule for updating would be to use smaller learning rates on the directions with \"big\" gradients. This would prevent us from overshooting on these directions if the decision of moving along them with a big step was wrong.\n",
    "\n",
    "$f(x,y)=20x^2+y^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta _x=0.05$, \n",
    "$\\eta _y=0.05$\n",
    "\n",
    "<img src='../nn/images/eta_x_0.05_eta_y_0.05.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta _x=0.015$, \n",
    "$\\eta _y=0.015$\n",
    "\n",
    "<img src='../nn/images/eta_x_0.015_eta_y_0.015.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta _x=0.015$, \n",
    "$\\eta _y=0.05$\n",
    "\n",
    "<img src='../nn/images/eta_x_0.015_eta_y_0.05.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w}_{t+1}= \\mathbf{w}_t - \\frac{\\alpha}{\\sqrt{\\epsilon  + \\text{diag}(G)}}\\,\\,g$\n",
    "\n",
    "\n",
    "Where **w** is the vector of parameters that we want to update, *g* is the gradient of the cost function w.r.t. these parameters **w**, *t* is the number of the iteration taking place and $G_t$ is given by:\n",
    "\n",
    "$G=\\sum_{\\tau=1}^{t} g_\\tau\\,g_\\tau^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if we have a look at one parameter $w_j$ from the vector $W=[w_1,w_2,...,w_n]^T$, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t+1}^j = w_t^j - \\frac{\\alpha}{\\sqrt{\\epsilon + G^{jj}}}\\,\\,g^j $\n",
    "\n",
    "$G^{jj}=\\sum_{\\tau=1}^{t+1}(g_\\tau^j)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *t* increase the learning rate related to the parameter $w_j$ will tend to decrease. This is because in the denominator we have the cummulative sum of the absolute value of its previous gradients. Adagrad will automatically give a lower learning rate to the updates in $x$ than on $y$.\n",
    "\n",
    "So if your function is convex it slow down as it goes toward minimum but if it is not non-convex, it will stuck in the saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to the more/less common features\n",
    "\n",
    "Refs [1](https://datascience.stackexchange.com/questions/82240/why-sparse-features-should-have-bigger-learning-rates-associated-and-how-adagra#:~:text=Adagrad%20allows%20us%20to%20give,lower%20gradients%20(in%20magnitude).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "Instead of leting the denominator get bigger and bigger over time we can have a decay factor, $\\gamma$ the reduce the importance of accumalted weights and current gradient.\n",
    "\n",
    "\n",
    "${\\displaystyle v(w,t)=\\gamma v(w,t-1)+(1-\\gamma )\\nabla f_{i}(w)^{2}}$\n",
    "\n",
    "\n",
    "${\\displaystyle w=w-{\\frac {\\eta }{\\sqrt {v(w,t)}}}\\nabla f_{i}(w)}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Refs [1](https://youtu.be/_JB0AO7QxSA?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&t=2137), [2](https://www.youtube.com/watch?v=oYWmIaKX_OY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "$\\text{first moment} \\leftarrow 0$\n",
    "\n",
    "$\\text{second moment} \\leftarrow 0$\n",
    "\n",
    "$\\beta_1 \\leftarrow 0.9$\n",
    "\n",
    "$\\beta_2 \\leftarrow 0.999$\n",
    "\n",
    "$\\eta \\leftarrow 0.001 \\text{ or } 0.0005$\n",
    "\n",
    "$\\text{first moment}=\\beta_1 \\times \\text{first moment} +(1-\\beta_1)\\times \\nabla f(w) \\leftarrow \\text{Momentum} $\n",
    "\n",
    "$\\text{second moment}=\\beta_2 \\times \\text{second moment} +(1-\\beta_2)\\times \\nabla f(w)^2  \\leftarrow \\text{AdaGrad/ RMSProp} $\n",
    "\n",
    "Since we start both $\\beta_1$ and $\\beta_2$ at 0, they will cause huge jump so we need **Bias Correction**:\n",
    "\n",
    "\n",
    "$\\text{Bias Correction} \\leftarrow  \\left\\{\\begin{matrix}\n",
    "\\text{first moment unbias}=\\frac{\\text{first moment}}{(1-\\beta_1^t)}\n",
    "\\\\\n",
    "\\text{second moment unbias}=\\frac{second moment}{(1-\\beta_2 ^t )} \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "$w=w- \\frac{\\eta* \\text{first moment unbias}}{\\sqrt[]{\\text{second moment unbias}+\\epsilon}}  \\leftarrow \\text{AdaGrad/ RMSProp}   $\n",
    "\n",
    "Refs: [1](https://youtu.be/_JB0AO7QxSA?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&t=2303),[2](https://www.youtube.com/watch?v=Syom0iwanHo&t=13s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov's Accelerated Gradient\n",
    "Refs: [1](https://www.youtube.com/watch?v=uHOTRHqnakQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n",
    "Refs [1](https://youtu.be/_JB0AO7QxSA?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&t=1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of available pytorch optimizers\n",
    "\n",
    "https://github.com/jettify/pytorch-optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
