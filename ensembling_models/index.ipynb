{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "editorial-relationship",
   "metadata": {},
   "source": [
    "##  1. What Is Ensembling?\n",
    "\n",
    "In **deep learning**, *ensembling* means **combining predictions from multiple models** to produce a final result that’s usually **more accurate and robust** than any single model.\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "> Asking 5 expert photographers to pick the best photo. Even if each one makes small mistakes, combining their opinions usually gets you closer to the best choice.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Ensemble?**\n",
    "\n",
    "You do it to **reduce error** caused by:\n",
    "\n",
    "* **Variance** → Model overfits differently on different data splits.\n",
    "* **Bias** → Combining models with different architectures can capture more complexity.\n",
    "* **Noise** → Random quirks in training that hurt one model are averaged out.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Higher accuracy\n",
    "* More robustness to outliers\n",
    "* Better generalization\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "* More computing at inference\n",
    "* More storage for multiple models\n",
    "* Harder to deploy in production\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18129898-6902-4ac2-8cc2-4d5676f805ab",
   "metadata": {},
   "source": [
    "## **2. Sampling With Replacement**\n",
    "\n",
    "**Sampling with replacement** means:\n",
    "\n",
    "* You **pick an item from the dataset**.\n",
    "* **You put it back** before picking the next item.\n",
    "* This means the **same item can be chosen more than once** in the sample.\n",
    "\n",
    "---\n",
    "\n",
    "####  Small Example\n",
    "\n",
    "Let’s say our dataset is:\n",
    "\n",
    "$$\n",
    "\\{A, B, C\\}\n",
    "$$\n",
    "\n",
    "We want to pick **3 samples** *with replacement*.\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. First pick → **B** (now we put B back into the pool).\n",
    "2. Second pick → **A** (A goes back into the pool).\n",
    "3. Third pick → **B** again (possible because we replaced it).\n",
    "\n",
    "**Final sample**: $\\{B, A, B\\}$\n",
    "Note:\n",
    "\n",
    "* **B** appears twice.\n",
    "* **C** was never picked.\n",
    "\n",
    "---\n",
    "\n",
    "####  Why “with replacement” Matters in Bagging\n",
    "\n",
    "In **Bagging (Bootstrap Aggregating)**:\n",
    "\n",
    "* The bootstrap sample is created by drawing **n** items **with replacement** from the original **n**-sized dataset.\n",
    "* Because we replace each time, the **same data point might be in the sample multiple times** and some points might be **missing entirely**.\n",
    "* This randomness means each model gets a **different view** of the data, adding diversity.\n",
    "\n",
    "---\n",
    "\n",
    "####  Probability Insight\n",
    "\n",
    "If we have $n$ data points and we sample $n$ times with replacement:\n",
    "\n",
    "* The probability that a given data point **is NOT selected** in a single draw = $\\frac{n-1}{n}$\n",
    "* Probability it is **never selected** in $n$ draws = $\\left(\\frac{n-1}{n}\\right)^n \\approx e^{-1} \\approx 0.368$\n",
    "\n",
    "➡ On average, **about 36.8% of the original dataset** will be missing from any given bootstrap sample, and about 63.2% will be present (with duplicates).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4621535-e8d3-4fd2-8e3c-8fcaa1b25617",
   "metadata": {},
   "source": [
    "##  **3. Common Ensembling Strategies**\n",
    "\n",
    "### **3.1. Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "* Train multiple models **independently** on different **random subsets** of the data (with replacement).\n",
    "* Average predictions (for regression) or use majority voting (for classification).\n",
    "* Example: **Random Forest** is bagging with decision trees.\n",
    "* In deep learning, bagging often means training the same architecture with different seeds + shuffled datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63f80e-27a1-4b87-9b56-02a00f98969c",
   "metadata": {},
   "source": [
    "### **3.1.1. Numerical Example for Bagging Using Voting**\n",
    "\n",
    "We’ll pretend we have:\n",
    "\n",
    "* **Dataset**: 6 data points\n",
    "* **Goal**: Predict whether a fruit is **Sweet (1)** or **Not Sweet (0)**\n",
    "* **Base models**: 3 weak classifiers (decision stumps in this example)\n",
    "* **Final decision rule**: Majority vote\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1 — Original Dataset**\n",
    "\n",
    "| ID | Feature (Sugar %) | Label (Sweet=1, Not=0) |\n",
    "| -- | ----------------- | ---------------------- |\n",
    "| 1  | 5%                | 0                      |\n",
    "| 2  | 8%                | 0                      |\n",
    "| 3  | 12%               | 1                      |\n",
    "| 4  | 15%               | 1                      |\n",
    "| 5  | 18%               | 1                      |\n",
    "| 6  | 20%               | 1                      |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2 — Create Bootstrap Samples**\n",
    "\n",
    "We randomly sample **with replacement** for each model.\n",
    "\n",
    "**Model 1’s training set** (random pick with replacement):\n",
    "\n",
    "* {ID 2, ID 3, ID 3, ID 4, ID 5, ID 6}\n",
    "\n",
    "**Model 2’s training set**:\n",
    "\n",
    "* {ID 1, ID 2, ID 4, ID 4, ID 5, ID 6}\n",
    "\n",
    "**Model 3’s training set**:\n",
    "\n",
    "* {ID 1, ID 3, ID 3, ID 4, ID 5, ID 5}\n",
    "\n",
    "⚠ Notice:\n",
    "\n",
    "* Some points appear multiple times.\n",
    "* Some points are missing in each sample.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3 — Train Models**\n",
    "\n",
    "We train a **simple classifier** on each bootstrap set.\n",
    "\n",
    "For example:\n",
    "\n",
    "* Model 1’s rule → “Sweet if sugar > 10%”\n",
    "* Model 2’s rule → “Sweet if sugar > 15%”\n",
    "* Model 3’s rule → “Sweet if sugar > 12%”\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4 — Make Predictions on a New Fruit**\n",
    "\n",
    "Let’s say our new fruit has **14% sugar**.\n",
    "\n",
    "* **Model 1** (Sweet if > 10%) → **Predict: 1**\n",
    "* **Model 2** (Sweet if > 15%) → **Predict: 0**\n",
    "* **Model 3** (Sweet if > 12%) → **Predict: 1**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5 — Aggregate Predictions**\n",
    "\n",
    "We take a **majority vote**:\n",
    "\n",
    "| Model | Prediction |\n",
    "| ----- | ---------- |\n",
    "| M1    | 1          |\n",
    "| M2    | 0          |\n",
    "| M3    | 1          |\n",
    "\n",
    "Sum = 1 + 0 + 1 = **2 votes for Sweet** out of 3 → **Final Prediction = Sweet (1)** ✅\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Points to Notice**\n",
    "\n",
    "* Bagging improves stability by **reducing variance** — each model sees a different “view” of the data.\n",
    "* If a single model makes an error on a point, other models can overrule it.\n",
    "* Works best when models are **unstable** (like decision trees).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ecbac6-9a8b-409c-b10a-603f24197df9",
   "metadata": {},
   "source": [
    "### **3.1.2. Numerical Example for Regression Bagging**\n",
    "\n",
    "\n",
    "We have one feature $x$ and a target $y$:\n",
    "\n",
    "| ID |  x |  y |\n",
    "| -: | -: | -: |\n",
    "|  1 |  1 |  2 |\n",
    "|  2 |  2 |  4 |\n",
    "|  3 |  3 |  7 |\n",
    "|  4 |  4 |  8 |\n",
    "|  5 |  5 | 10 |\n",
    "|  6 |  6 | 12 |\n",
    "\n",
    "**Base learner** (kept intentionally simple): a **regression stump** with a **fixed threshold** at $x=4.5$.\n",
    "\n",
    "* If $x \\le 4.5$: predict the **mean $y$** of points on the **left** (as seen in that model’s training sample).\n",
    "* If $x > 4.5$: predict the **mean $y$** of points on the **right** (as seen in that model’s training sample).\n",
    "\n",
    "We’ll predict at a new point $x^\\*=5.5$ (so we’ll use the right-branch mean).\n",
    "\n",
    "---\n",
    "\n",
    "#### Bagging: 3 bootstrap models\n",
    "\n",
    "Each model is trained on a **bootstrap sample (size 6, with replacement)**.\n",
    "\n",
    "#### Model A — sample: {2, 3, 3, 4, 5, 6}\n",
    "\n",
    "* Left (≤4.5): IDs 2,3,3,4 → $y=\\{4,7,7,8\\}$ → mean $= (4+7+7+8)/4 = 6.5$\n",
    "* Right (>4.5): IDs 5,6 → $y=\\{10,12\\}$ → mean $= 11$\n",
    "* **Prediction at $x^*=5.5$**: **11**\n",
    "\n",
    "#### Model B — sample: {1, 2, 4, 4, 5, 6}\n",
    "\n",
    "* Left: IDs 1,2,4,4 → $y=\\{2,4,8,8\\}$ → mean $= 22/4 = 5.5$\n",
    "* Right: IDs 5,6 → $y=\\{10,12\\}$ → mean $= 11$\n",
    "* **Prediction at $x^*=5.5$**: **11**\n",
    "\n",
    "#### Model C — sample: {1, 3, 3, 4, 5, 5}\n",
    "\n",
    "* Left: IDs 1,3,3,4 → $y=\\{2,7,7,8\\}$ → mean $= 24/4 = 6$\n",
    "* Right: IDs 5,5 → $y=\\{10,10\\}$ → mean $= 10$\n",
    "* **Prediction at $x^*=5.5$**: **10**\n",
    "\n",
    "---\n",
    "\n",
    "#### Aggregate (bagged) prediction\n",
    "\n",
    "Average the three model predictions:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\text{bag}}(5.5) = \\frac{11 + 11 + 10}{3} = \\frac{32}{3} \\approx \\mathbf{10.67}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Single (non-bagged) model baseline\n",
    "\n",
    "Train the **same regression stump** on the **full dataset**:\n",
    "\n",
    "* Left (IDs 1–4): $y=\\{2,4,7,8\\}$ → mean $= 21/4 = 5.25$\n",
    "* Right (IDs 5–6): $y=\\{10,12\\}$ → mean $= 11$\n",
    "* **Prediction at $x^*=5.5$**: **11**\n",
    "\n",
    "---\n",
    "\n",
    "#### What this shows\n",
    "\n",
    "* Each bootstrap sample yields **slightly different right-branch means** (11, 11, 10).\n",
    "* **Bagging averages** these to **10.67**, reducing variance relative to any single weak learner.\n",
    "* With more diverse base learners (different seeds/architectures/hyperparams), this effect is stronger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da55b1-1786-4f1e-a2f4-fbfce037eada",
   "metadata": {},
   "source": [
    "### **3.2. Boosting**\n",
    "\n",
    "* Train models **sequentially**.\n",
    "* Each new model focuses on **mistakes** made by previous ones.\n",
    "* Usually used in tree-based models (e.g., XGBoost), but also applicable in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3. Simple Averaging**\n",
    "\n",
    "* Train different models (same or different architectures).\n",
    "* Average their predicted probabilities:\n",
    "\n",
    "  $$\n",
    "  p_{\\text{final}} = \\frac{1}{N} \\sum_{i=1}^N p_i\n",
    "  $$\n",
    "* Easiest to implement.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4. Weighted Averaging**\n",
    "\n",
    "* Like above, but give **more weight** to better models:\n",
    "\n",
    "  $$\n",
    "  p_{\\text{final}} = \\sum_{i=1}^N w_i p_i\n",
    "  $$\n",
    "\n",
    "  where $\\sum w_i = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5. Stacking (Stacked Generalization)**\n",
    "\n",
    "* Train **level-1 models** (your base models).\n",
    "* Collect their predictions as **features**.\n",
    "* Train a **meta-model** (e.g., logistic regression, small NN) on these predictions to output the final result.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.6. Snapshot Ensembling**\n",
    "\n",
    "* Train **one model** with a cyclical learning rate.\n",
    "* Save **snapshots** at different points → they behave like different models due to being at different local minima.\n",
    "* Average their predictions at inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.7. Test Time Augmentation (TTA)**\n",
    "\n",
    "* Not exactly a multi-model ensemble.\n",
    "* At inference, create multiple augmented versions of the **same input** (e.g., flips, crops), pass them through the **same model**, and average the results.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97047153-45c8-448f-a061-94c62b821d5f",
   "metadata": {},
   "source": [
    "##  PyTorch Example — Simple Weighted Average\n",
    "\n",
    "Let’s say you have trained:\n",
    "\n",
    "* `modelA` = ResNet50\n",
    "* `modelB` = EfficientNet\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ensemble_predict(models, weights, x):\n",
    "    \"\"\"\n",
    "    models: list of trained PyTorch models\n",
    "    weights: list of weights for each model (sum to 1)\n",
    "    x: input batch\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for model, w in zip(models, weights):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = F.softmax(model(x), dim=1)  # probs\n",
    "            preds.append(w * output)\n",
    "    return torch.stack(preds).sum(dim=0)\n",
    "\n",
    "# Example usage:\n",
    "models = [modelA, modelB]\n",
    "weights = [0.6, 0.4]\n",
    "final_probs = ensemble_predict(models, weights, input_batch)\n",
    "final_labels = final_probs.argmax(dim=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. PyTorch Example — Stacking with Logistic Regression**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Assume we have validation predictions\n",
    "predsA = modelA(val_x).detach().numpy()\n",
    "predsB = modelB(val_x).detach().numpy()\n",
    "\n",
    "# Stack as features\n",
    "stacked_preds = np.hstack([predsA, predsB])\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_preds, val_labels)\n",
    "\n",
    "# At test time:\n",
    "test_predsA = modelA(test_x).detach().numpy()\n",
    "test_predsB = modelB(test_x).detach().numpy()\n",
    "stacked_test = np.hstack([test_predsA, test_predsB])\n",
    "final_preds = meta_model.predict(stacked_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Practical Tips\n",
    "\n",
    "* **Diversity matters** → Models should differ (architecture, initialization, training data order).\n",
    "* **Calibrate probabilities** → Ensures fair combination.\n",
    "* **Avoid overfitting in stacking** → Use out-of-fold predictions for meta-model training.\n",
    "* **Balance cost vs. accuracy** → Sometimes a single large model is better than a complex ensemble for deployment.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9ba29-77c4-41e4-bd2c-6ff11e2be6ed",
   "metadata": {},
   "source": [
    "## **5. Practical workflow for Ensembling Multiple Pretrained Models**\n",
    "\n",
    "Here’s a **practical, end-to-end workflow** to ensemble multiple **ImageNet-pretrained** models in **PyTorch** (e.g., ResNet50, DenseNet121, EfficientNet-B0). It’s the pattern I recommend for robust results without getting lost in glue code.\n",
    "\n",
    "---\n",
    "\n",
    "# 0) Goals & High-level plan\n",
    "\n",
    "* **Train** 3–5 diverse backbones with the **same dataset & splits**.\n",
    "* **Track** out-of-fold (OOF) predictions for proper validation.\n",
    "* **Choose** an ensemble method: simple average → weighted average → stacking.\n",
    "* **Polish**: TTA + probability calibration.\n",
    "* **Ship**: optionally **distill** the ensemble to one student for cheap inference.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Project skeleton\n",
    "\n",
    "```\n",
    "project/\n",
    "  data/\n",
    "  src/\n",
    "    dataset.py\n",
    "    models.py\n",
    "    train.py\n",
    "    infer.py\n",
    "    utils.py\n",
    "  oof/            # OOF predictions per fold & model\n",
    "  ckpt/           # checkpoints\n",
    "  cfg.yaml\n",
    "```\n",
    "\n",
    "### cfg.yaml (example)\n",
    "\n",
    "```yaml\n",
    "seed: 2025\n",
    "img_size: 224\n",
    "n_classes: 4\n",
    "folds: 5\n",
    "batch_size: 32\n",
    "epochs: 20\n",
    "models:\n",
    "  - name: resnet50\n",
    "    lr: 3e-4\n",
    "  - name: densenet121\n",
    "    lr: 3e-4\n",
    "  - name: efficientnet_b0\n",
    "    lr: 2e-4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Data & transforms (shared across models)\n",
    "\n",
    "```python\n",
    "# src/dataset.py\n",
    "import torch, torchvision as tv\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "def get_transforms(img_size):\n",
    "    train_tf = tv.transforms.Compose([\n",
    "        tv.transforms.Resize(int(img_size*1.15)),\n",
    "        tv.transforms.CenterCrop(img_size),\n",
    "        tv.transforms.RandomHorizontalFlip(),\n",
    "        tv.transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = tv.transforms.Compose([\n",
    "        tv.transforms.Resize(int(img_size*1.15)),\n",
    "        tv.transforms.CenterCrop(img_size),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df, tfm):\n",
    "        self.df, self.tfm = df.reset_index(drop=True), tfm\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(row.path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = torch.tensor(row.label, dtype=torch.long)\n",
    "        return x, y, row.index  # return index for OOF mapping\n",
    "```\n",
    "\n",
    "Use **StratifiedKFold** to create folds once and reuse for every backbone.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Load & adapt ImageNet backbones\n",
    "\n",
    "```python\n",
    "# src/models.py\n",
    "import torch.nn as nn\n",
    "import torchvision.models as M\n",
    "\n",
    "def make_model(name: str, n_classes: int, pretrained=True):\n",
    "    if name == \"resnet50\":\n",
    "        m = M.resnet50(weights=M.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, n_classes)\n",
    "    elif name == \"densenet121\":\n",
    "        m = M.densenet121(weights=M.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        m.classifier = nn.Linear(m.classifier.in_features, n_classes)\n",
    "    elif name == \"efficientnet_b0\":\n",
    "        m = M.efficientnet_b0(weights=M.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        m.classifier[1] = nn.Linear(m.classifier[1].in_features, n_classes)\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "    return m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Train each backbone with K-fold and save OOF predictions\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "* **OOF** = predictions for validation fold **not seen** during training → honest validation for ensembling/stacking.\n",
    "* Save both **checkpoint** and **OOF logits/probs** to disk.\n",
    "\n",
    "```python\n",
    "# src/train.py\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dataset import ImgDataset, get_transforms\n",
    "from models import make_model\n",
    "\n",
    "def train_one_fold(model_name, df, fold_idx, folds=5, n_classes=4, epochs=20, batch_size=32, lr=3e-4, img_size=224, device=\"cuda\"):\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=2025)\n",
    "    train_idx, val_idx = list(kf.split(df.path, df.label))[fold_idx]\n",
    "    tr_df, va_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "    tf_train, tf_val = get_transforms(img_size)\n",
    "    dl_tr = DataLoader(ImgDataset(tr_df, tf_train), batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    dl_va = DataLoader(ImgDataset(va_df, tf_val), batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = make_model(model_name, n_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "    best_acc, best_path = 0.0, f\"ckpt/{model_name}_fold{fold_idx}.pt\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x,y,_ in dl_tr:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x,y,_ in dl_va:\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                pred = logits.argmax(1)\n",
    "                correct += (pred==y).sum().item()\n",
    "                total += y.size(0)\n",
    "        acc = correct/total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    # Save OOF probabilities for stacking/weighting\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    model.eval()\n",
    "    oof_probs = np.zeros((len(va_df), n_classes), dtype=np.float32)\n",
    "    idxs = []\n",
    "    with torch.no_grad():\n",
    "        for x,y,ii in dl_va:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            oof_probs[len(idxs):len(idxs)+len(ii)] = probs\n",
    "            idxs.extend(ii.numpy().tolist())\n",
    "\n",
    "    oof_df = pd.DataFrame(oof_probs, index=va_df.index, columns=[f\"p{c}\" for c in range(n_classes)])\n",
    "    oof_df[\"label\"] = va_df.label.values\n",
    "    oof_df.to_csv(f\"oof/{model_name}_fold{fold_idx}.csv\", index=True)\n",
    "    print(model_name, fold_idx, \"best_acc\", best_acc)\n",
    "```\n",
    "\n",
    "Train **each model across all folds** (loop `model_name` in cfg and folds 0..K-1).\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Build the ensemble on OOF predictions\n",
    "\n",
    "## 5.1 Simple average (strong baseline)\n",
    "\n",
    "```python\n",
    "import pandas as pd, numpy as np, glob\n",
    "# load all OOF CSVs and align on index\n",
    "paths = sorted(glob.glob(\"oof/*.csv\"))  # e.g., resnet50_fold0.csv, ...\n",
    "dfs = [pd.read_csv(p, index_col=0) for p in paths]\n",
    "\n",
    "# group by fold/model as needed — here we just concatenate same rows (same indices)\n",
    "# compute per-row average of probabilities across models (not across folds)\n",
    "# (In practice: first merge OOF per model across folds -> one OOF per model, then average across models)\n",
    "def merge_folds(model_prefix):\n",
    "    files = [p for p in paths if model_prefix in p]\n",
    "    dd = [pd.read_csv(p, index_col=0) for p in files]\n",
    "    dfm = pd.concat(dd).sort_index()  # back to dataset index order\n",
    "    return dfm\n",
    "\n",
    "resnet_oof = merge_folds(\"resnet50\")\n",
    "dense_oof  = merge_folds(\"densenet121\")\n",
    "eff_oof    = merge_folds(\"efficientnet_b0\")\n",
    "\n",
    "all_probs = np.stack([resnet_oof.filter(like=\"p\").values,\n",
    "                      dense_oof.filter(like=\"p\").values,\n",
    "                      eff_oof.filter(like=\"p\").values], axis=0)\n",
    "avg_probs = all_probs.mean(axis=0)\n",
    "labels = resnet_oof[\"label\"].values  # same for all\n",
    "\n",
    "oof_acc = (avg_probs.argmax(1) == labels).mean()\n",
    "print(\"OOF accuracy (simple avg):\", oof_acc)\n",
    "```\n",
    "\n",
    "## 5.2 Learn **non-negative weights** for a weighted average\n",
    "\n",
    "We’ll parameterize weights via softmax so they’re ≥0 and sum to 1, and **optimize them on OOF**.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "probs_t = torch.tensor(all_probs)       # [M, N, C]\n",
    "labels_t = torch.tensor(labels)         # [N]\n",
    "w_logits = torch.zeros(len(probs_t), requires_grad=True)  # init equal weights\n",
    "opt = torch.optim.LBFGS([w_logits], max_iter=100)\n",
    "\n",
    "def closure():\n",
    "    opt.zero_grad()\n",
    "    w = torch.softmax(w_logits, dim=0)               # [M]\n",
    "    ens = (probs_t * w[:,None,None]).sum(dim=0)      # [N, C]\n",
    "    loss = torch.nn.functional.nll_loss(torch.log(ens+1e-8), labels_t)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "opt.step(closure)\n",
    "w = torch.softmax(w_logits.detach(), dim=0)\n",
    "print(\"Learned weights:\", w.tolist())\n",
    "```\n",
    "\n",
    "Use these weights at test time.\n",
    "\n",
    "## 5.3 Stacking (meta-learner)\n",
    "\n",
    "* Build a feature vector = **concatenate** model probabilities: `[p_resnet | p_densenet | p_efficientnet]`.\n",
    "* Train a **meta model** (e.g., LogisticRegression, small MLP) **on OOF**.\n",
    "* Predict with the meta model on test-time base probabilities.\n",
    "\n",
    "*(Skeleton, using scikit):*\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = np.concatenate([resnet_oof.filter(like=\"p\").values,\n",
    "                    dense_oof.filter(like=\"p\").values,\n",
    "                    eff_oof.filter(like=\"p\").values], axis=1)\n",
    "y = labels\n",
    "meta = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
    "meta.fit(X, y)\n",
    "print(\"OOF meta-acc:\", meta.score(X, y))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Inference with TTA + ensemble\n",
    "\n",
    "```python\n",
    "# src/infer.py\n",
    "import torch, torchvision as tv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def tta_transforms(img_size):\n",
    "    base = tv.transforms.Compose([\n",
    "        tv.transforms.Resize(int(img_size*1.15)),\n",
    "        tv.transforms.CenterCrop(img_size),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    hflip = tv.transforms.Compose([tv.transforms.RandomHorizontalFlip(p=1.0), *base.transforms])\n",
    "    return [base, hflip]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_single(models, img_path, weights=None, img_size=224, device=\"cuda\"):\n",
    "    tfs = tta_transforms(img_size)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    M = len(models)\n",
    "    if weights is None:\n",
    "        weights = [1.0/M]*M\n",
    "\n",
    "    # TTA per model\n",
    "    total = None\n",
    "    for m, w in zip(models, weights):\n",
    "        m.eval().to(device)\n",
    "        probs_sum = 0\n",
    "        for tf in tfs:\n",
    "            x = tf(img).unsqueeze(0).to(device)\n",
    "            logits = m(x)\n",
    "            probs_sum += torch.softmax(logits, dim=1)\n",
    "        probs = probs_sum / len(tfs)\n",
    "        total = probs * w if total is None else total + probs * w\n",
    "\n",
    "    return total.squeeze(0).cpu().numpy()  # [C]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Probability calibration (nice polish)\n",
    "\n",
    "Calibrate the **final ensemble** with **temperature scaling** on a held-out set (or OOF).\n",
    "\n",
    "```python\n",
    "class TemperatureScaler(torch.nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__(); self.t = torch.nn.Parameter(torch.ones(1))\n",
    "    def forward(self, logits): \n",
    "        return logits / self.t.clamp_min(1e-6)\n",
    "\n",
    "# Fit on logits+labels (use OOF logits from the ensemble, not probs)\n",
    "# minimize NLL by optimizing t\n",
    "```\n",
    "\n",
    "*(You can store per-model logits during OOF generation, combine to ensemble logits via log-weights, and fit one temperature.)*\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Speed/Memory options\n",
    "\n",
    "* **SWA/Polyak averaging** per model → better single checkpoints.\n",
    "* **Half-precision** (AMP) + **channels-last**.\n",
    "* **Prune** or **quantize** the **student** if you distill.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Distill the ensemble (optional, highly recommended for prod)\n",
    "\n",
    "Train a **student** to match the **ensemble’s soft targets**:\n",
    "\n",
    "```python\n",
    "# KL(student_logits / T, teacher_probs) + α * CE(student, hard_labels)\n",
    "```\n",
    "\n",
    "You keep ensemble quality but ship **one small model**.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Checklist / gotchas\n",
    "\n",
    "* Keep **identical folds** across backbones.\n",
    "* Always compute metrics on **OOF** (not per-fold val reported separately).\n",
    "* Prefer **probability-level** blending (not argmax of each).\n",
    "* Start with **simple average**, then try **learned weights**, then **stacking**.\n",
    "* Save seeds & versions; log runs (MLflow/W\\&B) for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR recipe\n",
    "\n",
    "1. Make 5 folds; train **ResNet50, DenseNet121, EfficientNet-B0** per fold, save **best ckpt** and **OOF probs**.\n",
    "2. **Simple average** OOF → baseline ensemble score.\n",
    "3. Fit **weights** on OOF via softmax-constrained optimization.\n",
    "4. (Optional) Fit a **stacking** meta-model on concatenated OOF probs.\n",
    "5. At inference: **TTA + weighted ensemble**.\n",
    "6. (Optional) **Calibrate** + **distill** to a single student.\n",
    "\n",
    "If you want, I can drop in a **minimal runnable script** that trains one fold and demonstrates OOF saving + weighted ensembling, ready to paste into your repo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
