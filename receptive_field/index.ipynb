{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf07ea5-a529-41f7-98fd-01296de249e2",
   "metadata": {},
   "source": [
    "The **receptive field** in deep learning — particularly in convolutional neural networks (CNNs) — is the **region of the input image that influences a given output activation** (for example, one neuron in a feature map).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Concept\n",
    "\n",
    "When a convolutional layer processes an image, each output pixel (or neuron) depends only on a **local neighborhood** in the input, defined by the **kernel size**.\n",
    "\n",
    "For example:\n",
    "\n",
    "* A **3×3 convolution** sees a **3×3 region** in the input.\n",
    "* A **5×5 convolution** sees a **5×5 region**, and so on.\n",
    "\n",
    "But when we **stack multiple layers**, the **effective receptive field** grows because each layer’s neurons depend on a region that itself depends on a region in the previous layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example: Stacked 3×3 Convolutions\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "* Stride = 1\n",
    "* Padding = 1 (so the spatial size stays constant)\n",
    "\n",
    "#### First convolution\n",
    "\n",
    "Each neuron in layer 1 sees a **3×3** patch of the input.\n",
    "\n",
    "#### Second convolution\n",
    "\n",
    "Each neuron in layer 2 sees **3×3** patch of layer 1,\n",
    "but each pixel in layer 1 already depends on a **3×3** region of the input.\n",
    "\n",
    "Thus, total receptive field:\n",
    "\n",
    "$$\n",
    "R = 3 + (3 - 1) = 5\n",
    "$$\n",
    "\n",
    "That means:\n",
    "Two stacked 3×3 convolutions → effective **5×5 receptive field**.\n",
    "\n",
    "#### Third convolution\n",
    "\n",
    "$$\n",
    "R = 5 + (3 - 1) = 7\n",
    "$$\n",
    "\n",
    "So three stacked 3×3 layers → effective **7×7 receptive field**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. General Formula\n",
    "\n",
    "For a stack of convolutional layers with kernel sizes ( k_i ), strides ( s_i ), the receptive field at layer ( L ) is:\n",
    "\n",
    "$$\n",
    "R_L = 1 + \\sum_{i=1}^{L} (k_i - 1) \\prod_{j=1}^{i-1} s_j\n",
    "$$\n",
    "\n",
    "For stride ( s_j = 1 ) everywhere, this simplifies to:\n",
    "\n",
    "$$\n",
    "R_L = 1 + \\sum_{i=1}^{L} (k_i - 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why Use 3×3 Convs Instead of Larger Kernels?\n",
    "\n",
    "Stacking multiple **3×3 convolutions** has several advantages over using a single large convolution (like 5×5 or 7×7):\n",
    "\n",
    "| Approach        | Effective Receptive Field | Parameters (for input channel C) |\n",
    "| --------------- | ------------------------- | -------------------------------- |\n",
    "| One 7×7 conv    | 7×7                       | ( 7^2 C^2 = 49C^2 )              |\n",
    "| Three 3×3 convs | 7×7                       | ( 3×(3^2C^2) = 27C^2 )           |\n",
    "\n",
    "Thus:\n",
    "\n",
    "* Same receptive field\n",
    "* Fewer parameters\n",
    "* More nonlinearity (ReLU after each conv)\n",
    "* Better feature abstraction\n",
    "\n",
    "That’s why **VGG** and many later networks (ResNet, EfficientNet, etc.) stack **3×3 convolutions** instead of using large kernels directly.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Relation to Stride and Pooling\n",
    "\n",
    "If you add **stride > 1** or **pooling**, the receptive field expands faster, because each neuron in deeper layers “jumps” over more pixels of the input.\n",
    "\n",
    "For example:\n",
    "\n",
    "* A stride of 2 doubles the spacing of receptive fields between adjacent neurons.\n",
    "* Pooling layers (like 2×2 max pooling) also expand the receptive field multiplicatively.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Visual Summary\n",
    "\n",
    "| Layers | Kernel | Stride | Padding | Effective Receptive Field |\n",
    "| ------ | ------ | ------ | ------- | ------------------------- |\n",
    "| 1      | 3×3    | 1      | 1       | 3×3                       |\n",
    "| 2      | 3×3    | 1      | 1       | 5×5                       |\n",
    "| 3      | 3×3    | 1      | 1       | 7×7                       |\n",
    "| 4      | 3×3    | 1      | 1       | 9×9                       |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d73cd-e269-4e95-abb8-5459b5789def",
   "metadata": {},
   "source": [
    "Let’s visualize **how the receptive field grows numerically** using a very small and concrete example.\n",
    "\n",
    "We’ll take a **5×5 input image**, apply **two 3×3 convolutions (stride = 1, padding = 1)**, and track which input pixels influence one specific output neuron.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Input feature map (5×5):\n",
    "Each element is labeled by its coordinates `(row, col)`.\n",
    "\n",
    "```\n",
    "(0,0) (0,1) (0,2) (0,3) (0,4)\n",
    "(1,0) (1,1) (1,2) (1,3) (1,4)\n",
    "(2,0) (2,1) (2,2) (2,3) (2,4)\n",
    "(3,0) (3,1) (3,2) (3,3) (3,4)\n",
    "(4,0) (4,1) (4,2) (4,3) (4,4)\n",
    "```\n",
    "\n",
    "Each convolution uses a **3×3 kernel**, stride = 1, padding = 1 → output is also 5×5.\n",
    "\n",
    "We’ll track the **center output neuron (2, 2)** after each convolution.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. First Convolution\n",
    "\n",
    "The output neuron at **(2, 2)** in **Conv 1** depends on these **3×3 input pixels**:\n",
    "\n",
    "```\n",
    "(1,1) (1,2) (1,3)\n",
    "(2,1) (2,2) (2,3)\n",
    "(3,1) (3,2) (3,3)\n",
    "```\n",
    "\n",
    "So the **receptive field = 3×3** at this point.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Second Convolution\n",
    "\n",
    "Now, take the **(2, 2)** neuron in **Conv 2’s output**.\n",
    "It depends on a 3×3 region of **Conv 1’s output**:\n",
    "\n",
    "```\n",
    "(1,1) (1,2) (1,3)\n",
    "(2,1) (2,2) (2,3)\n",
    "(3,1) (3,2) (3,3)\n",
    "```\n",
    "\n",
    "But **each** of those positions in Conv 1 already depends on its own 3×3 patch in the **original input**.\n",
    "\n",
    "Let’s compute their union.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Expand the Dependency\n",
    "\n",
    "* (1,1) in Conv 1 → depends on input rows [0–2], cols [0–2]\n",
    "* (1,2) → [0–2], [1–3]\n",
    "* (1,3) → [0–2], [2–4]\n",
    "* (2,1) → [1–3], [0–2]\n",
    "* (2,2) → [1–3], [1–3]\n",
    "* (2,3) → [1–3], [2–4]\n",
    "* (3,1) → [2–4], [0–2]\n",
    "* (3,2) → [2–4], [1–3]\n",
    "* (3,3) → [2–4], [2–4]\n",
    "\n",
    "Taking the union of all these ranges gives:\n",
    "\n",
    "* Rows covered: [0–4]\n",
    "* Cols covered: [0–4]\n",
    "\n",
    "Hence the center output neuron (2, 2) of **Conv 2** depends on **all input pixels (0–4, 0–4)** —\n",
    "that’s a **5×5 receptive field**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Formula Verification\n",
    "\n",
    "Using the receptive field formula:\n",
    "\n",
    "$$\n",
    "R = 1 + \\sum_{i=1}^{L}(k_i - 1)\n",
    "$$\n",
    "\n",
    "For two 3×3 convolutions (stride = 1):\n",
    "\n",
    "$$\n",
    "R = 1 + (3 - 1) + (3 - 1) = 5\n",
    "$$\n",
    "\n",
    "✅ Matches our numerical example.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Extension to Three Layers\n",
    "\n",
    "Add one more 3×3 convolution (stride = 1, padding = 1):\n",
    "\n",
    "$$\n",
    "R = 1 + 3×(3 - 1) = 7\n",
    "$$\n",
    "\n",
    "That means the center neuron of the third layer depends on a **7×7 region** of the original input (even though the input image may be only 5×5 here — conceptually, padding handles edges).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Intuitive Visualization\n",
    "\n",
    "You can think of the receptive field as expanding by **2 pixels per layer** on each side (since 3×3 adds one pixel of reach in every direction).\n",
    "\n",
    "| Layer | Kernel | Stride | Receptive Field |\n",
    "| ----- | ------ | ------ | --------------- |\n",
    "| 1     | 3×3    | 1      | 3×3             |\n",
    "| 2     | 3×3    | 1      | 5×5             |\n",
    "| 3     | 3×3    | 1      | 7×7             |\n",
    "| 4     | 3×3    | 1      | 9×9             |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **Python/PyTorch snippet** that prints or visualizes this dependency map (e.g., a small binary mask showing which input pixels contribute to one output neuron)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec647a-d827-42d8-bc52-ed74eb39d451",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=lxpQZRvfnCc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
