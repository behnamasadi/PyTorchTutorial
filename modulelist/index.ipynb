{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38245d4-9367-4ab0-b12d-c217127bd387",
   "metadata": {},
   "source": [
    "# Parameters Registration\n",
    "In PyTorch, all submodules that you assign as attributes inside `__init__` and that inherit from `nn.Module` become part of the model’s parameters **automatically**.\n",
    "\n",
    "Because this code does this:\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(...)\n",
    "self.FC    = nn.Sequential(...)\n",
    "```\n",
    "\n",
    "both `conv1` and the two Linear layers inside `FC` are registered automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e3d60c-ad23-49b7-b5d6-b0ec6b44e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([4, 3, 3, 3])\n",
      "FC.0.weight torch.Size([1000, 3136])\n",
      "FC.0.bias torch.Size([1000])\n",
      "FC.2.weight torch.Size([100, 1000])\n",
      "FC.2.bias torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=3, out_channels=4, kernel_size=3, padding=1, stride=2, bias=False)\n",
    "\n",
    "        self.FC = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=4*28*28, out_features=1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=1000, out_features=100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.FC(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "\n",
    "B, C, H, W = 1, 3, 56, 56\n",
    "x = torch.randn(B, C, H, W)\n",
    "\n",
    "# out = model(x)\n",
    "# print(out.shape)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15395471-f5f3-496e-ada1-6cedd3c88bf5",
   "metadata": {},
   "source": [
    "# Why they get registered automatically\n",
    "\n",
    "Every time you do:\n",
    "\n",
    "```python\n",
    "self.some_name = nn.Module()\n",
    "```\n",
    "\n",
    "PyTorch performs two things:\n",
    "\n",
    "1. **Registers `some_name` as a submodule**\n",
    "2. **Registers all trainable parameters inside it**\n",
    "\n",
    "This happens because `nn.Module` overrides `__setattr__`.\n",
    "\n",
    "So:\n",
    "\n",
    "* `self.conv1` → registered module\n",
    "* `self.FC` → registered module\n",
    "\n",
    "  * contains Linear(3136→1000) → parameters registered\n",
    "  * contains Linear(1000→100) → parameters registered\n",
    "  * contains ReLU → no parameters (but still registered)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc69620-569a-4b55-975e-5c63a8c1e6a0",
   "metadata": {},
   "source": [
    "# 1. What `nn.ModuleList` Actually Is\n",
    "\n",
    "`nn.ModuleList` is a **container that registers submodules** inside a PyTorch model.\n",
    "\n",
    "Any tensor parameters inside those submodules must be **registered** so that:\n",
    "\n",
    "1. They appear in\n",
    "   `model.parameters()`\n",
    "2. They are moved correctly with\n",
    "   `model.cuda()`, `model.to(device)`\n",
    "3. They appear in the state dictionary\n",
    "   `model.state_dict()`\n",
    "4. The optimizer receives their parameters\n",
    "\n",
    "A **normal Python list does not register anything**.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Why Registration Matters\n",
    "\n",
    "Suppose you have layers\n",
    "\n",
    "* $ W^{(1)} \\in \\mathbb{R}^{256 \\times 256} $\n",
    "* $ W^{(2)} \\in \\mathbb{R}^{256 \\times 256} $\n",
    "\n",
    "If they are inside a ModuleList, the model will collect them:\n",
    "\n",
    "$$\n",
    "\\theta = {W^{(1)}, W^{(2)}, \\dots}\n",
    "$$\n",
    "\n",
    "But if they are inside an ordinary list, these matrices **never appear** in (\\theta).\n",
    "That means:\n",
    "\n",
    "* Optimizer will never update them\n",
    "* State dict will not store them\n",
    "* `.cuda()` will not transfer them\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Minimal Example: What Goes Wrong with a Python List\n",
    "\n",
    "### Example (incorrect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770bb96e-93ef-4d4b-b023-918fd7425491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BadNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(10, 10), nn.Linear(10, 10)]  # Python list\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = BadNetwork()\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39c21a-d196-46dd-934a-15a32e0a54fb",
   "metadata": {},
   "source": [
    "\n",
    "The model has **zero parameters**.\n",
    "Both Linear layers were ignored.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Correct Version Using `ModuleList`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8220cf-266e-4f19-a9b4-1f30ee1ecc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight torch.Size([10, 10])\n",
      "layers.0.bias torch.Size([10])\n",
      "layers.1.weight torch.Size([10, 10])\n",
      "layers.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class GoodNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(10, 10),\n",
    "            nn.Linear(10, 10)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = GoodNetwork()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa31f6-6b96-4121-82b2-279f70b4dda2",
   "metadata": {},
   "source": [
    "Now both layers are registered.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. A Realistic Example: Deep Stacked Blocks\n",
    "\n",
    "You want multiple blocks:\n",
    "\n",
    "$$\n",
    "x_{l+1} = f_l(x_l)\n",
    "$$\n",
    "\n",
    "Instead of writing them manually:\n",
    "\n",
    "```python\n",
    "self.block1 = ...\n",
    "self.block2 = ...\n",
    "self.block3 = ...\n",
    "```\n",
    "\n",
    "You can do:\n",
    "\n",
    "```python\n",
    "self.blocks = nn.ModuleList([Block() for _ in range(6)])\n",
    "```\n",
    "\n",
    "And the forward:\n",
    "\n",
    "```python\n",
    "for block in self.blocks:\n",
    "    x = block(x)\n",
    "```\n",
    "\n",
    "This is **exactly** why PVT, Swin, FPN, U-Net, Mask2Former use ModuleList.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. How `ModuleList` Differs from `nn.Sequential`\n",
    "\n",
    "| Feature                     | `ModuleList` | `Sequential` |\n",
    "| --------------------------- | ------------ | ------------ |\n",
    "| Registers layers            | Yes          | Yes          |\n",
    "| Layers have **fixed order** | You define   | Automatic    |\n",
    "| You write your own forward  | Yes          | No           |\n",
    "| Useful for loops / branches | Yes          | Not ideal    |\n",
    "\n",
    "Example: U-Net decoder, FPN lateral layers, transformer blocks → use `ModuleList`.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Side-By-Side Comparison\n",
    "\n",
    "### Python List\n",
    "\n",
    "```python\n",
    "self.layers = [nn.Linear(32, 32)]\n",
    "```\n",
    "\n",
    "* No registration\n",
    "* No `parameters()`\n",
    "* No gradient updates\n",
    "* No `.cuda()` movement\n",
    "* No saving in `state_dict`\n",
    "\n",
    "### ModuleList\n",
    "\n",
    "```python\n",
    "self.layers = nn.ModuleList([nn.Linear(32, 32)])\n",
    "```\n",
    "\n",
    "* Fully registered\n",
    "* Appears in `parameters()`\n",
    "* Optimizable\n",
    "* Stored in checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Why PVT / FPN / ResNet use ModuleList\n",
    "\n",
    "### Example: PVT lateral layers:\n",
    "\n",
    "```python\n",
    "self.lateral = nn.ModuleList([\n",
    "    nn.Conv2d(c, out_channels, 1) for c in in_channels\n",
    "])\n",
    "```\n",
    "\n",
    "Why?\n",
    "Because PVT outputs are:\n",
    "\n",
    "* $P_1$\n",
    "* $P_2$\n",
    "* $P_3$\n",
    "* $P_4$\n",
    "\n",
    "And for each we need a 1×1 conv to unify channel dimensions:\n",
    "\n",
    "$$\n",
    "P_i' = \\text{Conv}_{1\\times1}(P_i)\n",
    "$$\n",
    "\n",
    "You cannot write 4 separate convs by hand each time.\n",
    "Also you cannot store them in a Python list.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Use `ModuleList` when:\n",
    "\n",
    "* You need **a variable or iterable number of layers**\n",
    "* You need loops inside forward\n",
    "* You build **transformer blocks**\n",
    "* You build **FPN lateral or top-down layers**\n",
    "* You build **U-Net decoder lists**\n",
    "* You need **skip connections** with lists of layers\n",
    "\n",
    "### Do *not* use a Python list when the elements are modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127c694-5f41-4f34-9572-2f8b36136be3",
   "metadata": {},
   "source": [
    "## Manually Registering Each Layer in  FPN \n",
    "\n",
    "you **can** manually register each layer like:\n",
    "\n",
    "```python\n",
    "self.lp1 = nn.Conv2d(in_channels[0], out_channels, 1)\n",
    "self.lp2 = nn.Conv2d(in_channels[1], out_channels, 1)\n",
    "self.lp3 = nn.Conv2d(in_channels[2], out_channels, 1)\n",
    "self.lp4 = nn.Conv2d(in_channels[3], out_channels, 1)\n",
    "```\n",
    "\n",
    "This works perfectly — PyTorch will register parameters because each one is assigned as an attribute.\n",
    "\n",
    "But here’s the key:\n",
    "\n",
    "# Why `ModuleList` Is Still Better\n",
    "\n",
    "## 1. Manual attributes do not scale\n",
    "\n",
    "If you have N layers:\n",
    "\n",
    "* Manual registration:\n",
    "\n",
    "  * Must define N attributes\n",
    "  * Must write N forward operations\n",
    "  * Error-prone and repetitive\n",
    "\n",
    "* ModuleList:\n",
    "\n",
    "  * Automatically registers each layer\n",
    "  * Makes forward loops easy\n",
    "\n",
    "---\n",
    "\n",
    "# Comparison Example\n",
    "\n",
    "## Manual Registration\n",
    "\n",
    "```python\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Conv2d(in_channels[0], out_channels, 1)\n",
    "        self.l2 = nn.Conv2d(in_channels[1], out_channels, 1)\n",
    "        self.l3 = nn.Conv2d(in_channels[2], out_channels, 1)\n",
    "        self.l4 = nn.Conv2d(in_channels[3], out_channels, 1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        p1 = self.l1(feats[0])\n",
    "        p2 = self.l2(feats[1])\n",
    "        p3 = self.l3(feats[2])\n",
    "        p4 = self.l4(feats[3])\n",
    "        return p1, p2, p3, p4\n",
    "```\n",
    "\n",
    "This works but is rigid.\n",
    "\n",
    "---\n",
    "\n",
    "## ModuleList Version (recommended)\n",
    "\n",
    "```python\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.lateral = nn.ModuleList(\n",
    "            [nn.Conv2d(c, out_channels, kernel_size=1) for c in in_channels]\n",
    "        )\n",
    "\n",
    "    def forward(self, feats):\n",
    "        return [layer(f) for layer, f in zip(self.lateral, feats)]\n",
    "```\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Cleaner\n",
    "* Can support variable number of inputs\n",
    "* Easy loops\n",
    "* Less code\n",
    "* Avoids mistakes\n",
    "\n",
    "---\n",
    "\n",
    "# 2. ModuleList is essential when you want dynamic number of layers\n",
    "\n",
    "Example in a Transformer:\n",
    "\n",
    "```python\n",
    "self.blocks = nn.ModuleList([Block() for _ in range(depth)])\n",
    "```\n",
    "\n",
    "You **cannot** do this with manual attributes unless you hard-code every depth.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ModuleList integrates better with flexible architectures\n",
    "\n",
    "Think of PVT, Swin, Mask2Former, U-Net, FPN — all use lists of layers.\n",
    "\n",
    "Because their depth and number of feature levels are often:\n",
    "\n",
    "* dynamic\n",
    "* dataset-dependent\n",
    "* architecture-dependent\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
