{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scheduled-louisville",
   "metadata": {},
   "source": [
    "# 1. **Regularization in Deep Learning**\n",
    "models with a large number of free parameters can describe\n",
    "an amazingly wide range of phenomena. Even if such a model agrees well with the available\n",
    "data, that doesn’t make it a good model. It may just mean there’s enough freedom in the\n",
    "model that it can describe almost any data set of the given size, without capturing any\n",
    "genuine insights into the underlying phenomenon. When that happens the model will work\n",
    "well for the existing data, but will fail to generalize to new situations.\n",
    "Regularization in deep learning is used to prevent **overfitting**, helping models generalize better to unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6f46a-28df-484e-a3d8-a29fc8d05bd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  **1.1 Weight-Based Regularization**\n",
    "These add a penalty to the loss function based on the model's weights.\n",
    "\n",
    "1. **L1 Regularization (Lasso)**\n",
    "   - Adds the absolute value of weights to the loss.\n",
    "   - Promotes sparsity (many weights become zero).\n",
    "\n",
    "2. **L2 Regularization (Ridge)**\n",
    "   - Adds the square of the weights to the loss.\n",
    "   - Keeps weights small but not sparse.\n",
    "\n",
    "3. **Elastic Net**\n",
    "   - Combines L1 and L2 penalties.\n",
    "\n",
    "---\n",
    "\n",
    "###  **1.2 Architecture-Based Regularization**\n",
    "Changes the structure or behavior of the network during training.\n",
    "\n",
    "4. **Dropout**\n",
    "   - Randomly \"drops\" units (sets them to zero) during training.\n",
    "   - Prevents co-adaptation of neurons.\n",
    "   - It prevents co-adaptation of features.\n",
    "   - It kind of like model ensembles within one model.\n",
    "\n",
    "5. **DropConnect**\n",
    "   - Instead of dropping activations, it randomly drops weights.\n",
    "\n",
    "6. **Batch Normalization (BN)**\n",
    "   - Normalizes layer inputs to stabilize training.\n",
    "   - Has some regularization effect but was not designed primarily for it.\n",
    "\n",
    "7. **Layer Normalization / Group Normalization**\n",
    "   - Similar to BN but works better for certain types of data (e.g., NLP, small batch sizes).\n",
    "\n",
    "---\n",
    "\n",
    "###  **1.3 Data-Based Regularization**\n",
    "Involves modifying the data to encourage better generalization.\n",
    "\n",
    "8. **Data Augmentation**\n",
    "   - Random transformations (e.g., rotation, cropping, flipping) applied to input data.\n",
    "   - Makes model invariant to these changes.\n",
    "\n",
    "9. **Mixup**\n",
    "   - Combines two input images and their labels to create a new training example.\n",
    "\n",
    "10. **Cutout / CutMix / Random Erasing**\n",
    "    - Removes or replaces parts of the input image.\n",
    "\n",
    "---\n",
    "\n",
    "###  **1.4 Early Training Control**\n",
    "Controlling training time or gradients to avoid overfitting.\n",
    "\n",
    "11. **Early Stopping**\n",
    "    - Stops training when validation performance stops improving.\n",
    "\n",
    "12. **Gradient Clipping**\n",
    "    - Limits the size of gradients to prevent exploding gradients.\n",
    "\n",
    "---\n",
    "\n",
    "###  **1.5 Noise-Based Regularization**\n",
    "\n",
    "13. **Label Smoothing**\n",
    "    - Softens one-hot labels (e.g., instead of `[0, 1, 0]`, use `[0.1, 0.8, 0.1]`).\n",
    "\n",
    "14. **Input Noise**\n",
    "    - Adds random noise to input during training.\n",
    "\n",
    "15. **Weight Noise**\n",
    "    - Adds noise directly to model weights during training.\n",
    "\n",
    "---\n",
    "\n",
    "###  **1.6 Advanced / Bayesian Approaches**\n",
    "\n",
    "16. **Variational Dropout / Bayesian Neural Networks**\n",
    "    - Models uncertainty by treating weights as distributions instead of fixed values.\n",
    "\n",
    "17. **Stochastic Depth**\n",
    "    - Randomly skips entire layers (used in ResNets).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95c503-892e-445e-8028-284ec808eef7",
   "metadata": {},
   "source": [
    "##  **2. When Regularization is Needed?**\n",
    "\n",
    "Regularization is needed when your model is **overfitting**—i.e., it performs well on the training data but poorly on validation or test data.\n",
    "\n",
    "---\n",
    "\n",
    "###  **2.1 Symptoms of Overfitting (Regularization is Needed )**\n",
    "\n",
    "####  1. **Large Gap Between Training and Validation Metrics**\n",
    "- **Training accuracy** is high, but **validation/test accuracy** is much lower.\n",
    "- Or, **training loss** is much lower than **validation loss**.\n",
    "\n",
    "####  2. **Validation Loss Increases While Training Loss Decreases**\n",
    "- A clear sign your model is memorizing the training data instead of generalizing.\n",
    "\n",
    "####  3. **Degrading F1 Score on Validation/Test**\n",
    "- Especially important in **imbalanced datasets**—you may have good accuracy but poor F1 score.\n",
    "\n",
    "---\n",
    "\n",
    "###  **2.2 Metrics to Watch**\n",
    "\n",
    "####  **Classification**\n",
    "| Metric        | What to Look For                                                                 |\n",
    "|---------------|----------------------------------------------------------------------------------|\n",
    "| Accuracy      | High train accuracy, low val/test accuracy ⇒ overfitting                         |\n",
    "| F1 Score      | More stable on imbalanced data; large drop from train to test ⇒ overfitting      |\n",
    "| Precision/Recall | If they drop significantly on validation set ⇒ model is too confident on training data |\n",
    "\n",
    "####  **Regression**\n",
    "| Metric        | What to Look For                                                                 |\n",
    "|---------------|----------------------------------------------------------------------------------|\n",
    "| MSE / MAE     | Low train error, high val/test error ⇒ overfitting                              |\n",
    "| R² Score      | Close to 1.0 on training, much lower on test ⇒ overfitting                       |\n",
    "\n",
    "---\n",
    "\n",
    "###  **2.3. Visualization Can Help Too**\n",
    "- **Learning curves** (plotting loss/accuracy vs. epochs for both training and validation):\n",
    "  - If the validation loss **starts increasing while training loss keeps dropping**, that’s a red flag.\n",
    "\n",
    "---\n",
    "\n",
    "###  Example Scenario\n",
    "You're training a neural net on image classification:\n",
    "\n",
    "| Epoch | Train Acc | Val Acc | Train Loss | Val Loss |\n",
    "|-------|-----------|---------|------------|----------|\n",
    "| 1     | 70%       | 68%     | 0.6        | 0.62     |\n",
    "| 5     | 95%       | 72%     | 0.2        | 0.58     |\n",
    "| 10    | 99%       | 65%     | 0.1        | 0.75     |\n",
    "\n",
    " You're overfitting. Time to apply regularization (e.g., Dropout, L2, Data Augmentation).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504808d-3671-430b-9ae7-c72448de5b17",
   "metadata": {},
   "source": [
    "##  L2 Regularization in Neural Networks\n",
    "\n",
    "L2 regularization is used **not only in linear regression** but also **in deep learning**, where we apply it to all weights in the network.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Example: Cross-Entropy Loss with L2**\n",
    "\n",
    "$\n",
    "C = -\\frac{1}{n} \\sum_{j} \\left[ y_j \\ln a^L_j + (1 - y_j) \\ln(1 - a^L_j) \\right] + \\frac{\\lambda}{2n} \\sum_w w^2\n",
    "$\n",
    "\n",
    "- First term: cross-entropy (how wrong our predictions are)\n",
    "- Second term: L2 penalty (sums squares of all weights)\n",
    "\n",
    "---\n",
    "\n",
    "###  **Example: MSE with L2**\n",
    "\n",
    "$\n",
    "C = \\frac{1}{2n} \\sum_x \\| y - a^L \\|^2 + \\frac{\\lambda}{2n} \\sum_w w^2\n",
    "$\n",
    "\n",
    "Both forms share this pattern:\n",
    "\n",
    "$\n",
    "C = C_0 + \\frac{\\lambda}{2n} \\sum_w w^2\n",
    "$\n",
    "\n",
    "- $ C_0 $: original loss (unregularized)\n",
    "- $ \\lambda $: regularization factor\n",
    "  - **Small $ \\lambda $** → prioritize fitting the training data\n",
    "  - **Large $ \\lambda $** → prioritize small weights\n",
    "\n",
    "---\n",
    "\n",
    "##  Gradient Descent with L2 Regularization\n",
    "\n",
    "Gradient update without regularization:\n",
    "\n",
    "$\n",
    "w_{\\text{new}} = w - \\eta \\frac{\\partial C_0}{\\partial w}\n",
    "$\n",
    "\n",
    "With L2 regularization:\n",
    "\n",
    "$\n",
    "w_{\\text{new}} = w - \\eta \\left( \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\right)\n",
    "= \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\eta \\frac{\\partial C_0}{\\partial w}\n",
    "$\n",
    "\n",
    "- The term $ \\left(1 - \\frac{\\eta \\lambda}{n} \\right) $ **shrinks** the weight on every update\n",
    "- This is known as **weight decay**\n",
    "\n",
    "---\n",
    "\n",
    "##  Mini-Batch Stochastic Gradient Descent (SGD)\n",
    "\n",
    "For a mini-batch of size $ m $:\n",
    "\n",
    "### Weight update:\n",
    "\n",
    "$\n",
    "w_{\\text{new}} = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial w}\n",
    "$\n",
    "\n",
    "### Bias update (no regularization term):\n",
    "\n",
    "$\n",
    "b_{\\text{new}} = b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b}\n",
    "$\n",
    "\n",
    "- We **don’t regularize biases**—only weights.\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "\n",
    "| Concept                          | Equation / Insight |\n",
    "|----------------------------------|---------------------|\n",
    "| OLS loss                         | $ \\|X\\beta - Y\\|^2 $ |\n",
    "| Ridge loss (L2)                  | $ \\|X\\beta - Y\\|^2 + \\lambda \\|\\beta\\|^2 $ |\n",
    "| Ridge solution                   | $ \\hat{\\beta}_R = (X^T X + \\lambda I)^{-1} X^T Y $ |\n",
    "| Neural network loss (w/ L2)      | $ C = C_0 + \\frac{\\lambda}{2n} \\sum_w w^2 $ |\n",
    "| Weight update rule (SGD + L2)    | $ w_{\\text{new}} = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\eta \\nabla_w C_0 $ |\n",
    "\n",
    "Let me know if you want me to do a **comparison with L1 (Lasso) regularization**, or explain how to **choose λ**, or **visualize the effect of L2 in weight space**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
