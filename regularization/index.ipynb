{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scheduled-louisville",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "models with a large number of free parameters can describe\n",
    "an amazingly wide range of phenomena. Even if such a model agrees well with the available\n",
    "data, that doesn’t make it a good model. It may just mean there’s enough freedom in the\n",
    "model that it can describe almost any data set of the given size, without capturing any\n",
    "genuine insights into the underlying phenomenon. When that happens the model will work\n",
    "well for the existing data, but will fail to generalize to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-vintage",
   "metadata": {},
   "source": [
    "## Linear least squares\n",
    "$X_{m\\times n}{\\vec {\\beta_{n\\times 1} }}=Y_{m\\times 1}$\n",
    "\n",
    "${\\displaystyle L(D,{\\vec {\\beta }})=||X{\\vec {\\beta }}-Y||^{2}=(X{\\vec {\\beta }}-Y)^{T}(X{\\vec {\\beta }}-Y)=Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "${\\displaystyle {\\frac {\\partial L(D,{\\vec {\\beta }})}{\\partial {\\vec {\\beta }}}}={\\frac {\\partial \\left(Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}=-2X^{T}Y+2X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "setting the gradient of the loss to zero and solving for ${\\displaystyle {\\vec {\\beta }}}$ we get: \n",
    "\n",
    "${\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y}{\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0}$\n",
    "\n",
    "\n",
    "$\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "## Tikhonov regularization (ridge regression) with L2 norm\n",
    "We add the magnitude of $\\beta$ to our cost to plenalize huge weights and keep the weights small (close to zero)  and all other things being equal. \n",
    "\n",
    "${\\displaystyle {\\hat {\\beta }}_{R}=(\\mathbf {X} ^{\\mathsf {T}}\\mathbf {X} +\\lambda \\mathbf {I} )^{-1}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {y} }$\n",
    "\n",
    "$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "The first term is the cross-entropy and the second term, is the squares of all the weights in the network. \n",
    "\n",
    "\n",
    "$\\begin{eqnarray} C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\n",
    "  \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "n both cases we can write the regularized cost function as:\n",
    "\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\n",
    "\\sum_w w^2,\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$C_0$ is the original, unregularized cost function.\n",
    "\n",
    "$\\lambda$: when $\\lambda$ is small we prefer to minimize the original cost function, but when $\\lambda$ is\n",
    "large we prefer small weights.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "b_{new} = b -\\eta \\frac{\\partial C_0}{\\partial b}.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= & & w-\\eta \\frac{\\partial C_0}{\\partial\n",
    "    w}-\\frac{\\eta \\lambda}{n} w   & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\n",
    "    C_0}{\\partial w}. \n",
    "\\end{eqnarray}$\n",
    "\n",
    "For stochastic gradient descent we can estimate $\\partial C_0 / \\partial w$ by averaging over a mini-batch of m training examples. Thus the regularized learning rule for stochastic gradient descent becomes:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\n",
    "  \\sum_x \\frac{\\partial C_x}{\\partial w}, \n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  b_{new} = b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\n",
    "\\end{eqnarray}$\n",
    "\n",
    "\n",
    "$n$ is, as usual, the size of our training set\n",
    "\n",
    "$m$ is size of the mini-batch training examples\n",
    "\n",
    "\n",
    "Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long, which is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-picnic",
   "metadata": {},
   "source": [
    "## Lasso with L1 norm\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "\n",
    "Refs: [1](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-conference",
   "metadata": {},
   "source": [
    "## Elastic net (L1+L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-resource",
   "metadata": {},
   "source": [
    "## Advanced Regularization\n",
    "[1](https://www.youtube.com/watch?v=ATo7vnzy5sY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-doctor",
   "metadata": {},
   "source": [
    "## Regularization and Deep Neural Nets\n",
    "Deep NNets, have a large number of degrees of freedom. So as a model, a NNet has a very large number of parameters, and if the number of parameters of the model is large relative to the number of training data points, there is an increased tendency to over fit. \n",
    "\n",
    "However regularization doesn't solve Deep Neural Nets hunger for data. Regularization helps to not fit to the noise, it doesn't do much in terms of determining the shape of the signal.\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/345737/why-doesnt-regularization-solve-deep-neural-nets-hunger-for-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-opinion",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "In each forward pass we randomly set zero a set of activation functions (In each iteration we select another set of activation to be set to zero). We usually do this in fully connected layers but seometimes we do that in convolutions layers. In the convolutions lasyers, sometimes instead of randomly turning off feature maps, we do it in a series, one by one.\n",
    "\n",
    "Interpertation why dropout works:\n",
    "\n",
    "1. It prevents co-adaptation of features.\n",
    "2. It kind of like model ensembles within one model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-attachment",
   "metadata": {},
   "source": [
    "## Pytorch  dropout\n",
    "`torch.count_nonzero(input, dim=None)`\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of $\\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driven-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of elemenst that set to zero:  tensor(0.2500)\n",
      "tensor([[-0.0000,  0.0000,  1.5350, -1.6167],\n",
      "        [-1.3246, -0.0000,  0.6490,  0.8528],\n",
      "        [-0.0000,  0.5941, -1.3181,  1.5891],\n",
      "        [ 0.6690, -0.0000,  0.4692,  0.3506],\n",
      "        [ 0.0000, -3.6877,  1.2310,  0.9887],\n",
      "        [ 0.6705, -0.8775,  0.6744,  2.7824]])\n",
      "tensor([[-0.0000,  0.0000,  1.2280, -1.2933],\n",
      "        [-1.0597, -0.0000,  0.5192,  0.6822],\n",
      "        [-0.0000,  0.4753, -1.0545,  1.2713],\n",
      "        [ 0.5352, -0.0000,  0.3754,  0.2805],\n",
      "        [ 0.0000, -2.9502,  0.9848,  0.7909],\n",
      "        [ 0.5364, -0.7020,  0.5395,  2.2259]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "width=4\n",
    "height=6\n",
    "p=0.2\n",
    "m = torch.nn.Dropout(p)\n",
    "input = torch.randn(height,width)\n",
    "output = m(input)\n",
    "print(\"The probability that elements set to zero is:\", p=0.2)\n",
    "print(\"The percentage of elemenst that set to zero: \",1-torch.count_nonzero(output)/(height*width))\n",
    "\n",
    "# the outputs are scaled by a factor of 1/(1-p) during training.\n",
    "print(output)\n",
    "print(output*(1-p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-thermal",
   "metadata": {},
   "source": [
    "Refs: [1](https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
