{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a910e9-d7e4-4f5d-af72-5c1c029c6fda",
   "metadata": {},
   "source": [
    "# **1. Neural Architecture Search (NAS)**\n",
    "**Neural Architecture Search (NAS)** is a subfield of **AutoML (Automated Machine Learning)** that aims to **automatically design neural network architectures** rather than hand-crafting them.\n",
    "\n",
    "The idea is to let an algorithm *search* through a space of possible network designs to find one that performs best for a given task (e.g., image classification, detection, or segmentation).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1. Motivation\n",
    "\n",
    "Traditionally, researchers manually design architectures like **ResNet**, **DenseNet**, or **Transformer** based on intuition and trial-and-error.\n",
    "However, there are many hyperparameters and design decisions:\n",
    "\n",
    "* Number of layers\n",
    "* Kernel sizes\n",
    "* Skip connections\n",
    "* Width/depth of the network\n",
    "* Type of blocks (conv, attention, etc.)\n",
    "\n",
    "This manual process is **time-consuming** and often **sub-optimal**.\n",
    "NAS automates this design process.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. General Workflow of NAS\n",
    "\n",
    "The process can be broken down into **three major components**:\n",
    "\n",
    "| Component               | Role                                                                                       |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| **Search Space**        | Defines *what* architectures can be explored (e.g., types of layers, connections, etc.)    |\n",
    "| **Search Strategy**     | Defines *how* architectures are sampled and improved (e.g., RL, evolution, gradient-based) |\n",
    "| **Evaluation Strategy** | Defines *how* to measure the performance (e.g., train model fully, or use proxy training)  |\n",
    "\n",
    "---\n",
    "\n",
    "### (a) **Search Space**\n",
    "\n",
    "This specifies the **building blocks** and **rules** for generating architectures.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Convolution types: 3×3, 5×5, depthwise conv, etc.\n",
    "* Skip connections allowed or not.\n",
    "* Number of channels, etc.\n",
    "\n",
    "Formally, NAS tries to find:\n",
    "$$\n",
    "\\arg\\min_{A \\in \\mathcal{A}} ; \\mathcal{L}_{val}(w^*(A), A)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "w^*(A) = \\arg\\min_{w} ; \\mathcal{L}_{train}(w, A)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $A$ = architecture from search space $\\mathcal{A}$\n",
    "* $w$ = its weights\n",
    "* $\\mathcal{L}_{train}$ = training loss\n",
    "* $\\mathcal{L}_{val}$ = validation loss\n",
    "\n",
    "So NAS must **optimize both architecture and its weights**.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **Search Strategy**\n",
    "\n",
    "How to explore the search space efficiently.\n",
    "\n",
    "#### Common Strategies:\n",
    "\n",
    "1. **Reinforcement Learning (RL) based NAS**\n",
    "\n",
    "   * A controller (e.g., RNN) generates architectures.\n",
    "   * Reward = validation accuracy.\n",
    "   * Example: **NASNet** (Zoph & Le, 2017).\n",
    "\n",
    "2. **Evolutionary Algorithms**\n",
    "\n",
    "   * Start with random architectures.\n",
    "   * Mutate and recombine top performers.\n",
    "   * Example: **AmoebaNet** (Real et al., 2019).\n",
    "\n",
    "3. **Gradient-Based (Differentiable NAS)**\n",
    "\n",
    "   * Represent architecture choices as continuous parameters.\n",
    "   * Optimize with gradient descent.\n",
    "   * Example: **DARTS (Differentiable Architecture Search)**.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) **Evaluation Strategy**\n",
    "\n",
    "Training each candidate model fully is **expensive**.\n",
    "So evaluation uses approximations:\n",
    "\n",
    "* **Early stopping:** Train a few epochs only.\n",
    "* **Weight sharing:** Train a “supernet” that contains all sub-architectures (e.g., **ENAS**).\n",
    "* **Performance prediction:** Use a small model to predict performance without full training.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key NAS Variants\n",
    "\n",
    "| Method                              | Type                            | Example Paper                    |\n",
    "| ----------------------------------- | ------------------------------- | -------------------------------- |\n",
    "| **RL-based**                        | Discrete search                 | NASNet (2017)                    |\n",
    "| **Evolutionary**                    | Discrete search                 | AmoebaNet (2019)                 |\n",
    "| **Differentiable (Gradient-based)** | Continuous relaxation           | DARTS (2018)                     |\n",
    "| **One-shot NAS**                    | Weight sharing                  | ENAS (2018), ProxylessNAS (2019) |\n",
    "| **Hardware-aware NAS**              | Adds latency/energy constraints | FBNet, MnasNet                   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b10d1c-287c-4fc1-bd09-a150802b5362",
   "metadata": {},
   "source": [
    "\n",
    "# **2. What Is a “Design Space”?**\n",
    "\n",
    "Traditionally, people talk about **a single architecture**:\n",
    "\n",
    "* ResNet-50\n",
    "* MobileNetV2\n",
    "* EfficientNet-B0\n",
    "* ViT-Base\n",
    "* etc.\n",
    "\n",
    "A **Design Space** is not one model — it is a **parametrized family of models** that follow a set of rules.\n",
    "\n",
    "A design space defines:\n",
    "\n",
    "1. Which hyperparameters are allowed\n",
    "2. Their ranges\n",
    "3. How these parameters interact\n",
    "4. How architecture components can be composed\n",
    "\n",
    "A model is **one point** inside that space.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1. Why Design Spaces?**\n",
    "\n",
    "Deep learning is 99% exploration:\n",
    "\n",
    "* number of layers\n",
    "* number of channels\n",
    "* kernel sizes\n",
    "* expansion ratios\n",
    "* attention heads\n",
    "* SE vs non-se\n",
    "* training schedules\n",
    "* etc.\n",
    "\n",
    "But this exploration is normally **ad-hoc**, **messy**, and inefficient.\n",
    "\n",
    "The RegNet authors realized something:\n",
    "\n",
    "### Instead of searching for a specific architecture that performs well,\n",
    "\n",
    "### it is better to design **a structured space** where *every model* is likely to perform well.\n",
    "\n",
    "This is a huge shift in thinking:\n",
    "\n",
    "**Search for good *rules*, not for a single good architecture.**\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2. Analogy**\n",
    "\n",
    "Think of a design space as:\n",
    "\n",
    "A **blueprint** defining the rules for building a whole family of houses:\n",
    "\n",
    "* same materials\n",
    "* same construction rules\n",
    "* same constraints\n",
    "\n",
    "But each house may vary in:\n",
    "\n",
    "* number of rooms\n",
    "* size\n",
    "* layout\n",
    "* number of floors\n",
    "\n",
    "In deep learning:\n",
    "\n",
    "* A model = a house\n",
    "* A design space = the blueprint for building all houses\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3. Why Design Spaces Improve Model Quality**\n",
    "\n",
    "The idea is that **architecture structure matters more than individual hyperparameters**.\n",
    "\n",
    "Example:\n",
    "\n",
    "If models perform better when:\n",
    "\n",
    "* the number of channels increases smoothly\n",
    "* bottleneck ratios stay in {1, 2, 4}\n",
    "* group sizes are bounded\n",
    "* depth per stage is not chaotic\n",
    "\n",
    "Then these principles should be **built into the design space**.\n",
    "\n",
    "This ensures:\n",
    "\n",
    "* all sampled models are good\n",
    "* scaling up/down is consistent\n",
    "* no random weird models exist\n",
    "* training and inference are efficient\n",
    "\n",
    "---\n",
    "\n",
    "## **2.4. How RegNet Discovered a Better Design Space**\n",
    "\n",
    "The authors conducted NAS (Neural Architecture Search) but instead of using the best architecture, they analyzed **population statistics**:\n",
    "\n",
    "### **2.4.1 Key empirical findings**\n",
    "\n",
    "From thousands of high-performing models:\n",
    "\n",
    "1. Width increases approximately **linearly**\n",
    "2. Depth per stage follows **simple distributions**\n",
    "3. Group sizes are **stable**\n",
    "4. Bottleneck ratios cluster around **few values**\n",
    "\n",
    "These “laws” inspired the **RegNet design space**, which formalizes them.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.5. Properties of a Good Design Space**\n",
    "\n",
    "According to the RegNet paper, a good design space should have:\n",
    "\n",
    "### **2.5.1 Predictability**\n",
    "\n",
    "A small change in parameters should produce a small change in model structure.\n",
    "\n",
    "### **2.5.2 Regularity**\n",
    "\n",
    "Smooth transitions in:\n",
    "\n",
    "* width\n",
    "* depth\n",
    "* kernel size\n",
    "* attention heads\n",
    "\n",
    "### **2.5.3 Scalability**\n",
    "\n",
    "Models can be:\n",
    "\n",
    "* very small (1 GFLOP)\n",
    "* medium (10 GFLOPs)\n",
    "* huge (100+ GFLOPs)\n",
    "\n",
    "… while still obeying the same rules.\n",
    "\n",
    "### **2.5.4 Consistency**\n",
    "\n",
    "All models in the space should share a similar look:\n",
    "\n",
    "* architecture layout\n",
    "* block type\n",
    "* stage structure\n",
    "\n",
    "### **2.5.5 Efficiency**\n",
    "\n",
    "Models from the space should be:\n",
    "\n",
    "* easy to implement\n",
    "* friendly to hardware (tensor cores, GPUs)\n",
    "* well-behaved during training\n",
    "\n",
    "---\n",
    "\n",
    "## **2.6. Design Space vs Neural Architecture Search (NAS)**\n",
    "\n",
    "NAS tries to find:\n",
    "\n",
    "**a single best architecture**\n",
    "\n",
    "RegNet’s philosophy is:\n",
    "\n",
    "**discover general principles that apply to an entire family of models**\n",
    "\n",
    "The advantages:\n",
    "\n",
    "| NAS                              | Design Space                          |\n",
    "| -------------------------------- | ------------------------------------- |\n",
    "| very expensive                   | fast, cheap                           |\n",
    "| produces irregular architectures | produces regular, clean architectures |\n",
    "| hard to scale                    | naturally scalable                    |\n",
    "| architecture may not generalize  | rules apply across scales             |\n",
    "\n",
    "Design spaces use **statistical analysis** from NAS but convert insights into structured rules.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.7. Examples of Design Spaces in Other Models**\n",
    "\n",
    "### **2.7.1 EfficientNet**\n",
    "\n",
    "Defines a scaling space with 3 dimensions:\n",
    "\n",
    "* depth\n",
    "* width\n",
    "* resolution\n",
    "\n",
    "This is a **scaling design space**.\n",
    "\n",
    "### **2.7.2 MobileNetV3**\n",
    "\n",
    "Defines a space of:\n",
    "\n",
    "* inverted bottlenecks\n",
    "* kernel sizes {3, 5, 7}\n",
    "* squeeze-excitation usage\n",
    "\n",
    "Then searched the best combination.\n",
    "\n",
    "### **2.7.3 ConvNeXt**\n",
    "\n",
    "Defines a space of:\n",
    "\n",
    "* patch size\n",
    "* depth per stage\n",
    "* spatial downsampling pattern\n",
    "\n",
    "### **2.7.4 ViT**\n",
    "\n",
    "Defines a transformer design space:\n",
    "\n",
    "* depth\n",
    "* hidden size\n",
    "* MLP expansion\n",
    "* number of heads\n",
    "\n",
    "---\n",
    "\n",
    "## **2.8. The RegNet Design Space (Key Components)**\n",
    "\n",
    "RegNet design space uses:\n",
    "\n",
    "1. Initial width $ w_0 $\n",
    "2. Width slope $ \\Delta w $\n",
    "3. Width quantization\n",
    "4. Bottleneck ratio $ b $\n",
    "5. Group size $ g $\n",
    "6. Depth (number of blocks)\n",
    "\n",
    "These parameters uniquely define:\n",
    "\n",
    "* stage widths\n",
    "* stage depths\n",
    "* block widths\n",
    "* growth pattern\n",
    "\n",
    "A model in the RegNet family is simply one sample from this space.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.9. Why This Matters in Practice**\n",
    "\n",
    "### **2.9.1 Practical benefits**\n",
    "\n",
    "* You can scale models cleanly (RegNet-400MF, 1.6GF, 3.2GF, 8GF, 16GF, 32GF)\n",
    "* Models are hardware-friendly (regular channel sizes)\n",
    "* No expensive architecture search needed\n",
    "* Architectures are interpretable\n",
    "\n",
    "### **2.9.2 Modern deep-learning perspective**\n",
    "\n",
    "Design spaces represent the idea that:\n",
    "\n",
    "**Good architectures follow structural laws.\n",
    "Our job is to discover those laws, not random architectures.**\n",
    "\n",
    "RegNet proved that simple rules outperform many NAS-generated models.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.10. Summary**\n",
    "\n",
    "A **Design Space** is a structured, rule-based framework that defines *families of architectures* instead of individual networks.\n",
    "\n",
    "RegNet found that top CNNs share simple statistical patterns, and converted those patterns into:\n",
    "\n",
    "* smooth width curves\n",
    "* fixed bottleneck ratios\n",
    "* stable group sizes\n",
    "* predictable stage transitions\n",
    "\n",
    "Sampling any architecture from this space gives a strong model with excellent compute–accuracy tradeoffs.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
