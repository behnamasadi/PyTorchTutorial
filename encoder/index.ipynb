{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ruled-europe",
   "metadata": {},
   "source": [
    "## The variational auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-ordinary",
   "metadata": {},
   "source": [
    "## Intractability\n",
    "Variational techniques are typically used to form an approximation for:\n",
    "\n",
    "${\\displaystyle P(\\mathbf {Z} \\mid \\mathbf {X} )={\\frac {P(\\mathbf {X} \\mid \\mathbf {Z} )P(\\mathbf {Z} )}{P(\\mathbf {X} )}}={\\frac {P(\\mathbf {X} \\mid \\mathbf {Z} )P(\\mathbf {Z} )}{\\int _{\\mathbf {Z} }P(\\mathbf {X} ,\\mathbf {Z} )\\,d\\mathbf {Z} }}}$\n",
    "\n",
    "\n",
    "The marginalization over ${\\mathbf  Z}$ to calculate ${\\displaystyle P(\\mathbf {X} )}$ in the denominator is typically intractable, because, for example, the search space of ${\\mathbf  Z}$ is combinatorially large. Therefore, we seek an approximation, using ${\\displaystyle Q(\\mathbf {Z} )\\approx P(\\mathbf {Z} \\mid \\mathbf {X} )}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-meaning",
   "metadata": {},
   "source": [
    "## Intractable probability distribution\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/4417/what-are-the-factors-that-cause-the-posterior-distributions-to-be-intractable), [2](https://arxiv.org/pdf/1601.00670.pdf), [3](https://stats.stackexchange.com/questions/208176/why-is-the-posterior-distribution-in-bayesian-inference-often-intractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-lindsay",
   "metadata": {},
   "source": [
    "## Variational Lower Bound\n",
    "\n",
    "Assume that $X$ are observations (data) and $Z$ are hidden variables. The hidden variables might include the \"parameters\". The relationship of these two variables can be represented using the following graphical model\n",
    "\n",
    "<img src='images/hidden_observed.jpg'>\n",
    "\n",
    "Moreover, uppercase $P(X)$ denotes the probability distribution over that variable, and\n",
    "lowercase $p(X)$ is the density function of the distribution of $X$.\n",
    "\n",
    "The posterior distribution of the hidden variables can then be written as follows:\n",
    " \n",
    "\n",
    "$p(Z|X)=\\frac{p(X|Z)p(Z)}{p(x)}=\\frac{p(X|Z)p(Z)}{\\int_{Z} p(X,Z)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-honolulu",
   "metadata": {},
   "source": [
    "### First derivation: The Jensen’s inequality\n",
    "\n",
    "$p(X)=\\int_{Z}p(X,Z)$\n",
    "\n",
    "$log(p(X))=log\\int_{Z}p(X,Z)$\n",
    "\n",
    "$=log\\int_{Z}p(X,Z)\\frac{q(Z)}{q(Z)} $\n",
    "\n",
    "Remember, expected value of a function:\n",
    "\n",
    "$\\mathbb{E}[h(X)]=\\int_x h(x) \\cdot p(x) \\ dx$\n",
    "\n",
    "\n",
    "$=log E_{q}[\\frac{p(X,Z)}{q(z)}]$\n",
    "\n",
    "We also know that:\n",
    "\n",
    "$f(E(X))\\leq E(f(X))$\n",
    "\n",
    "Therefore we have:\n",
    "\n",
    "\n",
    "$log p(x) \\geq E_{q}[log\\frac{p(X,Z)}{q(Z)}]=E_{q}[log(p(X,Z))]-E_{q}[log(q(z))]$\n",
    "\n",
    "\n",
    "$L= E_{q}[log(p(X,Z))]-E_{q}[log(q(z))]$\n",
    "\n",
    "Then it is obvious that $L$ is a lower bound of the log probability of the observations.\n",
    "As a result, if in some cases we want to maximize the marginal probability, we can instead\n",
    "maximize its variational lower bound $L$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-correction",
   "metadata": {},
   "source": [
    "### Second derivation: KL divergence\n",
    "\n",
    "The main idea behind variational methods is: to find some approximation distributions $q(Z)$ that are as closed as possible to the true posterior distribution $p(Z|X)$. These\n",
    "approximation distribution can have their own variational parameters: $q(Z|θ)$, and we\n",
    "try to find the setting of the parameters that make $q$ close to the posterior of interest.\n",
    "Obviously the distribution $q(Z)$ should be relatively easy and more tractable for inference.\n",
    "\n",
    "\n",
    "To measure the closeness of the two distribution $q(Z)$ and $p(Z|X)$, a common metric\n",
    "is the Kullback-Leibler (KL) divergence. \n",
    "\n",
    "$KL[q(Z) \\parallel p(Z|X)]= \\int_{Z} q(Z)log \\frac{q(Z)}{p(Z|X)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)\\log \\frac{p(Z|X)}{q(Z)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)\\log \\frac{p(Z,X)}{p(x)q(Z)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)( \\log \\frac{p(Z,X)}{q(Z)} -\\log(p(x)))$\n",
    "\n",
    "$= -\\int_{Z} q(Z) \\log \\frac{p(Z,X)}{q(Z)} +\\int_{Z} q(Z)\\log(p(x))$\n",
    "\n",
    "\n",
    "since $q(𝑍)$ is a pdf function:\n",
    "\n",
    "$= -\\int_{Z} q(Z) \\log \\frac{p(Z,X)}{q(Z)} + \\log(p(x)$\n",
    "\n",
    "$= -L + \\log(p(x)$\n",
    "\n",
    "$L$ is the variational lower bound.\n",
    "\n",
    "Rearranging will give us the following:\n",
    "\n",
    "$L = \\log p(X) − KL [q(Z)kp(Z|X)]$\n",
    "\n",
    "\n",
    "since $KL$ divergence is always $\\geq 0$, once again we get $L \\leq log p(X)$. therefore ur goal is to maximize $L $\n",
    "\n",
    "### Example\n",
    "We want to maximize the log likelihood of the class label: $\\log p(y|I,W)$. Here $I$ is the image, $W$ is the model parameters and $y$ is the class label. Then, the objective function above can be rewritten by\n",
    "marginalizing over the locations l (hidden variables):\n",
    "\n",
    "$\\log p(y|I,W)=\\log$\n",
    "\n",
    "Refs: [1](http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-guess",
   "metadata": {},
   "source": [
    " Refs: [1](https://www.youtube.com/watch?v=Tc-XfiDPLf4&ab_channel=MLExplained-AggregateIntellect-AI.SCIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-volleyball",
   "metadata": {},
   "source": [
    "## Marginal likelihood\n",
    "\n",
    "A marginal likelihood function (integrated likelihood), is a likelihood function in which some parameter variables have been marginalized. \n",
    "\n",
    "### In the context of Bayesian statistics\n",
    "Given a set of independent identically distributed data points ${\\displaystyle \\mathbf {X} =(x_{1},\\ldots ,x_{n}),}$, where $x_{i}\\sim p(x_{i}|\\theta )$ according to some probability distribution parameterized by $\\theta$ , where $\\theta$  itself is a random variable described by a distribution, i.e. ${\\displaystyle \\theta \\sim p(\\theta \\mid \\alpha ),}$ the marginal likelihood in general asks what the probability ${\\displaystyle p(\\mathbf {X} \\mid \\alpha )}$ is, where $\\theta$  has been marginalized out (integrated out): \n",
    "\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid \\alpha )=\\int _{\\theta }p(\\mathbf {X} \\mid \\theta )\\,p(\\theta \\mid \\alpha )\\ \\operatorname {d} \\!\\theta }$\n",
    "\n",
    "###  In classical statistics\n",
    "In In classical statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter ${\\displaystyle \\theta =(\\psi ,\\lambda )}$, where $\\psi$  is the actual parameter of interest, and $\\lambda$  is a non-interesting nuisance parameter.\n",
    "\n",
    "\n",
    "We know that:\n",
    "\n",
    "$P(B|C)=\\sum_{i} P(B|A_i,C)P(A_i|C) $\n",
    "\n",
    "And we also know \n",
    "\n",
    "${\\mathcal {L}}(\\theta|X)=p(X|\\theta)=p_{\\theta }(X)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "by marginalizing out $\\lambda$ :\n",
    "\n",
    "${\\displaystyle {\\mathcal {L}}(\\psi ;\\mathbf {X} )=p(\\mathbf {X} \\mid \\psi )=\\int _{\\lambda }p(\\mathbf {X} \\mid \\lambda ,\\psi )\\,p(\\lambda \\mid \\psi )\\ \\operatorname {d} \\!\\lambda }$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bayesian model comparison\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid M)=\\int p(\\mathbf {X} \\mid \\theta ,M)\\,p(\\theta \\mid M)\\,\\operatorname {d} \\!\\theta }$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-steps",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "$KL(q_\\phi(z|x) || P_\\theta(z|x))=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z|x)}=$\n",
    "\n",
    "$=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)p_\\theta(x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)} +\\int q_\\phi(z|x) \\log p_\\theta(x)$\n",
    "\n",
    "\n",
    "$=\\underbrace{ \\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}}_{-\\mathcal {L}}  +\\log p_\\theta(x)$\n",
    "\n",
    "\n",
    "$-\\mathcal {L}$, is variational lower bound.\n",
    "\n",
    "$\\mathcal {L}= -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$\\log p_\\theta(x)=\\mathcal {L}+KL(q_\\phi || P_\\theta)$\n",
    "\n",
    "$\\log p_\\theta(x) > \\mathcal {L}$\n",
    "\n",
    "The goal is minimize the $KL(q_\\phi || P_\\theta)$ w.r.t $\\phi$ ($p_{\\theta}$ is fixed w.r.t to $\\phi$) which means we have to maximize $\\mathcal {L}$\n",
    "\n",
    "\n",
    "$\\mathcal {L}= -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$P_\\theta(z,x)=p_\\theta(x|z)p_\\theta(z)$\n",
    "\n",
    "$\\mathcal {L}= E_q[\\log p_\\theta(x|z)]   -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z)}$\n",
    "\n",
    "\n",
    "$1) \\mathcal {L}= E_q[\\log p_\\theta(x,z) -  \\log q_\\phi(z|x)]$\n",
    "\n",
    "\n",
    "$2) \\mathcal {L}= E_q[\\log p_\\theta(x|z)] - KL( q_\\phi(z|x)||p_\\phi(z)) $\n",
    "\n",
    "\n",
    "\n",
    "(2) can be written as:\n",
    "\n",
    "\n",
    "$\\mathcal{L}(\\theta, \\phi;x^{(i)}) = -KL(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) + \\mathbb{E}_{z{\\tilde{}}q}[logp_{\\theta}(x|z)]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-credit",
   "metadata": {},
   "source": [
    "### The Optimization Procedure\n",
    "\n",
    "- And we need to maximize the expectation of the reconstruction of data points from the latent vector, $E_q[\\log p_\\theta(x|z)]$. Maximizing this means that the decoder is getting better at reconstruction, This means that we need to minimize reconstruction loss, which is $\\mathcal{L}_R$\n",
    "\n",
    "- We need to minimize the divergence between the estimated latent vector and the true latent vector, $KL( q_\\phi(z|x)||p_\\phi(z))$,  Let’s call this loss as $\\mathcal{L}_{KL}$\n",
    "\n",
    "\n",
    "$KL(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) = \\frac{1}{2}\\sum_{j=1}^{J}{(1+log(\\sigma_j)^2-(\\mu_j)^2-(\\sigma_j)^2)}$\n",
    "\n",
    "\n",
    "Here, $\\sigma_j$ is the standard deviation and $\\mu_j$ is the mean. We need $𝜎𝑗→1$ and $𝜇𝑗→1$\n",
    "\n",
    "\n",
    "To sum it up:\n",
    "\n",
    "1) $\\mathcal{L}_R = E_q[\\log p_\\theta(x|z)]$\n",
    "\n",
    "This is the reconstruction (decoder), i.e. pixel differences $|| x-f(x) ||^2$\n",
    "\n",
    "2) $\\mathcal{L}_{KL} = KL( q_\\phi(z|x)||p_\\phi(z)) = \\frac{1}{2}\\sum_{j=1}^{J}{(1+log(\\sigma_j)^2-(\\mu_j)^2-(\\sigma_j)^2)}$\n",
    "\n",
    "\n",
    "This is \n",
    "\n",
    "So, the final VAE loss that we need to optimize is:\n",
    "$\\mathcal{L}_{VAE} = \\mathcal{L}_R + \\mathcal{L}_{KL}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-blood",
   "metadata": {},
   "source": [
    "\n",
    "### Reparameterization trick\n",
    "\n",
    "$\\phi^{*},  \\theta^{*}=\\text{argmax} \\mathcal {L}(\\phi, \\theta;x) $\n",
    "\n",
    "Finally, we need to sample from the input space using the following formula (reparameterization trick ).\n",
    "\n",
    "$Sample = \\mu + \\epsilon\\sigma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-artist",
   "metadata": {},
   "source": [
    "<img src='images/vae.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-camcorder",
   "metadata": {},
   "source": [
    "Refs [1](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/), [2](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
