{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italic-campaign",
   "metadata": {},
   "source": [
    "## Probability space\n",
    "Probability space or a probability triple ${\\displaystyle (\\Omega ,{\\mathcal {F}},P)}$ \n",
    "\n",
    "1. A sample space, ${\\displaystyle \\Omega }$, which is the set of all possible outcomes.\n",
    "In the example of the throw of a standard dice, sample space is ${\\displaystyle \\{1,2,3,4,5,6\\}}$\n",
    "\n",
    "2. The σ-algebra ${\\displaystyle {\\mathcal {F}}}$ is a collection of all the events we would like to consider. In the example of the throwing the dice, dice lands on an even number ${\\displaystyle \\{2,4,6\\}}$\n",
    "\n",
    "3. A probability function, which assigns each event in the event space a probability, which is a number between 0 and 1.  ${\\displaystyle \\{2,4,6\\}}$ would be mapped to ${\\displaystyle 3/6=1/2}$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-probability",
   "metadata": {},
   "source": [
    "## Measure\n",
    "In mathematics, a measure on a set is a systematic way to assign a number, intuitively interpreted as its size, to some subsets of that set, called measurable sets.\n",
    "\n",
    "Let $X$ be a set and $\\Sigma$ a $\\sigma-algebra$ over $X$. A function $\\mu$ from $\\Sigma$ to the extended real number line is called a measure if it satisfies the following properties:\n",
    "\n",
    "\n",
    "1. Non-negativity: For all E in $\\Sigma$, we have $\\mu(E)$ ≥ 0.\n",
    "2. Null empty set: ${\\displaystyle \\mu (\\varnothing )=0}$.\n",
    "3. Countable additivity (or σ-additivity): For all countable collections ${\\displaystyle \\{E_{k}\\}_{k=1}^{\\infty }}$ of pairwise disjoint sets in $\\Sigma$,\n",
    "${\\displaystyle \\mu \\left(\\bigsqcup _{k=1}^{\\infty }E_{k}\\right)=\\sum _{k=1}^{\\infty }\\mu (E_{k}).}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-coalition",
   "metadata": {},
   "source": [
    "## Probability measure\n",
    "A probability measure is a real-valued function defined on a set of events in a probability space that satisfies measure properties such as countable additivity.\n",
    "\n",
    "For example, given three elements 1, 2 and 3 with probabilities 1/4, 1/4 and 1/2\n",
    "\n",
    "The requirements for a function $\\mu$  to be a probability measure on a probability space are that:\n",
    "<img src='images/Probability-measure.svg'> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-repair",
   "metadata": {},
   "source": [
    "## Expected Value\n",
    "\n",
    "\n",
    "$\\mathbb{E}[X] = \\int_x x \\cdot p(x) \\ dx$\n",
    "\n",
    "Indicates the \"average\" value of the random variable $X$. \n",
    "\n",
    "\n",
    "\n",
    "## Expected Value of a Function\n",
    "Sometimes interest will focus on the expected value of some function\n",
    "$h(X)$ rather than on just $E(X)$.\n",
    "\n",
    "If the random variable $X$ has a set of possible values $D$ and pmf $p(x)$, then the expected value of any function $h(X)$, denoted by $E[h(X)]$ or $\\mu_{h(X)}$:\n",
    "\n",
    "$E[h(X)]=\\sum_{D} h(x).p(x) $\n",
    "\n",
    "\n",
    "For a continuous function the expectation function is:\n",
    "\n",
    "$\\mathbb{E}[h(X)]=\\int_x h(x) \\cdot p(x) \\ dx$\n",
    "\n",
    "### Example\n",
    "A computer store has purchased three computers of a certain type at 500 USD apiece. It will sell them for 1000 USD apiece. The manufacturer has agreed to repurchase any computers still unsold after a specified period at 200 USD apiece.\n",
    "Let $X$ denote the number of computers sold, and suppose that: \n",
    "- $p(0) =0.1$ \n",
    "- $p(1) =0.2$ \n",
    "- $p(2) =0.3$\n",
    "- $p(3) =0.4$\n",
    "\n",
    "With $h(X)$ denoting the profit associated with selling $X$ units, the given information implies that:\n",
    "\n",
    "$h(X)= revenue-cost$\n",
    "\n",
    "$cost=3*500$\n",
    "\n",
    "$revenue=1000 \\times x + 200\\times(3 - x)$\n",
    "\n",
    "The expected profit is then:\n",
    "\n",
    "$E[h(X)]=p(0)\\times h(0) +p(1)\\times h(1) +p(2)\\times h(2) +p(3)\\times h(3)= (-900)(.1) + (- 100)(.2) + (700)(.3) + (1500)(.4)=700$\n",
    "\n",
    "\n",
    "\n",
    "Refs:[1](https://www.stat.purdue.edu/~zhanghao/STAT511/handout/Stt511%20Sec3.3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-recognition",
   "metadata": {},
   "source": [
    "## Conditional expectation\n",
    "Conditional expectation value, or conditional mean of a random variable is its expected value (the value it would take “on average” over an arbitrarily large number of occurrences) given that a certain set of \"conditions\" is known to occur. \n",
    "\n",
    "Depending on the context, the conditional expectation can be either a random variable  ${\\displaystyle E(X\\mid Y)}$\n",
    "or a function ${\\displaystyle E(X\\mid Y=y)}$ or ${\\displaystyle E(X\\mid Y)=f(Y)}$.\n",
    "\n",
    "\n",
    "### Discrete random variables\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\operatorname {E} (X\\mid Y=y)&=\\sum _{x}xP(X=x\\mid Y=y)\\\\&=\\sum _{x}x{\\frac {P(X=x,Y=y)}{P(Y=y)}}\\end{aligned}}}$\n",
    "\n",
    "${\\displaystyle P(X=x,Y=y)}$ is the **joint probability mass function** of $X$ and $Y$.\n",
    "\n",
    "\n",
    "The joint probability mass function of two discrete random variables ${\\displaystyle X,Y}$ is:\n",
    "\n",
    "${\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (X=x\\ \\mathrm {and} \\ Y=y)}$\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "### Continuous random variables\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\operatorname {E} (X\\mid Y=y)&=\\int _{-\\infty }^{\\infty }xf_{X|Y}(x,y)\\mathrm {d} x\\\\&=\\int _{-\\infty }^{\\infty }{\\frac {xf_{X,Y}(x,y)}{f_{Y}(y)}}\\mathrm {d} x\\end{aligned}}}$\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the roll of a fair die and let $A = 1$ if the number is even (i.e., 2, 4, or 6) and $A = 0$ otherwise. Furthermore, let $B = 1$ if the number is prime (i.e., 2, 3, or 5) and $B = 0$ otherwise.\n",
    "\n",
    "\n",
    "|   |1\t|2\t|3\t|4\t|5\t|6  |\n",
    "|---|---|---|---|---|---|---|\n",
    "|A\t|0\t|1\t|0\t|1\t|0\t|1  |\n",
    "|B\t|0\t|1\t|1\t|0\t|1\t|0  |\n",
    "\n",
    "\n",
    "1. The unconditional expectation of $A$ is ${\\displaystyle E[A]=(0+1+0+1+0+1)/6=1/2}$\n",
    "2. The expectation of A conditional on $B = 1$ (i.e., conditional on the die roll being 2, 3, or 5) is ${\\displaystyle E[A\\mid B=1]=(1+0+0)/3=1/3}$\n",
    "3. The expectation of A conditional on $B = 0$ (i.e., conditional on the die roll being 1, 4, or 6) is ${\\displaystyle E[A\\mid B=0]=(0+1+1)/3=2/3}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-frequency",
   "metadata": {},
   "source": [
    "## Expectations of Functions of Jointly Distributed Discrete Random Variables\n",
    "\n",
    "\n",
    "Suppose that  $X$  and  $Y$  are jointly distributed discrete random variables with joint pmf  $p(x,y)$.\n",
    "\n",
    "If  $g(X,Y)$  is a function of these two random variables, then its expected value is given by the following:\n",
    "\n",
    "$\\text{E}[g(X,Y)] = \\mathop{\\sum\\sum}_{(x,y)}g(x,y)p(x,y).\\notag$\n",
    "\n",
    "### Example\n",
    "\n",
    "\n",
    "We toss a fair coin three times and record the sequence of heads  $(h)$  and tails  $(t)$. Random variable  $X$  denote the number of heads obtained and random variable  $Y$  denote the winnings earned in a single play of a game with the following rules\n",
    "\n",
    "- $\\$1$ if first  $h$  occurs on the first toss\n",
    "- $\\$2$ if first $h$ occurs on the second toss\n",
    "- $\\$3$ if first $h$ occurs on the third toss\n",
    "- $\\$-1$ if no $h$ occur\n",
    "\n",
    "\n",
    "Note that the possible values of $X$ are  $x=0,1,2,3$ , and the possible values of  $Y$  are  $y=−1,1,2,3$. \n",
    "\n",
    "\n",
    "Joint pmf of $X$ and $Y$\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>p (x,y)</th>\n",
    "            <th  colspan=\"4\" rowspan=\"1\" scope=\"row\">\\(X\\)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">$Y$</th>\n",
    "            <th >0</th>\n",
    "            <th >1</th>\n",
    "            <th >2</th>\n",
    "            <th >3</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">-1</th>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">1</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td ><span  >2/8</span></td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">2</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">3</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. If, we define  $g(x,y)=xy$ , and compute the expected value of  $XY$ :\n",
    "$\\begin{align*} \n",
    "    \\text{E}[XY] = \\mathop{\\sum\\sum}_{(x,y)}xy\\cdot p(x,y) &= (0)(-1)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)(1)\\left(\\frac{1}{8}\\right) + (2)(1)\\left(\\frac{2}{8}\\right) + (3)(1)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)(2)\\left(\\frac{1}{8}\\right) + (2)(2)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)(3)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &= \\frac{17}{8} = 2.125 \n",
    "    \\end{align*}$\n",
    "\n",
    "2. Next, if we define  $g(x)=x$ , and compute the expected value of  $X$:\n",
    "$\\begin{align*} \n",
    "    \\text{E}[X] = \\mathop{\\sum\\sum}_{(x,y)}x\\cdot p(x,y) &= (0)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)\\left(\\frac{1}{8}\\right) + (2)\\left(\\frac{2}{8}\\right) + (3)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)\\left(\\frac{1}{8}\\right) + (2)\\left(\\frac{1}{8}\\right) \\\\ \n",
    "    &\\ + (1)\\left(\\frac{1}{8}\\right)\\\\ \n",
    "    &= \\frac{12}{8} = 1.5 \n",
    "    \\end{align*}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-inside",
   "metadata": {},
   "source": [
    "## Law of total probability\n",
    "If ${\\displaystyle \\left\\{{B_{n}:n=1,2,3,\\ldots }\\right\\}}$ is a finite or countably infinite partition of a sample space (in other words, a set of pairwise disjoint events whose union is the entire sample space) then for any event ${\\displaystyle A}$ of the same probability space:\n",
    "\n",
    "${\\displaystyle P(A)=\\sum _{n}P(A\\cap B_{n})}={\\displaystyle \\sum _{n}P(A\\mid B_{n})P(B_{n})}$\n",
    "\n",
    "The law of total probability, can also be stated for conditional probabilities.\n",
    "\n",
    "${\\displaystyle P(A\\mid C)=\\sum _{n}P(A\\mid C\\cap B_{n})P(B_{n}\\mid C)}$\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose that two factories supply light bulbs to the market. Factory X's bulbs work for over 5000 hours in 99% of cases, whereas factory Y's bulbs work for over 5000 hours in 95% of cases. It is known that factory X supplies 60% of the total bulbs available and Y supplies 40% of the total bulbs available. What is the chance that a purchased bulb will work for longer than 5000 hours?\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}P(A)&=P(A\\mid B_{X})\\cdot P(B_{X})+P(A\\mid B_{Y})\\cdot P(B_{Y})\\\\[4pt]&={99 \\over 100}\\cdot {6 \\over 10}+{95 \\over 100}\\cdot {4 \\over 10}={{594+380} \\over 1000}={974 \\over 1000}\\end{aligned}}}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-housing",
   "metadata": {},
   "source": [
    "## Law of total expectation\n",
    "If ${\\displaystyle X}$ is a random variable whose expected value ${\\displaystyle \\operatorname {E} (X)}$ is defined, and ${\\displaystyle Y}$ is any random variable on the same probability space, then\n",
    "\n",
    "${\\displaystyle \\operatorname {E} (X)=\\operatorname {E} (\\operatorname {E} (X\\mid Y))}$\n",
    "\n",
    "${\\displaystyle \\operatorname {E} (X)=\\sum _{i}{\\operatorname {E} (X\\mid A_{i})\\operatorname {P} (A_{i})}.}$\n",
    "\n",
    "### Example\n",
    "Factory ${\\displaystyle X}$'s bulbs work for an average of 5000 hours, whereas factory ${\\displaystyle Y}$'s bulbs work for an average of 4000 hours. It is known that factory ${\\displaystyle X}$ supplies 60% of the total bulbs available. What is the expected length of time that a purchased bulb will work for?\n",
    "\n",
    "By applying expected value for functions $E[h(X)]=\\sum_{D} h(x).p(x) $ and assuming $h(X)=E(X|Y)$\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\operatorname {E} (L)&=\\operatorname {E} (L\\mid X)\\operatorname {P} (X)+\\operatorname {E} (L\\mid Y)\\operatorname {P} (Y)\\\\[3pt]&=5000(0.6)+4000(0.4)\\\\[2pt]&=4600\\end{aligned}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-minute",
   "metadata": {},
   "source": [
    "## Law of total variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-security",
   "metadata": {},
   "source": [
    "## Conditional expectation of joint distribution\n",
    "\n",
    "Refs: [1](https://web.stanford.edu/class/archive/cs/cs109/cs109.1196/lectures/13%20-%20ConditionalJoints.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-andorra",
   "metadata": {},
   "source": [
    "## Subscript notation in expectations\n",
    "\n",
    "When many random variables are involved, and there is no subscript in the 𝐸 symbol, the expected value is taken with respect to their joint distribution:\n",
    "\n",
    "$E[h(X,Y)] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty h(x,y) f_{XY}(x,y) \\, dx \\, dy$\n",
    "\n",
    "When a subscript is present, it tells us on which variable we should condition.\n",
    "\n",
    "$E_X[h(X,Y)] = E[h(X,Y)\\mid X] = \\int_{-\\infty}^\\infty h(x,y) f_{h(X,Y)\\mid X}(h(x,y)\\mid x)\\,dy$\n",
    "\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/72613/subscript-notation-in-expectations)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-firewall",
   "metadata": {},
   "source": [
    "## Conditional expectation subscript notation\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/75024/conditional-expectation-subscript-notation?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-schema",
   "metadata": {},
   "source": [
    "## Take the expectation with respect to a probability measure\n",
    "\n",
    "In neural network architecture, the posterior probability of classes $\\mathbf{y}=y_1,y_2,...,y_K]$ given an input feature vector $\\mathbf{x}$ is $p(\\mathbf{y}|\\mathbf{x};\\mathbf{w})$ where $\\mathbf{w}$ are the parameters of the network. Note that $\\mathbf{y}$ is in one-hot encoding.\n",
    "\n",
    "\n",
    "This posterior probability is estimated using maximum likelihood estimation, and therefore the objective is to maximize $E_{p(\\mathbf{x},\\mathbf{y})}[log(p(\\mathbf{y}|\\mathbf{x};\\mathbf{w}))]$\n",
    "\n",
    "\n",
    "\n",
    "Let $f$ be a function and $\\mu$ be a probability measure. A notation $\\mathbb E_\\mu[f]$ means \n",
    "\n",
    "$\\mathbb E_\\mu[f]=\\int_\\mu f=\\int f(x)d(\\mu(x))$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x}|\\theta)}[X].$\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=9zKuYvjFFS8&ab_channel=ArxivInsights), [2](https://www.youtube.com/watch?v=2pEkWk-LHmU&ab_channel=JordanBoyd-GraberJordanBoyd-Graber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-september",
   "metadata": {},
   "source": [
    "## Cumulative distribution function\n",
    "CDF of a real-valued random variable ${\\displaystyle X}$,evaluated at ${\\displaystyle x}$, is the probability that ${\\displaystyle X}$ will take a value less than or equal to ${\\displaystyle x}$\n",
    "\n",
    " ${\\displaystyle F:\\mathbb {R} \\rightarrow [0,1]}$ satisfying \n",
    " \n",
    " ${\\displaystyle \\lim _{x\\rightarrow -\\infty }F(x)=0}$ \n",
    " \n",
    " and \n",
    " \n",
    " ${\\displaystyle \\lim _{x\\rightarrow \\infty }F(x)=1}$\n",
    " \n",
    " \n",
    " \n",
    "${\\displaystyle F_{X}(x)=\\operatorname {P} (X\\leq x)}$\n",
    "\n",
    " where the right-hand side represents the probability that the random variable $X$ takes on a value less than or equal to $x$\n",
    " \n",
    " ${\\displaystyle \\operatorname {P} (a<X\\leq b)=F_{X}(b)-F_{X}(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-graduation",
   "metadata": {},
   "source": [
    "## Probability mass function\n",
    "PMF is a function that gives the probability that a discrete random variable is exactly equal to some value\n",
    "\n",
    "\n",
    "## Joint probability distribution\n",
    "\n",
    "The joint probability mass function of two discrete random variables $X,Y$ is:\n",
    "\n",
    "${\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (X=x\\ \\mathrm {and} \\ Y=y)}$\n",
    "\n",
    "\n",
    "or written in terms of conditional distributions\n",
    "\n",
    "${\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (Y=y\\mid X=x)\\cdot \\mathrm {P} (X=x)=\\mathrm {P} (X=x\\mid Y=y)\\cdot \\mathrm {P} (Y=y)}$\n",
    "\n",
    "\n",
    "where ${\\displaystyle \\mathrm {P} (Y=y\\mid X=x)}\\mathrm {P} $ is the probability of ${\\displaystyle Y=y}$ given that ${\\displaystyle X=x}$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the roll of a fair die and let $A = 1$ if the number is even (i.e., 2, 4, or 6) and $A = 0$ otherwise. Furthermore, let $B = 1$ if the number is prime (i.e., 2, 3, or 5) and $B = 0$ otherwise.\n",
    "\n",
    "\n",
    "|   |1\t|2\t|3\t|4\t|5\t|6  |\n",
    "|---|---|---|---|---|---|---|\n",
    "|A\t|0\t|1\t|0\t|1\t|0\t|1  |\n",
    "|B\t|0\t|1\t|1\t|0\t|1\t|0  |\n",
    "\n",
    "Then, the joint distribution of $A$ and $B$, expressed as a probability mass function, is:\n",
    "\n",
    "$P(A=0,B=0)=P\\{1\\}=\\frac{1}{6}$\n",
    "\n",
    "$P(A=0,B=1)=P\\{3,5\\}=\\frac{2}{6}$\n",
    "\n",
    "$P(A=1,B=0)=P\\{4,6\\}=\\frac{2}{6}$\n",
    "\n",
    "$P(A=1,B=1)=P\\{2\\}=\\frac{1}{6}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-mileage",
   "metadata": {},
   "source": [
    "## Joint probability mass function (joint pmf)\n",
    "\n",
    "If discrete random variables  $X$  and  $Y$  are defined on the same sample space  $S$ , then their joint probability mass function (joint pmf) is given by\n",
    "$p(x,y) = P(X=x\\ \\ \\text{and}\\ \\ Y=y),\\notag$\n",
    " \n",
    "\n",
    "where  $(x,y)$  is a pair of possible values for the pair of random variables  $(x,y)$ , and  $p(x,y)$  satisfies the following conditions:\n",
    "\n",
    "- $0 \\leq p(x,y) \\leq 1$ \n",
    "- $\\displaystyle{\\mathop{\\sum\\sum}_{(x,y)}p(x,y) = 1}$\n",
    "- $\\displaystyle{P\\left((X,Y)\\in A\\right)) = \\mathop{\\sum\\sum}_{(x,y)\\in A} p(x,y)}$\n",
    "\n",
    "\n",
    "Refs: [1](https://stats.libretexts.org/Courses/Saint_Mary's_College_Notre_Dame/MATH_345__-_Probability_(Kuter)/5%3A_Probability_Distributions_for_Combinations_of_Random_Variables/5.1%3A_Joint_Distributions_of_Discrete_Random_Variables#:~:text=Suppose%20that%20X%20and%20Y,p(x%2Cy).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-review",
   "metadata": {},
   "source": [
    "## Joint cumulative distribution function (joint cdf)\n",
    "In the discrete case, we can obtain the joint cumulative distribution function (joint cdf) of  $X$  and  $Y$  by summing the joint pmf:\n",
    "\n",
    "$F(x,y) = P(X\\leq x\\ \\text{and}\\ Y\\leq y) = \\sum_{x_i \\leq x} \\sum_{y_j \\leq y} p(x_i, y_j),\\notag$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-strength",
   "metadata": {},
   "source": [
    "## Marginal probability mass functions (marginal pmf's)\n",
    "\n",
    "\n",
    "Suppose that discrete random variables  $X$  and  $Y$  have joint pmf  $p(x,y)$. Let  $y_1, y_2, \\ldots, y_j, \\ldots$  denote the possible values of  $Y$ , and let  $x_1, x_2, \\ldots, x_i, \\ldots$  denote the possible values of  $X$ . The marginal probability mass functions (marginal pmf's) of  $X$  and  $Y$  are respectively given by the following:\n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    "p_X(x) &= \\sum_j p(x, y_j) \\quad(\\text{fix a value of}\\ X\\ \\text{and sum over possible values of}\\ Y) \\\\ \n",
    "p_Y(y) &= \\sum_i p(x_i, y) \\quad(\\text{fix a value of}\\ Y\\ \\text{and sum over possible values of}\\ X) \n",
    "\\end{align*}$\n",
    "\n",
    "### Example joint pmf of  $X$  and  $Y$\n",
    "\n",
    "We toss a fair coin three times and record the sequence of heads  $(h)$  and tails  $(t)$. Random variable  $X$  denote the number of heads obtained and random variable  $Y$  denote the winnings earned in a single play of a game with the following rules\n",
    "\n",
    "- $\\$1$ if first  $h$  occurs on the first toss\n",
    "- $\\$2$ if first $h$ occurs on the second toss\n",
    "- $\\$3$ if first $h$ occurs on the third toss\n",
    "- $\\$-1$ if no $h$ occur\n",
    "\n",
    "\n",
    "Note that the possible values of $X$ are  $x=0,1,2,3$ , and the possible values of  $Y$  are  $y=−1,1,2,3$ . The joint pmf is:\n",
    "\n",
    " \n",
    "Joint pmf of $X$ and $Y$\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>p (x,y)</th>\n",
    "            <th  colspan=\"4\" rowspan=\"1\" scope=\"row\">\\(X\\)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">$Y$</th>\n",
    "            <th >0</th>\n",
    "            <th >1</th>\n",
    "            <th >2</th>\n",
    "            <th >3</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">-1</th>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">1</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td ><span  >2/8</span></td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">2</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th  scope=\"row\">3</th>\n",
    "            <td >0</td>\n",
    "            <td ><span  >1/8</span></td>\n",
    "            <td >0</td>\n",
    "            <td >0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "$S = \\{{ttt}, {htt}, {tht}, {tth}, {hht}, {hth}, {thh}, {hhh}\\}\\notag$\n",
    "\n",
    "$p(0,-1) = P(X=0\\ \\text{and}\\ Y=-1) = P(ttt) = \\frac{1}{8}.\\notag$\n",
    "\n",
    "\n",
    "$p(1,1) = P(X=1\\ \\text{and}\\ Y=1) = P(htt) = \\frac{1}{8}.\\notag$\n",
    "\n",
    "\n",
    "$p(2,1) = P(X=2\\ \\text{and}\\ Y=1) = P(\\text{tht or thh} ) = \\frac{2}{8}.\\notag$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-substance",
   "metadata": {},
   "source": [
    "### Example of marginal pmf's for  $X$  and  $Y$\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th  scope=\"col\">x</th>\n",
    "            <th  scope=\"col\">p_X(x)</th>\n",
    "            <th  scope=\"col\">y</th>\n",
    "            <th  scope=\"col\">p_Y(y)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td >0</td>\n",
    "            <td >1/8</td>\n",
    "            <td >-1</td>\n",
    "            <td >1/8</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >1</td>\n",
    "            <td >3/8</td>\n",
    "            <td >1</td>\n",
    "            <td >1/2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >2</td>\n",
    "            <td >3/8</td>\n",
    "            <td >2</td>\n",
    "            <td >1/4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >3</td>\n",
    "            <td >1/8</td>\n",
    "            <td >3</td>\n",
    "            <td >1/8</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-packing",
   "metadata": {},
   "source": [
    "## Chain rule (probability)\n",
    "\n",
    "For two random variables $X,Y$\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {P} (X,Y)=\\mathrm {P} (X\\mid Y)\\cdot P(Y)}{\\displaystyle \\mathrm {P} (X,Y)=\\mathrm {P} (X\\mid Y)\\cdot P(Y)}$\n",
    "\n",
    "More than two random variables:\n",
    "\n",
    "${\\displaystyle \\mathrm {P} (X_{n},\\ldots ,X_{1})=\\mathrm {P} (X_{n}|X_{n-1},\\ldots ,X_{1})\\cdot \\mathrm {P} (X_{n-1},\\ldots ,X_{1})}$\n",
    "\n",
    "For example:\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\mathrm {P} (X_{4},X_{3},X_{2},X_{1})&=\\mathrm {P} (X_{4}\\mid X_{3},X_{2},X_{1})\\cdot \\mathrm {P} (X_{3},X_{2},X_{1})\\\\&=\\mathrm {P} (X_{4}\\mid X_{3},X_{2},X_{1})\\cdot \\mathrm {P} (X_{3}\\mid X_{2},X_{1})\\cdot \\mathrm {P} (X_{2},X_{1})\\\\&=\\mathrm {P} (X_{4}\\mid X_{3},X_{2},X_{1})\\cdot \\mathrm {P} (X_{3}\\mid X_{2},X_{1})\\cdot \\mathrm {P} (X_{2}\\mid X_{1})\\cdot \\mathrm {P} (X_{1})\\end{aligned}}}$\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {P} \\left(\\bigcap _{k=1}^{n}X_{k}\\right)=\\prod _{k=1}^{n}\\mathrm {P} \\left(X_{k}\\,{\\Bigg |}\\,\\bigcap _{j=1}^{k-1}X_{j}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-hypothetical",
   "metadata": {},
   "source": [
    "## Variational Bayesian methods\n",
    "Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning.\n",
    "\n",
    "\n",
    "In variational inference, the posterior distribution over a set of unobserved variables ${\\displaystyle \\mathbf {Z} =\\{Z_{1}\\dots Z_{n}\\}}$ given some data ${\\displaystyle \\mathbf {X} }$  is approximated by a so-called variational distribution, ${\\displaystyle Q(\\mathbf {Z} )}$:\n",
    "\n",
    "${\\displaystyle P(\\mathbf {Z} \\mid \\mathbf {X} )\\approx Q(\\mathbf {Z} ).}$\n",
    "\n",
    "\n",
    "\n",
    "The distribution ${\\displaystyle Q(\\mathbf {Z} )}$ is restricted to belong to a family of distributions of simpler form (e.g. a family of Gaussian distributions) than ${\\displaystyle P(\\mathbf {Z} \\mid \\mathbf {X} )}$, selected with the intention of making ${\\displaystyle Q(\\mathbf {Z} )}$ similar to the true posterior, ${\\displaystyle P(\\mathbf {Z} \\mid \\mathbf {X} )}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-century",
   "metadata": {},
   "source": [
    "# KL divergence\n",
    "The most common type of variational Bayes uses the Kullback–Leibler divergence, which makes this minimization tractable.\n",
    "\n",
    "\n",
    "${\\displaystyle D_{\\mathrm {KL} }(Q\\parallel P)\\triangleq \\sum _{\\mathbf {Z} }Q(\\mathbf {Z} )\\log {\\frac {Q(\\mathbf {Z} )}{P(\\mathbf {Z} \\mid \\mathbf {X} )}}.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-degree",
   "metadata": {},
   "source": [
    "### KL Example\n",
    "\n",
    "|x |\t0   |\t1   |   2   |\n",
    "|---|-------|-------|-------|\n",
    "|Distribution P(x)| 9/25| 12/25|4/25|\n",
    "|Distribution Q(x)|1/3| 1/3|1/3|\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(P\\parallel Q)&=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{Q(x)}}\\right)\\\\&={\\frac {9}{25}}\\ln \\left({\\frac {9/25}{1/3}}\\right)+{\\frac {12}{25}}\\ln \\left({\\frac {12/25}{1/3}}\\right)+{\\frac {4}{25}}\\ln \\left({\\frac {4/25}{1/3}}\\right)\\\\&={\\frac {1}{25}}\\left(32\\ln(2)+55\\ln(3)-50\\ln(5)\\right)\\approx 0.0852996\\end{aligned}}}$\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(Q\\parallel P)&=\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{P(x)}}\\right)\\\\&={\\frac {1}{3}}\\ln \\left({\\frac {1/3}{9/25}}\\right)+{\\frac {1}{3}}\\ln \\left({\\frac {1/3}{12/25}}\\right)+{\\frac {1}{3}}\\ln \\left({\\frac {1/3}{4/25}}\\right)\\\\&={\\frac {1}{3}}\\left(-4\\ln(2)-6\\ln(3)+6\\ln(5)\\right)\\approx 0.097455\\end{aligned}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-ordinary",
   "metadata": {},
   "source": [
    "## Intractability\n",
    "Variational techniques are typically used to form an approximation for:\n",
    "\n",
    "${\\displaystyle P(\\mathbf {Z} \\mid \\mathbf {X} )={\\frac {P(\\mathbf {X} \\mid \\mathbf {Z} )P(\\mathbf {Z} )}{P(\\mathbf {X} )}}={\\frac {P(\\mathbf {X} \\mid \\mathbf {Z} )P(\\mathbf {Z} )}{\\int _{\\mathbf {Z} }P(\\mathbf {X} ,\\mathbf {Z} )\\,d\\mathbf {Z} }}}$\n",
    "\n",
    "\n",
    "The marginalization over ${\\mathbf  Z}$ to calculate ${\\displaystyle P(\\mathbf {X} )}$ in the denominator is typically intractable, because, for example, the search space of ${\\mathbf  Z}$ is combinatorially large. Therefore, we seek an approximation, using ${\\displaystyle Q(\\mathbf {Z} )\\approx P(\\mathbf {Z} \\mid \\mathbf {X} )}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-alexander",
   "metadata": {},
   "source": [
    "## Density Estimation\n",
    "\n",
    "<img src='images/density_estimation.jpg'>\n",
    "\n",
    "### Parametric Methods\n",
    "### Non-parametric Methods\n",
    "### Explicit Density Estimation\n",
    "### Implicit Density Estimation\n",
    "\n",
    "Refs: [1](https://www.kdnuggets.com/2019/10/overview-density-estimation.html#:~:text=Explicit%20Density%20Estimation%3A%20Estimates%20the,samples%20from%20the%20true%20distribution.), [2](https://arxiv.org/pdf/1701.00160.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-invalid",
   "metadata": {},
   "source": [
    "## Deep generative models\n",
    "Refs: [1](https://ermongroup.github.io/cs228-notes/extras/vae/)\n",
    "## Learning in latent variable models\n",
    "Refs: [1](https://ermongroup.github.io/cs228-notes/learning/latent/)\n",
    "\n",
    "## Variational inference\n",
    "Refs [1](https://ermongroup.github.io/cs228-notes/inference/variational/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-danger",
   "metadata": {},
   "source": [
    "# Bayesian network (Bayes network, belief network, or decision network, Directed graphical models)\n",
    "\n",
    "It is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).\n",
    "\n",
    "A compact Bayesian network is a distribution in which each factor on the right hand side depends only on a small number of ancestor variables $x_{A_i}$\n",
    "\n",
    "$p(x_i \\mid x_{i-1}, \\dotsc, x_1) = p(x_i \\mid x_{A_i}).$\n",
    "\n",
    "For example, in a model with five variables, we may choose to approximate the factor $p(x_5 \\mid x_4, x_3, x_2, x_1)$ with $p(x_5 \\mid x_4, x_3)$, meaning $x_{A_5} = \\{x_4, x_3\\}$.\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "<img width=\"300\" height=\"200\" src='images/grade-model.png'>\n",
    "\n",
    "$p(l, g, i, d, s) = p(l \\mid g)\\, p(g \\mid i, d)\\, p(i)\\, p(d)\\, p(s \\mid i).$\n",
    "\n",
    "<img src='images/win_rain_wet.jpg'>\n",
    "\n",
    "\n",
    "$P(L,R,W)=P(L)P(R)P(W|R)$\n",
    "\n",
    "\n",
    "<img src='images/rain_wet_car_slip.jpg'>\n",
    "\n",
    "$P(R,W,C,S)=P(R)P(C)P(W|C,R)P(S|W)$\n",
    "\n",
    "<img src='images/SimpleBayesNet.svg'>\n",
    "\n",
    "The chain rule will give us the followings:\n",
    "\n",
    "\n",
    "${\\displaystyle P(G,S,R)=P(G\\mid S,R)P(S\\mid R)P(R)}$\n",
    "\n",
    "\n",
    "What is the probability that it is raining, given the grass is wet? By applying bayes rule and then marginalisation:\n",
    "\n",
    "${\\displaystyle P(R=T\\mid G=T)={\\frac {P(G=T,R=T)}{P(G=T)}}={\\frac {\\sum _{x\\in \\{T,F\\}}P(G=T,S=x,R=T)}{\\sum _{x,y\\in \\{T,F\\}}P(G=T,S=x,R=y)}}}$\n",
    "\n",
    "Now using the expansion for the joint probability function ${\\displaystyle \\Pr(G,S,R)}$ and the conditional probabilities from the conditional probability tables:\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}P(G=T,S=T,R=T)&=P(G=T\\mid S=T,R=T)P(S=T\\mid R=T)P(R=T)\\\\&=0.99\\times 0.01\\times 0.2\\\\&=0.00198.\\end{aligned}}}$\n",
    "\n",
    "\n",
    "${\\displaystyle P(R=T\\mid G=T)={\\frac {0.00198_{TTT}+0.1584_{TFT}}{0.00198_{TTT}+0.288_{TTF}+0.1584_{TFT}+0.0_{TFF}}}={\\frac {891}{2491}}\\approx 35.77\\%.}$\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=TuGDMj43ehw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-digest",
   "metadata": {},
   "source": [
    "## Conditional probability tables\n",
    "When the variables are discrete  we may think of the factors $p(x_i\\mid x_{A_i})$ as probability tables. rows correspond to assignments to $x_{A_i}$ and columns correspond to values of $x_i$. the entries contain the actual probabilities $p(x_i\\mid x_{A_i})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-niagara",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "We have an evidence and we would like to know which $H_1, H_2, \\dots$ is more probable.\n",
    "\n",
    "$P(H|E)_{\\text{posterior probability}}=\\frac{P(E|H)_{ \\text{likelihood}} .P(H)_{\\text{prior probability}}}{P(E)_{evidence}}$\n",
    "\n",
    "\n",
    "- $H$:  Any hypothesis whose probability may be affected by data (evidence). Often there are competing hypotheses, and the task is to determine which is the most probable.\n",
    "- $P(H)$, the **prior** probability, is the estimate of the probability of the hypothesis $H$  before the data $E$.\n",
    "- $E$, the **evidence**, corresponds to new data that were not used in computing the prior probability.\n",
    "- $P(H\\mid E)$, the **posterior probability**, is the probability of $H$ H given $E$, i.e., after $E$ is observed. \n",
    "- $P(E\\mid H)$ is the probability of observing $E$ given $H$, and is called the **likelihood**.\n",
    "- $P(E)$ is sometimes termed the **marginal likelihood** or \"model evidence\". This factor is the same for all possible hypotheses being considered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-latvia",
   "metadata": {},
   "source": [
    "## Variable elimination\n",
    "## Inference in graphical models (Bayes net or a Markov random fields)\n",
    "### Marginal inference: \n",
    "what is the probability of a given variable in our model after we sum everything else out \n",
    "$p(y=1) = \\sum_{x_1} \\sum_{x_2} \\cdots \\sum_{x_n} p(y=1, x_1, x_2, \\dotsc, x_n).$\n",
    "\n",
    "### Maximum a posteriori (MAP) inference\n",
    "what is the most likely assignment to the variables in the model (possibly conditioned on evidence)?\n",
    "$\\max_{x_1, \\dotsc, x_n} p(y=1, x_1, \\dotsc, x_n)$\n",
    "\n",
    "Refs: [1](https://ermongroup.github.io/cs228-notes/inference/ve/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-prize",
   "metadata": {},
   "source": [
    "## Approximate solutions to the inference problem\n",
    "\n",
    "- Variational methods: Variational inference methods take their name from the calculus of variations, which deals with optimizing functions that take other functions as arguments., which formulate inference as an optimization problem\n",
    "- Sampling methods, which produce answers by repeatedly generating random numbers from a distribution of interest.\n",
    "\n",
    "$E_{x \\sim p}[f(x)] = \\sum_x f(x) p(x).$\n",
    "\n",
    "$E_{x \\sim p}[f(x)] \\approx I_T = \\frac{1}{T} \\sum_{t=1}^T f(x^t),$\n",
    "\n",
    "where $x^1, \\dotsc, x^T$ are samples drawn according to \n",
    "\n",
    "### unbiased estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-saint",
   "metadata": {},
   "source": [
    "# Approaches to Inference\n",
    "## 1) Approximate\n",
    "### 1-1) Randomized\n",
    "#### 1-1-1) importance sampling\n",
    "#### 1-1-2) MCMC\n",
    "#### 1-1-2-1) Gibbs\n",
    "### 1-2) Determinis\n",
    "#### 1-2-1) variational\n",
    "##### 1-2-1-1) mean field\n",
    "#### 1-2-2) loopy belief propagation\n",
    "#### 1-2-3) LP relaxations\n",
    "##### 1-2-3-1) dual decomp\n",
    "#### 1-2-4) beam search\n",
    "##### 1-2-4-1) local search\n",
    "\n",
    "## 2) Exact\n",
    "### 2-1) ILP\n",
    "### 2-2) variable Elimination\n",
    "#### 2-2-1) Dynamic Programing\n",
    "\n",
    "Refs [1](http://www.cs.cmu.edu/~nasmith/psnlp/lecture2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-meaning",
   "metadata": {},
   "source": [
    "## Intractable probability distribution\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/4417/what-are-the-factors-that-cause-the-posterior-distributions-to-be-intractable), [2](https://arxiv.org/pdf/1601.00670.pdf), [3](https://stats.stackexchange.com/questions/208176/why-is-the-posterior-distribution-in-bayesian-inference-often-intractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-payroll",
   "metadata": {},
   "source": [
    "## Inference and learning\n",
    "### Inferring unobserved variables\n",
    "### Parameter learning\n",
    "### Structure learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-lindsay",
   "metadata": {},
   "source": [
    "## Variational Lower Bound\n",
    "\n",
    "Assume that $X$ are observations (data) and $Z$ are hidden variables. The hidden variables might include the \"parameters\". The relationship of these two variables can be represented using the following graphical model\n",
    "\n",
    "<img src='images/hidden_observed.jpg'>\n",
    "\n",
    "Moreover, uppercase $P(X)$ denotes the probability distribution over that variable, and\n",
    "lowercase $p(X)$ is the density function of the distribution of $X$.\n",
    "\n",
    "The posterior distribution of the hidden variables can then be written as follows:\n",
    " \n",
    "\n",
    "$p(Z|X)=\\frac{p(X|Z)p(Z)}{p(x)}=\\frac{p(X|Z)p(Z)}{\\int_{Z} p(X,Z)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-honolulu",
   "metadata": {},
   "source": [
    "### First derivation: The Jensen’s inequality\n",
    "\n",
    "$p(X)=\\int_{Z}p(X,Z)$\n",
    "\n",
    "$log(p(X))=log\\int_{Z}p(X,Z)$\n",
    "\n",
    "$=log\\int_{Z}p(X,Z)\\frac{q(Z)}{q(Z)} $\n",
    "\n",
    "Remember, expected value of a function:\n",
    "\n",
    "$\\mathbb{E}[h(X)]=\\int_x h(x) \\cdot p(x) \\ dx$\n",
    "\n",
    "\n",
    "$=log E_{q}[\\frac{p(X,Z)}{q(z)}]$\n",
    "\n",
    "We also know that:\n",
    "\n",
    "$f(E(X))\\leq E(f(X))$\n",
    "\n",
    "Therefore we have:\n",
    "\n",
    "\n",
    "$log p(x) \\geq E_{q}[log\\frac{p(X,Z)}{q(Z)}]=E_{q}[log(p(X,Z))]-E_{q}[log(q(z))]$\n",
    "\n",
    "\n",
    "$L= E_{q}[log(p(X,Z))]-E_{q}[log(q(z))]$\n",
    "\n",
    "Then it is obvious that $L$ is a lower bound of the log probability of the observations.\n",
    "As a result, if in some cases we want to maximize the marginal probability, we can instead\n",
    "maximize its variational lower bound $L$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-correction",
   "metadata": {},
   "source": [
    "### Second derivation: KL divergence\n",
    "\n",
    "The main idea behind variational methods is: to find some approximation distributions $q(Z)$ that are as closed as possible to the true posterior distribution $p(Z|X)$. These\n",
    "approximation distribution can have their own variational parameters: $q(Z|θ)$, and we\n",
    "try to find the setting of the parameters that make $q$ close to the posterior of interest.\n",
    "Obviously the distribution $q(Z)$ should be relatively easy and more tractable for inference.\n",
    "\n",
    "\n",
    "To measure the closeness of the two distribution $q(Z)$ and $p(Z|X)$, a common metric\n",
    "is the Kullback-Leibler (KL) divergence. \n",
    "\n",
    "$KL[q(Z) \\parallel p(Z|X)]= \\int_{Z} q(Z)log \\frac{q(Z)}{p(Z|X)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)\\log \\frac{p(Z|X)}{q(Z)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)\\log \\frac{p(Z,X)}{p(x)q(Z)} $\n",
    "\n",
    "$= -\\int_{Z} q(Z)( \\log \\frac{p(Z,X)}{q(Z)} -\\log(p(x)))$\n",
    "\n",
    "$= -\\int_{Z} q(Z) \\log \\frac{p(Z,X)}{q(Z)} +\\int_{Z} q(Z)\\log(p(x))$\n",
    "\n",
    "\n",
    "since $q(𝑍)$ is a pdf function:\n",
    "\n",
    "$= -\\int_{Z} q(Z) \\log \\frac{p(Z,X)}{q(Z)} + \\log(p(x)$\n",
    "\n",
    "$= -L + \\log(p(x)$\n",
    "\n",
    "$L$ is the variational lower bound.\n",
    "\n",
    "Rearranging will give us the following:\n",
    "\n",
    "$L = \\log p(X) − KL [q(Z)kp(Z|X)]$\n",
    "\n",
    "\n",
    "since $KL$ divergence is always $\\geq 0$, once again we get $L \\leq log p(X)$. therefore ur goal is to maximize $L $\n",
    "\n",
    "### Example\n",
    "We want to maximize the log likelihood of the class label: $\\log p(y|I,W)$. Here $I$ is the image, $W$ is the model parameters and $y$ is the class label. Then, the objective function above can be rewritten by\n",
    "marginalizing over the locations l (hidden variables):\n",
    "\n",
    "$\\log p(y|I,W)=\\log$\n",
    "\n",
    "Refs: [1](http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-guess",
   "metadata": {},
   "source": [
    " Refs: [1](https://www.youtube.com/watch?v=Tc-XfiDPLf4&ab_channel=MLExplained-AggregateIntellect-AI.SCIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-crown",
   "metadata": {},
   "source": [
    "### probability measures vs. probability distributions vs. measure of probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-volleyball",
   "metadata": {},
   "source": [
    "## Marginal likelihood\n",
    "\n",
    "A marginal likelihood function (integrated likelihood), is a likelihood function in which some parameter variables have been marginalized. \n",
    "\n",
    "### In the context of Bayesian statistics\n",
    "Given a set of independent identically distributed data points ${\\displaystyle \\mathbf {X} =(x_{1},\\ldots ,x_{n}),}$, where $x_{i}\\sim p(x_{i}|\\theta )$ according to some probability distribution parameterized by $\\theta$ , where $\\theta$  itself is a random variable described by a distribution, i.e. ${\\displaystyle \\theta \\sim p(\\theta \\mid \\alpha ),}$ the marginal likelihood in general asks what the probability ${\\displaystyle p(\\mathbf {X} \\mid \\alpha )}$ is, where $\\theta$  has been marginalized out (integrated out): \n",
    "\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid \\alpha )=\\int _{\\theta }p(\\mathbf {X} \\mid \\theta )\\,p(\\theta \\mid \\alpha )\\ \\operatorname {d} \\!\\theta }$\n",
    "\n",
    "###  In classical statistics\n",
    "In In classical statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter ${\\displaystyle \\theta =(\\psi ,\\lambda )}$, where $\\psi$  is the actual parameter of interest, and $\\lambda$  is a non-interesting nuisance parameter.\n",
    "\n",
    "\n",
    "We know that:\n",
    "\n",
    "$P(B|C)=\\sum_{i} P(B|A_i,C)P(A_i|C) $\n",
    "\n",
    "And we also know \n",
    "\n",
    "${\\mathcal {L}}(\\theta|X)=p(X|\\theta)=p_{\\theta }(X)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "by marginalizing out $\\lambda$ :\n",
    "\n",
    "${\\displaystyle {\\mathcal {L}}(\\psi ;\\mathbf {X} )=p(\\mathbf {X} \\mid \\psi )=\\int _{\\lambda }p(\\mathbf {X} \\mid \\lambda ,\\psi )\\,p(\\lambda \\mid \\psi )\\ \\operatorname {d} \\!\\lambda }$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bayesian model comparison\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid M)=\\int p(\\mathbf {X} \\mid \\theta ,M)\\,p(\\theta \\mid M)\\,\\operatorname {d} \\!\\theta }$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-arthritis",
   "metadata": {},
   "source": [
    "## Some important extensions\n",
    "\n",
    "$P(x_1,...,x_n|y_1,...,y_m)=\\frac{P(x_1,...,x_n,y_1,...,y_m)}{P(y_1,...,y_m)}$\n",
    "\n",
    "\n",
    "$P(B|C)=\\frac{P(B,C)}{P(C)}=\\frac{\\sum_{A_i}P(B,A_i,C)}{P(C)}=\\sum_{A_i}P(B,A_i|C)$\n",
    "\n",
    "\n",
    "Proof:\n",
    "\n",
    "\n",
    "$P(A,B|C)=\\frac{P(A,B,C)}{P(C)}=\\frac{P(B,\\overbrace{A,C})}{P(C)}=\\frac{P(B|A,C)P(A,C)}{P(C)}=P(B|A,C)P(A|C)=P(A|B,C)P(B|C)$\n",
    "\n",
    "\n",
    "$P(B|C)=\\sum_{i} P(A_i|C)P(B|A_i,C) $\n",
    "\n",
    "\n",
    "$P(A|B,C)=\\frac{P(B|A,C)P(A|C)}{P(B|C)}$\n",
    "\n",
    "If $A$ and $B$ are conditionally independent of $C$\n",
    "\n",
    "$P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "$P(A|B,C)=P(A|C)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$P(A|B,C)=\\frac{P(A,B,C)}{P(B,C)}=\\frac{P(A,B|C)P(C)}{P(B|C)P(C)}$\n",
    "\n",
    "since $A$ and $B$ are conditionally independent of $C$, we have:  $P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "$\\frac{P(A,B|C)P(C)}{P(B|C)P(C)}=\\frac{P(A|C)P(B|C)P(C)}{P(B|C)P(C)}=P(A|C)$\n",
    "\n",
    "\n",
    "Refs: [1](http://users.ics.aalto.fi/harri/thesis/valpola_thesis/node16.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-personal",
   "metadata": {},
   "source": [
    "## Independent Event\n",
    "\n",
    "Two events $A,B$ are said to be statistically independent if and only if \n",
    "\n",
    "$P(A,B)=P(A)P(B)$\n",
    "\n",
    "$P(A|B)=\\frac{P(A,B)}{P(B)}=\\frac{P(A)P(B)}{P(B)}=P(A)$\n",
    "\n",
    "Also $\\bar{B}$ and $A$ are independent, $P(A,\\bar{B})=P(A)P(\\bar{B})$\n",
    "\n",
    "If $X$ and $Y$ are independent random variables, then the expectation operator $\\operatorname {E}$  has the property\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [XY]=\\operatorname {E} [X]\\operatorname {E} [Y]}$\n",
    "\n",
    "\n",
    "and the covariance ${\\displaystyle \\operatorname {cov} [X,Y]}$ is zero, as follows from\n",
    "\n",
    "${\\displaystyle \\operatorname {cov} [X,Y]=\\operatorname {E} [XY]-\\operatorname {E} [X]\\operatorname {E} [Y].}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-utility",
   "metadata": {},
   "source": [
    "## Conditionally Independent\n",
    "\n",
    "If $A$ and $B$ are conditionally independent of $C$, written symbolically as: ${\\displaystyle (A\\perp \\!\\!\\!\\perp B|C)}$\n",
    "\n",
    "$P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "$P(A|B,C)=P(A|C)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-first",
   "metadata": {},
   "source": [
    "## Semicolon notation in joint probability\n",
    "\n",
    "In $p_{\\theta} (x|z, y) = f(x; z, y, \\theta)$, \n",
    "\n",
    "$f(x; z, y, \\theta)$\n",
    "\n",
    "is a function of $x$ with \"parameters\" $y,x,\\theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-access",
   "metadata": {},
   "source": [
    "## Marginal distribution\n",
    "\n",
    "${\\displaystyle p_{X}(x_{i})=\\sum _{j}p(x_{i},y_{j})},$ and ${\\displaystyle \\ p_{Y}(y_{j})=\\sum _{i}p(x_{i},y_{j})}$\n",
    "\n",
    "A marginal probability can always be written as an expected value:\n",
    "\n",
    "${\\displaystyle p_{X}(x)=\\int _{y}p_{X\\mid Y}(x\\mid y)\\,p_{Y}(y)\\,\\mathrm {d} y=\\operatorname {E} _{Y}[p_{X\\mid Y}(x\\mid y)]\\;.}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-grace",
   "metadata": {},
   "source": [
    "## Expectation with respect to a probability distribution\n",
    "\n",
    "$\\mathbb{E}[X] = \\int_x x \\cdot p(x) \\ dx$\n",
    "\n",
    "$\\mathbb{E}[g(X)]=\\int_x g(x) \\cdot p(x) \\ dx$\n",
    "\n",
    "\n",
    "$\\mathbb{E}_{p(\\mathbf{x};\\mathbf{\\theta})}[f(\\mathbf{x};\\mathbf{\\phi})] = \\int p(\\mathbf{x};\\mathbf{\\theta}) f(\\mathbf{x};\\mathbf{\\phi}) d\\mathbf{x}$\n",
    "\n",
    "$\\mathbb{E}_{\\mathbf{x}}[f(\\mathbf{x};\\mathbf{\\phi})]$\n",
    "\n",
    "$\\mathbf{x} \\sim p(\\mathbf{x};\\mathbf{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-biography",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "$KL(q_\\phi(z|x) || P_\\theta(z|x))=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z|x)}=$\n",
    "\n",
    "$=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)p_\\theta(x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$=\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)} +\\int q_\\phi(z|x) \\log p_\\theta(x)$\n",
    "\n",
    "\n",
    "$=\\underbrace{ \\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}}_{-\\mathcal {L}}  +\\log p_\\theta(x)$\n",
    "\n",
    "\n",
    "$-\\mathcal {L}$, is variational lower bound.\n",
    "\n",
    "$\\mathcal {L}= -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$\\log p_\\theta(x)=\\mathcal {L}+KL(q_\\phi || P_\\theta)$\n",
    "\n",
    "$\\log p_\\theta(x) > \\mathcal {L}$\n",
    "\n",
    "The goal is minimize the $KL(q_\\phi || P_\\theta)$ w.r.t $\\phi$ ($p_{\\theta}$ is fixed w.r.t to $\\phi$) which means we have to maximize $\\mathcal {L}$\n",
    "\n",
    "\n",
    "$\\mathcal {L}= -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z,x)}$\n",
    "\n",
    "$P_\\theta(z,x)=p_\\theta(x|z)p_\\theta(z)$\n",
    "\n",
    "$\\mathcal {L}= E_q[\\log p_\\theta(x|z)]   -\\int q_\\phi(z|x) \\log \\frac{q_\\phi(z|x)}{P_\\theta(z)}$\n",
    "\n",
    "\n",
    "$1) \\mathcal {L}= E_q[\\log p_\\theta(x,z) -  \\log q_\\phi(z|x)]$\n",
    "\n",
    "\n",
    "$2) \\mathcal {L}= E_q[\\log p_\\theta(x|z)] - KL( q_\\phi(z|x)||p_\\phi(z)) $\n",
    "\n",
    "\n",
    "\n",
    "$\\log P(X) - D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)]=E[\\log P(X \\vert z)] - D_{KL}[Q(z \\vert X) \\Vert P(z)]$\n",
    "\n",
    "\n",
    "\n",
    "$\\mathcal{L}(\\theta, \\phi;x^{(i)}) = -D_{KL}(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) + \\mathbb{E}_{z{\\tilde{}}q}[logp_{\\theta}(x|z)]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-poland",
   "metadata": {},
   "source": [
    "### The Optimization Procedure\n",
    "\n",
    "- And we need to maximize the expectation of the reconstruction of data points from the latent vector, $E_q[\\log p_\\theta(x|z)]$. Maximizing this means that the decoder is getting better at reconstruction, This means that we need to minimize reconstruction loss, which is $\\mathcal{L}_R$\n",
    "\n",
    "- We need to minimize the divergence between the estimated latent vector and the true latent vector, $KL( q_\\phi(z|x)||p_\\phi(z))$,  Let’s call this loss as $\\mathcal{L}_{KL}$\n",
    "\n",
    "\n",
    "the KL divergence between those two distribution could be computed in closed form\n",
    "\n",
    "\n",
    "$D_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] = \\frac{1}{2} \\, \\left( \\textrm{tr}(\\Sigma(X)) + \\mu(X)^T\\mu(X) - k - \\log \\, \\det(\\Sigma(X)) \\right)$\n",
    "\n",
    "\n",
    "\n",
    "Above, $k$ is the dimension of our Gaussian. $tr(X)$ is trace function, i.e. sum of the diagonal of matrix $X$\n",
    ".The determinant of a diagonal matrix could be computed as product of its diagonal. So really, we could implement $\\Sigma(X)$, as just a vector as it’s a diagonal matrix:\n",
    "\n",
    "$D_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] $\n",
    "\n",
    "\n",
    "$= \\frac{1}{2} \\, \\left( \\sum_k \\Sigma(X) + \\sum_k \\mu^2(X) - \\sum_k 1 - \\log \\, \\prod_k \\Sigma(X) \\right)$\n",
    "\n",
    "\n",
    "$= \\frac{1}{2} \\, \\left( \\sum_k \\Sigma(X) + \\sum_k \\mu^2(X) - \\sum_k 1 - \\sum_k \\log \\Sigma(X) \\right)$ \n",
    "\n",
    "\n",
    "$= \\frac{1}{2} \\, \\sum_k \\left( \\Sigma(X) + \\mu^2(X) - 1 - \\log \\Sigma(X) \\right)$\n",
    "\n",
    "\n",
    "$-D_{KL}(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z))$\n",
    "\n",
    "\n",
    "Rewriting it as:\n",
    "\n",
    "$D_{KL}(q_{\\phi}(z|x^{(i)}) || p_{\\theta}(z)) = \\frac{1}{2}\\sum_{j=1}^{J}{(1+log(\\sigma_j)^2-(\\mu_j)^2-(\\sigma_j)^2)}$\n",
    "\n",
    "\n",
    "Here, $\\sigma_j$ is the standard deviation and $\\mu_j$ is the mean. We need $𝜎𝑗→1$ and $𝜇𝑗→1$\n",
    "\n",
    "\n",
    "\n",
    "So, the final VAE loss that we need to optimize is:\n",
    "$\\mathcal{L}_{VAE} = \\mathcal{L}_R + \\mathcal{L}_{KL}$\n",
    "\n",
    "\n",
    "Finally, we need to sample from the input space using the following formula.\n",
    "\n",
    "$Sample = \\mu + \\epsilon\\sigma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-cleaning",
   "metadata": {},
   "source": [
    "\n",
    "### Reparameterization trick\n",
    "\n",
    "$\\phi^{*},  \\theta^{*}=\\text{argmax} \\mathcal {L}(\\phi, \\theta;x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-ordinary",
   "metadata": {},
   "source": [
    "Refs [1](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/), [2](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
