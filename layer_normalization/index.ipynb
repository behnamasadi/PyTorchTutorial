{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf981a9-99ee-410c-9bb2-92ffeb0735f2",
   "metadata": {},
   "source": [
    "###  **Batch Normalization (BN)**\n",
    "\n",
    "**Where?**  \n",
    "- Commonly used in **CNNs** (Convolutional Neural Networks)  \n",
    "- Typically applied **between linear/convolution and activation layers**\n",
    "\n",
    "**How?**  \n",
    "- BN normalizes each feature **across the batch dimension**\n",
    "- Formula:  \n",
    "  $\n",
    "  \\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu^{(k)}}{\\sqrt{(\\sigma^{(k)})^2 + \\epsilon}}\n",
    "  $  \n",
    "  where:\n",
    "  - $ x^{(k)} $ is the k-th feature\n",
    "  - $ \\mu^{(k)} $, $ \\sigma^{(k)} $: mean and std over the batch\n",
    "  - $ \\epsilon $: small constant for numerical stability\n",
    "\n",
    "**Key Properties:**  \n",
    "- Uses **batch statistics** (mean and variance per feature across samples)\n",
    "- Has learnable **scale ($ \\gamma $)** and **shift ($ \\beta $)**\n",
    "- During inference, uses **running estimates** of mean/variance\n",
    "\n",
    "**Pros:**\n",
    "- Helps with **internal covariate shift**\n",
    "- Speeds up convergence\n",
    "- Often improves generalization\n",
    "\n",
    "**Cons:**\n",
    "- Depends on batch size (small batches may cause instability)\n",
    "- Not ideal for **RNNs** or **online/streaming data**\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Layer Normalization (LN)**\n",
    "\n",
    "**Where?**  \n",
    "- Used in **RNNs**, **Transformers**, **NLP models**, and **fully connected networks**\n",
    "\n",
    "**How?**  \n",
    "- LN normalizes **across features in a single sample**, **not across the batch**\n",
    "- Formula:  \n",
    "  $\n",
    "  \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $  \n",
    "  where:\n",
    "  - $ \\mu $, $ \\sigma $: mean and std over the features of one sample\n",
    "\n",
    "**Key Properties:**  \n",
    "- **Independent of batch size**\n",
    "- Always uses current sample statistics\n",
    "- Learnable **scale ($ \\gamma $)** and **shift ($ \\beta $)**\n",
    "\n",
    "**Pros:**\n",
    "- Works well with variable batch sizes\n",
    "- Stable for sequential models (RNNs, Transformers)\n",
    "\n",
    "**Cons:**\n",
    "- May be slightly slower convergence compared to BN in CNNs\n",
    "\n",
    "---\n",
    "\n",
    "###  **Quick Comparison**\n",
    "\n",
    "| Feature                  | BatchNorm                        | LayerNorm                        |\n",
    "|--------------------------|----------------------------------|----------------------------------|\n",
    "| Normalizes over          | Batch + feature axis             | Feature axis only (per sample)   |\n",
    "| Use case                 | CNNs                             | RNNs, Transformers, NLP          |\n",
    "| Batch size sensitive     | ‚úÖ Yes                           | ‚ùå No                            |\n",
    "| Inference mode           | Uses running stats               | Uses current sample              |\n",
    "| Sequential data          | ‚ùå Problematic                   | ‚úÖ Well suited                   |\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "\n",
    "- Use **BatchNorm** in CNNs with reasonably large batch sizes.\n",
    "- Use **LayerNorm** in NLP, RNNs, or attention-based models like Transformers.\n",
    "- For small batches or online learning, **LayerNorm** is more stable.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want code examples in PyTorch for either!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
