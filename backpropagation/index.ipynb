{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropagation for a single neuron layer\n",
    "<img src='images/nn_1_1_1_1_labled.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cost of a single sample from training set\n",
    "So the cost for one example in this network \n",
    "\n",
    "$C_0(...)=( a^{(L)}-y)^2$\n",
    "\n",
    "the $C_0$ means the cost for the first example\n",
    "\n",
    "\n",
    "$C_0(...)=( a^{(L)}-y)^2$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Output of neurons in each layer\n",
    "\n",
    "$z^{(L)}=w^{(L)}a^{(L-1)}+b^{(L)}$\n",
    "\n",
    "$a^{(L)}=\\sigma(z^{(L)}) $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Derivative of costs relative to weight\n",
    "\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial w^{(L)}  } = \\frac{ \\partial C_0   }{ \\partial a^{(L)}  } \\frac{ \\partial a^{(L)  } }{ \\partial z^{(L)}  }   \\frac{\\partial z^{(L)}} { \\partial w^{(L)} }  $\n",
    "\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial a^{(L)}}=2( a^{(L)}-y)   $\n",
    "\n",
    "$\\frac{ \\partial a^{(L)  } }{ \\partial z^{(L)} }= \\sigma' (z^{(L)}) $\n",
    "\n",
    "$\\frac{\\partial z^{(L)}} { \\partial w^{(L)} }=a^{(L-1)}$\n",
    "\n",
    "\n",
    "This is the cost for a specific example, \n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial w^{(L)}  } = 2( a^{(L)}-y) \\sigma' (z^{(L)})  a^{(L-1)}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Derivative of costs relative to bias\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial b^{(L)}  } = \\frac{ \\partial C_0   }{ \\partial a^{(L)}  } \\frac{ \\partial a^{(L)  } }{ \\partial z^{(L)}  }   \\frac{\\partial z^{(L)}} { \\partial b^{(L)} }  $\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial b^{(L)}  } =  2( a^{(L)}-y) \\sigma' (z^{(L)}) \\times 1$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Derivative of costs relative to the activation of previous layer\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial a^{(L-1)}  } = \\frac{ \\partial C_0   }{ \\partial a^{(L)}  } \\frac{ \\partial a^{(L)  } }{ \\partial z^{(L)}  }   \\frac{\\partial z^{(L)}} { \\partial a^{(L-1)} }  $\n",
    "\n",
    "$\\frac{ \\partial C_0   }{ \\partial b^{(L)}  } =  2( a^{(L)}-y) \\sigma' (z^{(L)}) w^{(L)}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Cost for all training\n",
    "The cost for all training example is average cost for all examples:\n",
    "\n",
    "\n",
    "$\\frac{ \\partial C}{ \\partial w^{(L)}  } = \\frac{1}{n} \\sum_{k=0}^{n-1} \\frac{ \\partial C_k   }{ \\partial w^{(L)}} $\n",
    "\n",
    "Which is a part of total derivative for all those weights and biases:\n",
    "\n",
    "$\\nabla C=\\begin{bmatrix} \\frac{ \\partial C}{ \\partial w^{(1)}  }  \\\\\n",
    "\\frac{ \\partial C}{ \\partial b^{(1)}  } \\\\ \\vdots   \\\\ \\frac{ \\partial C}{ \\partial w^{(L)}  } \\\\ \\frac{ \\partial C}{ \\partial b^{(L)}} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backpropagation for a multi neuron layer\n",
    "\n",
    "<img src='images/nn_a(1)_0.svg' height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subscript indicate which layer of neuron it is\n",
    "\n",
    "- $a_0^{(L-1)}$: First neuron in the layer $L-1$\n",
    "- $a_2^{(L)}$: Third neuron in the layer $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dimension of the MLP\n",
    "\n",
    "- Last layer $L$ has $n$ neuron, \n",
    "- Layer $L-1$ has $m$ neuron,\n",
    "- Therefore, the size of the weight matrix is: $\\textbf{W}^{(L)}_{n \\times m}$\n",
    "- Similarly layer $L-1$ has $m$ neuron, Layer $L-2$ has $p$ neuron, therefore: $\\textbf{W}^{(L-1)}_{m \\times p}$\n",
    "\n",
    "---\n",
    "\n",
    "$z^{(L)}=w^{(L)}a^{(L-1)}+b^{(L)}$\n",
    "\n",
    "$a^{(L)}=\\sigma(z^{(L)}) $\n",
    "\n",
    "---\n",
    "\n",
    "$z^{(L)}= \\textbf{W}^{(L)}_{n \\times m} \\times a^{(L-1)}_{m \\times 1} + b^{(L)}_{n \\times 1} $ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "For example, in the above example the first layer ($L=0$) has $6$ neurons  and the second layer  ($L=1$) has $4$ neurons:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "w_{0,0}^{(1)} &w_{0,1}^{(1)}  & \\cdots   &w_{0,5}^{(1)} \\\\ \n",
    "w_{1,0}^{(1)} &w_{1,1}^{(1)}  & \\cdots   &w_{1,5}^{(1)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{3,0}^{(1)} &w_{3,1}^{(1)}  & \\cdots   &w_{3,5}^{(1)} \n",
    "\\end{bmatrix}_{4 \\times 6} \\times$\n",
    "$\\begin{bmatrix}\n",
    "a^{(0)}_{0}\n",
    "\\\\ a^{(0)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ a^{(0)}_{5}\n",
    "\\end{bmatrix}_{6 \\times 1}\n",
    "+$\n",
    "$\\begin{bmatrix}\n",
    "b^{(1)}_{0}\n",
    "\\\\ b^{(1)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ b^{(1)}_{3}\n",
    "\\end{bmatrix}_{4 \\times 1}=$\n",
    "$\\begin{bmatrix}\n",
    "z^{(1)}_{0}\n",
    "\\\\ z^{(1)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ z^{(1)}_{3}\n",
    "\\end{bmatrix}_{4 \\times 1}$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(1)}_{0}=\\sigma( \n",
    "w_{0,0}\\times a^{(0)}_{0} +\n",
    "w_{0,1}\\times a^{(0)}_{1}+\n",
    "w_{0,2}\\times a^{(0)}_{2}+\n",
    "w_{0,3}\\times a^{(0)}_{3}+\n",
    "w_{0,4}\\times a^{(0)}_{4}+\n",
    "w_{0,5}\\times a^{(0)}_{5}+b_{0}^{(1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{j,k}^{(L)}$ indicates the weight that connect $k_{th}$ node in $L-1$ layer to $j_{th}$ node in the layer $L$ \n",
    "(we have $m$ neuron in the layer $L-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn_last_layer.svg' height=\"50%\" width=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{j}^{(L)}=w_{j,0}^{(L)}\\times a^{(L-1)}_{0} +\n",
    "w_{j,1}^{(L)}\\times a^{(L-1)}_{1}+\n",
    "\\cdots+\n",
    "w_{j,k}^{(L)}\\times a^{(L-1)}_{k}+\n",
    "\\cdots+\n",
    "w_{j,m-1}^{(L)}\\times a^{(L-1)}_{m-1}+b_{j}$\n",
    "\n",
    "$a^{(L)}_{j}=\\sigma(z_{j}^{(L)})$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last layer we have $n$ output, the cost for our $\\textbf{first}$ example in the training set, $c_{0}$ is:\n",
    "\n",
    "$c_{0}=\\sum_{j=0}^{n-1} (a_{j}^{(L)}-y_{j})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Computing error relative to the changes of weights in the last layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Computing error relative to the changes of biases in the last layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 $\\delta^{(L)}_{j}$: The error of neuron $j$ in layer $L$\n",
    "\n",
    "\n",
    "1) $\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})$\n",
    "\n",
    "2) $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}=\\sigma^{\\prime}(z_{j}^{(L)})$\n",
    "\n",
    "3) $\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}=a^{(L-1)}_{k}$\n",
    "\n",
    "4) $\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}=1$\n",
    "\n",
    "\n",
    "Since $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}$ is common in $\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}}$ and $\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}}$ we call it $delta$ $\\delta^{(L)}_{j}$ which is the error of neuron $j$ in layer $L$:\n",
    "\n",
    "$\\delta^{(L)}_{j} =\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}=\\frac{\\partial c}{\\partial z_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us:\n",
    "\n",
    "- $\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})a^{(L-1)}_{k}=\\delta^{(L)}_{j}a^{(L-1)}_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})=\\delta^{(L)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Writing backpropagation equations in matrix form\n",
    "\n",
    "### 2.5.1 $\\delta^L$: The error term at the output layer $L$\n",
    "\n",
    "\n",
    "The error term at the output layer:\n",
    "$\n",
    "\\delta^L = \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\frac{\\partial C}{\\partial a^{(L)}}\n",
    "$\n",
    "\n",
    "\n",
    "#### 1) $ \\frac{\\partial C}{\\partial a^{(L)}} $\n",
    "\n",
    "\n",
    "\n",
    "Let say our out put size is $2$:\n",
    "\n",
    "- $ a^{(L)} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ C \\in \\mathbb{R} $ (a scalar loss function)\n",
    "- So $ \\frac{\\partial C}{\\partial a^{(L)}} $ should have what size?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Cost function $ C \\in \\mathbb{R} $\n",
    "\n",
    "For scalar-valued functions, like the MSE loss:\n",
    "$\n",
    "C = \\frac{1}{2} \\| a^L - y \\|^2 = \\frac{1}{2} \\sum_i (a^L_i - y_i)^2\n",
    "$\n",
    "\n",
    "Then:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a^L} = a^L - y \\in \\mathbb{R}^{2 \\times 1}\n",
    "$\n",
    "\n",
    "- $ a^L $ is a $ 2 \\times 1 $ vector\n",
    "- So the gradient of scalar $ C $ with respect to a vector is also a $ 2 \\times 1 $ **vector of partial derivatives**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) $ \\frac{\\partial a^L}{\\partial z^L} $\n",
    "\n",
    "This is the derivative of the **activation function**, element-wise, What are the dimensions of $ \\partial a^{(L)} $ and $ \\partial z^{(L)} $, and what is the shape of their derivative?\n",
    "$\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Assume the output layer $ L $ has $ n_L $ neurons. Then:\n",
    "\n",
    "- $ a^{(L)} \\in \\mathbb{R}^{n_L \\times 1} $: activation (output) vector\n",
    "- $ z^{(L)} \\in \\mathbb{R}^{n_L \\times 1} $: pre-activation (weighted input) vector\n",
    "\n",
    "Each element:\n",
    "$\n",
    "a^{(L)}_i = \\sigma(z^{(L)}_i)\n",
    "$\n",
    "\n",
    "So $ a^{(L)} $ is computed **element-wise** from $ z^{(L)} $ via the sigmoid (or other) activation function.\n",
    "\n",
    "---\n",
    "\n",
    "What Is $ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} $?\n",
    "\n",
    "This is the derivative of a **vector-valued function** with respect to a **vector**.\n",
    "\n",
    "#### Full Jacobian Form:\n",
    "\n",
    "In general, for vector-valued $ a^{(L)} \\in \\mathbb{R}^{n_L \\times 1} $, and vector-valued $ z^{(L)} \\in \\mathbb{R}^{n_L \\times 1} $, the derivative is a **Jacobian matrix**:\n",
    "\n",
    "$\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\in \\mathbb{R}^{n_L \\times n_L}\n",
    "$\n",
    "\n",
    "But here's the trick:  \n",
    "Since the activation function is **applied element-wise**, the Jacobian is a **diagonal matrix**.\n",
    "\n",
    "For sigmoid:\n",
    "$\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z_1) & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma'(z_2) & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma'(z_{n_L})\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "So:\n",
    "- Shape of $ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} $: $ n_L \\times n_L $\n",
    "- Shape of $ \\partial a^{(L)} $ or $ a^{(L)} $: $ n_L \\times 1 $\n",
    "- Shape of $ \\partial z^{(L)} $: $ n_L \\times 1 $\n",
    "\n",
    "---\n",
    "**Example: If $ n_L = 2 $**\n",
    "\n",
    "Let‚Äôs say:\n",
    "- $ a^{(L)} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} $\n",
    "- $ z^{(L)} = \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} $\n",
    "\n",
    "Then:\n",
    "$\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial a_1}{\\partial z_1} & 0 \\\\\n",
    "0 & \\frac{\\partial a_2}{\\partial z_2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z_1) & 0 \\\\\n",
    "0 & \\sigma'(z_2)\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{2 \\times 2}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**So Why Do We Use Element-wise Product?**\n",
    "\n",
    "In neural network code (like NumPy or PyTorch), we **don't explicitly construct the Jacobian** because:\n",
    "$\n",
    "\\delta^L = \\left( \\frac{\\partial C}{\\partial a^L} \\right)^T \\cdot \\left( \\frac{\\partial a^L}{\\partial z^L} \\right)\n",
    "$\n",
    "\n",
    "Would become:\n",
    "$\n",
    "\\delta^L = J \\cdot v\n",
    "\\quad \\text{(Jacobian times gradient vector)}\n",
    "$\n",
    "\n",
    "But since $ J $ is diagonal, this is equivalent to **element-wise multiplication**:\n",
    "\n",
    "\n",
    "$\n",
    "\\delta^L = \\left( a^L - y \\right) \\odot \\sigma'(z^L)\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\frac{\\partial a^L}{\\partial z^L}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Expression | Meaning | Shape |\n",
    "|------------|---------|-------|\n",
    "| $ a^L $ | activation vector | $ n_L \\times 1 $ |\n",
    "| $ z^L $ | pre-activation vector | $ n_L \\times 1 $ |\n",
    "| $ \\frac{\\partial a^L}{\\partial z^L} $ | Jacobian (element-wise derivative) | $ n_L \\times n_L $, diagonal matrix |\n",
    "| $ \\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\sigma'(z^L) $ | vector of errors | $ n_L \\times 1 $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ a^L = \\sigma(z^L) $, then:\n",
    "\n",
    " \n",
    "$\n",
    "\\frac{\\partial a^L}{\\partial z^L} = \\sigma(z^L) \\odot (1 - \\sigma(z^L)) \\in \\mathbb{R}^{2 \\times 1}\n",
    "$\n",
    "\n",
    "Again, this is also a **vector**, not a matrix or Jacobian here, because the sigmoid is applied element-wise.\n",
    "\n",
    "\n",
    "$\n",
    "\\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\frac{\\partial a^L}{\\partial z^L}\n",
    "$\n",
    "\n",
    "So this is an **element-wise product** of two $ 2 \\times 1 $ vectors:\n",
    "$\n",
    "\\delta^L \\in \\mathbb{R}^{2 \\times 1}\n",
    "$\n",
    "\n",
    "\n",
    "- $ \\frac{\\partial C}{\\partial a^L} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\delta^L \\in \\mathbb{R}^{2 \\times 1} $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  What If We Were Doing Matrix Calculus?\n",
    "\n",
    "If you wrote everything using **Jacobian matrices**, then:\n",
    "- $ \\frac{\\partial C}{\\partial a^L} $ would be $ 1 \\times 2 $\n",
    "- $ \\frac{\\partial a^L}{\\partial z^L} $ would be $ 2 \\times 2 $ diagonal matrix\n",
    "- And:\n",
    "$\n",
    "\\delta^L = \\left( \\frac{\\partial C}{\\partial a^L} \\right) \\cdot \\left( \\frac{\\partial a^L}{\\partial z^L} \\right)\n",
    "\\in \\mathbb{R}^{1 \\times 2}\n",
    "$\n",
    "\n",
    "But in deep learning frameworks and hand-calculations, we **drop the Jacobians** and use element-wise Hadamard products ‚Äî which is what your original formula assumes (and correctly uses).\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Summary\n",
    "\n",
    "- $ C \\in \\mathbb{R} $\n",
    "- $ a^L \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\frac{\\partial C}{\\partial a^L} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\frac{\\partial a^L}{\\partial z^L} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- So:\n",
    "  $\n",
    "  \\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\sigma'(z^L) \\in \\mathbb{R}^{2 \\times 1}\n",
    "  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  The Two Worlds: Vector Calculus vs Matrix Calculus\n",
    "\n",
    "We Were Doing Matrix Calculus then the $\\delta^L$  is ${1 \\times 2}$ but if we don't use the Jacobian it is $\\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\sigma'(z^L) \\in \\mathbb{R}^{2 \\times 1}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are **two conventions** used in computing derivatives of vector functions:\n",
    "\n",
    "### ‚úÖ 1. **Vector (Coordinate-wise) Calculus**\n",
    "- This is what we typically use in deep learning.\n",
    "- Gradients are treated as **column vectors**.\n",
    "- Element-wise operations (like sigmoid) make life easier.\n",
    "- The error at the output is:\n",
    "  $\n",
    "  \\delta^L = \\frac{\\partial C}{\\partial a^L} \\odot \\sigma'(z^L) \\in \\mathbb{R}^{n_L \\times 1}\n",
    "  $\n",
    "\n",
    "This is the standard **backpropagation view**, and how it's implemented in frameworks like PyTorch, TensorFlow, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è 2. **Matrix Calculus (Jacobian-based)**\n",
    "\n",
    "In strict matrix calculus, you define gradients as:\n",
    "- For scalar function $ C: \\mathbb{R}^n \\rightarrow \\mathbb{R} $,\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial a^L} \\in \\mathbb{R}^{1 \\times n_L}\n",
    "  $\n",
    "  (a row vector)\n",
    "- And the Jacobian of $ \\sigma: \\mathbb{R}^{n_L} \\rightarrow \\mathbb{R}^{n_L} $ is:\n",
    "  $\n",
    "  \\frac{\\partial a^L}{\\partial z^L} \\in \\mathbb{R}^{n_L \\times n_L}\n",
    "  $\n",
    "\n",
    "Then, to compute $ \\delta^L $ (which is $ \\frac{\\partial C}{\\partial z^L} $), you would do:\n",
    "$\n",
    "\\delta^L = \\frac{\\partial C}{\\partial a^L} \\cdot \\frac{\\partial a^L}{\\partial z^L}\n",
    "\\in \\mathbb{R}^{1 \\times n_L}\n",
    "$\n",
    "\n",
    "So yes ‚Äî in this formal matrix calculus:\n",
    "- $ \\delta^L \\in \\mathbb{R}^{1 \\times n_L} $\n",
    "\n",
    "But that leads to:\n",
    "- Transposed weight matrices everywhere\n",
    "- Extra care for dimensions in chain rule\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Deep Learning Convention\n",
    "\n",
    "In deep learning, we **drop the full Jacobians** and use element-wise derivatives. So instead of:\n",
    "\n",
    "$\n",
    "\\delta^L = \\left( \\frac{\\partial C}{\\partial a^L} \\right) \\cdot \\left( \\frac{\\partial a^L}{\\partial z^L} \\right)\n",
    "\\quad (\\text{matrix calculus})\n",
    "$\n",
    "\n",
    "We use:\n",
    "$\n",
    "\\delta^L = \\left( \\frac{\\partial C}{\\partial a^L} \\right) \\odot \\sigma'(z^L)\n",
    "\\quad (\\text{element-wise / vector calculus})\n",
    "$\n",
    "\n",
    "Which gives:\n",
    "- $ \\delta^L \\in \\mathbb{R}^{n_L \\times 1} $ (a column vector)\n",
    "\n",
    "This is far more intuitive and easier to implement.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Clarification\n",
    "\n",
    "| Perspective | $ \\frac{\\partial C}{\\partial a^L} $ | $ \\delta^L $ | Notes |\n",
    "|-------------|------------------------------|----------------|-------|\n",
    "| Matrix calculus | $ 1 \\times n_L $ (row vector) | $ 1 \\times n_L $ | Uses full Jacobians |\n",
    "| Deep learning / coordinate-wise | $ n_L \\times 1 $ (column vector) | $ n_L \\times 1 $ | Uses Hadamard product |\n",
    "\n",
    "So you're exactly right: the **same concept** has **different dimensions depending on the convention**, and deep learning typically follows the column vector (Hadamard product) style.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see this distinction visualized with an example (e.g., comparing both forms on a small network)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can write the equation for $\\delta^{(L)}_{j}$ in vector form:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\boldsymbol{\\delta}^{(L)} &= (\\delta^{(L)}_{0},\\delta^{(L)}_{1},...,\\delta^{(L)}_{n-1} ) \\\\\n",
    "&= \\frac{\\partial \\textbf{c}}{\\partial \\textbf{z}^{(L)}} \\\\\n",
    "&= (\\frac{\\partial c}{\\partial z_{0}^{(L)}} ,\\frac{\\partial c}{\\partial z_{1}^{(L)}},... ,\\frac{\\partial c}{\\partial z_{n-1}^{(L)}} ) \\\\\n",
    "&= (\\frac{\\partial a_{0}^{(L)}}{\\partial z_{0}^{(L)}}\\frac{\\partial c}{\\partial a_{0}^{(L)}},\\frac{\\partial a_{1}^{(L)}}{\\partial z_{1}^{(L)}}\\frac{\\partial c}{\\partial a_{1}^{(L)}},...\\frac{\\partial a_{n-1}^{(L)}}{\\partial z_{n-1}^{(L)}}\\frac{\\partial c}{\\partial a_{n-1}^{(L)}}) \\\\\n",
    "&= (2(a_{0}^{(L)}-y_{0})\\sigma^{\\prime}(z_{0}^{(L)}),2(a_{1}^{(L)}-y_{1})\\sigma^{\\prime}(z_{1}^{(L)}),...2(a_{n-1}^{(L)}-y_{n-1})\\sigma^{\\prime}(z_{n-1}^{(L)})) \\\\\n",
    "&= 2(\\textbf{a}^{(L)}-\\textbf{y}) \\odot \\sigma^{\\prime}(\\textbf{z}^{(L)})\\tag{1}\n",
    "\\end{align*}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 $\\frac{\\partial C}{\\partial w^L}$: The error term relative to weight in layer $L$\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^L} = \\delta^L (a^{L-1})^T\n",
    "$\n",
    "\n",
    "\n",
    "At any layer $ L $, the **pre-activation** value (before the activation function) is:\n",
    "\n",
    "$\n",
    "z^L = w^L a^{L-1} + b^L\n",
    "\\quad \\text{where } w^L \\in \\mathbb{R}^{n_L \\times n_{L-1}}\n",
    "$\n",
    "\n",
    "Then:\n",
    "$\n",
    "a^L = \\sigma(z^L)\n",
    "$\n",
    "\n",
    "The loss $ C $ depends on the final output $ a^L $, and we're interested in how changing each weight in $ w^L $ affects the cost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^L_{ij}}\n",
    "$\n",
    "\n",
    "That is how does changing the weight from neuron $ j $ in layer $ L-1 $ to neuron $ i $ in layer $ L $ affect the cost?\n",
    "\n",
    "---\n",
    "\n",
    "**Chain Rule View**\n",
    "\n",
    "Use the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^L_{ij}} = \\frac{\\partial C}{\\partial z^L_i} \\cdot \\frac{\\partial z^l_i}{\\partial w^L_{ij}}\n",
    "$\n",
    "\n",
    "But:\n",
    "- $ \\frac{\\partial C}{\\partial z^l_i} = \\delta^l_i $\n",
    "- And $ z^L_i = \\sum_j w^L_{ij} a^{L-1}_j + b^l_i \\Rightarrow \\frac{\\partial z^L_i}{\\partial w^L_{ij}} = a^{L-1}_j $\n",
    "\n",
    "So:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^L_{ij}} = \\delta^L_i \\cdot a^{L-1}_j\n",
    "$\n",
    "\n",
    "From our previous section:\n",
    "\n",
    "- $\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})a^{(L-1)}_{k}=\\delta^{(L)}_{j}a^{(L-1)}_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write this in matrix form.\n",
    "\n",
    "\n",
    "\n",
    "Let:\n",
    "- $ \\delta^l \\in \\mathbb{R}^{n_l \\times 1} $\n",
    "- $ a^{l-1} \\in \\mathbb{R}^{n_{l-1} \\times 1} $\n",
    "\n",
    "We want to build a full matrix of all partial derivatives:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^l} \\in \\mathbb{R}^{n_l \\times n_{l-1}}\n",
    "$\n",
    "\n",
    "So we form the **outer product**:\n",
    "$\n",
    "\\delta^l (a^{l-1})^T \\in \\mathbb{R}^{n_l \\times n_{l-1}}\n",
    "$\n",
    "\n",
    "Each element is:\n",
    "$\n",
    "[\\delta^l (a^{l-1})^T]_{ij} = \\delta^l_i \\cdot a^{l-1}_j\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**So Why the Transpose?**\n",
    "\n",
    "Because:\n",
    "- $ a^{l-1} $ is a column vector\n",
    "- You want $ \\delta^l \\cdot (a^{l-1})^T $ to get a **matrix**, not a scalar\n",
    "\n",
    "### Example:\n",
    "\n",
    "If:\n",
    "- $ \\delta^L = \\begin{bmatrix} \\delta_1 \\\\ \\delta_2 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ a^{L-1} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 1} $\n",
    "\n",
    "Then:\n",
    "$\n",
    "\\delta^L (a^{L-1})^T = \n",
    "\\begin{bmatrix}\n",
    "\\delta_1 \\cdot a_1 & \\delta_1 \\cdot a_2 & \\delta_1 \\cdot a_3 \\\\\n",
    "\\delta_2 \\cdot a_1 & \\delta_2 \\cdot a_2 & \\delta_2 \\cdot a_3\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{2 \\times 3}\n",
    "$\n",
    "\n",
    "Which is the correct shape for $ w^L $ (2 neurons, each with 3 inputs).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- The gradient of the cost w.r.t. a weight is:\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial w^L_{ij}} = \\delta^L_i \\cdot a^{L-1}_j\n",
    "  $\n",
    "- In matrix form:\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial w^L} = \\delta^L (a^{L-1})^T\n",
    "  $\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $m-1$ neuron in the layer $L-1$ \n",
    "- $\\frac{\\partial \\textbf{c}}{\\partial \\textbf{w}_{j}^{(L)}}\n",
    "=(\\frac{\\partial z_{j}^{(L)}}{\\partial w_{j0}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}},\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{j1}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}},...,\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jm-1}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}})\n",
    "=(\\delta^{(L)}_{j}a^{(L-1)}_{0},\\delta^{(L)}_{j}a^{(L-1)}_{1},...,\\delta^{(L)}_{j}a^{(L-1)}_{m-1})\n",
    "=\\delta_{j}^{(L)} \\textbf{a}^{(L-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we write it in the matrix form:\n",
    "$ \\frac{\\partial \\textbf{c}}{\\partial \\textbf{W}^{(L)}}=\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}a^{(L-1)}_{0} & \\delta^{(L)}_{0}a^{(L-1)}_{1}&...&\\delta^{(L)}_{0}a^{(L-1)}_{m-1}\\\\ \n",
    "\\delta^{(L)}_{1}a^{(L-1)}_{0} & \\delta^{(L)}_{1}a^{(L-1)}_{1}&...&\\delta^{(L)}_{1}a^{(L-1)}_{m-1}\\\\\n",
    "\\vdots & \\vdots & \\vdots &\\vdots \\\\ \n",
    "\\delta^{(L)}_{j}a^{(L-1)}_{0} & \\delta^{(L)}_{j}a^{(L-1)}_{1}&...&\\delta^{(L)}_{j}a^{(L-1)}_{m-1}\\\\\n",
    "\\vdots & \\vdots & \\vdots &\\vdots \\\\ \n",
    "\\delta^{(L)}_{n-1}a^{(L-1)}_{0} & \\delta^{(L)}_{n-1}a^{(L-1)}_{1}&...&\\delta^{(L)}_{n-1}a^{(L-1)}_{m-1}\\\\ \n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "a^{(L-1)}_{0} & a^{(L-1)}_{1}&...&a^{(L-1)}_{m-1}\\\\ \n",
    "\\end{bmatrix}=\n",
    "\\boldsymbol{\\delta}^{(L)} \\cdot \\textbf{a}^{(L-1)}\\top \\tag{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 $\\frac{\\partial C}{\\partial b^L}$: The error relative to bias in layer $L$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\textbf{c}}{\\partial \\textbf{b}^{(L)}}=\\boldsymbol{\\delta}^{(L)} \\tag{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The error relative to the changes of activation in the previous layers\n",
    "<img src='images/nn_l-1_layer_a.svg' height=\"50%\" width=\"50%\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 $ \\frac{\\partial c_{0}}{\\partial a_{k}^{(L-1)}}$ error relative to the changes of single activation in the previous layers\n",
    "\n",
    "We know that $a_{k}^{(L-1)}$ (layer $L-1$ has $m$ neurons) has effect on all neuron in the layer $L$ layer (which has $n$ neurons), so to compute the rate of changes of error with respect to $a_{k}^{(L-1)}$ :\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial a_{k}^{(L-1)}} =\n",
    "\\sum_{j=0}^{n-1}( \n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial a_{k}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}})\n",
    "$\n",
    "\n",
    "\n",
    "This expression tells us the total influence of $ a_k^{(L-1)} $ on the cost is the **sum of its effects through all neurons in the next layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the rate of change of the cost with respect to an activation in the previous layer $ a_k^{(L-1)} $, we use the chain rule across all neurons in layer $ L $:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a_k^{(L-1)}} =\n",
    "\\sum_{j=0}^{n-1}\n",
    "\\left(\n",
    "\\underbrace{\\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}}}_{=w_{jk}^{(L)}} \\cdot\n",
    "\\underbrace{\\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}}}_{=\\sigma'(z_j^{(L)})} \\cdot\n",
    "\\underbrace{\\frac{\\partial C}{\\partial a_j^{(L)}}}_{\\text{from output or recursive}}\n",
    "\\right)\n",
    "$\n",
    "\n",
    "This explains how the signal \"flows backward\" from every neuron in the next layer back to neuron $ k $ in layer $ L-1 $.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Subsequently we have:\n",
    "\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial a_{k}^{(L-1)}} =\n",
    "\\sum_{i=0}^{n-1} 2(a_{i}^{(L)}-y_{i})\\sigma^{\\prime}(z_{i}^{(L)}) w_{ik}^{(L)}=\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ik}^{(L)}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a_k^{(L-1)}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ \\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}} = w_{jk}^{(L)} $,  \n",
    "\n",
    "We know:\n",
    "\n",
    "$z_{j}^{(L)}=w_{j,0}^{(L)}\\times a^{(L-1)}_{0} +\n",
    "w_{j,1}^{(L)}\\times a^{(L-1)}_{1}+\n",
    "\\cdots+\n",
    "w_{j,i}^{(L)}\\times a^{(L-1)}_{i}+\n",
    "\\cdots+\n",
    "w_{j,k}^{(L)}\\times a^{(L-1)}_{k}+b_{j}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{\\partial z_{j}^{(L)}}{\\partial a_{k}^{(L-1)}}=w_{jk}^{(L)}$\n",
    "\n",
    "  because $ z_j^{(L)} = \\sum_{i=0}^{m-1} w_{ji}^{(L)} a_i^{(L-1)} + b_j^{(L)} $\n",
    "\n",
    "\n",
    "- $ \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} = \\sigma'(z_j^{(L)}) $,  \n",
    "  the derivative of the activation function\n",
    "\n",
    "- $ \\frac{\\partial C}{\\partial a_j^{(L)}} $:  \n",
    "  how the cost depends on the activation of neuron $ j $ in layer $ L $\n",
    "\n",
    "So you can rewrite it as:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a_k^{(L-1)}} =\n",
    "\\sum_{j=0}^{n-1}\n",
    "\\left(\n",
    "w_{jk}^{(L)} \\cdot \\sigma'(z_j^{(L)}) \\cdot \\frac{\\partial C}{\\partial a_j^{(L)}}\n",
    "\\right)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Let‚Äôs say:\n",
    "- Layer $ L-1 $ has $ m = 3 $ neurons\n",
    "- Layer $ L $ has $ n = 2 $ neurons\n",
    "\n",
    "That is:\n",
    "\n",
    "- $ a^{(L-1)} \\in \\mathbb{R}^{3 \\times 1} $\n",
    "- $ w^{(L)} \\in \\mathbb{R}^{2 \\times 3} $\n",
    "- $ z^{(L)} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ a^{(L)} = \\sigma(z^{(L)}) \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\frac{\\partial C}{\\partial a^{(L)}} \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\sigma'(z^{(L)}) \\in \\mathbb{R}^{2 \\times 1} $\n",
    "- $ \\delta^L = \\frac{\\partial C}{\\partial a^{(L)}} \\odot \\sigma'(z^{(L)}) \\in \\mathbb{R}^{2 \\times 1} $\n",
    "\n",
    "---\n",
    "\n",
    "**Equation with Dimensions**\n",
    "\n",
    "You‚Äôre computing:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a_k^{(L-1)}} =\n",
    "\\sum_{j=0}^{n-1}\n",
    "\\left(\n",
    "\\underbrace{\\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}}}_{\\textcolor{blue}{\\mathbb{R}}} \\cdot\n",
    "\\underbrace{\\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}}}_{\\textcolor{green}{\\mathbb{R}}} \\cdot\n",
    "\\underbrace{\\frac{\\partial C}{\\partial a_j^{(L)}}}_{\\textcolor{red}{\\mathbb{R}}}\n",
    "\\right)\n",
    "$\n",
    "\n",
    "Now the dimensions:\n",
    "\n",
    "| Term | Description | Shape |\n",
    "|------|-------------|-------|\n",
    "| $ w^{(L)} $ | Weights from layer $ L-1 $ to $ L $ | $ \\mathbb{R}^{2 \\times 3} $ |\n",
    "| $ \\frac{\\partial z_j^{(L)}}{\\partial a_k^{(L-1)}} $ | One scalar: the weight $ w_{jk}^{(L)} $ | $ \\mathbb{R} $ |\n",
    "| $ \\frac{\\partial a_j^{(L)}}{\\partial z_j^{(L)}} $ | Derivative of activation function | $ \\mathbb{R} $ |\n",
    "| $ \\frac{\\partial C}{\\partial a_j^{(L)}} $ | Gradient of cost w.r.t. activation | $ \\mathbb{R} $ |\n",
    "| Whole sum | Gradient of cost w.r.t. $ a_k^{(L-1)} $ | $ \\mathbb{R} $ |\n",
    "\n",
    "So for each $ k \\in [0, 2] $, you're summing over the $ n = 2 $ neurons in the next layer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 $\\frac{\\partial C}{\\partial a^{(L-1)}}$ Error relative to the changes of activations in the previous layers in matrix Form\n",
    "\n",
    "\n",
    "\n",
    "When we write the above in vector/matrix form for the full gradient, we get:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial \\mathbf{a}^{(L-1)}} =\n",
    "(w^{(L)})^T \\cdot \\delta^L\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ w^{(L)} \\in \\mathbb{R}^{n \\times m} $\n",
    "- $ \\delta^L \\in \\mathbb{R}^{n \\times 1} $, with each entry:\n",
    "  $\n",
    "  \\delta_j^{(L)} = \\frac{\\partial C}{\\partial a_j^{(L)}} \\cdot \\sigma'(z_j^{(L)})\n",
    "  $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Quantity | Shape |\n",
    "|----------|-------|\n",
    "| $ w^{(L)} $ | $ \\mathbb{R}^{2 \\times 3} $ |\n",
    "| $ (w^{(L)})^T $ | $ \\mathbb{R}^{3 \\times 2} $ |\n",
    "| $ \\delta^L $ | $ \\mathbb{R}^{2 \\times 1} $ |\n",
    "| $ \\frac{\\partial C}{\\partial a^{(L-1)}} $ | $ \\mathbb{R}^{3 \\times 1} $ ‚úÖ |\n",
    "\n",
    "So the result has one scalar for **each activation** in layer $ L-1 $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting all together \n",
    "\n",
    "\n",
    "The way we compute the **error term $ \\delta^L $** at the **output layer** is different from how we compute the error $ \\delta^l $ in **hidden layers**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 The Purpose of $ \\delta^l $\n",
    "\n",
    "In backpropagation, the error term at each layer is:\n",
    "\n",
    "$\n",
    "\\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "$\n",
    "\n",
    "This is the **gradient of the cost function with respect to the weighted input** $ z^l $, not the activation $ a^l $.\n",
    "\n",
    "It‚Äôs used to:\n",
    "- Compute how much each neuron contributed to the final error\n",
    "- Update weights and biases:  \n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial w^l} = \\delta^l (a^{l-1})^T, \\quad \\frac{\\partial C}{\\partial b^l} = \\delta^l\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Case 1: Output Layer $ \\delta^L $\n",
    "\n",
    "Here, the cost function is **directly dependent** on the output activation $ a^L $, so you can directly compute:\n",
    "\n",
    "$\n",
    "\\delta^L = \\frac{\\partial C}{\\partial z^L}\n",
    "= \\frac{\\partial C}{\\partial a^L} \\odot \\sigma'(z^L)\n",
    "$\n",
    "\n",
    "If you‚Äôre using **mean squared error**:\n",
    "$\n",
    "C = \\frac{1}{2} \\| a^L - y \\|^2 \\quad \\Rightarrow \\quad \\frac{\\partial C}{\\partial a^L} = a^L - y\n",
    "$\n",
    "\n",
    "So:\n",
    "$\n",
    "\\delta^L = (a^L - y) \\odot \\sigma'(z^L)\n",
    "$\n",
    "\n",
    "This is **explicitly derived** from the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Case 2: Hidden Layers $ \\delta^l $, for $ l < L $\n",
    "\n",
    "For hidden layers, the cost **does not directly depend** on $ a^l $, so we must **propagate the error backward** through the next layer using the chain rule.\n",
    "\n",
    "We compute:\n",
    "\n",
    "$\n",
    "\\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "= \\left( \\frac{\\partial z^{l+1}}{\\partial a^l} \\right)^T \\cdot \\frac{\\partial C}{\\partial z^{l+1}} \\odot \\sigma'(z^l)\n",
    "$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$\n",
    "\\delta^l = (w^{l+1})^T \\delta^{l+1} \\odot \\sigma'(z^l)\n",
    "$\n",
    "\n",
    "This equation has three components:\n",
    "1. $ (w^{l+1})^T \\delta^{l+1} $: pulls the error **backward**\n",
    "2. $ \\odot $: element-wise product (Hadamard)\n",
    "3. $ \\sigma'(z^l) $: applies local sensitivity of activation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Layer Type     | Equation                                                  | Reason |\n",
    "|----------------|-----------------------------------------------------------|--------|\n",
    "| Output Layer   | $ \\delta^L = (a^L - y) \\odot \\sigma'(z^L) $             | Cost function is defined on $ a^L $ |\n",
    "| Hidden Layer   | $ \\delta^l = (w^{l+1})^T \\delta^{l+1} \\odot \\sigma'(z^l) $ | We propagate gradient backward using the chain rule |\n",
    "\n",
    "---\n",
    "\n",
    "**Intuitive Analogy**\n",
    "\n",
    "- At the **output layer**, we **see the error** directly (compare predicted vs target).\n",
    "- At **hidden layers**, we don‚Äôt see the error directly ‚Äî we only know **how errors in the next layer depend on these neurons**, so we propagate the error backward using the weights.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "\n",
    "- $ \\delta^L $ is computed **directly** from the loss\n",
    "- $ \\delta^l $ for $ l < L $ is computed **indirectly** using:\n",
    "  $\n",
    "  \\delta^l = (w^{l+1})^T \\delta^{l+1} \\odot \\sigma'(z^l)\n",
    "  $\n",
    "\n",
    "Would you like to see a **concrete numerical example** of both $ \\delta^L $ and $ \\delta^{L-1} $ to lock in the intuition?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Numerical Example\n",
    "\n",
    "### **Recap:**\n",
    "\n",
    "You're working with an MLP (a feedforward neural network), with layers labeled:\n",
    "\n",
    "- $ l = 1, 2, ..., L $, where $ L $ is the final layer (output layer)\n",
    "- Each layer has:\n",
    "  - **Weights** $ W^l $\n",
    "  - **Biases** $ b^l $\n",
    "  - **Input to activation** (pre-activation) $ z^l = W^l a^{l-1} + b^l $\n",
    "  - **Activation** $ a^l = \\sigma(z^l) $\n",
    "\n",
    "We define:\n",
    "- $ \\delta^l $ as the **error term** (gradient of the loss with respect to $ z^l $), i.e.:\n",
    "  $\n",
    "  \\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "### **The Backpropagation Rule:**\n",
    "\n",
    "To backpropagate the error from layer $ l+1 $ to layer $ l $, we use the chain rule. The rule is:\n",
    "\n",
    "$\n",
    "\\delta^l = (W^{l+1})^T \\delta^{l+1} \\odot \\sigma'(z^l)\n",
    "$\n",
    "\n",
    "Let‚Äôs explain this step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Derivation:**\n",
    "\n",
    "We want to compute:\n",
    "\n",
    "$\n",
    "\\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\n",
    "\\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "= \\frac{\\partial C}{\\partial a^l} \\cdot \\frac{\\partial a^l}{\\partial z^l}\n",
    "$\n",
    "\n",
    "Now:\n",
    "\n",
    "- $ \\frac{\\partial a^l}{\\partial z^l} = \\sigma'(z^l) $ ‚Äî elementwise derivative of the activation function\n",
    "- $ \\frac{\\partial C}{\\partial a^l} $ is harder, but we can express it using the next layer:\n",
    "  \n",
    "Since:\n",
    "\n",
    "$\n",
    "z^{l+1} = W^{l+1} a^l + b^{l+1}\n",
    "\\Rightarrow a^{l+1} = \\sigma(z^{l+1})\n",
    "\\Rightarrow C = C(a^{l+1})\n",
    "$\n",
    "\n",
    "By the chain rule again:\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a^l} = (W^{l+1})^T \\frac{\\partial C}{\\partial z^{l+1}} = (W^{l+1})^T \\delta^{l+1}\n",
    "$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\n",
    "\\delta^l = (W^{l+1})^T \\delta^{l+1} \\odot \\sigma'(z^l)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **What This Means Intuitively:**\n",
    "\n",
    "- $ \\delta^{l+1} $ tells you how the loss changes with respect to the next layer's inputs\n",
    "- $ (W^{l+1})^T \\delta^{l+1} $ propagates that error **backward through the weights**\n",
    "- $ \\odot \\sigma'(z^l) $ adjusts that backward-flowing error by how sensitive the activation is (via derivative of the activation function)\n",
    "\n",
    "---\n",
    "\n",
    "### **Example (ReLU):**\n",
    "\n",
    "If you're using ReLU:\n",
    "- $ \\sigma(z) = \\max(0, z) $\n",
    "- $ \\sigma'(z) = 1 $ if $ z > 0 $, otherwise 0\n",
    "\n",
    "So only the neurons that were active in the forward pass will get nonzero gradient in the backward pass ‚Äî which is how ReLU encourages sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a numeric example or want this in matrix form for an entire mini-batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Numerical Example\n",
    "\n",
    "Awesome, let‚Äôs work through a **fully detailed numerical example** of **backpropagation** for a small network, so you can see exactly how we compute the errors $ \\delta^l $ and gradients step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Network Architecture\n",
    "\n",
    "We'll use a small network to keep the math readable:\n",
    "\n",
    "$\n",
    "\\textbf{Network structure: } [2, 2, 1]\n",
    "$\n",
    "\n",
    "- Input layer: 2 neurons  \n",
    "- Hidden layer: 2 neurons  \n",
    "- Output layer: 1 neuron  \n",
    "\n",
    "We‚Äôll assume:\n",
    "- **Sigmoid** activation: $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **Quadratic cost**: $ C = \\frac{1}{2}(a^L - y)^2 $\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Step 1: Initialize Weights, Biases, Input\n",
    "\n",
    "We'll choose **fixed values** to simplify:\n",
    "\n",
    "### Input $ x $:\n",
    "$\n",
    "x = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Target $ y $:\n",
    "$\n",
    "y = \\begin{bmatrix} 0.8 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Weights and Biases:\n",
    "| Layer | Weights $ w^l $ | Biases $ b^l $ |\n",
    "|-------|-------------------|------------------|\n",
    "| 1 (Input ‚Üí Hidden) | $ w^1 = \\begin{bmatrix} 0.1 & 0.3 \\\\ 0.2 & 0.4 \\end{bmatrix} $ | $ b^1 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} $ |\n",
    "| 2 (Hidden ‚Üí Output) | $ w^2 = \\begin{bmatrix} 0.7 & -1.2 \\end{bmatrix} $ | $ b^2 = \\begin{bmatrix} 0.05 \\end{bmatrix} $ |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Step 2: Forward Pass\n",
    "\n",
    "### Hidden layer (layer 1):\n",
    "$\n",
    "z^1 = w^1 x + b^1\n",
    "= \\begin{bmatrix} 0.1 & 0.3 \\\\ 0.2 & 0.4 \\end{bmatrix} \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}\n",
    "+ \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.1 + 0.15 \\\\ 0.2 + 0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.35 \\\\ 0.3 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Apply sigmoid:\n",
    "$\n",
    "a^1 = \\sigma(z^1) \\approx \\begin{bmatrix} \\sigma(0.35) \\\\ \\sigma(0.3) \\end{bmatrix}\n",
    "\\approx \\begin{bmatrix} 0.5866 \\\\ 0.5744 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Output layer (layer 2):\n",
    "$\n",
    "z^2 = w^2 a^1 + b^2 = \\begin{bmatrix} 0.7 & -1.2 \\end{bmatrix} \\begin{bmatrix} 0.5866 \\\\ 0.5744 \\end{bmatrix} + 0.05\n",
    "= (0.4106 - 0.6893) + 0.05 = -0.2287\n",
    "$\n",
    "\n",
    "Apply sigmoid:\n",
    "$\n",
    "a^2 = \\sigma(z^2) = \\sigma(-0.2287) \\approx 0.4431\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step 3: Backward Pass\n",
    "\n",
    "---\n",
    "\n",
    "### Output layer (layer 2)\n",
    "\n",
    "Compute cost derivative:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial a^2} = a^2 - y = 0.4431 - 0.8 = -0.3569\n",
    "$\n",
    "\n",
    "$\n",
    "\\sigma'(z^2) = \\sigma(z^2)(1 - \\sigma(z^2)) = 0.4431 \\cdot (1 - 0.4431) \\approx 0.2467\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^2 = \\frac{\\partial C}{\\partial z^2} = \\frac{\\partial C}{\\partial a^2} \\cdot \\sigma'(z^2)\n",
    "= -0.3569 \\cdot 0.2467 \\approx -0.0880\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Hidden layer (layer 1)\n",
    "\n",
    "$\n",
    "\\delta^1 = ((w^2)^T \\delta^2) \\odot \\sigma'(z^1)\n",
    "$\n",
    "\n",
    "First:\n",
    "$\n",
    "(w^2)^T \\delta^2 = \\begin{bmatrix} 0.7 \\\\ -1.2 \\end{bmatrix} \\cdot -0.0880\n",
    "= \\begin{bmatrix} -0.0616 \\\\ 0.1056 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now compute $ \\sigma'(z^1) $:\n",
    "$\n",
    "\\sigma'(z^1) = \\sigma(z^1)(1 - \\sigma(z^1)) \\approx\n",
    "\\begin{bmatrix} 0.5866 \\cdot (1 - 0.5866) \\\\ 0.5744 \\cdot (1 - 0.5744) \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.2425 \\\\ 0.2445 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "So:\n",
    "$\n",
    "\\delta^1 = \\begin{bmatrix} -0.0616 \\\\ 0.1056 \\end{bmatrix}\n",
    "\\odot \\begin{bmatrix} 0.2425 \\\\ 0.2445 \\end{bmatrix}\n",
    "= \\begin{bmatrix} -0.0149 \\\\ 0.0258 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Step 4: Gradient Calculation\n",
    "\n",
    "### Gradients for output layer weights:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^2} = \\delta^2 \\cdot (a^1)^T\n",
    "= -0.0880 \\cdot \\begin{bmatrix} 0.5866 & 0.5744 \\end{bmatrix}\n",
    "= \\begin{bmatrix} -0.0516 & -0.0505 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Gradients for output layer biases:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial b^2} = \\delta^2 = -0.0880\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradients for hidden layer weights:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w^1} = \\delta^1 \\cdot (a^0)^T\n",
    "= \\begin{bmatrix} -0.0149 \\\\ 0.0258 \\end{bmatrix}\n",
    "\\cdot \\begin{bmatrix} 1.0 & 0.5 \\end{bmatrix}\n",
    "= \\begin{bmatrix} -0.0149 & -0.0075 \\\\ 0.0258 & 0.0129 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Gradients for hidden layer biases:\n",
    "$\n",
    "\\frac{\\partial C}{\\partial b^1} = \\delta^1 = \\begin{bmatrix} -0.0149 \\\\ 0.0258 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Summary\n",
    "\n",
    "| Layer | $ \\delta^l $ | $ \\frac{\\partial C}{\\partial w^l} $ | $ \\frac{\\partial C}{\\partial b^l} $ |\n",
    "|-------|----------------|--------------------------|------------------|\n",
    "| 2 (output) | -0.0880 | $[-0.0516, -0.0505]$ | -0.0880 |\n",
    "| 1 (hidden) | $[-0.0149, 0.0258]$ | $ \\begin{bmatrix} -0.0149 & -0.0075 \\\\ 0.0258 & 0.0129 \\end{bmatrix} $ | same as $ \\delta^1 $ |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to draw a **diagram** of this mini network with arrows and values to go with this numerical walkthrough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Let's walk through a **full forward and backward pass** for a neural network with architecture:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Network Structure: $[2, 3, 2]$\n",
    "\n",
    "| Layer      | # Neurons | Notation |\n",
    "|------------|-----------|----------|\n",
    "| Input      | 2         | $ a^0 $ |\n",
    "| Hidden     | 3         | $ a^1 $ |\n",
    "| Output     | 2         | $ a^2 $ |\n",
    "\n",
    "We'll assume:\n",
    "- Activation: **Sigmoid**\n",
    "- Cost: **Quadratic loss**\n",
    "$\n",
    "C = \\frac{1}{2} \\| a^2 - y \\|^2\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Step 1: Choose Input and Target\n",
    "\n",
    "### Input:\n",
    "$\n",
    "x = \\begin{bmatrix} 0.9 \\\\ 0.1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Target:\n",
    "$\n",
    "y = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 2: Weights and Biases\n",
    "\n",
    "### Layer 1: Input ‚Üí Hidden\n",
    "$\n",
    "w^1 = \\begin{bmatrix}\n",
    "0.1 & 0.4 \\\\\n",
    "0.2 & 0.3 \\\\\n",
    "0.5 & -0.1\n",
    "\\end{bmatrix}, \\quad\n",
    "b^1 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Layer 2: Hidden ‚Üí Output\n",
    "$\n",
    "w^2 = \\begin{bmatrix}\n",
    "0.3 & -0.7 & 0.2 \\\\\n",
    "0.6 & 0.1 & -0.5\n",
    "\\end{bmatrix}, \\quad\n",
    "b^2 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Step 3: Forward Pass\n",
    "\n",
    "### Hidden Layer\n",
    "$\n",
    "z^1 = w^1 x + b^1 = \n",
    "\\begin{bmatrix}\n",
    "0.1\\cdot0.9 + 0.4\\cdot0.1 + 0.1 \\\\\n",
    "0.2\\cdot0.9 + 0.3\\cdot0.1 - 0.1 \\\\\n",
    "0.5\\cdot0.9 + (-0.1)\\cdot0.1 + 0.2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.24 \\\\ 0.11 \\\\ 0.63 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Apply sigmoid:\n",
    "$\n",
    "a^1 = \\sigma(z^1) \\approx \\begin{bmatrix} 0.5597 \\\\ 0.5275 \\\\ 0.6522 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Output Layer\n",
    "$\n",
    "z^2 = w^2 a^1 + b^2\n",
    "= \\begin{bmatrix}\n",
    "0.3 & -0.7 & 0.2 \\\\\n",
    "0.6 & 0.1 & -0.5\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} 0.5597 \\\\ 0.5275 \\\\ 0.6522 \\end{bmatrix}\n",
    "+ \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Calculating:\n",
    "$\n",
    "z^2_1 = 0.3(0.5597) - 0.7(0.5275) + 0.2(0.6522) + 0.05 \\approx 0.167\n",
    "$\n",
    "$\n",
    "z^2_2 = 0.6(0.5597) + 0.1(0.5275) - 0.5(0.6522) - 0.05 \\approx 0.069\n",
    "$\n",
    "\n",
    "$\n",
    "z^2 = \\begin{bmatrix} 0.167 \\\\ 0.069 \\end{bmatrix}\n",
    "\\quad \\Rightarrow \\quad\n",
    "a^2 = \\sigma(z^2) \\approx \\begin{bmatrix} 0.5417 \\\\ 0.5172 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step 4: Backward Pass\n",
    "\n",
    "---\n",
    "\n",
    "### Output Layer Error:\n",
    "$\n",
    "\\delta^2 = (a^2 - y) \\odot \\sigma'(z^2)\n",
    "$\n",
    "\n",
    "$\n",
    "a^2 - y = \\begin{bmatrix} -0.4583 \\\\ 0.5172 \\end{bmatrix}\n",
    "$\n",
    "$\n",
    "\\sigma'(z^2) = a^2 \\odot (1 - a^2) \\approx \\begin{bmatrix} 0.5417(1 - 0.5417) \\\\ 0.5172(1 - 0.5172) \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.2483 \\\\ 0.2497 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\delta^2 \\approx \\begin{bmatrix} -0.1138 \\\\ 0.1291 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradients for Layer 2:\n",
    "\n",
    "#### Weight gradients:\n",
    "$\n",
    "\\nabla w^2 = \\delta^2 \\cdot (a^1)^T\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla w^2 \\approx \\begin{bmatrix}\n",
    "-0.1138 \\cdot 0.5597 & -0.1138 \\cdot 0.5275 & -0.1138 \\cdot 0.6522 \\\\\n",
    "0.1291 \\cdot 0.5597 & 0.1291 \\cdot 0.5275 & 0.1291 \\cdot 0.6522\n",
    "\\end{bmatrix}\n",
    "\\approx \\begin{bmatrix}\n",
    "-0.0637 & -0.0600 & -0.0743 \\\\\n",
    "0.0722 & 0.0681 & 0.0842\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Bias gradients:\n",
    "$\n",
    "\\nabla b^2 = \\delta^2 \\approx \\begin{bmatrix} -0.1138 \\\\ 0.1291 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Hidden Layer Error:\n",
    "\n",
    "$\n",
    "\\delta^1 = ((w^2)^T \\delta^2) \\odot \\sigma'(z^1)\n",
    "$\n",
    "\n",
    "First:\n",
    "$\n",
    "(w^2)^T \\delta^2 = \\begin{bmatrix}\n",
    "0.3 & 0.6 \\\\\n",
    "-0.7 & 0.1 \\\\\n",
    "0.2 & -0.5\n",
    "\\end{bmatrix}\n",
    "\\cdot \\begin{bmatrix} -0.1138 \\\\ 0.1291 \\end{bmatrix}\n",
    "\\approx \\begin{bmatrix}\n",
    "0.0278 \\\\\n",
    "-0.0907 \\\\\n",
    "-0.0900\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Then:\n",
    "$\n",
    "\\sigma'(z^1) = a^1 \\odot (1 - a^1) \\approx\n",
    "\\begin{bmatrix} 0.2464 \\\\ 0.2492 \\\\ 0.2267 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "So:\n",
    "$\n",
    "\\delta^1 = \\begin{bmatrix}\n",
    "0.0278 \\cdot 0.2464 \\\\\n",
    "-0.0907 \\cdot 0.2492 \\\\\n",
    "-0.0900 \\cdot 0.2267\n",
    "\\end{bmatrix}\n",
    "\\approx \\begin{bmatrix}\n",
    "0.0069 \\\\\n",
    "-0.0226 \\\\\n",
    "-0.0204\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradients for Layer 1:\n",
    "\n",
    "#### Weight gradients:\n",
    "$\n",
    "\\nabla w^1 = \\delta^1 \\cdot (x)^T =\n",
    "\\begin{bmatrix}\n",
    "0.0069 \\cdot 0.9 & 0.0069 \\cdot 0.1 \\\\\n",
    "-0.0226 \\cdot 0.9 & -0.0226 \\cdot 0.1 \\\\\n",
    "-0.0204 \\cdot 0.9 & -0.0204 \\cdot 0.1\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "0.0062 & 0.0007 \\\\\n",
    "-0.0204 & -0.0023 \\\\\n",
    "-0.0184 & -0.0020\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Bias gradients:\n",
    "$\n",
    "\\nabla b^1 = \\delta^1 \\approx \\begin{bmatrix} 0.0069 \\\\ -0.0226 \\\\ -0.0204 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Layer | $ \\delta $ | $ \\nabla w $ | $ \\nabla b $ |\n",
    "|-------|--------------|----------------|----------------|\n",
    "| Output (2) | $ \\begin{bmatrix}-0.1138\\\\0.1291\\end{bmatrix} $ | $ 2 \\times 3 $ matrix shown above | same as $ \\delta^2 $ |\n",
    "| Hidden (1) | $ \\begin{bmatrix}0.0069\\\\-0.0226\\\\-0.0204\\end{bmatrix} $ | $ 3 \\times 2 $ matrix shown above | same as $ \\delta^1 $ |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to create a diagram for this one too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Reminder:}$\n",
    "\n",
    "Last layer $L$ has $n$ neuron.\n",
    "\n",
    "Layer $L-1$ has $m$ neuron.\n",
    "\n",
    "Layer $L-2$ has $p$ neuron.\n",
    "\n",
    "$\\textbf{W}^{(L)}_{n \\times m}$\n",
    "\n",
    "$\\textbf{W}^{(L-1)}_{m \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{W}^{(L)}_{n\\times m}=\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{0,1}^{(L)}  & \\cdots   &w_{0,m-1}^{(L)} \\\\ \n",
    "w_{1,0}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,m-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{n-1,0}^{(L)} &w_{n-1,1}^{(L)}  & \\cdots   &w_{n-1,m-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{n\\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{W}^{(L)}_{m\\times n}\\top=\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{1,0}^{(L)}  & \\cdots   & w_{n-1,0}^{(L)}\\\\ \n",
    "w_{0,1}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,n-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{0,m-1}^{(L)} &w_{m-1,1}^{(L)}  & \\cdots   &w_{m-1,n-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{m\\times n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol\\delta^{(L)}=\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol \\sigma'(z^{(L-1)})=\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z^{(L-1)}_{0})\\\\ \n",
    "\\sigma'(z^{(L-1)}_{1})\\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma'(z^{(L-1)}_{m-1})\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{a}^{(L-2)}\\top=\n",
    "\\begin{bmatrix}\n",
    "a^{(L-2)}_{0} & a^{(L-2)}_{1}&...&a^{(L-2)}_{p-1}\\\\ \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compute the error relative to weight in layer $L-1$:\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L-1)}} =\n",
    "\\frac{\\partial z_{j}^{(L-1)}}{\\partial w_{jk}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L-1)}}{\\partial z_{j}^{(L-1)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L-1)}}=\n",
    "\\frac{\\partial z_{j}^{(L-1)}}{\\partial w_{jk}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L-1)}}{\\partial z_{j}^{(L-1)}}\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}=\n",
    "a_{k}^{(L-2)}\n",
    "\\sigma' (z_{j}^{(L-1)})\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L-1)}} =\n",
    "a_{k}^{(L-2)}\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}=\n",
    " \\left (   (\\textbf{W}^{(L)}_{(0:n-1,j)} \\top)\\cdot  \\boldsymbol\\delta^{(L)}\\right ) \\sigma' (z_{j}^{(L-1)})\n",
    " a_{k}^{(L-2)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was only one element in the $\\textbf{W}^{(L-1)}_{m \\times p}$, If we write it for entire matrix:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}_{m\\times p}=\n",
    "%W\n",
    "\\begin{eqnarray} \n",
    "   (\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{1,0}^{(L)}  & \\cdots   & w_{n-1,0}^{(L)}\\\\ \n",
    "w_{0,1}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,n-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{0,m-1}^{(L)} &w_{m-1,1}^{(L)}  & \\cdots   &w_{m-1,n-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{m\\times n}\n",
    "%delta \n",
    "\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}_{n \\times 1})\n",
    "\\odot \n",
    "%sigma prime\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z^{(L-1)}_{0})\\\\ \n",
    "\\sigma'(z^{(L-1)}_{1})\\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma'(z^{(L-1)}_{m-1})\n",
    "\\end{bmatrix}_{m\\times 1}\n",
    "\\end{eqnarray}\n",
    "\\cdot \n",
    "%a L-2 vector\n",
    "\\begin{bmatrix}\n",
    "a^{(L-2)}_{0} & a^{(L-2)}_{1}&...&a^{(L-2)}_{p-1}\\\\ \n",
    "\\end{bmatrix}_{p \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}=\n",
    "\\begin{eqnarray} \n",
    "   ((\\textbf{W}^{(L)})^T \\boldsymbol\\delta^{(L)}) \\odot \\boldsymbol\\sigma'(z^{(L-1)})\n",
    "\\end{eqnarray}\n",
    "\\cdot \\textbf{a}^{(L-2)}\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\delta_k^{(L-1)}$:\n",
    "\n",
    "\n",
    "$\\delta_k^{(L-1)} = \\frac{\\partial C}{\\partial z_k^{(L-1)}} = \\sum_j ^{n-1} \\frac{\\partial C}{\\partial z_j^{(L)}} \\frac{\\partial z_j^{(L)}}{\\partial z_k^{(L-1)}} = \\sum_j  \\delta_j^{(L)}\\frac{\\partial z_j^{(L)}}{\\partial z_k^{(L-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "$z^{(L)}_j = \\sum_{k} w_{jk}^{(L)} \\sigma(z_k^{(L) - 1}) + b_j^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative is:\n",
    "\n",
    "$\\frac{\\partial z_j^{(L)}}{\\partial z_K^{(L-1)}} = w_{jK}^{(L)}\\sigma'(z_{K}^{(L-1)})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these pieces together we have:\n",
    "\n",
    "$\\delta_k^{(L-1)} =\\sum_j^{n-1} \\delta_j^{(L)}\\frac{\\partial z_j^{(L)}}{\\partial z_K^{(L-1)}}= \\sum_j^{n-1}  (\\delta_j^{(L)}  w_{jK}^{(L)})\\sigma'(z_{K}^{(L-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\boldsymbol\\delta^{(L-1)}=\\frac{\\partial c}{\\partial z^{(L-1)}}  = ((\\textbf{W}^{(L)})^T \\boldsymbol\\delta^{(L)}) \\odot \\sigma'(z^{(L-1)})\n",
    "\\end{eqnarray}\\tag{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial\\textbf{c}}{\\partial \\textbf{b}^{(L-1)}}=\\boldsymbol\\delta^{(L-1)}\\tag{5}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}=\\boldsymbol\\delta^{(L-1)}\\cdot \\textbf{a}^{(L-2)}\\top\\tag{6}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was only for the $\\textbf{first}$ item in the training set, you feed the rest of the training set into the network and for each $w_{jk}^{(L)}$ you make an average over all of them. Then that's the vector that you should use in your gradient descent function. In the output layer use equations (1), (2), (3) and in the hidden layers use equation (4), (5) and 6:\n",
    "\n",
    "$\\textbf{W}_{new}=\\textbf{W}_{initial}-\\eta \\nabla (\\textbf{W}_{initial})$\n",
    "\n",
    "$\\textbf{b}_{new}=\\textbf{b}_{initial}-\\eta \\nabla (\\textbf{b}_{initial})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you usually randomly shuffle your training set into batches called mini batches and compute the gradient for every batch.\n",
    "\n",
    "Ref: [1](https://stats.stackexchange.com/questions/414825/how-exactly-is-the-error-backpropagated-in-backpropagation), [2](https://www.youtube.com/watch?v=xClK__CqZnQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable Gradient: Vanishing and Explosion Gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](https://www.youtube.com/watch?v=qO_NLVjD6zE), [2](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming activation function is:\n",
    "$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "$\\frac{\\sigma(x)}{\\partial x} = \\sigma(x)(1 - \\sigma(x))$\n",
    "Because:\n",
    "\n",
    "$\\begin{align}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
