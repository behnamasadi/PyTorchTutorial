{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ed833c-5e10-478d-b995-d551998e86c7",
   "metadata": {},
   "source": [
    "# 1. Deleting Large Tensors\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "PyTorch uses **reference counting**. A tensor stays in GPU memory as long as **one Python reference** points to it. If you overwrite or forget to delete a variable holding a large tensor, VRAM remains allocated.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdf020a-53a1-4567-b9d0-a63543951ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== start ===\n",
      "nvidia-smi used      : 872.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 722.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== del x ===\n",
      "nvidia-smi used      : 2398.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 1895.8 MB\n",
      "PyTorch reserved     : 2248.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== del y ===\n",
      "nvidia-smi used      : 2398.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 2248.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== calling empty_cache() ===\n",
      "nvidia-smi used      : 872.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 722.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "def get_gpu_memory_from_nvidia_smi():\n",
    "    \"\"\"\n",
    "    Returns total and used VRAM from nvidia-smi in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        ['nvidia-smi', '--query-gpu=memory.used,memory.total',\n",
    "         '--format=csv,nounits,noheader']\n",
    "    )\n",
    "    used, total = map(int, result.decode().strip().split(','))\n",
    "    return used, total\n",
    "\n",
    "\n",
    "def monitor(step=\"\"):\n",
    "    used, total = get_gpu_memory_from_nvidia_smi()\n",
    "\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    max_alloc = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"\\n=== {step} ===\")\n",
    "    print(f\"nvidia-smi used      : {used:.1f} MB / {total:.1f} MB\")\n",
    "    print(f\"PyTorch allocated    : {allocated:.1f} MB\")\n",
    "    print(f\"PyTorch reserved     : {reserved:.1f} MB\")\n",
    "    print(f\"PyTorch max allocated: {max_alloc:.1f} MB\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "monitor(step=\"start\")\n",
    "\n",
    "x = torch.randn(20000, 20000, device='cuda')  # ~1.6 GB VRAM\n",
    "y = x  # another reference\n",
    "\n",
    "\n",
    "del x\n",
    "monitor(step=\"del x\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "del y  # refcount is now 0 → GPU memory released\n",
    "\n",
    "monitor(step=\"del y\")\n",
    "torch.cuda.empty_cache()\n",
    "monitor(step=\"calling empty_cache()\")\n",
    "\n",
    "time.sleep(5)  # Sleep for 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6cd3c-d224-4c90-80b4-e75da54524f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What each line does\n",
    "\n",
    "* `del large_activation_maps`:\n",
    "  Removes the Python reference → PyTorch can now release the memory.\n",
    "\n",
    "* `torch.cuda.empty_cache()`:\n",
    "  Returns unused cached memory **back to CUDA**, reducing out-of-memory risks.\n",
    "\n",
    "### Important\n",
    "\n",
    "`empty_cache()` **does not free the memory used by live tensors**.\n",
    "It only frees memory that PyTorch cached *for reuse*.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Using `torch.no_grad()` for Inference\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "During inference, you do not need gradients.\n",
    "If gradients are tracked, PyTorch creates a **huge computation graph**, storing:\n",
    "\n",
    "* intermediate activations\n",
    "* backward links\n",
    "* gradient buffers\n",
    "\n",
    "This easily doubles or triples memory usage.\n",
    "\n",
    "### Wrong way (gradients tracked)\n",
    "\n",
    "```python\n",
    "pred = model(test_data)\n",
    "```\n",
    "\n",
    "This creates a full graph → memory explosion.\n",
    "\n",
    "### Correct way\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)\n",
    "```\n",
    "\n",
    "Inside this context:\n",
    "\n",
    "* No computation graph is built.\n",
    "* Intermediate activations are discarded immediately.\n",
    "* Memory use can drop by **40%–70%** depending on model.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Clearing Gradients with `zero_grad(set_to_none=True)`\n",
    "\n",
    "### PyTorch gradient clearing options\n",
    "\n",
    "There are two ways:\n",
    "\n",
    "## Option A: Standard gradient zeroing\n",
    "\n",
    "```python\n",
    "model.zero_grad()\n",
    "```\n",
    "\n",
    "This:\n",
    "\n",
    "* sets gradients to zeros tensors\n",
    "* keeps the memory allocated (VRAM remains occupied)\n",
    "\n",
    "So each parameter still owns a `grad` tensor of the same size as the weight.\n",
    "\n",
    "## Option B: Recommended: freeing gradients\n",
    "\n",
    "```python\n",
    "model.zero_grad(set_to_none=True)\n",
    "```\n",
    "\n",
    "This:\n",
    "\n",
    "* sets grad pointers to `None`\n",
    "* **frees the GPU memory**\n",
    "* PyTorch will reallocate them **only when needed** in the next backward pass\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "Weights like ConvNeXt (tens of millions of params) produce huge gradient buffers.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "| Operation                     | Gradients exist in VRAM? | Memory usage |\n",
    "| ----------------------------- | ------------------------ | ------------ |\n",
    "| `zero_grad()`                 | Yes, as zeros            | High         |\n",
    "| `zero_grad(set_to_none=True)` | No                       | Lower        |\n",
    "\n",
    "Especially useful when:\n",
    "\n",
    "* running gradient accumulation\n",
    "* switching between training and inference\n",
    "* performing validation inside training loop\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Bonus: How PyTorch GPU Memory Actually Works\n",
    "\n",
    "Understanding the internal mechanism helps avoid surprises.\n",
    "\n",
    "### PyTorch holds two kinds of GPU memory:\n",
    "\n",
    "### 1. **Allocated memory**\n",
    "\n",
    "Memory used by active tensors.\n",
    "Freed only when the last reference is deleted.\n",
    "\n",
    "### 2. **Cached memory** (PyTorch memory pool)\n",
    "\n",
    "For performance, PyTorch **does not return memory immediately to CUDA**.\n",
    "Instead, it keeps VRAM in a reusable pool.\n",
    "\n",
    "This is why `nvidia-smi` often shows high usage even after deleting variables.\n",
    "\n",
    "### To truly return it to CUDA:\n",
    "\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Putting It All Together (Best Practices Template)\n",
    "\n",
    "### Training loop best practice:\n",
    "\n",
    "```python\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    outputs = model(batch[\"input\"])\n",
    "    loss = criterion(outputs, batch[\"target\"])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del outputs, loss\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "### Validation inside training:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        preds = model(batch[\"input\"])\n",
    "        # compute metrics\n",
    "model.train()\n",
    "```\n",
    "\n",
    "### After a very large temporary tensor:\n",
    "\n",
    "```python\n",
    "tmp = some_heavy_operation()\n",
    "...\n",
    "del tmp\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Illustration of Memory Effects at Each Step\n",
    "\n",
    "Below is a conceptual diagram (VRAM allocation over time).\n",
    "It uses **blocks** to illustrate memory held.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Step Timeline\n",
    "\n",
    "### 1. `optimizer.zero_grad(set_to_none=True)`\n",
    "\n",
    "Before clearing:\n",
    "\n",
    "```\n",
    "[ weights | gradients | activations ]\n",
    "```\n",
    "\n",
    "After clearing:\n",
    "\n",
    "```\n",
    "[ weights ]   gradients removed → VRAM freed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Forward Pass\n",
    "\n",
    "```\n",
    "[ weights | activations | temporary buffers ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Backward Pass\n",
    "\n",
    "Adds more memory for gradient buffers:\n",
    "\n",
    "```\n",
    "[ weights | activations | grad buffers ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. After deleting outputs & loss\n",
    "\n",
    "```\n",
    "del outputs, loss\n",
    "```\n",
    "\n",
    "```\n",
    "[ weights | grad buffers ]\n",
    "```\n",
    "\n",
    "Activations from the forward are gone once backward finished.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `torch.cuda.empty_cache()`\n",
    "\n",
    "```\n",
    "[ weights | grad buffers ]\n",
    "unused cached blocks → returned to CUDA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Validation Step Timeline\n",
    "\n",
    "### 6. `with torch.no_grad():`\n",
    "\n",
    "No graph → no activations stored.\n",
    "\n",
    "```\n",
    "[ weights | small forward buffers ]\n",
    "```\n",
    "\n",
    "Memory footprint drops by **40–70%**.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. After deleting outputs & loss\n",
    "\n",
    "```\n",
    "[ weights ]\n",
    "```\n",
    "\n",
    "Cleaner than training, because no gradients or activations exist.\n",
    "\n",
    "---\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "| Step                          | What It Fixes            | Why It Matters                       |\n",
    "| ----------------------------- | ------------------------ | ------------------------------------ |\n",
    "| `zero_grad(set_to_none=True)` | frees gradient tensors   | reduces persistent VRAM load         |\n",
    "| `del outputs, loss`           | frees activations        | avoids unnecessary VRAM accumulation |\n",
    "| `empty_cache()`               | frees cached VRAM        | prevents out-of-memory errors        |\n",
    "| `no_grad()`                   | prevents graph creation  | cuts memory usage during validation  |\n",
    "| AMP                           | smaller FP16 activations | improves speed + memory              |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b033488-9443-4457-a1f0-f006fcbc01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype is bf16\n",
      "\n",
      "=== Before zero_grad ===\n",
      "nvidia-smi used      : 928.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 111.8 MB\n",
      "PyTorch reserved     : 778.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After zero_grad ===\n",
      "nvidia-smi used      : 928.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 111.8 MB\n",
      "PyTorch reserved     : 778.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After moving batch to GPU ===\n",
      "nvidia-smi used      : 928.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 122.1 MB\n",
      "PyTorch reserved     : 778.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After forward pass ===\n",
      "nvidia-smi used      : 3606.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 3245.6 MB\n",
      "PyTorch reserved     : 3456.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After backward pass ===\n",
      "nvidia-smi used      : 3678.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 212.8 MB\n",
      "PyTorch reserved     : 3528.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After optimizer.step ===\n",
      "nvidia-smi used      : 3678.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 379.7 MB\n",
      "PyTorch reserved     : 3528.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After del + empty_cache ===\n",
      "nvidia-smi used      : 780.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 379.6 MB\n",
      "PyTorch reserved     : 630.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== Before zero_grad ===\n",
      "nvidia-smi used      : 780.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 379.6 MB\n",
      "PyTorch reserved     : 630.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After zero_grad ===\n",
      "nvidia-smi used      : 780.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 630.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After moving batch to GPU ===\n",
      "nvidia-smi used      : 780.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 630.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After forward pass ===\n",
      "nvidia-smi used      : 3668.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 3415.4 MB\n",
      "PyTorch reserved     : 3518.0 MB\n",
      "PyTorch max allocated: 3427.1 MB\n",
      "\n",
      "=== After backward pass ===\n",
      "nvidia-smi used      : 1698.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.7 MB\n",
      "PyTorch reserved     : 1548.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After optimizer.step ===\n",
      "nvidia-smi used      : 1726.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.7 MB\n",
      "PyTorch reserved     : 1576.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After del + empty_cache ===\n",
      "nvidia-smi used      : 840.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.7 MB\n",
      "PyTorch reserved     : 690.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== Before zero_grad ===\n",
      "nvidia-smi used      : 840.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.7 MB\n",
      "PyTorch reserved     : 690.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After zero_grad ===\n",
      "nvidia-smi used      : 840.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 690.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After moving batch to GPU ===\n",
      "nvidia-smi used      : 840.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 690.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After forward pass ===\n",
      "nvidia-smi used      : 3670.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 3412.7 MB\n",
      "PyTorch reserved     : 3520.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After backward pass ===\n",
      "nvidia-smi used      : 3762.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 379.0 MB\n",
      "PyTorch reserved     : 3612.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After optimizer.step ===\n",
      "nvidia-smi used      : 886.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 379.0 MB\n",
      "PyTorch reserved     : 736.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After del + empty_cache ===\n",
      "nvidia-smi used      : 858.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.9 MB\n",
      "PyTorch reserved     : 708.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== Before zero_grad ===\n",
      "nvidia-smi used      : 858.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 378.9 MB\n",
      "PyTorch reserved     : 708.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After zero_grad ===\n",
      "nvidia-smi used      : 858.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 708.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After moving batch to GPU ===\n",
      "nvidia-smi used      : 858.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 708.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After forward pass ===\n",
      "nvidia-smi used      : 3672.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 3412.0 MB\n",
      "PyTorch reserved     : 3522.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After backward pass ===\n",
      "nvidia-smi used      : 3764.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 382.3 MB\n",
      "PyTorch reserved     : 3614.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After optimizer.step ===\n",
      "nvidia-smi used      : 926.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 382.3 MB\n",
      "PyTorch reserved     : 776.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After del + empty_cache ===\n",
      "nvidia-smi used      : 898.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 382.2 MB\n",
      "PyTorch reserved     : 748.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== Before zero_grad ===\n",
      "nvidia-smi used      : 898.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 382.2 MB\n",
      "PyTorch reserved     : 748.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After zero_grad ===\n",
      "nvidia-smi used      : 898.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 289.0 MB\n",
      "PyTorch reserved     : 748.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After moving batch to GPU ===\n",
      "nvidia-smi used      : 898.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 278.7 MB\n",
      "PyTorch reserved     : 748.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After forward pass ===\n",
      "nvidia-smi used      : 2210.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 2004.5 MB\n",
      "PyTorch reserved     : 2060.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After backward pass ===\n",
      "nvidia-smi used      : 2256.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 2106.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After optimizer.step ===\n",
      "nvidia-smi used      : 2264.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 2114.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n",
      "\n",
      "=== After del + empty_cache ===\n",
      "nvidia-smi used      : 872.0 MB / 4096.0 MB\n",
      "PyTorch allocated    : 369.8 MB\n",
      "PyTorch reserved     : 722.0 MB\n",
      "PyTorch max allocated: 3428.3 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"tf_efficientnetv2_s\"\n",
    "model = timm.create_model(model_name=model_name, pretrained=True).to(device)\n",
    "\n",
    "# Get model configuration\n",
    "cfg = model.default_cfg\n",
    "\n",
    "\n",
    "C, H, W = list(cfg['input_size'])\n",
    "B = 10\n",
    "\n",
    "\n",
    "num_class = cfg['num_classes']\n",
    "num_samples = 100\n",
    "X = torch.randn(num_samples, C, H, W)\n",
    "Y = torch.randint(0, num_class, (num_samples,))\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "batch_size = 22\n",
    "\n",
    "data_loader = DataLoader(batch_size=batch_size, dataset=dataset,\n",
    "                         pin_memory=True, num_workers=4)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Detect optimal dtype\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "    print(\"dtype is bf16\")\n",
    "else:\n",
    "    dtype = torch.float16\n",
    "    print(\"dtype is f16\")\n",
    "\n",
    "\n",
    "# Use GradScaler only for FP16\n",
    "use_scaler = (dtype == torch.float16)\n",
    "scaler = GradScaler() if use_scaler else None\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        monitor(\"Before zero_grad\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        monitor(\"After zero_grad\")\n",
    "\n",
    "        images, labels = batch\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        monitor(\"After moving batch to GPU\")\n",
    "\n",
    "        # Use autocast with detected dtype (bf16 or fp16)\n",
    "        with torch.amp.autocast('cuda', dtype=dtype):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        monitor(\"After forward pass\")\n",
    "\n",
    "        # Use GradScaler for FP16, regular backward for BF16\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            monitor(\"After backward pass\")\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            monitor(\"After backward pass\")\n",
    "            optimizer.step()\n",
    "        monitor(\"After optimizer.step\")\n",
    "\n",
    "        del outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        monitor(\"After del + empty_cache\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
