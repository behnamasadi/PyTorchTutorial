{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2fa6f7-fa5c-4bc5-82e6-5cfe7adde958",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Checkpointing — Deep Explanation\n",
    "\n",
    "Gradient checkpointing is a technique that **reduces GPU memory usage by not storing some intermediate activations during the forward pass**.\n",
    "\n",
    "Instead, during backward pass, PyTorch **recomputes** those activations on-the-fly.\n",
    "\n",
    "You **trade more compute** for **less memory**.\n",
    "\n",
    "---\n",
    "\n",
    "# Why do we need it?\n",
    "\n",
    "Normally, for backpropagation, PyTorch must keep **all intermediate activations** because gradients require them:\n",
    "\n",
    "Backprop step for a layer:\n",
    "\n",
    "$$\n",
    "\\nabla W = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W}\n",
    "$$\n",
    "\n",
    "Inside this, the term\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial W}\n",
    "$$\n",
    "depends on **the activation x**, so **x must be stored**.\n",
    "\n",
    "For a deep model:\n",
    "\n",
    "* Saving all activations costs many GB.\n",
    "* Even if weights fit, activations explode in memory usage.\n",
    "\n",
    "Gradient checkpointing solves this by:\n",
    "\n",
    "* **Saving fewer activations**\n",
    "* **Recomputing forward pass for selected layers** during backward\n",
    "\n",
    "---\n",
    "\n",
    "# What exactly happens?\n",
    "\n",
    "### Without checkpointing\n",
    "\n",
    "Forward pass stores activations:\n",
    "\n",
    "```\n",
    "x → layer1 → a1   (save a1)\n",
    "a1 → layer2 → a2  (save a2)\n",
    "a2 → layer3 → a3  (save a3)\n",
    "```\n",
    "\n",
    "Memory usage:\n",
    "\n",
    "* Save a1\n",
    "* Save a2\n",
    "* Save a3\n",
    "  Total = large.\n",
    "\n",
    "### With checkpointing\n",
    "\n",
    "```\n",
    "x → layer1 → a1   (NOT saved)\n",
    "a1 → layer2 → a2  (NOT saved)\n",
    "a2 → layer3 → a3  (saved only for final layer)\n",
    "```\n",
    "\n",
    "Backward pass:\n",
    "\n",
    "* To compute gradients for layer2, PyTorch **re-runs layer2 + layer1 forward** to reconstruct the missing activations.\n",
    "\n",
    "---\n",
    "\n",
    "# Memory savings\n",
    "\n",
    "Approximate:\n",
    "\n",
    "* Without checkpointing:\n",
    "  Memory ≈ activations of all layers\n",
    "* With checkpointing:\n",
    "  Memory ≈ activations only at checkpoints + outputs of checkpointed segments\n",
    "\n",
    "Rule of thumb:\n",
    "\n",
    "**You cut activation memory roughly by √2 to 2× depending on structure.**\n",
    "\n",
    "---\n",
    "\n",
    "# Cost?\n",
    "\n",
    "Extra compute:\n",
    "\n",
    "* During backward, checkpointed layers are recomputed.\n",
    "* Backward compute becomes roughly up to **1.5×–2× slower**.\n",
    "\n",
    "---\n",
    "\n",
    "# When is checkpointing useful?\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* You run out of GPU memory\n",
    "* Your model is deep (Transformers, ConvNeXt, ViT, ResNets, diffusion models)\n",
    "* Large resolution inputs (512–1024+)\n",
    "* Large batch sizes\n",
    "\n",
    "Do NOT use it when:\n",
    "\n",
    "* Model is small (no need)\n",
    "* You are already compute-bound\n",
    "* You need maximum throughput\n",
    "\n",
    "---\n",
    "\n",
    "# How to apply checkpointing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65548e0-1e6f-4379-8aa2-ddeea4114acb",
   "metadata": {},
   "source": [
    "Here is a clean rewrite showing **two versions of your model**:\n",
    "\n",
    "1. **Normal forward** (no checkpointing)\n",
    "2. **Checkpointed forward** (with nonlinearities, as a realistic network)\n",
    "\n",
    "Both versions use:\n",
    "\n",
    "* Linear → ReLU → Linear → ReLU → Linear\n",
    "* And checkpoint the expensive parts only in the second version.\n",
    "\n",
    "No emojis used.\n",
    "\n",
    "---\n",
    "\n",
    "# Version 1: Normal forward (no checkpointing)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "        self.layer3 = nn.Linear(1024, 10)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normal forward with standard activation flow\n",
    "        x = self.layer1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "This forward pass stores **all** intermediate activations for backward.\n",
    "\n",
    "---\n",
    "\n",
    "# Version 2: Forward with gradient checkpointing\n",
    "\n",
    "Checkpointing is applied to the *Linear + ReLU* blocks.\n",
    "\n",
    "Important: We wrap the sequence into a function because\n",
    "checkpointing requires a function that recomputes the exact operations.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class MyModelCheckpointed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "        self.layer3 = nn.Linear(1024, 10)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward_block1(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def forward_block2(self, x):\n",
    "        x = self.layer2(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Checkpoint the expensive blocks\n",
    "        x = checkpoint.checkpoint(self.forward_block1, x)\n",
    "        x = checkpoint.checkpoint(self.forward_block2, x)\n",
    "\n",
    "        # Last layer is cheap, so usually not checkpointed\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Notes\n",
    "\n",
    "1. In checkpointing, you must wrap multiple operations inside a function (`forward_block1`, `forward_block2`).\n",
    "   You cannot simply do:\n",
    "\n",
    "   ```\n",
    "   checkpoint.checkpoint(self.layer1, x)\n",
    "   checkpoint.checkpoint(self.act, x)\n",
    "   ```\n",
    "\n",
    "   because recomputing must follow the same sequence.\n",
    "\n",
    "2. You decide which blocks to checkpoint.\n",
    "   Usually, you checkpoint large or repeated layers, not the last linear classifier.\n",
    "\n",
    "3. Activations inside checkpointed blocks are not saved\n",
    "   They will be recomputed during backward.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also create:\n",
    "\n",
    "* A wrapper that automatically checkpoint-wraps arbitrary sequential blocks\n",
    "* A version using `checkpoint_sequential`\n",
    "* A version with ConvNeXt blocks to show exactly how it would look in timm models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123970c-137c-40d0-b2b3-ebf04fa8b7d5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Important rule: The function must be “pure”\n",
    "\n",
    "A checkpointed function:\n",
    "\n",
    "* must not modify global state\n",
    "* must not use random operations unless deterministic\n",
    "* must not use in-place ops on inputs\n",
    "\n",
    "PyTorch will rerun it during backward, so result must be identical.\n",
    "\n",
    "---\n",
    "\n",
    "# Example for larger models (Transformers)\n",
    "\n",
    "Typical structure:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    for block in self.transformer_blocks:\n",
    "        x = checkpoint.checkpoint(block, x)\n",
    "    x = self.final_layer(x)\n",
    "    return x\n",
    "```\n",
    "\n",
    "This is where checkpointing gives **huge** memory savings because Transformer blocks are expensive.\n",
    "\n",
    "---\n",
    "\n",
    "# How many layers to checkpoint?\n",
    "\n",
    "A good guideline:\n",
    "\n",
    "* Checkpoint **middle layers** of deep networks.\n",
    "* Do not checkpoint:\n",
    "\n",
    "  * the very first layer (cheap)\n",
    "  * the very last layer (cheap)\n",
    "  * layers with small activation sizes\n",
    "\n",
    "Example for 12-layer Transformer:\n",
    "\n",
    "```\n",
    "No checkpoint: Layer 1\n",
    "Checkpoint: Layers 2–11\n",
    "No checkpoint: Layer 12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Advanced: checkpointing a *sequence* of layers (more efficient)\n",
    "\n",
    "Instead of wrapping each layer individually, you can group them:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    def block1(x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    x = checkpoint.checkpoint(block1, x)\n",
    "    x = self.layer3(x)\n",
    "    return x\n",
    "```\n",
    "\n",
    "This reduces the overhead of calling checkpoint many times.\n",
    "\n",
    "---\n",
    "\n",
    "# When checkpointing gives the best memory reduction\n",
    "\n",
    "When activations dominate memory:\n",
    "\n",
    "* ViT (very good benefit)\n",
    "* MLP-Mixer\n",
    "* ConvNeXt\n",
    "* ResNet-50+\n",
    "* UNet (diffusion, medical imaging)\n",
    "* Any sequence of >10 layers\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also give you:\n",
    "\n",
    "* A visualization \"before vs after\" of memory usage\n",
    "* A version using gradient checkpointing for ConvNeXt or ViT\n",
    "* Code that automatically wraps N layers with checkpointing\n",
    "* A PyTorch Lightning example\n",
    "* A demonstration of memory usage using `torch.cuda.memory_allocated()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
