{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1899b1dd-db89-401e-a0ad-b26b0a65a63b",
   "metadata": {},
   "source": [
    "# Efficient Data Loading in PyTorch\n",
    "\n",
    "The `DataLoader` controls how batches are fetched from your dataset during training. Bad settings can slow your entire model because the GPU waits for data.\n",
    "\n",
    "A well-configured loader overlaps **CPU data preparation** with **GPU training**, ensuring the GPU never starves.\n",
    "\n",
    "---\n",
    "\n",
    "# Explaining Each Setting\n",
    "\n",
    "## 1. `num_workers`\n",
    "\n",
    "Controls how many **separate background processes** load and preprocess data.\n",
    "\n",
    "### When `num_workers > 0`\n",
    "\n",
    "* PyTorch starts multiple subprocesses.\n",
    "* Each worker:\n",
    "\n",
    "  * Reads files\n",
    "  * Applies transforms\n",
    "  * Prepares batches\n",
    "* This happens **in parallel** with GPU training.\n",
    "* Result: **Higher throughput**, fewer data-loading bottlenecks.\n",
    "\n",
    "### When `num_workers = 0`\n",
    "\n",
    "This is the simplest and slowest mode.\n",
    "**No subprocesses** are created.\n",
    "All data loading happens **in the main training thread**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `pin_memory=True`\n",
    "\n",
    "This activates **page-locked (pinned) memory**, which enables faster DMA transfer to GPU:\n",
    "\n",
    "* CPU → GPU transfer becomes faster and asynchronous.\n",
    "* Useful when using GPUs.\n",
    "\n",
    "If the DataLoader outputs tensors on CPU first (typical), enabling this improves throughput.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `persistent_workers=True`\n",
    "\n",
    "Workers stay alive across epochs instead of being restarted every time.\n",
    "\n",
    "Without this:\n",
    "\n",
    "* Workers are killed at epoch end.\n",
    "* Restarting them costs time.\n",
    "\n",
    "With `persistent_workers=True`:\n",
    "\n",
    "* Start once → reused every epoch.\n",
    "* Saves overhead.\n",
    "\n",
    "Works **only if num_workers > 0**\n",
    "(does nothing when num_workers = 0, because there are no workers to persist).\n",
    "\n",
    "---\n",
    "\n",
    "# Visual Intuition for num_workers\n",
    "\n",
    "### num_workers = 0\n",
    "\n",
    "```\n",
    "[Load batch] → [Train batch] → [Load next batch] → [Train] → ...\n",
    "```\n",
    "\n",
    "CPU and GPU take turns → slow.\n",
    "\n",
    "---\n",
    "\n",
    "### num_workers = 4\n",
    "\n",
    "```\n",
    "Worker 1: Load batch 1 -----\\\n",
    "Worker 2: Load batch 2 ------|--> GPU trains continuously\n",
    "Worker 3: Load batch 3 ------|\n",
    "Worker 4: Load batch 4 -----/\n",
    "```\n",
    "\n",
    "CPU prepares data in parallel while GPU trains → fast.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad92fa3-8c1e-4117-97d6-a3c38d090c76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1. General Rule of Thumb\n",
    "\n",
    "### Start with:\n",
    "\n",
    "```python\n",
    "num_workers = min(8, number_of_CPU_cores)\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "prefetch_factor = 2\n",
    "```\n",
    "\n",
    "Then tune based on symptoms (GPU idle vs CPU overload).\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Recommended Settings per System\n",
    "\n",
    "## A. **Your Machine (Behnam)**\n",
    "\n",
    "You usually train on:\n",
    "\n",
    "* **RTX 3050 / similar GPUs**\n",
    "* **Laptop with ~8 CPU cores (4 physical + SMT)** or desktop with 8–12 cores\n",
    "* Heavy augmentations (ConvNeXt, ViTs), medium dataset sizes\n",
    "\n",
    "### Recommended:\n",
    "\n",
    "```python\n",
    "num_workers = 4   # safe + fast parallelism\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "prefetch_factor = 2\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* 4 workers is optimal for 8 logical cores without oversubscribing.\n",
    "* Great balance for image datasets (224×224).\n",
    "* Good when doing color jitter, resize, crop, flips, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. System Type Breakdown\n",
    "\n",
    "## A. **Small Laptop CPU (2–4 cores)**\n",
    "\n",
    "Use fewer workers:\n",
    "\n",
    "```python\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "```\n",
    "\n",
    "Why?\n",
    "\n",
    "* Too many workers create overhead.\n",
    "* Laptops throttle quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## B. **Mid-Range Desktop (6–12 cores)**\n",
    "\n",
    "Optimal settings:\n",
    "\n",
    "```python\n",
    "num_workers = 4–8\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "prefetch_factor = 2\n",
    "```\n",
    "\n",
    "This gives:\n",
    "\n",
    "* Continuous data feeding\n",
    "* Zero GPU starvation\n",
    "* No CPU overload\n",
    "\n",
    "---\n",
    "\n",
    "## C. **Workstation / Server (16–64 cores)**\n",
    "\n",
    "Large parallelism helps **especially with heavy augmentations**:\n",
    "\n",
    "```python\n",
    "num_workers = 8–16\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "prefetch_factor = 4\n",
    "```\n",
    "\n",
    "Why?\n",
    "\n",
    "* Many cores can decode images (jpg/png) much faster.\n",
    "* You get full GPU utilization (95–100%).\n",
    "\n",
    "---\n",
    "\n",
    "## D. **Cluster training or very large images (medical, 1K–4K)**\n",
    "\n",
    "You need strong parallelism:\n",
    "\n",
    "```python\n",
    "num_workers = 8–32\n",
    "pin_memory = True\n",
    "persistent_workers = True\n",
    "prefetch_factor = 4\n",
    "```\n",
    "\n",
    "Because:\n",
    "\n",
    "* Image decoding becomes the bottleneck.\n",
    "* MONAI transforms (RandomElasticDeformation, RandFlipd, etc.) are expensive.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Additional Settings You Should Use\n",
    "\n",
    "## A. `prefetch_factor`\n",
    "\n",
    "Defaults to 2; usually fine.\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Each worker prepares N batches ahead.\n",
    "* Prevents GPU stalling.\n",
    "\n",
    "Recommended:\n",
    "\n",
    "```python\n",
    "prefetch_factor = 2\n",
    "```\n",
    "\n",
    "Increase to 3–4 only for very large images.\n",
    "\n",
    "---\n",
    "\n",
    "## B. `batch_size`\n",
    "\n",
    "Bigger batch size = fewer DataLoader calls per epoch.\n",
    "\n",
    "Since your RTX 3050 has limited VRAM (4–6 GB), optimal:\n",
    "\n",
    "### For ConvNeXt / ViT:\n",
    "\n",
    "```python\n",
    "batch_size = 16\n",
    "```\n",
    "\n",
    "### For ResNet / EfficientNet:\n",
    "\n",
    "```python\n",
    "batch_size = 32\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Quick Decision Matrix\n",
    "\n",
    "| CPU Cores | Recommended num_workers |\n",
    "| --------- | ----------------------- |\n",
    "| 2         | 1–2                     |\n",
    "| 4         | 2–4                     |\n",
    "| 6         | 4–6                     |\n",
    "| 8         | 4–8                     |\n",
    "| 12        | 6–8                     |\n",
    "| 16+       | 8–16                    |\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Detect if DataLoader is the bottleneck\n",
    "\n",
    "### Symptom: GPU utilization is low (<60%)\n",
    "\n",
    "Cause: Too few workers\n",
    "Fix:\n",
    "\n",
    "```python\n",
    "increase num_workers\n",
    "```\n",
    "\n",
    "### Symptom: CPU at 100% and training slows\n",
    "\n",
    "Cause: Too many workers fighting each other\n",
    "Fix:\n",
    "\n",
    "```python\n",
    "reduce num_workers\n",
    "```\n",
    "\n",
    "### Symptom: Epochs take long but GPU is fine\n",
    "\n",
    "Cause: Transform cost too high\n",
    "Fix:\n",
    "\n",
    "* Move augmentations to GPU (Kornia, DALI)\n",
    "* Increase workers\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Recommended Final Configuration for You\n",
    "\n",
    "### Best for Behnam's typical system:\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "```\n",
    "\n",
    "This matches:\n",
    "\n",
    "* Your GPU (mobile mid-range)\n",
    "* Your CPU (8 logical cores)\n",
    "* Your augmentation pipeline\n",
    "* Image sizes (224×224)\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also provide:\n",
    "\n",
    "✅ A script to auto-detect the best num_workers\n",
    "✅ A benchmark utility to test DataLoader speed on your system\n",
    "✅ A diagram explaining async loading vs GPU compute\n",
    "\n",
    "Just tell me which one you want.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
