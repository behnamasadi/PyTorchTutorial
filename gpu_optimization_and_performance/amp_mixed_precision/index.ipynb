{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f47000-d25d-47fe-a686-7c4644142ae5",
   "metadata": {},
   "source": [
    "# Automatic Mixed Precision (AMP)\n",
    "\n",
    "## 1. What AMP Does\n",
    "\n",
    "AMP enables **mixed-precision training** by automatically choosing lower precision (`float16` or `bfloat16`) where it is safe, and full precision (`float32`) where necessary.\n",
    "This reduces memory use and often increases training speed while preserving model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What `torch.amp.autocast` Does\n",
    "\n",
    "`autocast` intercepts operations during the forward pass and:\n",
    "\n",
    "* **casts them to lower precision** when stable\n",
    "* **keeps them in float32** when precision is important (e.g., softmax, batch-norm)\n",
    "\n",
    "This gives the benefits of mixed precision **without manually rewriting operations**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Basic Syntax\n",
    "\n",
    "```python\n",
    "from torch.amp import autocast\n",
    "\n",
    "with autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "```\n",
    "\n",
    "or, if you want PyTorch to choose the safe precision automatically:\n",
    "\n",
    "```python\n",
    "with autocast('cuda'):\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. When to Use Autocast\n",
    "\n",
    "AMP is applied **only around the forward pass**:\n",
    "\n",
    "* model(input)\n",
    "* loss computation\n",
    "\n",
    "The backward pass is **never** inside autocast.\n",
    "\n",
    "When using float16, the backward pass must be protected by **GradScaler**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Dynamic AMP Training Loop \n",
    "\n",
    "Below is the clean, correct version of your script with explanations integrated.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Decide the safest dtype automatically\n",
    "# Modern GPUs (A100/H100/RTX40xx) prefer bfloat16\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "    print(\"dtype is bf16\")\n",
    "else:\n",
    "    dtype = torch.float16\n",
    "    print(\"dtype is f16\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Simple model\n",
    "model = torch.nn.Linear(10, 2).to(device)\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.randn(40, 10)\n",
    "Y = torch.randint(0, 2, (40,))     # CrossEntropyLoss expects integer labels\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# GradScaler is needed only when dtype=float16\n",
    "use_scaler = (dtype == torch.float16)\n",
    "scaler = GradScaler() if use_scaler else None\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass in mixed precision\n",
    "        with autocast(device_type='cuda', dtype=dtype):\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "        # Backward pass\n",
    "        if use_scaler:                     # float16 only\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:                               # bfloat16 or float32\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} finished\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why Use `GradScaler` Only for float16?\n",
    "\n",
    "* `float16` has a **very limited exponent range**, making underflow/overflow common.\n",
    "  `GradScaler` multiplies the loss by a large factor to keep gradients numerically stable.\n",
    "\n",
    "* `bfloat16` has the **same exponent range as float32**, so gradients almost never underflow.\n",
    "  Therefore **no scaler is needed**.\n",
    "\n",
    "Summary:\n",
    "\n",
    "| dtype      | Use GradScaler? | Reason                       |\n",
    "| ---------- | --------------- | ---------------------------- |\n",
    "| `float16`  | Yes             | narrow dynamic range         |\n",
    "| `bfloat16` | No              | float32-level exponent range |\n",
    "| `float32`  | No              | full precision               |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Updated Explanation of the Key Steps\n",
    "\n",
    "| Step  | What Happens                                     | Why It Matters                                    |\n",
    "| ----- | ------------------------------------------------ | ------------------------------------------------- |\n",
    "| **1** | `with autocast(device_type='cuda', dtype=dtype)` | Enables mixed precision only for forward/loss.    |\n",
    "| **2** | Scaled backward (float16 only)                   | Protects against NaNs in gradients.               |\n",
    "| **3** | `scaler.step(optimizer)`                         | Applies unscaled gradients to parameters.         |\n",
    "| **4** | `scaler.update()`                                | Adjusts scaling factor dynamically for stability. |\n",
    "| **5** | In bf16 mode: normal backward                    | bf16 does not need scaling.                       |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Best Practices\n",
    "\n",
    "1. Use `bfloat16` whenever available on your GPU.\n",
    "2. Use `GradScaler` only when training in float16.\n",
    "3. Never wrap the backward pass inside autocast.\n",
    "4. For specific layers that must run in float32, disable autocast manually:\n",
    "\n",
    "   ```python\n",
    "   with autocast(enabled=False):\n",
    "       output = numerically_sensitive_layer(x)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Inference Example (No Scaler Needed)\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad(), autocast('cuda', dtype=dtype):\n",
    "    output = model(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. CPU Autocast (Mostly for bf16)\n",
    "\n",
    "```python\n",
    "with autocast(device_type='cpu', dtype=torch.bfloat16):\n",
    "    output = model(x)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
