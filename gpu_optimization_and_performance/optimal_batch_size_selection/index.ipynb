{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd59db2c-2f14-41c3-94d0-153884186208",
   "metadata": {},
   "source": [
    "## 1. What **Dynamic Batch Size Optimization** Means\n",
    "\n",
    "The goal is simple:\n",
    "\n",
    "You want to automatically find the **largest batch size** that fits in GPU memory.\n",
    "\n",
    "A larger batch size gives:\n",
    "\n",
    "* more stable gradients\n",
    "* better utilization of GPU\n",
    "* fewer iterations per epoch\n",
    "\n",
    "But if the batch is too large, you get a CUDA OOM error:\n",
    "\n",
    "```\n",
    "RuntimeError: CUDA out of memory\n",
    "```\n",
    "\n",
    "So we can probe the GPU by trying different batch sizes.\n",
    "\n",
    "The best strategy is **binary search**:\n",
    "start big (like 256), if OOM → try half, if fits → try the midpoint, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 2. Correct Implementation\n",
    "\n",
    "```python\n",
    "def find_optimal_batch_size(\n",
    "    model,\n",
    "    sample_batch,\n",
    "    criterion,\n",
    "    max_batch_size=512,\n",
    "    dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the largest batch size that fits into GPU memory.\n",
    "    Tests both forward and backward passes using AMP autocast.\n",
    "    \n",
    "    sample_batch: one (inputs, labels) batch from dataloader\n",
    "    criterion: loss function, e.g. nn.CrossEntropyLoss()\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs, labels = sample_batch\n",
    "    one_input = inputs[:1].to(device)\n",
    "    one_label = labels[:1].to(device)\n",
    "\n",
    "    low, high = 1, max_batch_size\n",
    "    best = 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "\n",
    "        try:\n",
    "            # Construct synthetic batch of size `mid`\n",
    "            test_inputs = one_input.repeat(mid, 1, 1, 1)\n",
    "            test_labels = one_label.repeat(mid)\n",
    "\n",
    "            # Clear gradients\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Forward + backward under AMP\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                output = model(test_inputs)\n",
    "                loss = criterion(output, test_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            print(f\"✓ Fits: {mid}\")\n",
    "            best = mid\n",
    "            low = mid + 1\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"✗ OOM at: {mid}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    return best\n",
    "\n",
    "```\n",
    "\n",
    "#### Usage\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "optimal_bs = find_optimal_batch_size(\n",
    "    model,\n",
    "    sample_batch,\n",
    "    criterion,\n",
    "    max_batch_size=512,\n",
    "    dtype=torch.float16,     # or torch.bfloat16 if supported\n",
    ")\n",
    "\n",
    "print(\"Optimal batch size =\", optimal_bs)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc496a-d0e4-4c2c-9b1f-9228e1eb5181",
   "metadata": {},
   "source": [
    "## Meaning of `inputs[:1]`\n",
    "`inputs[:1]` **always means taking the first element along the **batch dimension**.\n",
    "\n",
    "So it means:\n",
    "\n",
    "$$\n",
    "\\text{inputs}[0:1,;:,;:,;:]\n",
    "$$\n",
    "\n",
    "Not slicing the last dimension.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "If your input tensor has shape:\n",
    "\n",
    "```\n",
    "inputs.shape = (B, C, H, W)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "`inputs[:1]`\n",
    "\n",
    "→ keeps **batch dimension first**, slices only on axis 0\n",
    "\n",
    "Equivalent to:\n",
    "\n",
    "```\n",
    "inputs[0:1, :, :, :]\n",
    "```\n",
    "\n",
    "Resulting shape:\n",
    "\n",
    "```\n",
    "(1, C, H, W)\n",
    "```\n",
    "\n",
    "This selects a **batch of size 1**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e67d8-4a53-4434-8d80-18271f4650c4",
   "metadata": {},
   "source": [
    "## Why next(model.parameters()).device\n",
    "The expression \n",
    "\n",
    "```python\n",
    "device = next(model.parameters()).device\n",
    "```\n",
    "\n",
    "is a **Python trick** to quickly get the device (CPU/GPU) that the model is currently on.\n",
    "\n",
    "Let’s break down *exactly* what’s happening and why we use `next()`.\n",
    "\n",
    "---\n",
    "\n",
    "A PyTorch model has many parameters\n",
    "\n",
    "A model may have thousands of parameters inside layers:\n",
    "\n",
    "```\n",
    "Conv2d.weight\n",
    "Conv2d.bias\n",
    "Linear.weight\n",
    "Linear.bias\n",
    "BatchNorm.running_mean\n",
    "...\n",
    "```\n",
    "\n",
    "All of these live inside:\n",
    "\n",
    "```\n",
    "model.parameters()   # returns an iterator over all parameters\n",
    "```\n",
    "\n",
    "This is **not a list** — it’s an iterator.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "list(model.parameters())   # → [param1, param2, param3, ...]\n",
    "```\n",
    "\n",
    "But internally, `parameters()` returns something like:\n",
    "\n",
    "```\n",
    "<generator object Module.parameters>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Why we use `next(model.parameters())`**\n",
    "\n",
    "Because `model.parameters()` is an **iterator**, not a list.\n",
    "To get the *first* parameter from this iterator, we call:\n",
    "\n",
    "```\n",
    "next(model.parameters())\n",
    "```\n",
    "\n",
    "This returns the first parameter tensor in the model.\n",
    "\n",
    "That tensor has a `.device` property, for example:\n",
    "\n",
    "```\n",
    "device(type='cuda', index=0)\n",
    "device(type='cpu')\n",
    "device(type='cuda', index=1)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "device = next(model.parameters()).device\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "**Take the first parameter of the model and read what device it is stored on.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
