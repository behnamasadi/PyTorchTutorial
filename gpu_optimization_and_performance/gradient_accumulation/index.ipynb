{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00b79e-65af-4fd0-81c1-1ebca9192520",
   "metadata": {},
   "source": [
    "# 1. Why do we need gradient accumulation?\n",
    "\n",
    "If your GPU cannot fit a large batch in memory (for example batch size 128), you can simulate this large batch by splitting it into several smaller batches (micro-batches) and *accumulating* their gradients before updating the weights.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "* Load batch of size $B$\n",
    "* Forward\n",
    "* Backward\n",
    "* Optimizer step (weights update)\n",
    "\n",
    "We do:\n",
    "\n",
    "* Load micro-batch of size $b$\n",
    "* Forward\n",
    "* Backward (accumulate gradients)\n",
    "* Repeat for $k$ micro-batches\n",
    "* Optimizer step once\n",
    "\n",
    "Where\n",
    "$$B = k \\cdot b.$$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Intuition\n",
    "\n",
    "Imagine the “true” gradient for batch size 128 is:\n",
    "\n",
    "$$\n",
    "g = \\frac{1}{128}\\sum_{i=1}^{128} \\nabla_\\theta \\ell(x_i)\n",
    "$$\n",
    "\n",
    "But you can only process 32 samples at once.\n",
    "So you compute:\n",
    "\n",
    "* Micro-batch 1 (32 samples) → gradient $g_1$\n",
    "* Micro-batch 2 (32 samples) → gradient $g_2$\n",
    "* Micro-batch 3 (32 samples) → gradient $g_3$\n",
    "* Micro-batch 4 (32 samples) → gradient $g_4$\n",
    "\n",
    "PyTorch *adds* these gradients automatically when you call `.backward()` repeatedly.\n",
    "\n",
    "So you want:\n",
    "\n",
    "$$\n",
    "g = \\frac{1}{4}(g_1 + g_2 + g_3 + g_4)\n",
    "$$\n",
    "\n",
    "To achieve this, you will **divide the loss by accumulation_steps**.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. ASCII Diagram (Clear Visualization)\n",
    "\n",
    "```\n",
    "   Big Batch (B = 128)\n",
    "   -----------------------------------------\n",
    "   | 32 | 32 | 32 | 32 |    micro-batches   |\n",
    "   -----------------------------------------\n",
    "\n",
    "   step 1: forward(b1) → backward(loss/4) → grads += g1/4\n",
    "   step 2: forward(b2) → backward(loss/4) → grads += g2/4\n",
    "   step 3: forward(b3) → backward(loss/4) → grads += g3/4\n",
    "   step 4: forward(b4) → backward(loss/4) → grads += g4/4\n",
    "   step 5: optimizer.step()  → uses g = (g1+g2+g3+g4)/4\n",
    "   step 6: optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Correct PyTorch implementation\n",
    "\n",
    "### Core rule\n",
    "\n",
    "❗ Always scale the loss:\n",
    "\n",
    "$$\n",
    "\\mathrm{loss\\_scaled} = \\frac{\\mathrm{loss}}{\\mathrm{accum\\_steps}}\n",
    "$$\n",
    "\n",
    "\n",
    "so the final gradient matches the large-batch gradient.\n",
    "\n",
    "### Full minimal example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "accum_steps = 4   # Simulated batch size = 32 * 4 = 128\n",
    "model.train()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for step, (images, labels) in enumerate(train_loader):\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss = loss / accum_steps   # scale the loss\n",
    "\n",
    "    loss.backward()             # gradients accumulate\n",
    "\n",
    "    if (step + 1) % accum_steps == 0:\n",
    "        optimizer.step()        # update weights\n",
    "        optimizer.zero_grad()   # reset for next accumulation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. How PyTorch accumulates gradients internally\n",
    "\n",
    "Each `.backward()` adds to `param.grad`:\n",
    "\n",
    "```\n",
    "param.grad = param.grad + grad_from_this_backward_call\n",
    "```\n",
    "\n",
    "Gradients are **not cleared** automatically.\n",
    "Only `optimizer.zero_grad()` clears them.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Common mistakes (and how to avoid them)\n",
    "\n",
    "### Mistake 1\n",
    "\n",
    "Not scaling the loss.\n",
    "\n",
    "❌ Wrong\n",
    "\n",
    "```\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "Weights get updated with gradients that are too large.\n",
    "\n",
    "### Correct\n",
    "\n",
    "```\n",
    "(loss / accum_steps).backward()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 2\n",
    "\n",
    "Doing optimizer.step() every iteration.\n",
    "That breaks accumulation.\n",
    "\n",
    "---\n",
    "\n",
    "### Mistake 3\n",
    "\n",
    "Forgetting to reset gradients at the right moment.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. With mixed precision / GradScaler\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "accum_steps = 4\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for step, (x, y) in enumerate(train_loader):\n",
    "    with autocast():\n",
    "        loss = criterion(model(x), y)\n",
    "        loss = loss / accum_steps\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    if (step + 1) % accum_steps == 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8. How to choose accumulation steps\n",
    "\n",
    "Use:\n",
    "\n",
    "$$\n",
    "\\mathrm{accum\\_steps} = \\frac{\\text{wanted batch size}}{\\text{actual batch size}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "* model fits only batch size 16\n",
    "* want effective batch size 128\n",
    "\n",
    "accum_steps = 128 / 16 = 8\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Verifying gradient equality (mathematically)\n",
    "\n",
    "Let $g_i$ be gradient of micro-batch $i$.\n",
    "\n",
    "In accumulation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L = \\sum_{i=1}^{k} \\frac{1}{k} g_i = \\frac{1}{k} \\sum_{i=1}^k g_i.\n",
    "$$\n",
    "\n",
    "This equals the gradient of a single batch of size $B = k b$.\n",
    "\n",
    "If you do not divide loss by $k$, you get:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L = \\sum g_i\n",
    "$$\n",
    "\n",
    "which is too large.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Summary\n",
    "\n",
    "### Gradient accumulation simulates large batch training:\n",
    "\n",
    "* Split batch into micro-batches.\n",
    "* Scale loss by $1 / accum_steps$.\n",
    "* Call backward() on each micro-batch.\n",
    "* Step optimizer once per “full accumulated batch”.\n",
    "\n",
    "It’s simple and works with any model, any dataset, any optimizer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbc82a-e1d9-4f23-a6da-5a4fe54a6985",
   "metadata": {},
   "source": [
    "# Gradient Accumulation for Large Batch Training\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project demonstrates how to simulate training with large batch sizes when GPU memory is limited using **gradient accumulation**. We achieve the training dynamics of `batch_size=80` while only fitting `batch_size=10` in memory at a time.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Goal:** Train a model with effective `batch_size=80` for better gradient estimates and training stability.\n",
    "\n",
    "**Constraint:** GPU can only fit `batch_size=20` under normal training conditions.\n",
    "\n",
    "**Solution:** Use gradient accumulation to simulate larger batches by accumulating gradients over multiple smaller batches before updating weights.\n",
    "\n",
    "## How Gradient Accumulation Works\n",
    "\n",
    "Instead of updating weights after every batch, we:\n",
    "\n",
    "1. Perform multiple forward and backward passes\n",
    "2. Accumulate gradients across these passes\n",
    "3. Update weights only after N accumulation steps\n",
    "\n",
    "**Mathematical representation:**\n",
    "\n",
    "For `accum_steps = N`:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\text{effective}} = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_i\n",
    "$$\n",
    "\n",
    "This simulates the gradient of a batch that is N times larger.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| **Normal batch size** | 20 | What fits without gradient accumulation |\n",
    "| **Actual batch size** | 10 | Reduced to accommodate accumulation overhead |\n",
    "| **Accumulation steps** | 8 | Number of batches to accumulate |\n",
    "| **Effective batch size** | 80 | `10 × 8 = 80` |\n",
    "\n",
    "### Memory Overhead Analysis\n",
    "\n",
    "**Why reduce batch size from 20 to 10?**\n",
    "\n",
    "Gradient accumulation requires keeping gradients in memory across multiple iterations, which adds overhead:\n",
    "\n",
    "- **Accumulated gradients:** Stored until optimizer step\n",
    "- **Optimizer state:** Maintained throughout accumulation\n",
    "- **Model activations:** Kept for backward pass\n",
    "\n",
    "**Memory trade-off:**\n",
    "- ✅ Gained: 4× larger effective batch size (20 → 80)\n",
    "- ❌ Cost: 50% batch size reduction (20 → 10) to fit accumulated gradients\n",
    "\n",
    "### Key Implementation Steps\n",
    "\n",
    "```python\n",
    "accum_steps = 8\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for step, (images, labels) in enumerate(data_loader):\n",
    "    # 1. Forward pass with mixed precision\n",
    "    with torch.amp.autocast('cuda', dtype=dtype):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 2. Scale loss by accumulation steps\n",
    "    loss = loss / accum_steps\n",
    "    \n",
    "    # 3. Backward pass (gradients accumulate)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Cleanup to save memory\n",
    "    del outputs, loss, images, labels\n",
    "    \n",
    "    # 5. Update weights every N steps\n",
    "    if (step + 1) % accum_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "## Memory Optimization Techniques\n",
    "\n",
    "### 1. Mixed Precision Training\n",
    "- **BF16/FP16:** Reduces memory by ~50%\n",
    "- **Auto-detected:** Uses BF16 if supported, otherwise FP16\n",
    "- **GradScaler:** Applied only for FP16 to prevent underflow\n",
    "\n",
    "### 2. Aggressive Memory Cleanup\n",
    "- `del` intermediate tensors after use\n",
    "- `torch.cuda.empty_cache()` after optimizer steps\n",
    "- `optimizer.zero_grad(set_to_none=True)` to free gradient memory\n",
    "\n",
    "### 3. DataLoader Optimization\n",
    "- `pin_memory=True`: Faster CPU-to-GPU transfer\n",
    "- `num_workers=4`: Parallel data loading\n",
    "\n",
    "## Results\n",
    "\n",
    "### Batch Size Comparison\n",
    "\n",
    "| Method | Batch Size | Updates/Epoch | Effective Batch | Memory Usage |\n",
    "|--------|-----------|---------------|-----------------|--------------|\n",
    "| **Baseline** | 20 | 5 | 20 | ~2.0 GiB |\n",
    "| **Gradient Accumulation** | 10 | 1.25 | 80 | ~2.0 GiB |\n",
    "\n",
    "*(100 samples total, 100/20=5 updates vs 100/10/8≈1.25 updates)*\n",
    "\n",
    "### Benefits Achieved\n",
    "\n",
    "✅ **4× larger effective batch size** (20 → 80)\n",
    "✅ **Better gradient estimates** from larger batches\n",
    "✅ **More stable training** with reduced gradient noise\n",
    "✅ **Same memory footprint** as baseline training\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "❌ **Slower training:** Fewer weight updates per epoch\n",
    "❌ **Increased complexity:** More hyperparameters to tune\n",
    "❌ **Batch size reduction:** 50% smaller per-step batches\n",
    "\n",
    "## When to Use Gradient Accumulation\n",
    "\n",
    "### Good Use Cases ✅\n",
    "- Training with very large batch sizes (>128)\n",
    "- Limited GPU memory but need stable gradients\n",
    "- Reproducing results from papers with large batches\n",
    "- Distributed training simulation on single GPU\n",
    "\n",
    "### Not Recommended ❌\n",
    "- Already fitting desired batch size\n",
    "- Training with small models\n",
    "- When training speed is critical\n",
    "- Real-time inference applications\n",
    "\n",
    "## Running the Code\n",
    "\n",
    "```bash\n",
    "# Activate environment\n",
    "conda activate PyTorchTutorial\n",
    "\n",
    "# Run the script\n",
    "python src/gpu_optimization_and_performance/gradient_accumulation/scripts/main.py\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient accumulation simulates larger batches** without requiring more memory for activations\n",
    "2. **You still need to reduce batch size** by ~50% to accommodate gradient accumulation overhead\n",
    "3. **Loss must be scaled** by `1/accum_steps` to maintain correct gradient magnitudes\n",
    "4. **Mixed precision is essential** for maximizing memory efficiency\n",
    "5. **The effective batch size** is `batch_size × accum_steps`\n",
    "\n",
    "## References\n",
    "\n",
    "- [PyTorch Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html)\n",
    "- [Gradient Accumulation in Deep Learning](https://arxiv.org/abs/1711.00489)\n",
    "- [Memory-Efficient Training Techniques](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
