{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23b7f7d-fcbe-4340-a9bc-e04c002006dd",
   "metadata": {},
   "source": [
    "# **Knowledge Distillation (KD)**\n",
    "\n",
    "**Knowledge Distillation (KD)** is a training technique in deep learning where a **smaller (student) model** learns to imitate a **larger (teacher) model**.\n",
    "It was introduced by **Geoffrey Hinton et al. (2015)** in *“Distilling the Knowledge in a Neural Network”*, with the goal of transferring the **“dark knowledge”** (soft, informative distributions) from a large model or ensemble into a compact, efficient model — preserving performance while reducing size and computation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "Large models (teachers) have high capacity and accuracy but are computationally expensive to deploy.\n",
    "Small models (students) are efficient but typically less accurate.\n",
    "\n",
    "**Knowledge Distillation** bridges this gap by **compressing** the teacher’s knowledge into the student so that:\n",
    "\n",
    "* The student learns the teacher’s **generalization behavior**.\n",
    "* The student achieves accuracy close to the teacher while being much smaller.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Core Idea\n",
    "\n",
    "Instead of learning only from **hard one-hot labels**, the student learns from the **soft output distribution** of the teacher.\n",
    "\n",
    "### Teacher and Student Outputs\n",
    "\n",
    "$$\n",
    "p_t = \\text{softmax}\\left(\\frac{z_t}{T}\\right), \\qquad\n",
    "p_s = \\text{softmax}\\left(\\frac{z_s}{T}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ z_t, z_s $: logits from teacher and student\n",
    "* $ T $: **temperature** — controls how smooth the probabilities are\n",
    "\n",
    "Higher $ T $ (e.g. 2–4) produces **softer distributions**, revealing **inter-class similarities** — the so-called *dark knowledge*.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Loss Function\n",
    "\n",
    "The student minimizes a **weighted combination** of two losses:\n",
    "\n",
    "1. **Distillation Loss** — match soft outputs of teacher and student:\n",
    "   $$\n",
    "   \\mathcal{L}_{KD} = T^2 \\cdot \\text{KL}(p_t || p_s)\n",
    "   $$\n",
    "\n",
    "2. **Cross-Entropy Loss** — match student to hard labels:\n",
    "   $$\n",
    "   \\mathcal{L}*{CE} = -\\sum_i y_i \\log p*{s,i}(T=1)\n",
    "   $$\n",
    "\n",
    "Final combined objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}*{\\text{total}} = \\alpha \\mathcal{L}*{CE} + (1-\\alpha) \\mathcal{L}_{KD}\n",
    "$$\n",
    "\n",
    "where $ \\alpha \\in [0,1] $ balances between dataset labels and teacher supervision.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Soft Targets Help\n",
    "\n",
    "Hard labels only tell the model which class is correct.\n",
    "Soft targets reveal **how confident** the teacher is about all classes.\n",
    "\n",
    "| Class | Hard Label | Teacher (T=3) |\n",
    "| ----- | ---------- | ------------- |\n",
    "| Cat   | 1          | 0.85          |\n",
    "| Dog   | 0          | 0.10          |\n",
    "| Fox   | 0          | 0.05          |\n",
    "\n",
    "This gives the student richer gradient information, such as:\n",
    "\n",
    "* “Dog” is more similar to “Cat” than “Fox.”\n",
    "* Leads to **better generalization**, **faster convergence**, and **less overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Classical Knowledge Distillation Setup\n",
    "\n",
    "### 1. **Teacher is pretrained and frozen**\n",
    "\n",
    "The teacher is first trained on the dataset and then kept fixed:\n",
    "\n",
    "$$\n",
    "z_t = f_{\\text{teacher}}(x), \\qquad\n",
    "p_t = \\text{softmax}\\left(\\frac{z_t}{T}\\right)\n",
    "$$\n",
    "\n",
    "No gradients are computed for the teacher:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\theta_{\\text{teacher}}}{\\partial t} = 0\n",
    "$$\n",
    "\n",
    "### 2. **Student learns to mimic the teacher**\n",
    "\n",
    "The student is trained from scratch or pretrained weights.\n",
    "For each input sample:\n",
    "\n",
    "1. Teacher produces soft probabilities (no gradients).\n",
    "2. Student predicts its own outputs.\n",
    "3. KD and CE losses are combined.\n",
    "4. Only the student’s parameters are updated:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{student}} \\leftarrow \\theta_{\\text{student}} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{\\text{student}}}\n",
    "$$\n",
    "\n",
    "### 3. **Simplified PyTorch Loop**\n",
    "\n",
    "```python\n",
    "teacher.eval()\n",
    "for p in teacher.parameters(): p.requires_grad = False\n",
    "\n",
    "student.train()\n",
    "opt = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "for imgs, labels in dataloader:\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher(imgs)\n",
    "    student_logits = student(imgs)\n",
    "    loss = distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.5)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Variants of Distillation\n",
    "\n",
    "| Type                          | Description                                             | Example                  |\n",
    "| ----------------------------- | ------------------------------------------------------- | ------------------------ |\n",
    "| **Response-based**            | Student mimics teacher’s output probabilities           | Hinton et al., 2015      |\n",
    "| **Feature-based**             | Student matches intermediate activations                | FitNets                  |\n",
    "| **Relation-based**            | Student matches relations between samples/features      | Attention Transfer, CRD  |\n",
    "| **Online / EMA Distillation** | Teacher and student co-train (teacher = EMA of student) | DINO, BYOL, Mean Teacher |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. EMA Teacher (Used in DINO/BYOL)\n",
    "\n",
    "In self-distillation (no labels), the teacher is updated as an **Exponential Moving Average (EMA)** of the student:\n",
    "\n",
    "$$\n",
    "\\theta_t \\leftarrow \\tau \\theta_t + (1-\\tau)\\theta_s\n",
    "$$\n",
    "\n",
    "* $ \\tau  ≈ 0.996–0.9995$ controls update smoothness\n",
    "* The teacher evolves slowly, providing **stable targets**\n",
    "* Prevents collapse in label-free training\n",
    "\n",
    "PyTorch implementation:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for t_p, s_p in zip(teacher.parameters(), student.parameters()):\n",
    "        t_p.data = tau * t_p.data + (1 - tau) * s_p.data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Numerical Example — Hard vs. Soft Targets\n",
    "\n",
    "### Setup\n",
    "\n",
    "3-class problem, true label = “dog”\n",
    "Hard label: $ y = [0, 1, 0] $\n",
    "\n",
    "### Case A — Hard labels\n",
    "\n",
    "$$\n",
    "p_s = [0.1, 0.7, 0.2], \\quad\n",
    "\\mathcal{L}*{CE} = -\\log(0.7)=0.357\n",
    "$$\n",
    "Gradient:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}*{CE}}{\\partial z_s} = [0.1, -0.3, 0.2]\n",
    "$$\n",
    "\n",
    "### Case B — Teacher soft labels\n",
    "\n",
    "Teacher: $ p_t = [0.2, 0.6, 0.2] $, $ T=2 $\n",
    "\n",
    "Softened:\n",
    "$$\n",
    "p_t(T=2)=[0.29,0.43,0.29], \\quad p_s(T=2)=[0.26,0.38,0.36]\n",
    "$$\n",
    "Distillation gradient:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{KD}}{\\partial z_s} = p_s(T)-p_t(T)=[-0.03,-0.05,0.07]\n",
    "$$\n",
    "\n",
    "| Class | Hard Gradient | Soft Gradient |\n",
    "| ----- | ------------- | ------------- |\n",
    "| Cat   | +0.10         | –0.03         |\n",
    "| Dog   | –0.30         | –0.05         |\n",
    "| Wolf  | +0.20         | +0.07         |\n",
    "\n",
    "Soft targets yield **smoother gradients** and capture class similarity, improving stability and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Soft Labels Without a Teacher\n",
    "\n",
    "You can use **soft labels manually**, known as **label smoothing**:\n",
    "\n",
    "$$\n",
    "y_i^{\\text{smooth}} =\n",
    "\\begin{cases}\n",
    "1-\\varepsilon & \\text{if } i=y_{\\text{true}} \\\n",
    "\\frac{\\varepsilon}{C-1} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Example ($ \\varepsilon=0.1 $):\n",
    "\n",
    "$$\n",
    "y^{\\text{smooth}} = [0.05, 0.90, 0.05]\n",
    "$$\n",
    "\n",
    "### Difference from KD\n",
    "\n",
    "| Aspect                | Label Smoothing | Knowledge Distillation              |\n",
    "| --------------------- | --------------- | ----------------------------------- |\n",
    "| Source                | Manual          | Teacher-generated                   |\n",
    "| Same for all samples? | Yes             | No                                  |\n",
    "| Semantic info?        | No              | Yes                                 |\n",
    "| Data-dependent?       | No              | Yes                                 |\n",
    "| Effect                | Regularization  | Knowledge transfer + regularization |\n",
    "\n",
    "Thus:\n",
    "\n",
    "* **Label smoothing** says: “be a little less certain about all classes.”\n",
    "* **Distillation** says: “be uncertain in the *same way the teacher is uncertain*.”\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Applications\n",
    "\n",
    "| Task             | Teacher      | Student       | Benefit                            |\n",
    "| ---------------- | ------------ | ------------- | ---------------------------------- |\n",
    "| Classification   | ResNet-152   | ResNet-18     | 4× smaller, near-equal accuracy    |\n",
    "| Object Detection | Faster R-CNN | MobileNet-SSD | Real-time on edge devices          |\n",
    "| NLP              | BERT-large   | DistilBERT    | 40% smaller, 97% accuracy          |\n",
    "| Self-supervised  | EMA teacher  | DINO, BYOL    | Label-free representation learning |\n",
    "\n",
    "---\n",
    "\n",
    "## 11. PyTorch Example (Simplified)\n",
    "\n",
    "```python\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.5):\n",
    "    ce = F.cross_entropy(student_logits, labels)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    p_s = F.log_softmax(student_logits / T, dim=1)\n",
    "    kd = F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "    return alpha * ce + (1 - alpha) * kd\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Summary Intuition\n",
    "\n",
    "| Concept            | Analogy                           |\n",
    "| ------------------ | --------------------------------- |\n",
    "| Teacher            | Expert explaining reasoning       |\n",
    "| Student            | Learner imitating reasoning       |\n",
    "| Temperature        | Softness of teacher’s explanation |\n",
    "| Distillation Loss  | Match reasoning                   |\n",
    "| Cross-Entropy Loss | Match answers                     |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "Knowledge Distillation teaches the student **how the teacher thinks**, not just **what it predicts** — transferring structured, data-dependent knowledge that improves efficiency and generalization.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
