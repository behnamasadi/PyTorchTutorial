{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0235e23-c7ba-4ed6-b291-2adb4913eaba",
   "metadata": {},
   "source": [
    "### (b) **Edge-Aware Depth Smoothness Loss**\n",
    "\n",
    "Encourages locally smooth depth while preserving depth discontinuities along image edges.\n",
    "\n",
    "**Equation:**\n",
    "$\n",
    "L_\\text{smooth} = |\\partial_x d_t^*| e^{-|\\partial_x I_t|} + |\\partial_y d_t^*| e^{-|\\partial_y I_t|}\n",
    "$\n",
    "\n",
    "where $d_t^* = d_t / \\bar{d_t}$ (normalized disparity).\n",
    "\n",
    " This loss prevents noisy disparity, and the edge weighting keeps depth edges aligned with color edges.\n",
    "\n",
    "---\n",
    "\n",
    "**Combined core loss:**\n",
    "$\n",
    "L_\\text{unsup} = L_\\text{photo} + \\lambda_\\text{smooth} L_\\text{smooth}\n",
    "$\n",
    "Typical λₛₘₒₒₜₕ ≈ 0.001 – 0.01.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d71ea6-e33c-42f4-8c4e-4dd5d3efcd5a",
   "metadata": {},
   "source": [
    "## 1.6 Smoothness Loss\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.1 What it is (and why)\n",
    "\n",
    "Photometric+SSIM losses make the network match views, but monocular depth has many plausible solutions (esp. in textureless regions). **Smoothness loss** is a regularizer that encourages **piecewise-smooth disparity/depth** while **preserving edges** aligned with image gradients.\n",
    "\n",
    "### 1.6.2 Common formulations\n",
    "\n",
    "Let $I\\in\\mathbb{R}^{H\\times W\\times 3}$ be the target image, and $d$ the predicted **disparity** (often smoother than raw depth; $d=1/z$). Finite differences:\n",
    "$\\partial_x f_{i,j}=f_{i,j}-f_{i,j-1}$, $\\partial_y f_{i,j}=f_{i,j}-f_{i-1,j}$.\n",
    "\n",
    "### 1.6.3 First-order, edge-aware (most used)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{sm}}^{(1)}=\n",
    "\\frac{1}{HW}\\sum_{i,j}\\Big(\n",
    "\\left|\\partial_x d_{i,j}\\right|\\,e^{-\\alpha\\,\\|\\partial_x I_{i,j}\\|}\n",
    "+\n",
    "\\left|\\partial_y d_{i,j}\\right|\\,e^{-\\alpha\\,\\|\\partial_y I_{i,j}\\|}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "* The exponential **down-weights** the penalty at strong image edges so you **don’t over-smooth boundaries**.\n",
    "* Typical $\\alpha\\in[5,10]$. Use grayscale $I$ or per-channel gradient norm.\n",
    "\n",
    "### 1.6.4 Second-order (curvature) variant\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{sm}}^{(2)}=\n",
    "\\frac{1}{HW}\\sum_{i,j}\\Big(\n",
    "\\left|\\partial_{xx} d_{i,j}\\right| e^{-\\alpha \\|\\partial_x I_{i,j}\\|}\n",
    "+\n",
    "\\left|\\partial_{yy} d_{i,j}\\right| e^{-\\alpha \\|\\partial_y I_{i,j}\\|}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "* Penalizes **changes of slope**; good for avoiding “staircasing”.\n",
    "\n",
    "### 1.6.5 Robust penalty / Charbonnier\n",
    "\n",
    "Replace $|x|$ with $\\rho(x)=\\sqrt{x^2+\\epsilon^2}$ (e.g., $\\epsilon=10^{-3}$) for stability.\n",
    "\n",
    "### 1.6.6 Scale-invariant normalization\n",
    "\n",
    "Disparity amplitude can drift. Common tricks:\n",
    "\n",
    "* Use disparity $d$ instead of depth $z$.\n",
    "* Or divide by mean disparity per image: $\\tilde d = d / (\\bar d + \\varepsilon)$ before taking gradients.\n",
    "\n",
    "### 1.6.7 Multi-scale\n",
    "\n",
    "Compute $\\mathcal{L}_{\\text{sm}}$ at pyramid levels $s=0..S-1$ (coarsest $\\to$ finest). Weight by $w_s$ (e.g., $w_s=1/2^s$) and sum.\n",
    "\n",
    "### 1.6.8 How it plugs into monocular VO (with a ViT)\n",
    "\n",
    "Even if your depth/pose networks are **ViT-based**, the smoothness term is unchanged—just compute it on the **full-resolution** disparity map (after your ViT’s upsampling head).\n",
    "\n",
    "**Total loss** (typical self-supervised monocular pipeline):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \n",
    "\\lambda_{\\text{photo}} \\, \\mathcal{L}_{\\text{photo}}\n",
    "+ \\lambda_{\\text{ssim}} \\, \\mathcal{L}_{\\text{ssim}}\n",
    "+ \\lambda_{\\text{sm}} \\, \\mathcal{L}_{\\text{sm}}\n",
    "\\,(+ \\text{other terms: automask, occlusion, geometry})\n",
    "$$\n",
    "\n",
    "Reasonable starting weights (tune per dataset):\n",
    "\n",
    "* $\\lambda_{\\text{photo}}=1.0$\n",
    "* $\\lambda_{\\text{ssim}}=0.15$ (if photo is L1)\n",
    "* $\\lambda_{\\text{sm}} \\in [0.001, 0.1]$ (start small; increase if depth is noisy)\n",
    "\n",
    "**ViT-specific tips**\n",
    "\n",
    "* Upsample tokens to image space (conv+pixelshuffle or interpolation) **before** smoothness.\n",
    "* If you see block boundaries, add a tiny second-order term or anti-blocking conv in the upsampling head.\n",
    "* Detach image gradients (no backprop through $I$).\n",
    "\n",
    "\n",
    "> Notes\n",
    "\n",
    "* Provide `image` in **grayscale** or compute gradient norm channel-wise and average (as above).\n",
    "* If you want Charbonnier: replace `.abs()` with `torch.sqrt(x*x + eps*eps)`.\n",
    "\n",
    "### 1.6.9 Practical tuning & pitfalls\n",
    "\n",
    "* **Start small** $\\lambda_{\\text{sm}}$: too large $\\Rightarrow$ over-smoothed, “melted” geometry; too small $\\Rightarrow$ noisy depth.\n",
    "* Use **disparity** rather than depth; it naturally stabilizes scale.\n",
    "* **Detach** image gradients (as shown) to prevent weird coupling.\n",
    "* Consider **second-order** term if you see “staircase” artifacts.\n",
    "* Compute on **multiple scales**; strongest impact at coarse scales.\n",
    "* Dynamic objects/occlusions: combine with **auto-masking / per-pixel min reprojection** to avoid penalizing impossible warps.\n",
    "* For ViT heads, ensure good **anti-aliasing upsampling**; otherwise smoothness fights token blocking.\n",
    "\n",
    "### 1.6.10 Minimal recipe (drop-in)\n",
    "\n",
    "1. Predict multi-scale disparities $d^{(s)}$ from your ViT depth head.\n",
    "2. For each scale, compute $\\mathcal{L}_{\\text{sm}}^{(1)}$ with $\\alpha=10$, normalize by mean disparity, weight by $w_s=1/2^s$.\n",
    "3. Set $\\lambda_{\\text{sm}}=0.01$ as a starting point; tune against validation photometric error and scale drift.\n",
    "4. Keep your usual $\\mathcal{L}_{\\text{photo}} + \\mathcal{L}_{\\text{SSIM}}$ and occlusion handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28411248-53df-4f97-88a7-7efd328fd8ea",
   "metadata": {},
   "source": [
    "## 1.3 Edge-Aware Smoothness\n",
    "\n",
    "\n",
    "**weight the smoothness loss by image gradients.**: Intuition: if the image has a strong edge (large intensity gradient), we expect a depth discontinuity there, so we should relax the smoothness penalty.\n",
    "\n",
    "The edge-aware smoothness loss becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{edge-aware}} = \n",
    "\\sum_{i,j} \n",
    "\\Big(|\\partial_x D_{i,j}| \\cdot e^{-|\\partial_x I_{i,j}|}\\Big)\n",
    "+\n",
    "\\Big(|\\partial_y D_{i,j}| \\cdot e^{-|\\partial_y I_{i,j}|}\\Big)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $D_{i,j}$: predicted depth/disparity\n",
    "* $I_{i,j}$: input image (grayscale or per-channel average)\n",
    "* $\\partial_x, \\partial_y$: gradients along x and y\n",
    "\n",
    "####  Interpretation\n",
    "\n",
    "* If $I$ has **low gradient** → weight ≈ 1 → enforce smoothness strongly\n",
    "* If $I$ has **high gradient** (edge) → weight ≈ 0 → let depth change abruptly\n",
    "\n",
    "This preserves object boundaries and prevents depth bleeding across edges.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1.3.1 Numerical Example\n",
    "\n",
    "let’s treat $D$ as **Z-depth in meters** and compute the **full edge-aware smoothness loss** step-by-step on tiny 3×3 grids.\n",
    "Depth (meters)\n",
    "\n",
    "$$\n",
    "D=\\begin{bmatrix}\n",
    "1.0 & 1.1 & 1.2\\\\\n",
    "1.0 & 1.1 & 1.3\\\\\n",
    "1.0 & 1.0 & 1.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Grayscale image\n",
    "\n",
    "$$\n",
    "I=\\begin{bmatrix}\n",
    "0.2 & 0.2 & 0.8\\\\\n",
    "0.2 & 0.2 & 0.9\\\\\n",
    "0.2 & 0.2 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We’ll use **forward differences** and the common formulation:\n",
    "\n",
    "$$\n",
    "\\mathcal L_{\\text{edge}}=\\underbrace{\\big\\langle |\\partial_x D|\\;e^{-|\\partial_x I|}\\big\\rangle}_{\\text{x-term mean}}\n",
    "\\;+\\;\n",
    "\\underbrace{\\big\\langle |\\partial_y D|\\;e^{-|\\partial_y I|}\\big\\rangle}_{\\text{y-term mean}}.\n",
    "$$\n",
    "\n",
    "(Angle brackets $\\langle\\cdot\\rangle$ = mean over all valid entries.)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Gradients**\n",
    "\n",
    "Forward diffs (left–right for $x$, top–down for $y$):\n",
    "\n",
    "$$\n",
    "\\partial_x D=\n",
    "\\begin{bmatrix}\n",
    "-0.1 & -0.1\\\\\n",
    "-0.1 & -0.2\\\\\n",
    "0.0 & -0.2\n",
    "\\end{bmatrix},\\quad\n",
    "\\partial_y D=\n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & -0.1\\\\\n",
    "0.0 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\partial_x I=\n",
    "\\begin{bmatrix}\n",
    "0.0 & -0.6\\\\\n",
    "0.0 & -0.7\\\\\n",
    "0.0 & -0.7\n",
    "\\end{bmatrix},\\quad\n",
    "\\partial_y I=\n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & -0.1\\\\\n",
    "0.0 & 0.0 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We’ll use magnitudes $|\\cdot|$ in the loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Edge-aware weights**\n",
    "\n",
    "Weights are $w=e^{-|\\partial I|}$.\n",
    "\n",
    "Useful constants (rounded):\n",
    "$e^{-0.6}\\approx \\mathbf{0.548811}$, $e^{-0.7}\\approx \\mathbf{0.496585}$, $e^{-0.1}\\approx \\mathbf{0.904837}$, $e^{0}=1$.\n",
    "\n",
    "**X-weights $w_x=e^{-|\\partial_x I|}$:**\n",
    "\n",
    "$$\n",
    "w_x=\n",
    "\\begin{bmatrix}\n",
    "1.000000 & 0.548812\\\\\n",
    "1.000000 & 0.496585\\\\\n",
    "1.000000 & 0.496585\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Y-weights $w_y=e^{-|\\partial_y I|}$:**\n",
    "\n",
    "$$\n",
    "w_y=\n",
    "\\begin{bmatrix}\n",
    "1.000000 & 1.000000 & 0.904837\\\\\n",
    "1.000000 & 1.000000 & 1.000000\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Weighted gradients**\n",
    "\n",
    "Take absolute value of the depth gradients, multiply by weights.\n",
    "\n",
    "**X term** ($|\\partial_x D|\\cdot w_x$):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.1\\cdot1      & 0.1\\cdot0.548812\\\\\n",
    "0.1\\cdot1      & 0.2\\cdot0.496585\\\\\n",
    "0.0\\cdot1      & 0.2\\cdot0.496585\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{0.100000} & \\mathbf{0.054881}\\\\\n",
    "\\mathbf{0.100000} & \\mathbf{0.099317}\\\\\n",
    "\\mathbf{0.000000} & \\mathbf{0.099317}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Y term** ($|\\partial_y D|\\cdot w_y$):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.0\\cdot1 & 0.0\\cdot1 & 0.1\\cdot0.904837\\\\\n",
    "0.0\\cdot1 & 0.1\\cdot1 & 0.1\\cdot1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{0.000000} & \\mathbf{0.000000} & \\mathbf{0.090484}\\\\\n",
    "\\mathbf{0.000000} & \\mathbf{0.100000} & \\mathbf{0.100000}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Means and final loss**\n",
    "\n",
    "Each matrix above has 6 valid entries. Compute means:\n",
    "\n",
    "* $\\text{mean}_x = \\frac{0.100000+0.054881+0.100000+0.099317+0.000000+0.099317}{6} = \\mathbf{0.075585}$\n",
    "\n",
    "* $\\text{mean}_y = \\frac{0.000000+0.000000+0.090484+0.000000+0.100000+0.100000}{6} = \\mathbf{0.048415}$\n",
    "\n",
    "**Edge-aware smoothness loss**\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal L_{\\text{edge}}=\\text{mean}_x+\\text{mean}_y\n",
    "= \\mathbf{0.075585}+\\mathbf{0.048415}\n",
    "= \\mathbf{0.123999}\\ (\\approx 0.124)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**For comparison: naïve smoothness (no edge weights)**\n",
    "\n",
    "$$\n",
    "\\langle|\\partial_x D|\\rangle + \\langle|\\partial_y D|\\rangle\n",
    "= \\frac{0.1+0.1+0.1+0.2+0.0+0.2}{6} + \\frac{0+0+0.1+0+0.1+0.1}{6}\n",
    "= 0.116667+0.050000\n",
    "= \\mathbf{0.166667}\n",
    "$$\n",
    "\n",
    "So edge-aware weighting **reduced** the penalty from **0.1667 → 0.124**, because it down-weighted depth changes where the image has strong edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976041b-c87c-407e-8c2d-74c5e2bef298",
   "metadata": {},
   "source": [
    "### Size of $D$, and $I$ For Real Size Image Input \n",
    "Let’s walk through this carefully for an image of **224 × 224** (like a ResNet-18 input) and see what the shapes look like for $D$, $I$, and their gradients.\n",
    "\n",
    "---\n",
    "\n",
    "#### Input Image $I$\n",
    "\n",
    "If you feed a single RGB image to your network:\n",
    "\n",
    "* $I$ shape (PyTorch): **\\[B, C, H, W] = \\[1, 3, 224, 224]**\n",
    "* For smoothness loss, we usually convert to **grayscale or take per-channel mean** to get shape:\n",
    "  **\\[1, 1, 224, 224]**\n",
    "\n",
    "---\n",
    "\n",
    "#### Predicted Depth/Disparity Map $D$\n",
    "\n",
    "Most monocular depth networks **output one value per pixel** (dense prediction):\n",
    "\n",
    "* $D$ shape: **\\[1, 1, 224, 224]**\n",
    "* This means $D[i,j]$ is the predicted depth/disparity for pixel $(i,j)$.\n",
    "\n",
    "> ⚠️ If you’re using a network like ResNet-18 as backbone, you often upsample the final feature map back to 224×224 so that $D$ has same resolution as $I$.\n",
    "\n",
    "---\n",
    "\n",
    "####  Computing Gradients\n",
    "\n",
    "Gradients are local finite differences — they don’t change resolution much.\n",
    "\n",
    "#### Horizontal gradient $∂x$:\n",
    "\n",
    "* Compute $D[:,:, :, :-1] - D[:,:, :, 1:]$\n",
    "* Resulting size: **\\[1, 1, 224, 223]** (one fewer column)\n",
    "\n",
    "#### Vertical gradient $∂y$:\n",
    "\n",
    "* Compute $D[:,:, :-1, :] - D[:,:, 1:, :]$\n",
    "* Resulting size: **\\[1, 1, 223, 224]** (one fewer row)\n",
    "\n",
    "Same for image $I$ — we compute $∂x I$ and $∂y I$ with same operators, so they match the shapes of depth gradients.\n",
    "\n",
    "---\n",
    "\n",
    "#### Element-wise Weighting\n",
    "\n",
    "Once you have:\n",
    "\n",
    "* $|\\partial_x D|$  → shape \\[1, 1, 224, 223]\n",
    "* $|\\partial_x I|$  → shape \\[1, 1, 224, 223]\n",
    "\n",
    "You compute:\n",
    "\n",
    "$$\n",
    "\\text{weighted}_x = |\\partial_x D| \\cdot e^{-|\\partial_x I|}\n",
    "$$\n",
    "\n",
    "Element-wise multiplication → stays **\\[1, 1, 224, 223]**\n",
    "\n",
    "Then you take mean over all elements.\n",
    "Same for $y$-direction.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Quantity        | Shape (for 224×224 input)      | Meaning                   |\n",
    "| --------------- | ------------------------------ | ------------------------- |\n",
    "| $I$ (grayscale) | \\[B, 1, 224, 224]              | Input image intensity     |\n",
    "| $D$             | \\[B, 1, 224, 224]              | Predicted depth/disparity |\n",
    "| $∂xD, ∂xI$      | \\[B, 1, 224, 223]              | Horizontal differences    |\n",
    "| $∂yD, ∂yI$      | \\[B, 1, 223, 224]              | Vertical differences      |\n",
    "| Weighted terms  | Same as corresponding gradient | Used in loss computation  |\n",
    "| Final loss      | Scalar                         | Mean over all entries     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb45d02-6e26-4a44-826e-b69524a2671e",
   "metadata": {},
   "source": [
    "#### Could We Use Better Edge Detection?\n",
    "\n",
    "Yes — some papers do. Variants include:\n",
    "\n",
    "* **Gradient magnitude (isotropic):**\n",
    "\n",
    "  $$\n",
    "  |\\nabla I| = \\sqrt{(\\partial_x I)^2 + (\\partial_y I)^2}\n",
    "  $$\n",
    "\n",
    "  This captures diagonal edges more naturally, instead of separating x and y.\n",
    "\n",
    "* **Sobel / Scharr filters**\n",
    "  Better gradient estimation (uses 3×3 kernels for smoothing + derivative).\n",
    "\n",
    "* **Learned edge weights**\n",
    "  Some works learn an \"edge mask\" jointly with depth to adaptively weight smoothness.\n",
    "\n",
    "* **Perceptual / semantic edges**\n",
    "  Use segmentation boundaries or learned feature maps instead of raw intensity.\n",
    "\n",
    "But they all must remain **differentiable** — so binary detectors like Canny are generally avoided in training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08891f38-73a2-4d65-9e9a-cd169f0b9dd1",
   "metadata": {},
   "source": [
    "\n",
    "##  Convention Used in Most Papers (e.g. SfMLearner, Monodepth2)\n",
    "\n",
    "| **Frame**                    | **Role**                                                          |\n",
    "| ---------------------------- | ----------------------------------------------------------------- |\n",
    "| $I_i$                        | **Target frame** → you predict depth $D_i$ for this frame         |\n",
    "| $I_{i+1}$ (and/or $I_{i-1}$) | **Reference frame(s)** → you warp them into frame $i$’s viewpoint |\n",
    "\n",
    "---\n",
    "\n",
    "##  What Happens Step by Step\n",
    "\n",
    "1. **Depth Prediction:**\n",
    "   $D_i = \\text{DepthNet}(I_i)$\n",
    "   → per-pixel depth map **for frame $i$**.\n",
    "\n",
    "2. **Pose Prediction:**\n",
    "   $(R, \\mathbf{t}) = \\text{PoseNet}(I_i, I_{i+1})$\n",
    "   → relative transform $T_{i \\rightarrow i+1}$ (from frame $i$ to frame $i+1$).\n",
    "\n",
    "3. **Back-Project:**\n",
    "   Use $D_i$ and camera intrinsics $K$ to get 3D points $P_i$ in frame $i$.\n",
    "\n",
    "4. **Transform:**\n",
    "   Move points to frame $i+1$:\n",
    "   $P_{i+1} = T_{i \\rightarrow i+1} \\cdot P_i$.\n",
    "\n",
    "5. **Project:**\n",
    "   Project $P_{i+1}$ to 2D using intrinsics $K$ → get pixel coords $(u_{i+1}, v_{i+1})$.\n",
    "\n",
    "6. **Sample:**\n",
    "   Bilinear sample $I_{i+1}$ at these coords → reconstructed image $\\hat{I}_i$.\n",
    "\n",
    "7. **Loss:**\n",
    "   Compare $I_i$ (target) and $\\hat{I}_i$:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{photo}} = \\frac{1}{N}\\sum |I_i - \\hat{I}_i|\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "##  Intuition\n",
    "\n",
    "* **Target frame:** $I_i$ — you are trying to reproduce this image.\n",
    "* **Reference frame:** $I_{i+1}$ — you are \"borrowing\" its pixels, warping them into $i$’s viewpoint.\n",
    "* If reconstruction is good, $I_i \\approx \\hat{I}_i$.\n",
    "\n",
    "---\n",
    "\n",
    "##  You Can Also Swap\n",
    "\n",
    "You can just as well make $I_{i+1}$ the target and $I_i$ the reference — as long as you're consistent.\n",
    "But by convention:\n",
    "\n",
    "* **Depth is always predicted for the target frame.**\n",
    "* **Reference frames are the ones you warp into the target’s viewpoint.**\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR (Answer to Your Question)\n",
    "\n",
    "**With two consecutive frames $I_i, I_{i+1}$:**\n",
    "\n",
    "* **Target frame:** $I_i$ (depth $D_i$ is predicted for this one)\n",
    "* **Reference frame:** $I_{i+1}$ (warped into frame $i$’s viewpoint to create $\\hat{I}_i$)\n",
    "* **Loss computed between:** $I_i$ and $\\hat{I}_i$\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to draw a small diagram (camera frustums for $i$ and $i+1$, showing how a 3D point projects to each, and how we warp $I_{i+1}$ to reconstruct $I_i$)? It usually makes this concept stick immediately.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
