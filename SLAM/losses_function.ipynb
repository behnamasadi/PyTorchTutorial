{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa5949b-6373-4254-a10a-fda9b237421f",
   "metadata": {},
   "source": [
    "# Loss Functions for Depth + Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13843bf-c0cf-40ea-ae97-b8a441c1c927",
   "metadata": {},
   "source": [
    "\n",
    "| **Category**                          | **Loss name**                                                         | **Type**                    | **Mandatory?** | **Purpose / Description**                                                    |\n",
    "| ------------------------------------- | --------------------------------------------------------------------- | --------------------------- | -------------- | ---------------------------------------------------------------------------- |\n",
    "| **Photometric consistency**           | *Photometric loss* (SSIM + L1)                                        | Self-supervised             | ✅              | Image reconstruction via reprojection-based self-supervision                 |\n",
    "| **Geometry regularization**           | *Edge-aware depth smoothness*                                         | Regularization              | ✅              | Enforces spatial coherence, sharp and stable depth maps                      |\n",
    "| **Pose supervision / regularization** | *Geodesic loss*, *Quaternion loss*, *SE(3) transform loss*            | Pose supervision            | optional       | Penalize rotation and translation errors if GT/pseudo-GT poses are available |\n",
    "| **Motion priors / dynamics**          | *Rotation magnitude loss*, *velocity smoothness*, *Motion prior (L2)* | Regularization              | optional       | Regularize PoseNet outputs, prevent large or inconsistent motion jumps       |\n",
    "| **Additional (optional)**             | *Depth consistency*, *multi-scale weighting*, *explainability mask*   | Refinement / Regularization | optional       | Advanced refinement to improve robustness and interpretability               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2a761-ceef-40fa-9dff-bd86b824603c",
   "metadata": {},
   "source": [
    "##  Self-Supervised Losses (always used)\n",
    "\n",
    "They drive both DepthNet and PoseNet when you train from monocular videos **without ground truth**.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) **Photometric Reprojection Loss** (a.k.a. View Synthesis Loss)\n",
    "\n",
    "**Definition:**\n",
    "$\n",
    "L_\\text{photo} = \\min_s \\Big( \\alpha \\frac{1 - \\text{SSIM}(I_t, I_s')}{2} + (1 - \\alpha) | I_t - I_s' |_1 \\Big)\n",
    "$\n",
    "\n",
    "* $I_s'$: Source image warped into target frame using predicted depth + pose.\n",
    "* $\\alpha = 0.85$ works best (from Monodepth2).\n",
    "* Take **minimum reprojection** across multiple source frames (to ignore occlusions).\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded924a-e6c1-4e4d-a054-f439c070d199",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e777af5-f877-4970-bf76-f5fa63667437",
   "metadata": {},
   "source": [
    "## 1.4 Structural Similarity Index Measure (SSIM)\n",
    "\n",
    "\n",
    "The **Structural Similarity Index Measure (SSIM)** is a widely used metric for measuring the similarity between two images. Unlike simple metrics such as Mean Squared Error (MSE) or Peak Signal-to-Noise Ratio (PSNR), SSIM is designed to model the way humans perceive image quality — focusing on structural information, contrast, and luminance rather than raw pixel differences.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.1 The Idea Behind SSIM\n",
    "\n",
    "SSIM tries to answer: *“How similar are two images in terms of structure, contrast, and brightness?”*\n",
    "\n",
    "It decomposes similarity into **three components**:\n",
    "\n",
    "1. **Luminance similarity** $l(x, y)$:\n",
    "\n",
    "   Are the two images equally bright on average?\n",
    "\n",
    "   $$\n",
    "   l(x,y) = \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}\n",
    "   $$\n",
    "\n",
    "- If both images have similar brightness, this term is near 1.\n",
    "- If one image is much darker, it will drop below 1.   \n",
    "\n",
    "2. **Contrast similarity** $c(x, y)$:\n",
    "\n",
    "   Do the two images have the same amount of contrast?\n",
    "\n",
    "   $$\n",
    "   c(x,y) = \\frac{2 \\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}\n",
    "   $$\n",
    "\n",
    "- If both images have similar contrast (variability), this is near 1.\n",
    "- If one is flat (low contrast) and the other is textured (high contrast), the similarity decreases.\n",
    "\n",
    "3. **Structural similarity** $s(x, y)$:\n",
    "\n",
    "   Do the two images have the same patterns and textures?\n",
    "\n",
    "   $$\n",
    "   s(x,y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "   $$\n",
    "- If $x$ and $y$ rise and fall together (high correlation), this is near $1$.\n",
    "- If they are uncorrelated or inverted (noise, wrong edges), this value becomes smaller or even negative.   \n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\mu_x, \\mu_y$ are the mean intensities,\n",
    "* $\\sigma_x, \\sigma_y$ are standard deviations,\n",
    "* $\\sigma_{xy}$ is covariance between $x$ and $y$,\n",
    "* $C_1, C_2, C_3$ are small constants to stabilize division.\n",
    "\n",
    "The final SSIM is:\n",
    "\n",
    "$$\n",
    "SSIM(x, y) = [l(x, y)]^\\alpha \\cdot [c(x, y)]^\\beta \\cdot [s(x, y)]^\\gamma\n",
    "$$\n",
    "\n",
    "Usually $\\alpha = \\beta = \\gamma = 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a979f2-92f1-4346-99db-b2b54d03c18c",
   "metadata": {},
   "source": [
    "### 1.4.2 Normalized Correlation\n",
    "The **normalized correlation** part of SSIM is the most “structural” component, so it’s worth understanding carefully.\n",
    "\n",
    "**Step 1: Represent the Images**\n",
    "\n",
    "Suppose you have two image patches $x$ and $y$ of size $N$ pixels (can be grayscale or single channel).\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, \\dots, x_N], \\quad y = [y_1, y_2, \\dots, y_N]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Compute the Mean**\n",
    "\n",
    "Compute the mean intensity (average brightness) of each patch:\n",
    "\n",
    "$$\n",
    "\\mu_x = \\frac{1}{N} \\sum_{i=1}^{N} x_i, \\qquad\n",
    "\\mu_y = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Compute the Standard Deviations**\n",
    "\n",
    "Compute how much pixel values vary around the mean:\n",
    "\n",
    "$$\n",
    "\\sigma_x = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu_x)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_y = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\mu_y)^2}\n",
    "$$\n",
    "\n",
    "These represent the **contrast** of each image.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Compute the Covariance**\n",
    "\n",
    "Covariance measures how much the two patches vary *together*:\n",
    "\n",
    "$$\n",
    "\\sigma_{xy} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu_x)(y_i - \\mu_y)\n",
    "$$\n",
    "\n",
    "* If $x$ and $y$ increase/decrease together → covariance is **positive**.\n",
    "* If one increases when the other decreases → covariance is **negative**.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 5: Normalize → Get the Correlation**\n",
    "\n",
    "The **Pearson correlation coefficient** is just the covariance normalized by the product of standard deviations:\n",
    "\n",
    "$$\n",
    "\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "* This value ranges between **-1 and 1**.\n",
    "\n",
    "  * $1.0 \\Rightarrow$ perfect positive linear correlation (structures align perfectly).\n",
    "  * $0 \\Rightarrow$ no linear correlation (structures unrelated).\n",
    "  * $-1.0 \\Rightarrow$ perfect negative correlation (inverted contrast).\n",
    "\n",
    "---\n",
    "\n",
    "**Step 6: Add Stabilization for SSIM**\n",
    "\n",
    "In SSIM, to avoid division by zero when contrast is very low, we use a small constant $C_3$:\n",
    "\n",
    "$$\n",
    "s(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "$$\n",
    "\n",
    "This keeps the measure well-defined even for very flat patches (e.g., almost uniform gray).\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* **Covariance** tells you whether pixel intensities move together.\n",
    "* **Normalization** by $\\sigma_x \\sigma_y$ removes the effect of scale/contrast so you focus purely on **structure** (edges, textures, gradients).\n",
    "\n",
    "This is why SSIM can still give a high similarity score if one image is slightly brighter/darker — because the **pattern** of variations is the same.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71665822-6aad-41fc-8c27-52fb57a8f6b1",
   "metadata": {},
   "source": [
    "SSIM tries to mimic the **human visual system** by:\n",
    "\n",
    "* **Normalizing for lighting** (so small brightness changes are ignored).\n",
    "* **Normalizing for contrast** (so small contrast changes are less penalized).\n",
    "* **Measuring structure** (so it cares about edges, patterns, textures).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e714f9-5318-49e9-940c-92abffa639f8",
   "metadata": {},
   "source": [
    "### 1.4.3 Numerical SSIM Example\n",
    "Awesome—let’s do a fully worked **numerical SSIM example** with two $3\\times3$ grayscale image patches and compute every piece: mean, std, covariance, normalized correlation, the three SSIM terms (luminance/contrast/structure), and the final SSIM.\n",
    "\n",
    "**Images (grayscale, 8-bit scale assumed)**\n",
    "\n",
    "$$\n",
    "x=\\begin{bmatrix}\n",
    "10&20&30\\\\\n",
    "20&30&40\\\\\n",
    "30&40&50\n",
    "\\end{bmatrix},\\quad\n",
    "y=\\begin{bmatrix}\n",
    "12&22&32\\\\\n",
    "21&31&41\\\\\n",
    "29&39&49\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We’ll treat the whole $3\\times3$ window as one patch (i.e., a single SSIM window).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) Basic statistics\n",
    "\n",
    "Let $N=9$.\n",
    "\n",
    "**Means**\n",
    "\n",
    "$$\n",
    "\\mu_x=30.0000,\\qquad \\mu_y=30.6667\n",
    "$$\n",
    "\n",
    "**Sample standard deviations** (ddof=1)\n",
    "\n",
    "$$\n",
    "\\sigma_x=\\sqrt{150}=12.2474,\\qquad \\sigma_y=\\sqrt{129.25}=11.3688\n",
    "$$\n",
    "\n",
    "**Sample covariance**\n",
    "\n",
    "$$\n",
    "\\sigma_{xy}=\\frac{1}{N-1}\\sum (x_i-\\mu_x)(y_i-\\mu_y)=138.75\n",
    "$$\n",
    "\n",
    "**Normalized correlation (Pearson $\\rho$)**\n",
    "\n",
    "$$\n",
    "\\rho=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}=\\frac{138.75}{12.2474\\cdot 11.3688}\\approx 0.9965\n",
    "$$\n",
    "\n",
    "> Intuition: the patches vary together almost perfectly (very strong structural agreement).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) SSIM components\n",
    "\n",
    "Use the standard SSIM constants for 8-bit images:\n",
    "\n",
    "$$\n",
    "L=255,\\quad C_1=(0.01L)^2=6.5025,\\quad C_2=(0.03L)^2=58.5225,\\quad C_3=\\frac{C_2}{2}=29.26125\n",
    "$$\n",
    "\n",
    "#### (a) Luminance term $l(x,y)$\n",
    "\n",
    "$$\n",
    "l=\\frac{2\\mu_x\\mu_y+C_1}{\\mu_x^2+\\mu_y^2+C_1}\n",
    "=\\frac{2\\cdot 30\\cdot 30.6667+6.5025}{30^2+30.6667^2+6.5025}\n",
    "\\approx 0.99976\n",
    "$$\n",
    "\n",
    "#### (b) Contrast term $c(x,y)$\n",
    "\n",
    "$$\n",
    "c=\\frac{2\\sigma_x\\sigma_y+C_2}{\\sigma_x^2+\\sigma_y^2+C_2}\n",
    "=\\frac{2\\cdot 12.2474\\cdot 11.3688+58.5225}{12.2474^2+11.3688^2+58.5225}\n",
    "\\approx 0.99771\n",
    "$$\n",
    "\n",
    "#### (c) Structure term $s(x,y)$\n",
    "\n",
    "$$\n",
    "s=\\frac{\\sigma_{xy}+C_3}{\\sigma_x\\sigma_y+C_3}\n",
    "=\\frac{138.75+29.26125}{12.2474\\cdot 11.3688+29.26125}\n",
    "\\approx 0.99710\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Final SSIM\n",
    "\n",
    "$$\n",
    "SSIM=l\\cdot c\\cdot s\\approx 0.99976\\cdot 0.99771\\cdot 0.99710\\approx \\mathbf{0.99458}\n",
    "$$\n",
    "\n",
    "**Takeaway:** Despite small brightness/contrast differences, the structures match extremely well (high $\\rho$ and high SSIM ≈ **0.995**). This is exactly the kind of case where SSIM (and the structure term) shines compared to plain MSE/PSNR.\n",
    "\n",
    "If you want, I can also compute **MSE/PSNR** for the same pair so you can see how they react differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14310ab-96f4-4f5c-abd7-2081315611f6",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5 LPIPS\n",
    "\n",
    "So far, we talked about **SSIM** (hand-crafted metric). Now, **LPIPS (Learned Perceptual Image Patch Similarity)** goes a step further: instead of manually designing similarity measures, it uses **deep features** from pretrained networks (e.g., AlexNet, VGG, SqueezeNet) to capture perceptual similarity.\n",
    "\n",
    "\n",
    "* Proposed in **\"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\"** (Zhang et al., 2018).\n",
    "* Idea: Humans judge images by *perceptual similarity*, not pixel-wise equality.\n",
    "* LPIPS measures distance in the **feature space** of a pretrained CNN rather than raw pixels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.1 How It Works\n",
    "\n",
    "1. Take two images $x$ and $y$.\n",
    "2. Pass both through a **pretrained network** (e.g., VGG).\n",
    "3. Extract activations from multiple layers (feature maps).\n",
    "4. Normalize features and compute **L2 distance** per spatial location.\n",
    "5. Average distances across spatial positions and layers.\n",
    "6. Optionally, train small linear weights to better align with human judgments.\n",
    "\n",
    "$$\n",
    "LPIPS(x,y) = \\sum_l \\frac{1}{H_l W_l} \\sum_{h,w} w_l \\; \\| \\hat{f}_l(x)_{h,w} - \\hat{f}_l(y)_{h,w} \\|_2^2\n",
    "$$\n",
    "\n",
    "* $f_l$: feature map from layer $l$.\n",
    "* $\\hat{f}_l$: channel-wise normalized.\n",
    "* $w_l$: learned weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.2 Why LPIPS is Important\n",
    "\n",
    "* **MSE/PSNR**: pixel-wise, not perceptual.\n",
    "* **SSIM**: structural but still hand-crafted.\n",
    "* **LPIPS**: learned perceptual similarity, matches human perception much better.\n",
    "\n",
    "In practice, LPIPS is considered **state-of-the-art** for evaluating perceptual image quality (GANs, super-resolution, style transfer, inpainting).\n",
    "\n",
    "---\n",
    "\n",
    "**LPIPS in Deep Learning Workflows**\n",
    "\n",
    "* Used as an **evaluation metric** for generative models (GANs, diffusion, etc.).\n",
    "* Sometimes used as a **loss function** (LPIPS loss) for training perceptual similarity.\n",
    "* Often combined with pixel losses (L1/L2) or SSIM.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.3 Comparison: SSIM vs LPIPS\n",
    "\n",
    "| Metric       | Based on                           | Pros                                                         | Cons                                             |\n",
    "| ------------ | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------ |\n",
    "| **MSE/PSNR** | Pixel differences                  | Simple, fast                                                 | Not perceptual, sensitive to shifts              |\n",
    "| **SSIM**     | Luminance, contrast, structure     | Better perceptual alignment                                  | Hand-crafted, less robust to complex distortions |\n",
    "| **LPIPS**    | Deep features (VGG, AlexNet, etc.) | Best matches human perception, widely used in GAN evaluation | Heavier, requires pretrained nets                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91544ea3-86ed-479d-8038-832f742f8d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8baec-6240-4c43-ad3f-45a6258e3bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0235e23-c7ba-4ed6-b291-2adb4913eaba",
   "metadata": {},
   "source": [
    "### (b) **Edge-Aware Depth Smoothness Loss**\n",
    "\n",
    "Encourages locally smooth depth while preserving depth discontinuities along image edges.\n",
    "\n",
    "**Equation:**\n",
    "$\n",
    "L_\\text{smooth} = |\\partial_x d_t^*| e^{-|\\partial_x I_t|} + |\\partial_y d_t^*| e^{-|\\partial_y I_t|}\n",
    "$\n",
    "\n",
    "where $d_t^* = d_t / \\bar{d_t}$ (normalized disparity).\n",
    "\n",
    " This loss prevents noisy disparity, and the edge weighting keeps depth edges aligned with color edges.\n",
    "\n",
    "---\n",
    "\n",
    "**Combined core loss:**\n",
    "$\n",
    "L_\\text{unsup} = L_\\text{photo} + \\lambda_\\text{smooth} L_\\text{smooth}\n",
    "$\n",
    "Typical λₛₘₒₒₜₕ ≈ 0.001 – 0.01.\n",
    "\n",
    "---\n",
    "\n",
    "##  3. Optional pose-related losses (for better motion consistency)\n",
    "\n",
    "These are **not mandatory** for self-supervised training,\n",
    "but useful if you have **pseudo ground truth poses** (e.g., from KITTI odometry or IMU).\n",
    "\n",
    "---\n",
    "\n",
    "### (a) **Geodesic Rotation Loss**\n",
    "\n",
    "Encourages PoseNet’s predicted rotation (R_\\text{pred}) to be close to ground truth (R_\\text{gt}) on SO(3):\n",
    "\n",
    "$\n",
    "L_R = | \\log(R_\\text{gt}^T R_\\text{pred}) |_2\n",
    "$\n",
    "\n",
    "Where `log()` is the matrix logarithm mapping to so(3).\n",
    "This gives a **rotation angle error** in radians.\n",
    "\n",
    "```python\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    R_rel = R_pred.transpose(-1, -2) @ R_gt\n",
    "    log_R = torch.linalg.logm(R_rel)\n",
    "    return torch.norm(log_R, dim=(1,2)).mean()\n",
    "```\n",
    "\n",
    "Use if you have ground-truth rotations from KITTI or IMU fusion.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **Quaternion Loss**\n",
    "\n",
    "If you represent rotations as quaternions (q):\n",
    "\n",
    "$\n",
    "L_q = 1 - \\langle q_\\text{pred}, q_\\text{gt} \\rangle^2\n",
    "$\n",
    "\n",
    "It penalizes quaternion misalignment.\n",
    "\n",
    "```python\n",
    "def quaternion_loss(q_pred, q_gt):\n",
    "    return 1 - torch.sum(q_pred * q_gt, dim=-1).pow(2).mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### (c) **Full SE(3) Transformation Loss**\n",
    "\n",
    "Combines rotation and translation errors in one expression:\n",
    "\n",
    "$\n",
    "L_{SE3} = | \\log(T_\\text{gt}^{-1} T_\\text{pred}) |_2\n",
    "$\n",
    "This measures the 6D twist vector (ξ) difference between two SE(3) transforms.\n",
    "\n",
    " Great for fine-tuning PoseNet if you have ground-truth or pseudo ground-truth trajectories.\n",
    "\n",
    "---\n",
    "\n",
    "### (d) **Rotation Magnitude / Motion Prior Loss**\n",
    "\n",
    "Encourages PoseNet outputs to have small, realistic motion per frame:\n",
    "\n",
    "$\n",
    "L_\\text{motion} = |r_\\text{pred}|*2 + |t*\\text{pred}|_2\n",
    "$\n",
    "\n",
    "This acts like a regularizer and avoids large jumps in estimated pose.\n",
    "\n",
    "---\n",
    "\n",
    "##  4. Optional advanced terms (for refinement or stability)\n",
    "\n",
    "| Loss                           | Purpose                                                          |\n",
    "| ------------------------------ | ---------------------------------------------------------------- |\n",
    "| **Explainability mask loss**   | Downweights moving objects and occlusions.                       |\n",
    "| **Depth consistency loss**     | Enforces consistency between multi-scale depth predictions.      |\n",
    "| **Temporal smoothness loss**   | Penalizes acceleration/jerk between consecutive predicted poses. |\n",
    "| **Scale-invariant depth loss** | Used when GT depths are available but relative scale is unknown. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Recommended combination for your project\n",
    "\n",
    "Since you’re currently using **KITTI** and your setup is **self-supervised** (DepthNet + PoseNet trained together):\n",
    "\n",
    "###  **Use these always**\n",
    "\n",
    "| Loss                                      | Weight | Purpose              |\n",
    "| ----------------------------------------- | ------ | -------------------- |\n",
    "| Photometric (SSIM + L1, min reprojection) | 1.0    | Core supervision     |\n",
    "| Edge-aware depth smoothness               | 0.001  | Depth regularization |\n",
    "\n",
    "###  **Add these if GT poses available (optional fine-tuning)**\n",
    "\n",
    "| Loss                   | Weight | Purpose               |\n",
    "| ---------------------- | ------ | --------------------- |\n",
    "| Geodesic rotation loss | 1.0    | Rotation accuracy     |\n",
    "| Translation (L1) loss  | 0.1    | Motion scale accuracy |\n",
    "| SE(3) transform loss   | 0.5    | Joint refinement      |\n",
    "\n",
    "---\n",
    "\n",
    "##  6. Example final total loss\n",
    "\n",
    "```python\n",
    "λ_smooth = 0.001\n",
    "λ_se3 = 0.5\n",
    "λ_geo = 1.0\n",
    "\n",
    "total_loss = photo_loss + λ_smooth * smooth_loss\n",
    "\n",
    "if use_pose_supervision:\n",
    "    total_loss += λ_geo * geo_loss + λ_se3 * se3_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a9196-9276-48a7-97c4-792d39f053c8",
   "metadata": {},
   "source": [
    "When you train a network to predict **rotations**, you want a loss function that measures “how far apart” two rotations are.\n",
    "Rotations live on the **special orthogonal group** SO(3), which is not a flat Euclidean space — so we need to be careful.\n",
    "\n",
    "## 1.1 Rotation Loss $SO(3)$\n",
    "\n",
    "###  1.1.1 Rotation Representation \n",
    "\n",
    "* **Quaternions** (unit 4D vectors, $q \\in \\mathbb{R}^4, \\|q\\|=1$)\n",
    "* **Rotation matrices** ($R \\in SO(3)$, orthogonal 3×3 with det=+1)\n",
    "* **Axis-angle** ($\\theta, \\mathbf{u}$)\n",
    "\n",
    "For deep learning, **quaternions** are often used because:\n",
    "\n",
    "* They are continuous (no singularities like Euler angles).\n",
    "* They are compact (4 parameters).\n",
    "* Easy to normalize to unit norm after network output.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
