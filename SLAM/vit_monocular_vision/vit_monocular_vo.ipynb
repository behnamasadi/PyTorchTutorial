{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9aebb7-14ee-4d75-9f88-b3c53fff58b8",
   "metadata": {},
   "source": [
    "# Overview of Approaches and Deep Learning Architecture For Visual Odometry\n",
    "\n",
    "###  Deep Learning VO: main families\n",
    "\n",
    "**A. Pose regression (end-to-end)**\n",
    "\n",
    "* **Idea:** CNN (or CNN+RNN) regresses $(\\mathbf{R},\\mathbf{t}) \\in SE(3)$ directly from stacked frames.\n",
    "* **Architectures:** PoseNet-style CNNs → ConvLSTM/GRU for temporal context.\n",
    "* **Losses:** supervised $\\ell_1/\\ell_2$ on translation, geodesic loss on rotation; or **self-supervised photometric** (see §3).\n",
    "* **Pros:** simple inference; can be fast.\n",
    "* **Cons:** scale drift (monocular), weak geometry priors, generalization risk.\n",
    "\n",
    "**B. Depth+Pose joint learning (self-supervised SfM-style)**\n",
    "\n",
    "* **Idea:** one net predicts **depth** $D_t$; another predicts **relative pose** $T_{t\\rightarrow s}$. Reproject source $\\mathbf{I}_s$ into target with $D_t$ and $T$; train by minimizing **photometric/SSIM** reconstruction.\n",
    "* **Architectures:** U-Net depth backbones; small PoseNet; sometimes **cost volumes** (stereo) or **transformers**.\n",
    "* **Extras:** auto-masking for non-rigid pixels, **explainability masks**, multi-scale supervision, **edge-aware smoothness** on depth.\n",
    "* **Pros:** no GT poses needed; geometry-aware; scales well with data.\n",
    "* **Cons:** moving objects/occlusions need handling; absolute scale ambiguous (mono).\n",
    "\n",
    "**C. Geometry-aware networks (differentiable optimization inside)**\n",
    "\n",
    "* **Idea:** embed **PnP/BA/ICP** as differentiable layers (Gauss-Newton blocks, differentiable bundle adjustment, learned Jacobians/weights).\n",
    "* **Examples vibe:** DeepV2D-like depth-pose iterative refinement, BA-Net-style layers, **DROID-SLAM-like** dense matching + iterative pose/structure updates.\n",
    "* **Pros:** better inductive bias; stronger generalization; better consistency.\n",
    "* **Cons:** more complex; heavier training; careful stability engineering.\n",
    "\n",
    "**D. Flow- or correspondence-driven VO**\n",
    "\n",
    "* **Idea:** learn dense optical flow or correspondences; then recover pose via differentiable epipolar geometry, or train end-to-end.\n",
    "* **Pros:** good on dynamic scenes with robust matchers; integrates with cost volumes/transformers.\n",
    "* **Cons:** scale ambiguity (mono); need rigidity masks or scene flow.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Core training losses (self-supervised mono/stereo)\n",
    "\n",
    "Let $I_t$ target, $I_s$ source; predict $D_t$ and $T_{t\\rightarrow s}$. For pixel $p$ in $I_t$:\n",
    "\n",
    "1. **Back-project:** $\\mathbf{X} = D_t(p)\\,K^{-1}\\tilde{p}$\n",
    "2. **Transform:** $\\mathbf{X}_s = T_{t\\rightarrow s}\\,\\mathbf{X}$\n",
    "3. **Project:** $p' \\sim K\\,\\mathbf{X}_s$ → sample $\\hat{I}_t(p) = I_s(p')$\n",
    "\n",
    "**Photometric loss:**\n",
    "$\\mathcal{L}_{pho} = \\alpha \\frac{1 - \\mathrm{SSIM}(I_t,\\hat{I}_t)}{2} + (1-\\alpha)\\|I_t-\\hat{I}_t\\|_1$\n",
    "\n",
    "**Depth smoothness (edge-aware):**\n",
    "$\\mathcal{L}_{sm} = \\sum |\\partial_x D_t| e^{-|\\partial_x I_t|} + |\\partial_y D_t| e^{-|\\partial_y I_t|}$\n",
    "\n",
    "**Geometry consistency:**\n",
    "\n",
    "* Epipolar loss: $\\tilde{p}'^\\top F \\tilde{p} \\approx 0$ on learned correspondences.\n",
    "* Cycle/reprojection min over multiple sources to handle occlusions.\n",
    "* **Scale constraints:** stereo baseline, IMU priors, or learned scale head.\n",
    "\n",
    "**Rotation loss (geodesic) for supervised/regularized pose:**\n",
    "$\\ell_R(R,\\hat{R}) = \\|\\log(R^\\top\\hat{R})\\|_2$\n",
    "\n",
    "---\n",
    "\n",
    "###  Popular architectural building blocks\n",
    "\n",
    "* **Backbones:** ResNet/EfficientNet/MobileNet, or ViT/convnext-style hybrids.\n",
    "* **Temporal:** ConvLSTM/GRU; 1D temporal convs; attention over clips.\n",
    "* **Transformers:** for long-range associations, global cost volumes, memory (e.g., recurrent matching + pose/BA heads).\n",
    "* **Cost volumes:** stereo/monocular multi-hypothesis depth refinement.\n",
    "* **Differentiable solvers:** Gauss-Newton layers, differentiable PnP/ICP, learned robust weights (M-estimation).\n",
    "* **Uncertainty heads:** aleatoric/epistemic to weight residuals and poses.\n",
    "\n",
    "---\n",
    "\n",
    "###  Practical training & engineering tips\n",
    "\n",
    "* **Data curation:** varied motion/illumination; ensure exposure consistency or use brightness augmentation & learned photometric invariance.\n",
    "* **Non-rigidity handling:** auto-mask moving objects; min-reprojection over multiple sources; per-pixel uncertainty weighting.\n",
    "* **Scale:** prefer stereo or occasional depth supervision/IMU to anchor scale; else learn a scale head or post-scale with a known height/velocity prior.\n",
    "* **Drift control:** keyframes + temporal windows; small **differentiable BA** blocks every N frames; pose-graph fine-tuning at segment ends.\n",
    "* **Initialization:** identity or gyro-seeded initial pose; pyramids and coarse-to-fine warping reduce bad local minima.\n",
    "* **Numerics:** SE(3) parametrization via **Lie algebra** $\\boldsymbol{\\xi} \\in \\mathbb{R}^6$; compose poses with $\\exp(\\cdot)$ / $\\log(\\cdot)$; use geodesic rotation losses.\n",
    "* **Speed:** share encoders for depth/pose; mixed precision; tile-based warping; keep cost volumes shallow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5f333-c58c-4cd7-bffc-8d563a71e725",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6fefdc-33d5-4deb-be8d-29372d7df701",
   "metadata": {},
   "source": [
    "###  1.1.2 Quaternion Loss (Naïve Euclidean Loss)\n",
    "\n",
    "The simplest approach is to minimize the **L2 distance between quaternions**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{quat} = \\| q_\\text{pred} - q_\\text{gt} \\|_2\n",
    "$$\n",
    "\n",
    "But there are **two problems**:\n",
    "\n",
    "1. **Double cover**: $q$ and $-q$ represent the same rotation.\n",
    "   → If the network predicts $-q_\\text{gt}$, the loss will be **large**, even though the rotation is exactly correct.\n",
    "2. **Euclidean mismatch**: The Euclidean distance between quaternions does not exactly correspond to the **geodesic distance** (shortest path on SO(3)).\n",
    "\n",
    "**Fix for double cover:**\n",
    "\n",
    "Take the shorter path:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{quat} = 1 - |\\langle q_\\text{pred}, q_\\text{gt} \\rangle|\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot, \\cdot \\rangle$ is the quaternion dot product.\n",
    "This gives a loss proportional to the **cosine of half the rotation angle**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d351f-eaeb-44be-9348-de57f135aa82",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1.3 Geodesic Loss (Rotation-Angle Loss)\n",
    "\n",
    "The **geodesic distance** between two rotations $R_1, R_2 \\in SO(3)$ is the smallest rotation angle that aligns them.\n",
    "If $R = R_1^\\top R_2$, then:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1}\\!\\left(\\frac{\\text{trace}(R) - 1}{2}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "If $R_1, R_2$ are very close, then in $R = R_1^\\top R_2$, their transpose are perpendicular, mening $R=I$, which means the $\\text{trace(R)}=3$ therefor $\\frac{\\text{trace}(R_1^\\top R_2) - 1}{2}$ is $1$, and  $\\cos^{-1}(1)=0$ \n",
    "\n",
    "\n",
    "This is the true **shortest path distance on SO(3)** (a proper Riemannian metric).\n",
    "Loss is typically defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = \\theta = \\cos^{-1}\\!\\left(\\frac{\\text{trace}(R_1^\\top R_2) - 1}{2}\\right)\n",
    "$$\n",
    "\n",
    "If using quaternions, you can avoid matrices:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = 2 \\cos^{-1} \\!\\big( |\\langle q_\\text{pred}, q_\\text{gt} \\rangle| \\big)\n",
    "$$\n",
    "\n",
    "where again we take the absolute value to fix the double-cover issue.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc43f87-347c-470c-84d7-8659de60e83f",
   "metadata": {},
   "source": [
    "### 1.1.4 Geodesic Loss Numerical Example\n",
    "\n",
    "**Ground truth rotation**: 90° around z-axis\n",
    "\n",
    "  $$\n",
    "  q_\\text{gt} = \\left[\\cos(45°), 0, 0, \\sin(45°)\\right] = [0.7071, 0, 0, 0.7071]\n",
    "  $$\n",
    "\n",
    "**Predicted rotation**: 60° around z-axis\n",
    "\n",
    "  $$\n",
    "  q_\\text{pred} = \\left[\\cos(30°), 0, 0, \\sin(30°)\\right] = [0.8660, 0, 0, 0.5]\n",
    "  $$\n",
    "\n",
    "Both are already normalized.\n",
    "\n",
    "---\n",
    "\n",
    "**Compute Dot Product**\n",
    "\n",
    "$$\n",
    "\\langle q_\\text{pred}, q_\\text{gt} \\rangle\n",
    "= (0.8660)(0.7071) + (0)(0) + (0)(0) + (0.5)(0.7071)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.6124 + 0.3536 = 0.9660\n",
    "$$\n",
    "\n",
    "Take absolute value (to handle sign ambiguity):\n",
    "\n",
    "$$\n",
    "|\\langle q_\\text{pred}, q_\\text{gt} \\rangle| = 0.9660\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Compute Geodesic Loss (Angle)**\n",
    "\n",
    "Geodesic loss (in radians):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = 2 \\cos^{-1}(0.9660)\n",
    "$$\n",
    "\n",
    "Compute step-by-step:\n",
    "\n",
    "* $\\cos^{-1}(0.9660) ≈ 0.2618 \\, \\text{rad}$\n",
    "* Multiply by 2 → $\\mathcal{L}_\\text{geo} ≈ 0.5236 \\, \\text{rad}$\n",
    "\n",
    "Convert to degrees:\n",
    "\n",
    "$$\n",
    "0.5236 \\, \\text{rad} × \\frac{180°}{\\pi} ≈ 30°\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a007e1-12be-40b5-aae3-6ee61fcebd71",
   "metadata": {},
   "source": [
    "###  1.1.5 Practical Advice\n",
    "\n",
    "* **If you only care about small orientation errors** (e.g., fine-tuning a network near correct pose):\n",
    "  Quaternion dot-product loss or L2 loss is usually fine (cheaper, smooth gradients).\n",
    "\n",
    "* **If you care about accurate global orientation** (e.g., SLAM, pose estimation, camera relocalization):\n",
    "  **Geodesic loss is strongly preferred** because it reflects the real physical difference between two orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52cded-0a15-417c-ada0-0b896a2278b6",
   "metadata": {},
   "source": [
    "## 1.2 Full Transformation Loss $SE(3)$\n",
    "\n",
    "\n",
    "To make an **SE(3) loss** you typically combine:\n",
    "\n",
    "1. a **rotation term** that measures distance on SO(3) (your geodesic loss), and\n",
    "2. a **translation term** that measures distance in $\\mathbb{R}^3$.\n",
    "\n",
    "There are two common (and solid) ways to do this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a2081-777a-4297-932d-dc15f6d7c77e",
   "metadata": {},
   "source": [
    "### 1.2.1 Option A — Simple & Effective (weighted sum)\n",
    "\n",
    "Use the geodesic **rotation angle** (in radians) plus a norm on translation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SE(3)}} \\;=\\; \\lambda_R \\,\\underbrace{\\big(2\\arccos(|\\langle q_{\\text{pred}}, q_{\\text{gt}}\\rangle|)\\big)}_{\\text{SO(3) geodesic angle}}\n",
    "\\;+\\; \\lambda_t \\,\\underbrace{\\|\\,t_{\\text{pred}}-t_{\\text{gt}}\\,\\|_2}_{\\text{meters}}\n",
    "$$\n",
    "\n",
    "* $\\lambda_R$ and $\\lambda_t$ balance **units** (radians vs meters).\n",
    "  Rules of thumb:\n",
    "\n",
    "  * If typical translation errors are \\~0.05–0.2 m and angular errors are \\~2–10°, try $\\lambda_R\\in[0.5,2.0]$, $\\lambda_t\\in[1,10]$.\n",
    "  * Tune so both terms contribute similar magnitude early in training.\n",
    "* Often use **robust norms** (e.g., Huber) on translation.\n",
    "\n",
    "This is the **go-to baseline**: simple, stable, and strong in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b551f3c-3684-48be-af3f-e4b9962d3538",
   "metadata": {},
   "source": [
    "### 1.2.2 Option B — True SE(3) geodesic via Lie Log (advanced)\n",
    "\n",
    "Compute the **relative transform** $\\Delta T = T_{\\text{gt}}^{-1}T_{\\text{pred}}$, take the **matrix logarithm** to get a 6-vector $\\xi = [\\omega, v]\\in \\mathbb{R}^6$ (rotation/translation in the tangent space), then penalize it:\n",
    "\n",
    "$$\n",
    "\\xi \\;=\\; \\log(\\Delta T) \\;=\\; \n",
    "\\begin{bmatrix} \\omega \\\\ v \\end{bmatrix},\\quad\n",
    "\\mathcal{L} \\;=\\; \\|\\; W\\,\\xi \\;\\|_2\n",
    "\\quad\\text{or}\\quad\n",
    "\\mathcal{L} \\;=\\; \\|W_\\omega \\omega\\|_2 + \\|W_v v\\|_2.\n",
    "$$\n",
    "\n",
    "* Here $W$ (or $W_\\omega, W_v$) sets the relative weighting/units.\n",
    "* This treats rotation and translation **on the same manifold footing** and is invariant to **left/right multiplication** (choose consistently).\n",
    "* Slightly more math and careful numerics (small-angle handling).\n",
    "\n",
    "\n",
    "This version gives you a **true SE(3) tangent-space error**. Use if you want strict group-theoretic consistency (e.g., in pose-graph optimization or when composition/invariance properties matter).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222b334-e085-435d-8bb9-fbba03d51440",
   "metadata": {},
   "source": [
    "### 1.2.3 Numerical Example SE(3) Option-B\n",
    "\n",
    "\n",
    "We’ll compute\n",
    "$\\Delta T = T_{\\text{gt}}^{-1}T_{\\text{pred}}$, then $\\xi=\\log(\\Delta T)=[\\omega,\\,v]\\in\\mathbb{R}^6$, and a loss $\\|\\omega\\|_2+\\|v\\|_2$.\n",
    "\n",
    "---\n",
    "\n",
    "**Poses**\n",
    "\n",
    "* Ground truth: rotation **+90° about z**, translation $t_\\text{gt}=[1,\\,0,\\,0]$\n",
    "* Prediction: rotation **+60° about z**, translation $t_\\text{pred}=[1.2,\\,0.1,\\,0]$\n",
    "\n",
    "Rotation matrices:\n",
    "\n",
    "$$\n",
    "R_z(\\phi)=\\begin{bmatrix}\\cos\\phi & -\\sin\\phi & 0\\\\ \\sin\\phi & \\cos\\phi & 0\\\\ 0&0&1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_\\text{gt}=R_z(90^\\circ),\\quad\n",
    "R_\\text{pred}=R_z(60^\\circ)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Relative transform $\\Delta T$\n",
    "\n",
    "$$\n",
    "R_{\\text{rel}} = R_\\text{gt}^\\top R_\\text{pred}\n",
    "= \\begin{bmatrix}\n",
    "0.8660254 & 0.5 & 0\\\\\n",
    "-0.5 & 0.8660254 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad(\\text{a }-30^\\circ\\text{ rotation about }z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "t_{\\text{rel}} = R_\\text{gt}^\\top (t_\\text{pred}-t_\\text{gt})\n",
    "= \\begin{bmatrix}0.1\\\\ -0.2\\\\ 0\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### $\\log(\\Delta T)\\Rightarrow [\\omega,\\,v]$\n",
    "\n",
    "**SO(3) log (rotation):**\n",
    "\n",
    "* $\\theta=\\arccos\\big((\\mathrm{tr}(R_{\\text{rel}})-1)/2\\big)=\\arccos(0.8660254)=\\;0.5235988$ rad $=30^\\circ$\n",
    "* Axis $=\\ -\\hat z$, so\n",
    "\n",
    "$$\n",
    "\\omega = \\theta\\cdot(-\\hat z) = \\begin{bmatrix}0\\\\0\\\\-0.5235988\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**SE(3) translation log:**\n",
    "\n",
    "$$\n",
    "V = I + \\frac{1-\\cos\\theta}{\\theta^2}[\\omega]_\\times\n",
    "      + \\frac{\\theta-\\sin\\theta}{\\theta^3}[\\omega]_\\times^2,\n",
    "\\qquad v = V^{-1} t_{\\text{rel}}\n",
    "$$\n",
    "\n",
    "Numerically (for $\\theta=0.5236$ rad):\n",
    "\n",
    "$$\n",
    "V \\approx\n",
    "\\begin{bmatrix}\n",
    "0.95492966 & 0.25587263 & 0\\\\\n",
    "-0.25587263 & 0.95492966 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "v \\approx\n",
    "\\begin{bmatrix}\n",
    "0.1500647\\\\\n",
    "-0.1692298\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\xi=\\log(\\Delta T)=\n",
    "\\big[\\,\\omega;\\,v\\,\\big]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "-0.5235988\\\\\n",
    "0.1500647\\\\\n",
    "-0.1692298\\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Example loss\n",
    "\n",
    "With $\\mathcal L = \\|\\omega\\|_2 + \\|v\\|_2$:\n",
    "\n",
    "* $\\|\\omega\\|_2 = 0.5235988$ (30° in radians)\n",
    "* $\\|v\\|_2 \\approx \\sqrt{0.1500647^2 + (-0.1692298)^2} \\approx 0.2261817$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal L \\approx 0.5236 + 0.2262 = 0.7498}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f7044-0fa9-426c-8800-a01ef3f977a5",
   "metadata": {},
   "source": [
    "### 1.2.3 How to pick weights (very important)\n",
    "\n",
    "* Units differ: **radians vs meters**. You must balance them.\n",
    "* Three common strategies:\n",
    "\n",
    "  1. **Manual tuning** (start with $\\lambda_R=1, \\lambda_t\\in[1,10]$).\n",
    "  2. **Normalize by dataset scale** (e.g., divide translation by scene extent).\n",
    "  3. **Learned homoscedastic uncertainty** (Kendall & Cipolla):\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L} = \\frac{1}{2\\sigma_R^2} \\, \\mathcal{L}_R + \\frac{1}{2\\sigma_t^2}\\, \\mathcal{L}_t + \\log \\sigma_R + \\log \\sigma_t\n",
    "     $$\n",
    "\n",
    "     with $\\log \\sigma_R, \\log \\sigma_t$ as learnable scalars.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.4 Quick recommendations\n",
    "\n",
    "* Start with **Option A** (weighted sum, Huber on translation). It’s robust and easy to tune.\n",
    "* If you need **group-consistent** errors (e.g., enforcing trajectory smoothness with relative poses), use **Option B** (Lie log).\n",
    "* Always monitor **angle (deg)** and **translation (m)** separately as metrics, even if your loss is a combination.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dc364-1987-424c-b61f-29156e841050",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0926ecff-f001-445a-b4d2-3a4a1f142182",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca1119fd-9181-47c6-a440-89ab777687c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2057d02d-32d2-4d2f-9239-7cc3fca57e8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63cf38b0-8f8e-4853-bd5f-fc71b997f1e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c019d7-5e6e-456b-99a6-0d0a0587158e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec6af079-e1c2-4b47-a991-631cfe2afb4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec69c051-1557-4ffc-b665-c2ae0b4170b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8922ffd-8683-44e2-b112-689a9f89d734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d396f6c-1e6a-4aff-a212-c17f566c38aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6b81bb-ee87-471f-9bd2-e7f17991e325",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e41ed0d-20d6-4121-a983-68feb0c24655",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d626a9-7797-4af8-8828-5671926b2878",
   "metadata": {},
   "source": [
    "## Popular Monocular Depth Datasets\n",
    "\n",
    "| Dataset          | Description                        | License / Notes                    |\n",
    "| ---------------- | ---------------------------------- | ---------------------------------- |\n",
    "| **NYU Depth V2** | Indoor scenes, Kinect RGB-D images | ✔️ Standard for indoor depth       |\n",
    "| **KITTI**        | Outdoor driving scenes (LiDAR)     | ✔️ Standard for autonomous driving |\n",
    "| **Make3D**       | Outdoor stills (Stanford)          | Older, smaller                     |\n",
    "| **DIML/CVT**     | Outdoor depth from stereo          | Large and high-resolution          |\n",
    "| **TUM RGB-D**    | Indoor SLAM dataset                | ✔️ Camera + depth                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955963d-507f-4316-a676-c81074b4c07e",
   "metadata": {},
   "source": [
    "## DepthNet\n",
    "\n",
    "DepthNet usually refers to a **deep learning model that predicts per-pixel depth from an image (or image pair)**, and there are several variants of \"DepthNet\" depending on the paper or implementation you are looking at.\n",
    "Let’s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What DepthNet Does**\n",
    "\n",
    "DepthNet takes as input:\n",
    "\n",
    "* **Single image** (monocular depth estimation)\n",
    "* or **stereo pair** (left & right image)\n",
    "* or even **consecutive frames** (for self-supervised depth + ego-motion)\n",
    "\n",
    "and outputs:\n",
    "\n",
    "* A **dense depth map** – one depth value per pixel (usually inverse depth/disparity for numerical stability).\n",
    "\n",
    "So, it is a CNN that learns to \"understand\" the scene geometry and infer how far each pixel is from the camera.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Typical Architecture**\n",
    "\n",
    "DepthNet usually follows an **encoder-decoder (U-Net-like) architecture**:\n",
    "\n",
    "1. **Encoder (Feature Extractor):**\n",
    "\n",
    "   * Often a backbone CNN like ResNet-18/34/50, EfficientNet, etc.\n",
    "   * Extracts multi-scale features from the image.\n",
    "   * Captures semantics and context (helps to know if a region is road, wall, sky).\n",
    "\n",
    "2. **Decoder (Upsampling):**\n",
    "\n",
    "   * Series of up-convolution (transpose convolution) or interpolation layers.\n",
    "   * Skip connections from encoder layers help recover spatial details.\n",
    "   * Produces a per-pixel prediction of depth or disparity.\n",
    "\n",
    "3. **Output:**\n",
    "\n",
    "   * Final layer applies `sigmoid` (or `ReLU`) to constrain depth to a valid range.\n",
    "   * Sometimes outputs **multi-scale depth predictions** (coarse → fine).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Training Approaches**\n",
    "\n",
    "#### (A) **Supervised DepthNet**\n",
    "\n",
    "* Trained with ground-truth depth maps (e.g., KITTI LiDAR scans).\n",
    "* Loss function: L1/L2 loss between predicted depth and ground truth.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{depth} = \\frac{1}{N} \\sum_{i=1}^{N} \\| D_{pred}(i) - D_{gt}(i) \\|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### (B) **Self-Supervised (Monocular) DepthNet**\n",
    "\n",
    "If ground truth depth is not available, DepthNet is trained **unsupervised**:\n",
    "\n",
    "1. **DepthNet** predicts disparity (inverse depth).\n",
    "2. **PoseNet** predicts camera motion between consecutive frames.\n",
    "3. **View synthesis loss:** reconstruct one view from the other using predicted depth + pose via differentiable warping.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{photo} = \\| I_{t} - \\hat{I}_{t} \\|\n",
    "$$\n",
    "\n",
    "Additional regularizers:\n",
    "\n",
    "* **Smoothness Loss** (encourages locally smooth depth)\n",
    "* **Edge-aware Loss** (preserves object boundaries)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Why DepthNet Works**\n",
    "\n",
    "It learns geometric priors:\n",
    "\n",
    "* Parallel lines converge at vanishing points → infers depth.\n",
    "* Objects of known shape/size → learns perspective cues.\n",
    "* Motion parallax (in self-supervised mode) → deduces scene structure.\n",
    "\n",
    "Because CNNs can capture global context, DepthNet generalizes well beyond traditional stereo matching.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Example: Monodepth2 (Popular DepthNet)**\n",
    "\n",
    "* Encoder: ResNet-18/50\n",
    "* Decoder: U-Net-like\n",
    "* Multi-scale disparity prediction\n",
    "* Trained with photometric reprojection loss + smoothness loss\n",
    "\n",
    "Result: **Real-time monocular depth estimation** with good generalization.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
