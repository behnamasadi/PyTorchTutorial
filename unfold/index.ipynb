{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c535a6-38c8-4cd3-a6fe-c3679c80d742",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "In convolutional neural networks (CNNs), each convolution can be thought of as:\n",
    "\n",
    "* Taking **local patches** (receptive fields) from the input,\n",
    "* Flattening them,\n",
    "* Multiplying by the filter weights (matrix multiplication),\n",
    "* Then reshaping the result back to the output feature map.\n",
    "\n",
    "The operations **`torch.nn.Unfold`** and **`torch.nn.Fold`** allow you to explicitly perform those “patch extraction” and “reconstruction” steps.\n",
    "\n",
    "✅ In short:\n",
    "\n",
    "* `unfold` = **extract patches**\n",
    "* `fold` = **reconstruct image from patches**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `torch.nn.Unfold`\n",
    "\n",
    "### Concept\n",
    "\n",
    "`Unfold` takes a 4D tensor\n",
    "$$x \\in \\mathbb{R}^{(B, C, H, W)}$$\n",
    "and returns a 3D tensor of **flattened sliding local blocks**.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.arange(1, 17, dtype=torch.float32).view(1, 1, 4, 4)\n",
    "print(x)\n",
    "```\n",
    "\n",
    "```\n",
    "[[[[ 1,  2,  3,  4],\n",
    "   [ 5,  6,  7,  8],\n",
    "   [ 9, 10, 11, 12],\n",
    "   [13, 14, 15, 16]]]]\n",
    "```\n",
    "\n",
    "Let’s extract 2×2 patches with stride 2:\n",
    "\n",
    "```python\n",
    "unfold = nn.Unfold(kernel_size=(2, 2), stride=2)\n",
    "patches = unfold(x)\n",
    "print(patches.shape)\n",
    "print(patches)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "torch.Size([1, 4, 4])\n",
    "tensor([[[ 1.,  3.,  9., 11.],\n",
    "         [ 2.,  4., 10., 12.],\n",
    "         [ 5.,  7., 13., 15.],\n",
    "         [ 6.,  8., 14., 16.]]])\n",
    "```\n",
    "\n",
    "Let’s interpret this:\n",
    "\n",
    "* There are **4 patches** (since 4 windows fit in 4×4 with stride 2).\n",
    "* Each patch has **4 elements** (2×2 = 4).\n",
    "* The shape `[B, C×kernel_height×kernel_width, num_patches]` = `[1, 4, 4]`.\n",
    "\n",
    "If you transpose:\n",
    "\n",
    "```python\n",
    "patches_T = patches.squeeze(0).T\n",
    "print(patches_T)\n",
    "```\n",
    "\n",
    "you’ll see each row = flattened patch:\n",
    "\n",
    "```\n",
    "[[ 1,  2,  5,  6],\n",
    " [ 3,  4,  7,  8],\n",
    " [ 9, 10, 13, 14],\n",
    " [11, 12, 15, 16]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `torch.nn.Fold`\n",
    "\n",
    "### Concept\n",
    "\n",
    "`Fold` is the *inverse* of `Unfold`:\n",
    "It reconstructs the original image (or feature map) from patches.\n",
    "\n",
    "```python\n",
    "fold = nn.Fold(output_size=(4, 4), kernel_size=(2, 2), stride=2)\n",
    "reconstructed = fold(patches)\n",
    "print(reconstructed)\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "```\n",
    "tensor([[[[ 1.,  2.,  3.,  4.],\n",
    "          [ 5.,  6.,  7.,  8.],\n",
    "          [ 9., 10., 11., 12.],\n",
    "          [13., 14., 15., 16.]]]])\n",
    "```\n",
    "\n",
    "✅ Perfect reconstruction, since the patches don’t overlap.\n",
    "\n",
    "If there **is overlap** (e.g., stride < kernel_size), overlapping pixels will be **summed** during `fold`.\n",
    "\n",
    "To fix that, you can normalize by the overlap count (explained below).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Handling Overlaps (stride < kernel_size)\n",
    "\n",
    "When patches overlap, `Fold` adds contributions together.\n",
    "You can compute a **normalization mask** like this:\n",
    "\n",
    "```python\n",
    "ones = torch.ones_like(x)\n",
    "patches_ones = unfold(ones)\n",
    "reconstruction_mask = fold(patches_ones)\n",
    "reconstructed /= reconstruction_mask\n",
    "```\n",
    "\n",
    "This ensures that overlapping regions are averaged correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Matrix Multiplication View of Convolution\n",
    "\n",
    "CNN convolution can be written as matrix multiplication:\n",
    "\n",
    "$$\n",
    "Y = W \\times X_{\\text{unfolded}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* ( X_{\\text{unfolded}} ) = patches extracted by `Unfold` → shape `[B, C×k_h×k_w, L]`\n",
    "* ( W ) = filter weights → shape `[out_channels, C×k_h×k_w]`\n",
    "* Result ( Y ) = `[B, out_channels, L]` → reshaped back using `Fold`\n",
    "\n",
    "So, convolution is basically:\n",
    "\n",
    "```python\n",
    "Y = W @ unfold(X)\n",
    "```\n",
    "\n",
    "and then reshape.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Typical Use Cases\n",
    "\n",
    "✅ **ViT or Patch-based Models**\n",
    "`unfold` is used to create patch tokens:\n",
    "\n",
    "```python\n",
    "patches = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "patches = patches.contiguous().view(B, C, -1, patch_size*patch_size)\n",
    "```\n",
    "\n",
    "✅ **Image Reconstruction / Super-resolution**\n",
    "`fold` helps reassemble overlapping patches from the processed output.\n",
    "\n",
    "✅ **Implementing Custom Convolutions**\n",
    "You can explicitly compute convolution using `unfold`, matrix multiplication, and `fold`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary Table\n",
    "\n",
    "| Operation   | Purpose                  | Input Shape       | Output Shape      | Example              |\n",
    "| ----------- | ------------------------ | ----------------- | ----------------- | -------------------- |\n",
    "| `nn.Unfold` | Extract local patches    | `[B, C, H, W]`    | `[B, C*kH*kW, L]` | Feature extraction   |\n",
    "| `nn.Fold`   | Reconstruct from patches | `[B, C*kH*kW, L]` | `[B, C, H, W]`    | Image reconstruction |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show a **numerical example of `unfold` + weight multiplication + `fold` to simulate a 2D convolution manually** (so you see exactly how it matches PyTorch’s `nn.Conv2d`)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df890aeb-65fa-4f29-bffe-e32687b5d588",
   "metadata": {},
   "source": [
    "## 9. Convolution as Matrix Multiplication\n",
    "\n",
    "\n",
    "Instead of sliding a kernel over the image manually, we can:\n",
    "\n",
    "1. Use `Unfold` to extract all sliding **patches** from the input.\n",
    "2. **Flatten the kernel** into a row vector.\n",
    "3. Multiply: **(kernel matrix) × (unfolded input patches)**.\n",
    "\n",
    "This is much faster, and how many deep learning libraries implement convolutions behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337d3c0-0bfe-4b3a-ad00-19e78f782f1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "####  Input Image $ \\mathbf{X} \\in \\mathbb{R}^{3 \\times 3} $\n",
    "\n",
    "$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "####  Kernel $ \\mathbf{K} \\in \\mathbb{R}^{2 \\times 2} $\n",
    "\n",
    "$\n",
    "\\mathbf{K} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "We'll do a **valid convolution** (no padding), **stride 1**, and treat it as **cross-correlation** (no flipping of kernel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2902d4ec-e617-4ed0-a98e-ed468d9c71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input image: shape (1, 1, 3, 3) (batch, channels, height, width)\n",
    "img = torch.tensor([[[[1, 2, 3],\n",
    "                      [4, 5, 6],\n",
    "                      [7, 8, 9]]]], dtype=torch.float32)\n",
    "\n",
    "print(\"Image shape:\", img.shape)  # (N=1, C=1, H=3, W=3)\n",
    "\n",
    "# Kernel: shape (2, 2), no channel yet\n",
    "kernel = torch.tensor([[1, 0],\n",
    "                       [0, -1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a710-db8b-43da-92b9-a5f6d95e924b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "####  Step 1: Unfold (im2col)\n",
    "\n",
    "We extract all possible $ 2 \\times 2 $ patches from $ \\mathbf{X} $ and flatten each into a column vector:\n",
    "\n",
    "####  Sliding Patches:\n",
    "\n",
    "1. Patch at top-left:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\begin{bmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 5 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "2. Patch at top-middle:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\\\ 6 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "3. Patch at middle-left:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "4 & 5 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\begin{bmatrix} 4 \\\\ 5 \\\\ 7 \\\\ 8 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "4. Patch at middle-middle:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "8 & 9\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow \\begin{bmatrix} 5 \\\\ 6 \\\\ 8 \\\\ 9 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8f260-d282-4f5f-a5ac-03d75ab1f1cf",
   "metadata": {},
   "source": [
    "####  Step 2: Build Unfolded Matrix $ \\mathbf{X}_{\\text{unfold}} \\in \\mathbb{R}^{4 \\times 4} $\n",
    "\n",
    "Each column is a flattened patch:\n",
    "\n",
    "$\n",
    "\\mathbf{X}_{\\text{unfold}} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 4 & 5 \\\\\n",
    "2 & 3 & 5 & 6 \\\\\n",
    "4 & 5 & 7 & 8 \\\\\n",
    "5 & 6 & 8 & 9 \\\\\n",
    "\\end{bmatrix}^\\top =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 4 & 5 \\\\\n",
    "2 & 3 & 5 & 6 \\\\\n",
    "4 & 5 & 7 & 8 \\\\\n",
    "5 & 6 & 8 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d18ddeb-2b5f-4579-8602-7244489441a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfolded patches:\n",
      " tensor([[[1., 2., 4., 5.],\n",
      "         [2., 3., 5., 6.],\n",
      "         [4., 5., 7., 8.],\n",
      "         [5., 6., 8., 9.]]])\n"
     ]
    }
   ],
   "source": [
    "unfold = torch.nn.Unfold(kernel_size=(2, 2))  # no padding, stride=1\n",
    "patches = unfold(img)  # shape: (N, C*kH*kW, L), here (1, 4, 4)\n",
    "print(\"Unfolded patches:\\n\", patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b0156-a437-479a-a962-75e54d652eaf",
   "metadata": {},
   "source": [
    "####  Step 3: Flatten Kernel into Row Vector\n",
    "\n",
    "$\n",
    "\\mathbf{k}_{\\text{flat}} = \\begin{bmatrix} 1 & 0 & 0 & -1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591ce8c2-4a42-4441-ad99-99ad16f4d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "kernel_flat = kernel.view(1, -1)  # shape: (1, 4)\n",
    "print(kernel_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437595d-4d23-4a51-8c54-97226bed991a",
   "metadata": {},
   "source": [
    "#### Step 4: Matrix Multiplication\n",
    "\n",
    "We compute:\n",
    "\n",
    "$\\mathbf{y} = \\mathbf{k}_{\\text{flat}} \\cdot \\mathbf{X}_{\\text{unfold}}^\\top$\n",
    "\n",
    "$\\mathbf{y} = \n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 0 & -1 \n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 4 & 5 \\\\\n",
    "2 & 3 & 5 & 6 \\\\\n",
    "4 & 5 & 7 & 8 \\\\\n",
    "5 & 6 & 8 & 9 \\\\\n",
    "\\end{bmatrix}^\\top\n",
    "= \\begin{bmatrix}\n",
    "1 - 5, & 2 - 6, & 4 - 8, & 5 - 9\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-4 & -4 & -4 & -4\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea22163-1348-4ab2-8190-919d6563cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4., -4., -4., -4.]]])\n"
     ]
    }
   ],
   "source": [
    "out = kernel_flat @ patches  # shape: (1, 4)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b42612-2185-4a35-9ad6-36c223734be5",
   "metadata": {},
   "source": [
    "####  Step 5: Reshape Output\n",
    "\n",
    "Reshape to $ 2 \\times 2 $ (since original image is $ 3 \\times 3 $, kernel is $ 2 \\times 2 $, and we used stride 1):\n",
    "\n",
    "$\n",
    "\\mathbf{Y} = \\begin{bmatrix}\n",
    "-4 & -4 \\\\\n",
    "-4 & -4\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "####  Final Output\n",
    "\n",
    "$\\boxed{\n",
    "\\mathbf{Y} = \\begin{bmatrix}\n",
    "-4 & -4 \\\\\n",
    "-4 & -4\n",
    "\\end{bmatrix}}$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d442d2bc-abfb-42ae-bff6-861ad12fbac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image:\n",
      " tensor([[[[-4., -4.],\n",
      "          [-4., -4.]]]])\n"
     ]
    }
   ],
   "source": [
    "out_image = out.view(1, 1, 2, 2)\n",
    "print(\"Output image:\\n\", out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8d0c1-87dc-42dc-b50a-cc21c26cbd86",
   "metadata": {},
   "source": [
    "| Step            | Shape                         |\n",
    "|----------------|-------------------------------|\n",
    "| `x`            | (1, 1, 3, 3)                   |\n",
    "| `x_unfold`     | (1, 4, 4) → 4 values per patch |\n",
    "| `kernel_flat`  | (1, 4)                         |\n",
    "| `out`          | (1, 4)                         |\n",
    "| `out_image`    | (1, 1, 2, 2)                   |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5d215-433c-47ad-8758-ac5d0135e4b9",
   "metadata": {},
   "source": [
    "# **`torch.nn.Unfold`**\n",
    "\n",
    "`Unfold` (also called **im2col**) turns a 2D input image into a set of flattened sliding patches (columns), which makes convolution a **matrix multiplication**.\n",
    "\n",
    "\n",
    "In PyTorch, **you don't *need* to use `unfold`** when doing convolutions because PyTorch already provides highly optimized convolution operations via `torch.nn.Conv2d`, `F.conv2d`, etc. However, you might want to use `unfold` when you want to:\n",
    "\n",
    "Sometimes you want to do something **patch-wise**, like:\n",
    "- Applying attention over local windows (like in Swin Transformers).\n",
    "- Doing non-standard convolutions (e.g., dynamic or deformable convolutions).\n",
    "- Manually implementing **convolutional backpropagation** or other gradient tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7577a-8d39-4a20-abe6-7fc1cf541e35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Assume:\n",
    "- Input: `x` with shape **[N, C_in=3, H, W]**\n",
    "- Kernel: `w` with shape **[C_out, C_in=3, kH=3, kW=3]**\n",
    "- Stride: `s=1`\n",
    "- Padding: `p=0`\n",
    "- Dilation: `d=1`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74000f8-9214-47b2-8518-abadae68ce48",
   "metadata": {},
   "source": [
    "```python\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "out = conv(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e951659-302d-421d-b093-8e476508a358",
   "metadata": {},
   "source": [
    "\n",
    "**Shape math:**\n",
    "- Input: `[N, 3, H, W]`\n",
    "- Kernel: `[8, 3, 3, 3]` (8 filters)\n",
    "- Output: `[N, 8, H_out, W_out]` where:\n",
    "\n",
    "$\n",
    "H_{out} = \\left\\lfloor \\frac{H + 2p - d(kH - 1) - 1}{s} + 1 \\right\\rfloor = H - 2\n",
    "$\n",
    "$\n",
    "W_{out} = W - 2\n",
    "$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f8243-bb38-4af3-ad20-ec16b2054918",
   "metadata": {},
   "source": [
    "**Simulating Conv2d with `Unfold` and `Fold`**\n",
    "\n",
    "\n",
    "```python\n",
    "unfold = nn.Unfold(kernel_size=(3, 3), stride=1, padding=0)\n",
    "patches = unfold(x)  # [N, C_in * kH * kW, L], where L = H_out * W_out\n",
    "```\n",
    "\n",
    "**Resulting shape:**\n",
    "- `patches`: `[N, 3*3*3 = 27, H_out * W_out]`\n",
    "\n",
    "So you get all sliding `3x3` patches in flattened form.\n",
    "\n",
    "---\n",
    "\n",
    "**Multiply with weights**\n",
    "\n",
    "Let's say weights `w` have shape `[8, 3, 3, 3]`. We flatten each kernel:\n",
    "\n",
    "```python\n",
    "w_flat = w.view(8, -1)  # [8, 27]\n",
    "```\n",
    "\n",
    "We now do matrix multiplication for each image in the batch:\n",
    "\n",
    "```python\n",
    "# x_unfold: [N, 27, L]\n",
    "# w_flat.T: [27, 8]\n",
    "out_unfold = torch.einsum('nkl,ok->nol', patches, w_flat)  # [N, 8, L]\n",
    "```\n",
    "\n",
    "Or simply:\n",
    "```python\n",
    "out_unfold = w_flat @ patches  # [8, L] per image, batched\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Reshape back to image using `Fold`**\n",
    "\n",
    "You can reshape it back if needed:\n",
    "```python\n",
    "fold = nn.Fold(output_size=(H_out, W_out), kernel_size=(1,1))  # Since already computed\n",
    "out = out_unfold.view(N, 8, H_out, W_out)\n",
    "```\n",
    "\n",
    "However, usually `Fold` is used when you've done an operation **in patch space** and want to re-aggregate.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Operation | Shape | Equivalent |\n",
    "|----------|-------------------------|-----------|\n",
    "| `nn.Conv2d` | `[N, C_out, H_out, W_out]` | Implicit |\n",
    "| `Unfold(x)` | `[N, C_in * kH * kW, L]` | All patches as columns |\n",
    "| Weight flattening | `[C_out, C_in * kH * kW]` | Filters as rows |\n",
    "| Multiply | `[N, C_out, L]` | Output in flattened spatial domain |\n",
    "| Reshape | `[N, C_out, H_out, W_out]` | Final output |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
