{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a910e9-d7e4-4f5d-af72-5c1c029c6fda",
   "metadata": {},
   "source": [
    "# **Neural Architecture Search (NAS)**\n",
    "**Neural Architecture Search (NAS)** is a subfield of **AutoML (Automated Machine Learning)** that aims to **automatically design neural network architectures** rather than hand-crafting them.\n",
    "\n",
    "The idea is to let an algorithm *search* through a space of possible network designs to find one that performs best for a given task (e.g., image classification, detection, or segmentation).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "Traditionally, researchers manually design architectures like **ResNet**, **DenseNet**, or **Transformer** based on intuition and trial-and-error.\n",
    "However, there are many hyperparameters and design decisions:\n",
    "\n",
    "* Number of layers\n",
    "* Kernel sizes\n",
    "* Skip connections\n",
    "* Width/depth of the network\n",
    "* Type of blocks (conv, attention, etc.)\n",
    "\n",
    "This manual process is **time-consuming** and often **sub-optimal**.\n",
    "NAS automates this design process.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. General Workflow of NAS\n",
    "\n",
    "The process can be broken down into **three major components**:\n",
    "\n",
    "| Component               | Role                                                                                       |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| **Search Space**        | Defines *what* architectures can be explored (e.g., types of layers, connections, etc.)    |\n",
    "| **Search Strategy**     | Defines *how* architectures are sampled and improved (e.g., RL, evolution, gradient-based) |\n",
    "| **Evaluation Strategy** | Defines *how* to measure the performance (e.g., train model fully, or use proxy training)  |\n",
    "\n",
    "---\n",
    "\n",
    "### (a) **Search Space**\n",
    "\n",
    "This specifies the **building blocks** and **rules** for generating architectures.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Convolution types: 3×3, 5×5, depthwise conv, etc.\n",
    "* Skip connections allowed or not.\n",
    "* Number of channels, etc.\n",
    "\n",
    "Formally, NAS tries to find:\n",
    "$$\n",
    "\\arg\\min_{A \\in \\mathcal{A}} ; \\mathcal{L}_{val}(w^*(A), A)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "w^*(A) = \\arg\\min_{w} ; \\mathcal{L}_{train}(w, A)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $A$ = architecture from search space $\\mathcal{A}$\n",
    "* $w$ = its weights\n",
    "* $\\mathcal{L}_{train}$ = training loss\n",
    "* $\\mathcal{L}_{val}$ = validation loss\n",
    "\n",
    "So NAS must **optimize both architecture and its weights**.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **Search Strategy**\n",
    "\n",
    "How to explore the search space efficiently.\n",
    "\n",
    "#### Common Strategies:\n",
    "\n",
    "1. **Reinforcement Learning (RL) based NAS**\n",
    "\n",
    "   * A controller (e.g., RNN) generates architectures.\n",
    "   * Reward = validation accuracy.\n",
    "   * Example: **NASNet** (Zoph & Le, 2017).\n",
    "\n",
    "2. **Evolutionary Algorithms**\n",
    "\n",
    "   * Start with random architectures.\n",
    "   * Mutate and recombine top performers.\n",
    "   * Example: **AmoebaNet** (Real et al., 2019).\n",
    "\n",
    "3. **Gradient-Based (Differentiable NAS)**\n",
    "\n",
    "   * Represent architecture choices as continuous parameters.\n",
    "   * Optimize with gradient descent.\n",
    "   * Example: **DARTS (Differentiable Architecture Search)**.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) **Evaluation Strategy**\n",
    "\n",
    "Training each candidate model fully is **expensive**.\n",
    "So evaluation uses approximations:\n",
    "\n",
    "* **Early stopping:** Train a few epochs only.\n",
    "* **Weight sharing:** Train a “supernet” that contains all sub-architectures (e.g., **ENAS**).\n",
    "* **Performance prediction:** Use a small model to predict performance without full training.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key NAS Variants\n",
    "\n",
    "| Method                              | Type                            | Example Paper                    |\n",
    "| ----------------------------------- | ------------------------------- | -------------------------------- |\n",
    "| **RL-based**                        | Discrete search                 | NASNet (2017)                    |\n",
    "| **Evolutionary**                    | Discrete search                 | AmoebaNet (2019)                 |\n",
    "| **Differentiable (Gradient-based)** | Continuous relaxation           | DARTS (2018)                     |\n",
    "| **One-shot NAS**                    | Weight sharing                  | ENAS (2018), ProxylessNAS (2019) |\n",
    "| **Hardware-aware NAS**              | Adds latency/energy constraints | FBNet, MnasNet                   |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. DARTS Example (Differentiable NAS)\n",
    "\n",
    "In DARTS, instead of picking a single operation (like 3×3 conv or skip), we use a **weighted sum** of all possible ops:\n",
    "\n",
    "$$\n",
    "\\tilde{o}(x) = \\sum_{i} \\alpha_i , o_i(x)\n",
    "$$\n",
    "\n",
    "* $\\alpha_i$ are learnable architecture parameters (softmax normalized)\n",
    "* After training, the operation with the highest $\\alpha_i$ is chosen for the final architecture.\n",
    "\n",
    "This makes NAS **differentiable**, so we can train both network weights and architecture weights via backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Benefits and Challenges\n",
    "\n",
    "✅ **Pros**\n",
    "\n",
    "* Automates architecture design\n",
    "* Can outperform manually designed models\n",
    "* Can adapt to specific tasks and hardware constraints\n",
    "\n",
    "❌ **Cons**\n",
    "\n",
    "* Very computationally expensive (can require thousands of GPU-hours)\n",
    "* Search may overfit to validation data\n",
    "* Interpretability of found architectures is limited\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Modern Trends\n",
    "\n",
    "* **Efficiency**: Using weight sharing (ENAS, Once-for-All Networks)\n",
    "* **Task-specific NAS**: For segmentation, detection, NLP\n",
    "* **Multi-objective NAS**: Optimize accuracy + latency + energy\n",
    "* **Zero-cost NAS**: Predict performance without training at all\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Example in Practice\n",
    "\n",
    "1. Define a cell (micro-architecture):\n",
    "\n",
    "   * Combination of convolutions, pooling, skip connections.\n",
    "2. Use NAS algorithm (e.g., DARTS) to find the best combination.\n",
    "3. Stack the discovered cell into a full network.\n",
    "4. Train the final model from scratch.\n",
    "\n",
    "Example architectures discovered by NAS:\n",
    "\n",
    "* **NASNet**, **AmoebaNet**, **MnasNet**, **FBNet**, **EfficientNet**, **Once-for-All (OFA)**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91c3af-d4ea-4285-988d-6183428ef64e",
   "metadata": {},
   "source": [
    "## **Minimal PyTorch Differentiable NAS** \n",
    "\n",
    "\n",
    "Perfect — let’s build a **minimal differentiable NAS (DARTS-style)** example in PyTorch so you can see how both **network weights** and **architecture parameters** are optimized jointly.\n",
    "\n",
    "We’ll design a small “search cell” that chooses between a few candidate operations (e.g., conv3×3, conv5×5, and skip-connection), all weighted by learnable coefficients **α**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Core Idea\n",
    "\n",
    "For each edge (input → output), instead of picking one operation manually, we take a **soft mixture** of all candidate operations:\n",
    "\n",
    "$$\n",
    "\\tilde{o}(x) = \\sum_{i} \\frac{e^{\\alpha_i}}{\\sum_j e^{\\alpha_j}} , o_i(x)\n",
    "$$\n",
    "\n",
    "So the network learns both:\n",
    "\n",
    "* **Weights (W)** — as usual (Conv filters, etc.)\n",
    "* **Architecture parameters (α)** — which operation to prefer\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Minimal Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Candidate operations -----\n",
    "class Conv3x3(nn.Module):\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super().__init__()\n",
    "        self.op = nn.Conv2d(C_in, C_out, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.op(x))\n",
    "\n",
    "class Conv5x5(nn.Module):\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super().__init__()\n",
    "        self.op = nn.Conv2d(C_in, C_out, 5, padding=2)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.op(x))\n",
    "\n",
    "class SkipConnect(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# ----- Mixed operation (weighted sum of candidates) -----\n",
    "class MixedOp(nn.Module):\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super().__init__()\n",
    "        self.ops = nn.ModuleList([\n",
    "            Conv3x3(C_in, C_out),\n",
    "            Conv5x5(C_in, C_out),\n",
    "            SkipConnect()\n",
    "        ])\n",
    "        # α parameters (one per op) — initialized to zero\n",
    "        self.alpha = nn.Parameter(torch.zeros(len(self.ops)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = F.softmax(self.alpha, dim=0)\n",
    "        return sum(w * op(x) for w, op in zip(weights, self.ops))\n",
    "\n",
    "# ----- Small NAS network -----\n",
    "class NASNet(nn.Module):\n",
    "    def __init__(self, C=16, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(3, C, 3, padding=1)\n",
    "        self.mixed = MixedOp(C, C)\n",
    "        self.fc = nn.Linear(C*8*8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.stem(x))\n",
    "        x = F.avg_pool2d(self.mixed(x), 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ----- Simple toy training -----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net = NASNet().to(device)\n",
    "\n",
    "# Separate parameters: weights vs architecture\n",
    "arch_params = [p for n, p in net.named_parameters() if 'alpha' in n]\n",
    "weight_params = [p for n, p in net.named_parameters() if 'alpha' not in n]\n",
    "\n",
    "opt_weights = torch.optim.SGD(weight_params, lr=0.01)\n",
    "opt_alpha = torch.optim.Adam(arch_params, lr=0.003)\n",
    "\n",
    "# Fake data\n",
    "x = torch.randn(8, 3, 32, 32).to(device)\n",
    "y = torch.randint(0, 10, (8,)).to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    # --- Step 1: Update weights (train loss) ---\n",
    "    net.train()\n",
    "    opt_weights.zero_grad()\n",
    "    preds = net(x)\n",
    "    loss = F.cross_entropy(preds, y)\n",
    "    loss.backward()\n",
    "    opt_weights.step()\n",
    "\n",
    "    # --- Step 2: Update architecture (validation loss) ---\n",
    "    # (Here we just reuse same data for simplicity)\n",
    "    opt_alpha.zero_grad()\n",
    "    preds_val = net(x)\n",
    "    val_loss = F.cross_entropy(preds_val, y)\n",
    "    val_loss.backward()\n",
    "    opt_alpha.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss={loss.item():.3f} | val_loss={val_loss.item():.3f}\")\n",
    "    print(\"α:\", net.mixed.alpha.data.tolist())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What Happens Here\n",
    "\n",
    "* The **`MixedOp`** contains 3 candidate operations:\n",
    "\n",
    "  * 3×3 convolution\n",
    "  * 5×5 convolution\n",
    "  * Skip connection\n",
    "\n",
    "* The architecture parameters **α = [α₁, α₂, α₃]** are trained via gradient descent.\n",
    "\n",
    "* The **softmax(α)** gives the mixture weights for each operation.\n",
    "\n",
    "* During training, **network weights (W)** and **architecture parameters (α)** are updated alternately:\n",
    "\n",
    "  1. Update **W** using training loss.\n",
    "  2. Update **α** using validation loss (to simulate DARTS’ bilevel optimization).\n",
    "\n",
    "After training, the operation with the **highest α** becomes the chosen one for the final architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Typical Workflow in Real NAS\n",
    "\n",
    "| Step                       | Description                                                     |\n",
    "| -------------------------- | --------------------------------------------------------------- |\n",
    "| **1. Search phase**        | Train the supernet with both W and α parameters.                |\n",
    "| **2. Derive architecture** | Pick top operations (argmax α).                                 |\n",
    "| **3. Retrain phase**       | Train the derived architecture from scratch for final accuracy. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Expected Output Example\n",
    "\n",
    "```\n",
    "Epoch 1 | train_loss=2.21 | val_loss=2.09\n",
    "α: [0.05, 0.03, -0.02]\n",
    "Epoch 5 | train_loss=1.12 | val_loss=1.07\n",
    "α: [1.24, 0.87, -0.31]\n",
    "```\n",
    "\n",
    "After training, the model learns to prefer one op (for example α₁ → Conv3x3).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to extend this toy example to **two cells connected in sequence** (so you can see how DARTS builds a graph of multiple “mixed” edges like in real NAS)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
