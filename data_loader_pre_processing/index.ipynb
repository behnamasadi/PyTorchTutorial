{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization Vs Normalization\n",
    "\n",
    "## Normalization (min-max Normalization or Feature Scaling)\n",
    "Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale.\n",
    "\n",
    "$X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "\n",
    "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization (Z-Score Normalization)\n",
    "Scaling to normal distribution $\\mu=0$ and $\\sigma^2=1$\n",
    "\n",
    "$X_{standard}=\\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does **not** have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects\n",
    "\n",
    "In theory, regression is insensitive to standardization since any linear transformation of input data can be counteracted by adjusting model parameters.\n",
    "\n",
    "Despite the fact that in theroy standardization plays little role in regression, it is used in regression because of the followings:\n",
    "\n",
    "1) Standardization improves the numerical stability of your model\n",
    "\n",
    "2) Standardization may speed up the training process\n",
    "if different features have drastically different ranges, the learning rate is determined by the feature with the largest range. This leads to another advantage of standardization: speeds up the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows us to normalize our dataset using the standardization process we've just seen by passing in the mean and standard deviation values for each color channel to the Normalize() transform.\n",
    "\n",
    "torchvision.transforms.Normalize(\n",
    "      [meanOfChannel1, meanOfChannel2, meanOfChannel3] \n",
    "    , [stdOfChannel1, stdOfChannel2, stdOfChannel3] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0), [2](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/), [3](https://en.wikipedia.org/wiki/Correlation_and_dependence), [4](https://deeplizard.com/learn/video/lu7TCu7HeYc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Normalization\n",
    "In pytorch normalization means we transform our data such that aftrwards our data becomes : $\\mu=0, \\sigma^2=1$.\n",
    "If you read the data directly from pytorch, they are in range of [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR10_train_dataset.data.min():  0\n",
      "CIFAR10_train_dataset.data.max():  255\n",
      "mean of r, g, b channel: 125.306918046875 122.950394140625 113.86538318359375\n",
      "standard deviation  of r, g, b channel: 62.99321927813685 62.088707640014405 66.70489964063101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "train_transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "CIFAR10_train_dataset=torchvision.datasets.CIFAR10(root='../data',download=True,transform=train_transform,train=True)\n",
    "\n",
    "min_value=CIFAR10_train_dataset.data.min()\n",
    "max_value=CIFAR10_train_dataset.data.max()\n",
    "\n",
    "print(\"CIFAR10_train_dataset.data.min(): \",min_value)\n",
    "print(\"CIFAR10_train_dataset.data.max(): \",max_value)\n",
    "\n",
    "r_mean, g_mean, b_mean=CIFAR10_train_dataset.data.mean(axis=(0,1,2))\n",
    "r_std, g_std, b_std=CIFAR10_train_dataset.data.std(axis=(0,1,2))\n",
    "\n",
    "print(\"mean of r, g, b channel:\",r_mean, g_mean, b_mean)\n",
    "print(\"standard deviation  of r, g, b channel:\",r_std, g_std, b_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load the data with DataLoader without using any transformer they will be in the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.min():  tensor(0.)\n",
      "images.max():  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images.min(): \",images.min())\n",
    "print(\"images.max(): \", images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you want to load the input to your network in the form of normal distribution with $\\mu=0, \\sigma^2=1$\n",
    "you should compute the mean and std of your data in advance from dataset directly divide it by max value (since DataLoader will make it in the range of [0,1] ) and use it when loading data from DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Now data are in the form of normal distribution\n",
      "\n",
      "images.min():  tensor(-1.9892)\n",
      "images.max():  tensor(2.0430)\n",
      "shape of batch: batch_size x channel x row x column:  torch.Size([4, 3, 32, 32])\n",
      "shape of training dataset:  (50000, 32, 32, 3)\n",
      "size of images: row x column x channel:  (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "r_mean, g_mean, b_mean=[r_mean/max_value,  g_mean/max_value, b_mean/max_value]\n",
    "r_std, std_g, b_std=[r_std/max_value, g_std/max_value, b_std/max_value]\n",
    "\n",
    "train_transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize(\n",
    "                                                    (r_mean, g_mean, b_mean),\n",
    "                                                    (r_std, b_std, g_std)  ) ])\n",
    "\n",
    "CIFAR10_train_dataset=torchvision.datasets.CIFAR10(root='../data',download=True,transform=train_transform,train=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print('\\nNow data are in the form of normal distribution\\n')\n",
    "\n",
    "print(\"images.min(): \",images.min())\n",
    "print(\"images.max(): \", images.max())\n",
    "print(\"shape of batch: batch_size x channel x row x column: \",images.shape)\n",
    "print(\"shape of training dataset: \",CIFAR10_train_dataset.data.shape)\n",
    "print(\"size of images: row x column x channel: \",CIFAR10_train_dataset.data[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete source code: [1](index.py), [2](datasets_normalization_preprocessing), [3](custome_dataset.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Tensor Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
