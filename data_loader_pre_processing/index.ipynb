{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standardization vs. Normalization\n",
    "\n",
    "### 1.1 Normalization (Min-Max Scaling or Feature Scaling)\n",
    "Normalization rescales the feature values to a fixed range, usually $[0, 1]$. The formula is:\n",
    "\n",
    "$\n",
    "X_{\\text{norm}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$\n",
    "\n",
    "- **Purpose**: Useful when you want all features to contribute equally to the model, especially when they are on different scales.\n",
    "- **Assumptions**: Does **not** assume any particular distribution of the data.\n",
    "- **Sensitive to outliers**: Yes — since it relies on the minimum and maximum values, outliers can significantly affect the scaling.\n",
    "- **Use cases**: Algorithms that rely on distances or assume bounded input features, such as:\n",
    "  - K-Nearest Neighbors (KNN)\n",
    "  - Neural Networks (e.g., when using sigmoid/tanh activations)\n",
    "  - Principal Component Analysis (PCA), when interpretability is not affected by bounded scale\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Standardization (Z-score Normalization)\n",
    "Standardization transforms the data to have zero mean and unit variance. The formula is:\n",
    "\n",
    "$\n",
    "X_{\\text{standard}} = \\frac{X - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "- **Purpose**: Useful when features have different means and variances and you want to center them around 0.\n",
    "- **Assumptions**: Works well if the data is approximately normally distributed, but this is **not a strict requirement**.\n",
    "- **Sensitive to outliers**: Less than min-max normalization, but outliers still affect mean and standard deviation.\n",
    "- **Use cases**: Algorithms that assume data is centered or use covariance:\n",
    "  - Linear Regression\n",
    "  - Logistic Regression\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Principal Component Analysis (PCA) (for preserving variance direction)\n",
    "  - K-Means Clustering\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Aspect               | Normalization \\([0,1]\\)                  | Standardization ($\\mu=0$, $\\sigma=1$) |\n",
    "|----------------------|-------------------------------------------|--------------------------------------------|\n",
    "| Range                | Bounded $([0,1]$)                      | Unbounded                                 |\n",
    "| Sensitive to outliers | High                                    | Medium                                     |\n",
    "| Assumes normality    | No                                       | No (but benefits from it)                 |\n",
    "| Preserves outliers   | No                                       | Yes (to an extent)                        |\n",
    "| Use cases            | KNN, Neural Nets                         | SVM, Linear Models, PCA, K-Means          |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Transforms\n",
    "\n",
    "These transforms convert your raw images into tensors and prepare them for model input. They generally include:\n",
    "\n",
    "- **Resize:**  \n",
    "  Scales the image to a target size. This is useful for ensuring that all images in a batch have consistent dimensions.  \n",
    "  ```python\n",
    "  transforms.Resize((256, 256))\n",
    "  ```  \n",
    "  Alternatively, you might use:\n",
    "  ```python\n",
    "  transforms.Resize(256)  # Maintains aspect ratio if a single number is provided.\n",
    "  ```\n",
    "\n",
    "- **Center Crop / Random Crop:**  \n",
    "  When working with images that have been resized, cropping helps to focus on the central region (for validation/testing) or to introduce variability (for training).  \n",
    "  - **CenterCrop:** Used during evaluation to ensure a consistent crop.  \n",
    "    ```python\n",
    "    transforms.CenterCrop(224)\n",
    "    ```\n",
    "  - **RandomCrop:** Provides a random crop and is part of data augmentation during training.\n",
    "    ```python\n",
    "    transforms.RandomCrop(224)\n",
    "    ```\n",
    "\n",
    "- **ToTensor:**  \n",
    "  Converts a PIL image or NumPy array into a PyTorch tensor and scales pixel values to \\[0, 1\\].  \n",
    "  ```python\n",
    "  transforms.ToTensor()\n",
    "  ```\n",
    "\n",
    "- **Normalization:**  \n",
    "  Adjusts pixel values by subtracting the dataset’s mean and dividing by its standard deviation. This normalization is key for training stability, especially when working with pretrained models. For example, the ImageNet dataset uses:\n",
    "  ```python\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "  ```\n",
    "  Note that the mean and std should match the dataset your model was trained on or adjusted for your custom dataset.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **PyTorch**, the term **\"normalization\" often refers to standardization**, i.e., **Z-score normalization**.\n",
    "\n",
    "### Specifically:\n",
    "When using `torchvision.transforms.Normalize(mean, std)` in PyTorch, the transformation applied is:\n",
    "\n",
    "$\n",
    "X' = \\frac{X - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "This is exactly **Z-score standardization**, where:\n",
    "- `mean` = $\\mu$ (per channel)\n",
    "- `std` = $\\sigma$ (per channel)\n",
    "\n",
    "So even though it's **called \"Normalize\"**, it's actually **standardizing** the image pixel values, usually with:\n",
    "```python\n",
    "transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "```\n",
    "which scales image values from $[0, 1]$ to $[-1, 1]$, or:\n",
    "```python\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "```\n",
    "which is standardization using ImageNet statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you use `transforms.ToTensor()`, it **does not involve any normalization using mean or standard deviation**. It simply rescales the pixel values from `[0, 255]` to `[0.0, 1.0]` using:\n",
    "\n",
    "$\n",
    "X_{\\text{norm}} = \\frac{X}{255}\n",
    "$\n",
    "\n",
    "So your assumption is correct — it's not applying:\n",
    "\n",
    "$\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "To apply mean/std normalization (standardization), you would explicitly add `transforms.Normalize(mean, std)` to your transform pipeline, like this:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 mean\n",
    "                         std=[0.2023, 0.1994, 0.2010])   # CIFAR-10 std\n",
    "])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `transforms.ToTensor()` in PyTorch, it **automatically converts the image pixel values from the range [0, 255] (uint8)** to **floating-point values in the range [0.0, 1.0]**.\n",
    "\n",
    "So here:\n",
    "\n",
    "```python\n",
    "CIFAR10_train_dataset = datasets.CIFAR10(root='../data', train=True,\n",
    "                                         download=True, transform=transforms.ToTensor())\n",
    "```\n",
    "\n",
    "Each image in the dataset will be a `torch.Tensor` of shape `(3, 32, 32)` with values **in the range [0.0, 1.0]**.\n",
    "\n",
    "If you want to keep the original pixel values in the `[0, 255]` range (e.g., for visualization or custom processing), you should avoid `transforms.ToTensor()` and instead use a custom transform or read the raw image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the  $\\mu$ and $\\sigma$:\n",
    "If you need to calculate the  $\\mu$ and $\\sigma$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def calculate_mean_std(dataset,  batch_size=64, num_workers=2):\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images = 0\n",
    "\n",
    "    for images, lables in tqdm(loader, desc='Computing mean and std'):\n",
    "        print(\"images shape is: B, C, H, W\", images.shape)\n",
    "\n",
    "        # so you will get something like: images shape is: B, C, H, W torch.Size([64, 3, 224, 224]\n",
    "\n",
    "        images = images.view(images.size(0), images.size(1), -1)  # (B, C, H*W)\n",
    "\n",
    "        # since we want the mean over the image, we call images.mean(2), since \n",
    "        # images.mean(0) -> B\n",
    "        # images.mean(1) -> C\n",
    "\n",
    "        # now images.mean(2) is the mean of each channel for all batches so it is something like:\n",
    "        # [[0.0691, 0.0691, 0.0691],\n",
    "        # [0.1690, 0.1690, 0.1690],\n",
    "        # .\n",
    "        # .\n",
    "        # .\n",
    "        # [0.1031, 0.1031, 0.1031],\n",
    "        # [0.1088, 0.1088, 0.1088],\n",
    "\n",
    "        # so  images.mean(2).sum(0) will collapse the 0 dimention which rows, so you get sum along rows: \n",
    "        # images.mean(2).sum(0) -> tensor([7.3089, 7.3089, 7.3089])\n",
    "        \n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        \n",
    "        total_images += images.size(0)\n",
    "\n",
    "    # Here we have the sum of the avg of all images, so just divide by the total number of images\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "    return mean, std\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on **Windows you must wrap your DataLoader code inside a `if __name__ == '__main__':` block**, or you'll run into issues due to how multiprocessing works.\n",
    "\n",
    "Unlike Linux/macOS, Windows **does not fork processes** — it **spawns them**, which means the entire script is re-imported in each subprocess. If your `DataLoader` with `num_workers > 0` is not guarded properly, it can cause **recursive subprocess creation** or errors like:\n",
    "\n",
    "```\n",
    "RuntimeError: An attempt has been made to start a new process before the current process has finished its bootstrapping phase.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Here's how to fix it:\n",
    "in the file `data_utils.py`\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def calculate_mean_std(dataset, batch_size=64, num_workers=2):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in tqdm(loader, desc='Computing mean and std'):\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += images.size(0)\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "    return mean, std\n",
    "```\n",
    "\n",
    "This is totally fine because you’re **just defining the function** — it won’t be executed when the module is imported. Then, in your `train.py`, you do:\n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "from data_utils import calculate_mean_std\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "    mean, std = calculate_mean_std(dataset, num_workers=2)  # Safe here\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Std: {std}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Always put `DataLoader` usage inside `if __name__ == '__main__':` when `num_workers > 0` on Windows.\n",
    "- **Never create a `DataLoader` with `num_workers > 0` at the top level of a module if that module might be imported.**\n",
    "- For quick tests or small datasets, setting `num_workers=0` avoids this hassle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Augmentation Transforms\n",
    "\n",
    "Data augmentation increases the diversity of your training data and can help prevent overfitting. Common augmentation strategies include:\n",
    "\n",
    "- **Random Horizontal Flip:**  \n",
    "  Flips the image horizontally with a given probability (by default 0.5).  \n",
    "  ```python\n",
    "  transforms.RandomHorizontalFlip(p=0.5)\n",
    "  ```\n",
    "\n",
    "- **Random Vertical Flip:**  \n",
    "  Flips the image vertically. Use this judiciously – it can be useful for datasets where vertical orientation is not semantically important (e.g., some aerial or medical images).  \n",
    "  ```python\n",
    "  transforms.RandomVerticalFlip(p=0.5)\n",
    "  ```\n",
    "\n",
    "- **Random Rotation:**  \n",
    "  Rotates the image within a specified degree range.  \n",
    "  ```python\n",
    "  transforms.RandomRotation(degrees=15)\n",
    "  ```\n",
    "  A small degree range is often sufficient unless you know your objects can appear at wide angles.\n",
    "\n",
    "- **Random Resized Crop:**  \n",
    "  This transform combines random cropping with resizing. It randomly crops a portion of the image and then scales it to a target size. This is very popular for training on natural images.  \n",
    "  ```python\n",
    "  transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(0.75, 1.33))\n",
    "  ```\n",
    "  The scale parameter specifies the range of size of the cropped image relative to the original, and ratio controls the aspect ratio range.\n",
    "\n",
    "- **Color Jitter:**  \n",
    "  Adjusts brightness, contrast, saturation, and hue. This augmentation is helpful when lighting conditions vary.  \n",
    "  ```python\n",
    "  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "  ```\n",
    "\n",
    "- **Random Affine / Perspective:**  \n",
    "  - **RandomAffine:** Applies random translations, rotations, scaling, and shearing.  \n",
    "    ```python\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5)\n",
    "    ```\n",
    "  - **RandomPerspective:** Simulates perspective distortions.  \n",
    "    ```python\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "    ```\n",
    "\n",
    "- **Additional Transforms:**  \n",
    "  - **GaussianBlur:** For datasets where blurring might simulate realistic scenarios (especially in noisy environments).  \n",
    "    ```python\n",
    "    transforms.GaussianBlur(kernel_size=3)\n",
    "    ```\n",
    "  - **Random Erasing:** For occlusion augmentation, which randomly erases a portion of the image. This can help the network become robust to missing parts of an image.  \n",
    "    ```python\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Building the Transform Pipeline\n",
    "\n",
    "It is common practice to define separate pipelines for training and evaluation (validation/testing). Below are examples:\n",
    "\n",
    "### Training Transform Pipeline\n",
    "When building the training pipeline, the focus is on introducing variability:\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),                # Randomly crop and resize\n",
    "    transforms.RandomHorizontalFlip(),                # Random flip horizontally\n",
    "    transforms.RandomRotation(degrees=15),            # Random rotation\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # Vary colors\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.5)                   # Randomly erase parts of the image\n",
    "])\n",
    "```\n",
    "\n",
    "### Evaluation Transform Pipeline\n",
    "For evaluation, you want consistency and reproducibility:\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),                           # Resize images to a consistent scale\n",
    "    transforms.CenterCrop(224),                       # Crop the center of the image\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Choosing and Tuning Transforms\n",
    "\n",
    "- **Dataset Characteristics:**  \n",
    "  Not every augmentation is beneficial for every dataset. For example, if your dataset has objects with strict orientation (like digits or text), heavy rotations might hurt performance. Analyze your dataset to decide which augmentations are appropriate.\n",
    "\n",
    "- **Model Architecture:**  \n",
    "  Some models are sensitive to the scale or other characteristics of the input. Make sure your normalization values and the spatial dimensions match the pretrained model’s expected input.\n",
    "\n",
    "- **Experimentation:**  \n",
    "  It is common to experiment with a subset of these transforms and tune their parameters (e.g., rotation degrees, crop sizes, probability parameters) to see what optimally improves your validation performance.\n",
    "\n",
    "- **Data Augmentation Libraries:**  \n",
    "  While torchvision.transforms covers many use cases, for complex scenarios you might explore libraries like [albumentations](https://albumentations.ai/) which offer a more extensive suite of augmentation methods and more flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "A well-crafted transform pipeline can significantly improve your model’s ability to generalize while keeping training efficient. Experiment with these suggestions, and tailor them to your specific problem domain to achieve the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
