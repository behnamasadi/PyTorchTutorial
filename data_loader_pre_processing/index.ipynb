{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Transforms\n",
    "\n",
    "These transforms convert your raw images into tensors and prepare them for model input. They generally include:\n",
    "\n",
    "- **Resize:**  \n",
    "  Scales the image to a target size. This is useful for ensuring that all images in a batch have consistent dimensions.  \n",
    "  ```python\n",
    "  transforms.Resize((256, 256))\n",
    "  ```  \n",
    "  Alternatively, you might use:\n",
    "  ```python\n",
    "  transforms.Resize(256)  # Maintains aspect ratio if a single number is provided.\n",
    "  ```\n",
    "\n",
    "- **Center Crop / Random Crop:**  \n",
    "  When working with images that have been resized, cropping helps to focus on the central region (for validation/testing) or to introduce variability (for training).  \n",
    "  - **CenterCrop:** Used during evaluation to ensure a consistent crop.  \n",
    "    ```python\n",
    "    transforms.CenterCrop(224)\n",
    "    ```\n",
    "  - **RandomCrop:** Provides a random crop and is part of data augmentation during training.\n",
    "    ```python\n",
    "    transforms.RandomCrop(224)\n",
    "    ```\n",
    "\n",
    "- **ToTensor:**  \n",
    "  Converts a PIL image or NumPy array into a PyTorch tensor and scales pixel values to \\[0, 1\\].  \n",
    "  ```python\n",
    "  transforms.ToTensor()\n",
    "  ```\n",
    "\n",
    "- **Normalization:**  \n",
    "  Adjusts pixel values by subtracting the dataset’s mean and dividing by its standard deviation. This normalization is key for training stability, especially when working with pretrained models. For example, the ImageNet dataset uses:\n",
    "  ```python\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "  ```\n",
    "  Note that the mean and std should match the dataset your model was trained on or adjusted for your custom dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Augmentation Transforms\n",
    "\n",
    "Data augmentation increases the diversity of your training data and can help prevent overfitting. Common augmentation strategies include:\n",
    "\n",
    "- **Random Horizontal Flip:**  \n",
    "  Flips the image horizontally with a given probability (by default 0.5).  \n",
    "  ```python\n",
    "  transforms.RandomHorizontalFlip(p=0.5)\n",
    "  ```\n",
    "\n",
    "- **Random Vertical Flip:**  \n",
    "  Flips the image vertically. Use this judiciously – it can be useful for datasets where vertical orientation is not semantically important (e.g., some aerial or medical images).  \n",
    "  ```python\n",
    "  transforms.RandomVerticalFlip(p=0.5)\n",
    "  ```\n",
    "\n",
    "- **Random Rotation:**  \n",
    "  Rotates the image within a specified degree range.  \n",
    "  ```python\n",
    "  transforms.RandomRotation(degrees=15)\n",
    "  ```\n",
    "  A small degree range is often sufficient unless you know your objects can appear at wide angles.\n",
    "\n",
    "- **Random Resized Crop:**  \n",
    "  This transform combines random cropping with resizing. It randomly crops a portion of the image and then scales it to a target size. This is very popular for training on natural images.  \n",
    "  ```python\n",
    "  transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(0.75, 1.33))\n",
    "  ```\n",
    "  The scale parameter specifies the range of size of the cropped image relative to the original, and ratio controls the aspect ratio range.\n",
    "\n",
    "- **Color Jitter:**  \n",
    "  Adjusts brightness, contrast, saturation, and hue. This augmentation is helpful when lighting conditions vary.  \n",
    "  ```python\n",
    "  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "  ```\n",
    "\n",
    "- **Random Affine / Perspective:**  \n",
    "  - **RandomAffine:** Applies random translations, rotations, scaling, and shearing.  \n",
    "    ```python\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5)\n",
    "    ```\n",
    "  - **RandomPerspective:** Simulates perspective distortions.  \n",
    "    ```python\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5)\n",
    "    ```\n",
    "\n",
    "- **Additional Transforms:**  \n",
    "  - **GaussianBlur:** For datasets where blurring might simulate realistic scenarios (especially in noisy environments).  \n",
    "    ```python\n",
    "    transforms.GaussianBlur(kernel_size=3)\n",
    "    ```\n",
    "  - **Random Erasing:** For occlusion augmentation, which randomly erases a portion of the image. This can help the network become robust to missing parts of an image.  \n",
    "    ```python\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Building the Transform Pipeline\n",
    "\n",
    "It is common practice to define separate pipelines for training and evaluation (validation/testing). Below are examples:\n",
    "\n",
    "### Training Transform Pipeline\n",
    "When building the training pipeline, the focus is on introducing variability:\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),                # Randomly crop and resize\n",
    "    transforms.RandomHorizontalFlip(),                # Random flip horizontally\n",
    "    transforms.RandomRotation(degrees=15),            # Random rotation\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # Vary colors\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.5)                   # Randomly erase parts of the image\n",
    "])\n",
    "```\n",
    "\n",
    "### Evaluation Transform Pipeline\n",
    "For evaluation, you want consistency and reproducibility:\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),                           # Resize images to a consistent scale\n",
    "    transforms.CenterCrop(224),                       # Crop the center of the image\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Choosing and Tuning Transforms\n",
    "\n",
    "- **Dataset Characteristics:**  \n",
    "  Not every augmentation is beneficial for every dataset. For example, if your dataset has objects with strict orientation (like digits or text), heavy rotations might hurt performance. Analyze your dataset to decide which augmentations are appropriate.\n",
    "\n",
    "- **Model Architecture:**  \n",
    "  Some models are sensitive to the scale or other characteristics of the input. Make sure your normalization values and the spatial dimensions match the pretrained model’s expected input.\n",
    "\n",
    "- **Experimentation:**  \n",
    "  It is common to experiment with a subset of these transforms and tune their parameters (e.g., rotation degrees, crop sizes, probability parameters) to see what optimally improves your validation performance.\n",
    "\n",
    "- **Data Augmentation Libraries:**  \n",
    "  While torchvision.transforms covers many use cases, for complex scenarios you might explore libraries like [albumentations](https://albumentations.ai/) which offer a more extensive suite of augmentation methods and more flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "For deep learning image classification with PyTorch, a combination of the following is usually recommended:\n",
    "\n",
    "- **Normalization:** Essential for proper convergence.  \n",
    "- **Size Standardization (Resize/CenterCrop/RandomResizedCrop):** To ensure input size consistency.  \n",
    "- **Random Augmentation Techniques:** Such as horizontal (or vertical) flip, random crop, rotation, color jitter, and advanced techniques like Random Erasing, to improve model robustness.\n",
    "\n",
    "A well-crafted transform pipeline can significantly improve your model’s ability to generalize while keeping training efficient. Experiment with these suggestions, and tailor them to your specific problem domain to achieve the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization Vs Normalization\n",
    "\n",
    "## Normalization (min-max Normalization or Feature Scaling)\n",
    "Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale.\n",
    "\n",
    "$X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "\n",
    "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization (Z-Score Normalization)\n",
    "Scaling to normal distribution $\\mu=0$ and $\\sigma^2=1$\n",
    "\n",
    "$X_{standard}=\\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does **not** have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects\n",
    "\n",
    "In theory, regression is insensitive to standardization since any linear transformation of input data can be counteracted by adjusting model parameters.\n",
    "\n",
    "Despite the fact that in theroy standardization plays little role in regression, it is used in regression because of the followings:\n",
    "\n",
    "1) Standardization improves the numerical stability of your model\n",
    "\n",
    "2) Standardization may speed up the training process\n",
    "if different features have drastically different ranges, the learning rate is determined by the feature with the largest range. This leads to another advantage of standardization: speeds up the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows us to normalize our dataset using the standardization process we've just seen by passing in the mean and standard deviation values for each color channel to the Normalize() transform.\n",
    "\n",
    "torchvision.transforms.Normalize(\n",
    "      [meanOfChannel1, meanOfChannel2, meanOfChannel3] \n",
    "    , [stdOfChannel1, stdOfChannel2, stdOfChannel3] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0), [2](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/), [3](https://en.wikipedia.org/wiki/Correlation_and_dependence), [4](https://deeplizard.com/learn/video/lu7TCu7HeYc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Normalization\n",
    "In pytorch normalization means we transform our data such that aftrwards our data becomes : $\\mu=0, \\sigma^2=1$.\n",
    "If you read the data directly from pytorch, they are in range of [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR10_train_dataset.data.min():  0\n",
      "CIFAR10_train_dataset.data.max():  255\n",
      "mean of r, g, b channel: 125.306918046875 122.950394140625 113.86538318359375\n",
      "standard deviation  of r, g, b channel: 62.99321927813685 62.088707640014405 66.70489964063101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "train_transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "CIFAR10_train_dataset=torchvision.datasets.CIFAR10(root='../data',download=True,transform=train_transform,train=True)\n",
    "\n",
    "min_value=CIFAR10_train_dataset.data.min()\n",
    "max_value=CIFAR10_train_dataset.data.max()\n",
    "\n",
    "print(\"CIFAR10_train_dataset.data.min(): \",min_value)\n",
    "print(\"CIFAR10_train_dataset.data.max(): \",max_value)\n",
    "\n",
    "r_mean, g_mean, b_mean=CIFAR10_train_dataset.data.mean(axis=(0,1,2))\n",
    "r_std, g_std, b_std=CIFAR10_train_dataset.data.std(axis=(0,1,2))\n",
    "\n",
    "print(\"mean of r, g, b channel:\",r_mean, g_mean, b_mean)\n",
    "print(\"standard deviation  of r, g, b channel:\",r_std, g_std, b_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load the data with DataLoader without using any transformer they will be in the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.min():  tensor(0.)\n",
      "images.max():  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images.min(): \",images.min())\n",
    "print(\"images.max(): \", images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you want to load the input to your network in the form of normal distribution with $\\mu=0, \\sigma^2=1$\n",
    "you should compute the mean and std of your data in advance from dataset directly divide it by max value (since DataLoader will make it in the range of [0,1] ) and use it when loading data from DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Now data are in the form of normal distribution\n",
      "\n",
      "images.min():  tensor(-1.9892)\n",
      "images.max():  tensor(2.0430)\n",
      "shape of batch: batch_size x channel x row x column:  torch.Size([4, 3, 32, 32])\n",
      "shape of training dataset:  (50000, 32, 32, 3)\n",
      "size of images: row x column x channel:  (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "r_mean, g_mean, b_mean=[r_mean/max_value,  g_mean/max_value, b_mean/max_value]\n",
    "r_std, std_g, b_std=[r_std/max_value, g_std/max_value, b_std/max_value]\n",
    "\n",
    "train_transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize(\n",
    "                                                    (r_mean, g_mean, b_mean),\n",
    "                                                    (r_std, b_std, g_std)  ) ])\n",
    "\n",
    "CIFAR10_train_dataset=torchvision.datasets.CIFAR10(root='../data',download=True,transform=train_transform,train=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print('\\nNow data are in the form of normal distribution\\n')\n",
    "\n",
    "print(\"images.min(): \",images.min())\n",
    "print(\"images.max(): \", images.max())\n",
    "print(\"shape of batch: batch_size x channel x row x column: \",images.shape)\n",
    "print(\"shape of training dataset: \",CIFAR10_train_dataset.data.shape)\n",
    "print(\"size of images: row x column x channel: \",CIFAR10_train_dataset.data[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete source code: [1](index.py), [2](datasets_normalization_preprocessing), [3](custome_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
