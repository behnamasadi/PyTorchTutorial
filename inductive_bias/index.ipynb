{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcd5df3-ac50-4225-9b2c-a47b915e7aba",
   "metadata": {},
   "source": [
    "## **Inductive Bias**\n",
    "\n",
    "**Inductive bias** in deep learning refers to the set of assumptions that a model makes to **generalize** from the training data to unseen data.\n",
    "\n",
    "Since learning from data alone is impossible without *some* prior assumptions (as shown by the **No Free Lunch Theorem**), every learning algorithm has an inductive bias‚Äîwhether explicit or implicit.\n",
    "\n",
    "---\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "> Inductive bias is the **preference or assumptions** a learning algorithm uses to predict outputs for **inputs it hasn't seen** before.\n",
    "\n",
    "In deep learning, the model architecture, training setup, and optimization procedure all contribute to the inductive bias.\n",
    "\n",
    "---\n",
    "\n",
    "### Examples of Inductive Biases in Deep Learning\n",
    "\n",
    "| Bias Type                  | Description                                             | Example                                                                       |\n",
    "| -------------------------- | ------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **Locality**               | Nearby pixels or values are more likely to be related.  | CNNs assume local features matter (e.g. edges).                               |\n",
    "| **Translation Invariance** | A pattern is the same no matter where it appears.       | CNNs with weight sharing across spatial locations.                            |\n",
    "| **Compositionality**       | Complex structures are made from simpler parts.         | Transformers assume a sequence has hierarchical meaning (words ‚Üí sentences).  |\n",
    "| **Smoothness**             | Small changes in input lead to small changes in output. | Regularization and SGD promote smooth functions.                              |\n",
    "| **Hierarchy**              | Data can be represented at multiple levels.             | Deep networks learn hierarchical representations (low-level edges ‚Üí objects). |\n",
    "\n",
    "---\n",
    "\n",
    "###  Why It Matters\n",
    "\n",
    "* **Without inductive bias**, the model can fit the training data perfectly but fail to generalize.\n",
    "* **With the right bias**, the model can learn meaningful patterns, even from limited data.\n",
    "\n",
    "---\n",
    "\n",
    "###  Inductive Bias Sources in Deep Learning\n",
    "\n",
    "| Source             | Examples                                                                |\n",
    "| ------------------ | ----------------------------------------------------------------------- |\n",
    "| **Architecture**   | CNNs (spatial), RNNs (temporal), Transformers (attention over sequence) |\n",
    "| **Loss Function**  | Cross-entropy, MSE, contrastive loss                                    |\n",
    "| **Optimization**   | SGD tends to find flat minima (implicit bias)                           |\n",
    "| **Regularization** | Dropout, weight decay, data augmentation                                |\n",
    "| **Initialization** | Influences which minima are found                                       |\n",
    "| **Training Data**  | Augmentations and preprocessing encode assumptions                      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94016803-cc13-4dee-a892-77a55b24fc3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### MLPs: The ‚ÄúMinimal Bias‚Äù Baseline\n",
    "\n",
    "Multilayer Perceptrons have virtually **no built-in vision-related inductive bias**. They flatten the input (e.g. raw images), losing information about spatial relationships and operating without weight-sharing or locality.  MLPs don't encode any assumptions about structure in the input (like spatial locality).\n",
    "\n",
    "---\n",
    "\n",
    "### CNNs: Strong Vision-Centric Bias\n",
    "\n",
    "CNNs embed strong built-in assumptions ideal for vision tasks:\n",
    "\n",
    "* **Locality:** Filters operate on small regions (receptive fields) of the input.\n",
    "* **Weight sharing:** The same filters are applied across spatial locations.\n",
    "* **Translation equivariance:** Shifting the input image leads to a similarly shifted response in feature maps.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Vision Transformers (ViTs): A ‚ÄúPatch-Level‚Äù Bias\n",
    "\n",
    "ViTs offer a different kind of inductive bias:\n",
    "\n",
    "* **Weakened vision bias:** ViTs lack built-in translation equivariance‚Äîbut they do embed structure through **patching** and **parameter sharing** across patches .\n",
    "* **Emergent robustness and shape bias:** Empirical evidence shows Vision Transformers are often **more robust to corruption** (e.g., image noise, blur) and **exhibit higher shape bias** compared to classic CNNs‚Äîeven when having **fewer parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparative Summary\n",
    "\n",
    "| Architecture | Inductive Bias Style                         | Vision Strengths                                   | Limitations                                    |\n",
    "| ------------ | -------------------------------------------- | -------------------------------------------------- | ---------------------------------------------- |\n",
    "| **MLP**      | Minimal, no structure                        | Possible high performance at scale                 | Data-hungry; poor inductive bias               |\n",
    "| **CNN**      | Strong built-in bias (locality, translation) | Sample-efficient; structured inductive assumptions | Texture-bias; may struggle with corruption     |\n",
    "| **ViT**      | Patch-based; flexible attention              | More robust to corruptions; shape-aware            | Requires more data; less inherently structured |\n",
    "\n",
    "---\n",
    "\n",
    "### Why These Biases Matter in Practice\n",
    "\n",
    "* **CNNs** excel when training data is limited or structured‚Äîbias helps generalization.\n",
    "* **ViTs** shine in scenarios requiring robustness or flexibility with large datasets and compute.\n",
    "* **MLPs**, while the simplest, can approximate complex behaviors‚Äîbut only with enough scale and data augmentation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a22cf-6e12-4c52-b25b-3ae50beb2c4f",
   "metadata": {},
   "source": [
    "## Example: Detecting a Bicycle (Wheel + Handlebar far apart)\n",
    "\n",
    "Imagine a **32√ó32 image**, divided into **4√ó4 patches** (each patch = 8√ó8 pixels).\n",
    "We want to detect that the **wheel** (bottom left) and the **handlebar** (top right) belong to the same object.\n",
    "\n",
    "---\n",
    "\n",
    "### **CNN**\n",
    "\n",
    "* A **3√ó3 convolution kernel** looks at a small local patch.\n",
    "* To connect wheel (bottom-left corner) ‚Üí handlebar (top-right corner):\n",
    "\n",
    "  * The information must pass through **many layers** of convolutions + pooling.\n",
    "  * Each layer increases the receptive field slowly:\n",
    "\n",
    "    * 1st layer: sees 3√ó3 pixels\n",
    "    * 2nd layer: maybe 7√ó7 pixels\n",
    "    * After \\~5‚Äì6 layers, the receptive field finally covers the whole image.\n",
    "\n",
    "üëâ Problem: The CNN only *learns the relation between wheel & handlebar indirectly* through deep stacking. It‚Äôs biased toward local patterns (edges, corners, textures).\n",
    "\n",
    "---\n",
    "\n",
    "### **ViT**\n",
    "\n",
    "* Split the image into **16 patches** (4√ó4).\n",
    "* The first **self-attention layer** compares *every patch with every other patch*.\n",
    "* That means **wheel patch** can directly attend to the **handlebar patch**, even though they are far apart.\n",
    "* Attention weight example:\n",
    "\n",
    "  * Similarity between wheel patch and handlebar patch = **0.82** (high)\n",
    "  * Similarity between wheel patch and background patch = **0.05** (low)\n",
    "\n",
    "üëâ Result: ViT **immediately learns a global relationship**: \"wheel + handlebar = likely bicycle\" ‚Äî no need to wait for many layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Numerical Contrast\n",
    "\n",
    "* CNN: relation strength grows only after multiple layers, e.g.\n",
    "\n",
    "  * Layer 1 relation strength (wheel ‚Üí handlebar): \\~0.0\n",
    "  * Layer 3: \\~0.2\n",
    "  * Layer 6: \\~0.7\n",
    "* ViT:\n",
    "\n",
    "  * Layer 1 relation strength (wheel ‚Üí handlebar): \\~0.82 already.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Takeaway:**\n",
    "CNNs start local ‚Üí slowly become global.\n",
    "ViTs are **global from the start**, which makes them better at modeling objects whose parts are **spatially distant**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f114e1-1a87-4829-9796-7686a5d07690",
   "metadata": {},
   "source": [
    "[Example](https://x.com/zhaisf/status/1956689364408549522)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
