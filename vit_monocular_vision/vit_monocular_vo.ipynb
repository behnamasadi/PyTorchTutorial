{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9aebb7-14ee-4d75-9f88-b3c53fff58b8",
   "metadata": {},
   "source": [
    "# Overview of Approaches and Deep Learning Architecture For Visual Odometry\n",
    "\n",
    "###  Deep Learning VO: main families\n",
    "\n",
    "**A. Pose regression (end-to-end)**\n",
    "\n",
    "* **Idea:** CNN (or CNN+RNN) regresses $(\\mathbf{R},\\mathbf{t}) \\in SE(3)$ directly from stacked frames.\n",
    "* **Architectures:** PoseNet-style CNNs → ConvLSTM/GRU for temporal context.\n",
    "* **Losses:** supervised $\\ell_1/\\ell_2$ on translation, geodesic loss on rotation; or **self-supervised photometric** (see §3).\n",
    "* **Pros:** simple inference; can be fast.\n",
    "* **Cons:** scale drift (monocular), weak geometry priors, generalization risk.\n",
    "\n",
    "**B. Depth+Pose joint learning (self-supervised SfM-style)**\n",
    "\n",
    "* **Idea:** one net predicts **depth** $D_t$; another predicts **relative pose** $T_{t\\rightarrow s}$. Reproject source $\\mathbf{I}_s$ into target with $D_t$ and $T$; train by minimizing **photometric/SSIM** reconstruction.\n",
    "* **Architectures:** U-Net depth backbones; small PoseNet; sometimes **cost volumes** (stereo) or **transformers**.\n",
    "* **Extras:** auto-masking for non-rigid pixels, **explainability masks**, multi-scale supervision, **edge-aware smoothness** on depth.\n",
    "* **Pros:** no GT poses needed; geometry-aware; scales well with data.\n",
    "* **Cons:** moving objects/occlusions need handling; absolute scale ambiguous (mono).\n",
    "\n",
    "**C. Geometry-aware networks (differentiable optimization inside)**\n",
    "\n",
    "* **Idea:** embed **PnP/BA/ICP** as differentiable layers (Gauss-Newton blocks, differentiable bundle adjustment, learned Jacobians/weights).\n",
    "* **Examples vibe:** DeepV2D-like depth-pose iterative refinement, BA-Net-style layers, **DROID-SLAM-like** dense matching + iterative pose/structure updates.\n",
    "* **Pros:** better inductive bias; stronger generalization; better consistency.\n",
    "* **Cons:** more complex; heavier training; careful stability engineering.\n",
    "\n",
    "**D. Flow- or correspondence-driven VO**\n",
    "\n",
    "* **Idea:** learn dense optical flow or correspondences; then recover pose via differentiable epipolar geometry, or train end-to-end.\n",
    "* **Pros:** good on dynamic scenes with robust matchers; integrates with cost volumes/transformers.\n",
    "* **Cons:** scale ambiguity (mono); need rigidity masks or scene flow.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Core training losses (self-supervised mono/stereo)\n",
    "\n",
    "Let $I_t$ target, $I_s$ source; predict $D_t$ and $T_{t\\rightarrow s}$. For pixel $p$ in $I_t$:\n",
    "\n",
    "1. **Back-project:** $\\mathbf{X} = D_t(p)\\,K^{-1}\\tilde{p}$\n",
    "2. **Transform:** $\\mathbf{X}_s = T_{t\\rightarrow s}\\,\\mathbf{X}$\n",
    "3. **Project:** $p' \\sim K\\,\\mathbf{X}_s$ → sample $\\hat{I}_t(p) = I_s(p')$\n",
    "\n",
    "**Photometric loss:**\n",
    "$\\mathcal{L}_{pho} = \\alpha \\frac{1 - \\mathrm{SSIM}(I_t,\\hat{I}_t)}{2} + (1-\\alpha)\\|I_t-\\hat{I}_t\\|_1$\n",
    "\n",
    "**Depth smoothness (edge-aware):**\n",
    "$\\mathcal{L}_{sm} = \\sum |\\partial_x D_t| e^{-|\\partial_x I_t|} + |\\partial_y D_t| e^{-|\\partial_y I_t|}$\n",
    "\n",
    "**Geometry consistency:**\n",
    "\n",
    "* Epipolar loss: $\\tilde{p}'^\\top F \\tilde{p} \\approx 0$ on learned correspondences.\n",
    "* Cycle/reprojection min over multiple sources to handle occlusions.\n",
    "* **Scale constraints:** stereo baseline, IMU priors, or learned scale head.\n",
    "\n",
    "**Rotation loss (geodesic) for supervised/regularized pose:**\n",
    "$\\ell_R(R,\\hat{R}) = \\|\\log(R^\\top\\hat{R})\\|_2$\n",
    "\n",
    "---\n",
    "\n",
    "###  Popular architectural building blocks\n",
    "\n",
    "* **Backbones:** ResNet/EfficientNet/MobileNet, or ViT/convnext-style hybrids.\n",
    "* **Temporal:** ConvLSTM/GRU; 1D temporal convs; attention over clips.\n",
    "* **Transformers:** for long-range associations, global cost volumes, memory (e.g., recurrent matching + pose/BA heads).\n",
    "* **Cost volumes:** stereo/monocular multi-hypothesis depth refinement.\n",
    "* **Differentiable solvers:** Gauss-Newton layers, differentiable PnP/ICP, learned robust weights (M-estimation).\n",
    "* **Uncertainty heads:** aleatoric/epistemic to weight residuals and poses.\n",
    "\n",
    "---\n",
    "\n",
    "###  Practical training & engineering tips\n",
    "\n",
    "* **Data curation:** varied motion/illumination; ensure exposure consistency or use brightness augmentation & learned photometric invariance.\n",
    "* **Non-rigidity handling:** auto-mask moving objects; min-reprojection over multiple sources; per-pixel uncertainty weighting.\n",
    "* **Scale:** prefer stereo or occasional depth supervision/IMU to anchor scale; else learn a scale head or post-scale with a known height/velocity prior.\n",
    "* **Drift control:** keyframes + temporal windows; small **differentiable BA** blocks every N frames; pose-graph fine-tuning at segment ends.\n",
    "* **Initialization:** identity or gyro-seeded initial pose; pyramids and coarse-to-fine warping reduce bad local minima.\n",
    "* **Numerics:** SE(3) parametrization via **Lie algebra** $\\boldsymbol{\\xi} \\in \\mathbb{R}^6$; compose poses with $\\exp(\\cdot)$ / $\\log(\\cdot)$; use geodesic rotation losses.\n",
    "* **Speed:** share encoders for depth/pose; mixed precision; tile-based warping; keep cost volumes shallow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5f333-c58c-4cd7-bffc-8d563a71e725",
   "metadata": {},
   "source": [
    "# 1. Loss Functions\n",
    "\n",
    "When you train a network to predict **rotations**, you want a loss function that measures “how far apart” two rotations are.\n",
    "Rotations live on the **special orthogonal group** SO(3), which is not a flat Euclidean space — so we need to be careful.\n",
    "\n",
    "## 1.1 Rotation Loss $SO(3)$\n",
    "\n",
    "###  1.1.1 Rotation Representation \n",
    "\n",
    "* **Quaternions** (unit 4D vectors, $q \\in \\mathbb{R}^4, \\|q\\|=1$)\n",
    "* **Rotation matrices** ($R \\in SO(3)$, orthogonal 3×3 with det=+1)\n",
    "* **Axis-angle** ($\\theta, \\mathbf{u}$)\n",
    "\n",
    "For deep learning, **quaternions** are often used because:\n",
    "\n",
    "* They are continuous (no singularities like Euler angles).\n",
    "* They are compact (4 parameters).\n",
    "* Easy to normalize to unit norm after network output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6fefdc-33d5-4deb-be8d-29372d7df701",
   "metadata": {},
   "source": [
    "###  1.1.2 Quaternion Loss (Naïve Euclidean Loss)\n",
    "\n",
    "The simplest approach is to minimize the **L2 distance between quaternions**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{quat} = \\| q_\\text{pred} - q_\\text{gt} \\|_2\n",
    "$$\n",
    "\n",
    "But there are **two problems**:\n",
    "\n",
    "1. **Double cover**: $q$ and $-q$ represent the same rotation.\n",
    "   → If the network predicts $-q_\\text{gt}$, the loss will be **large**, even though the rotation is exactly correct.\n",
    "2. **Euclidean mismatch**: The Euclidean distance between quaternions does not exactly correspond to the **geodesic distance** (shortest path on SO(3)).\n",
    "\n",
    "**Fix for double cover:**\n",
    "\n",
    "Take the shorter path:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{quat} = 1 - |\\langle q_\\text{pred}, q_\\text{gt} \\rangle|\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot, \\cdot \\rangle$ is the quaternion dot product.\n",
    "This gives a loss proportional to the **cosine of half the rotation angle**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d351f-eaeb-44be-9348-de57f135aa82",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1.3 Geodesic Loss (Rotation-Angle Loss)\n",
    "\n",
    "The **geodesic distance** between two rotations $R_1, R_2 \\in SO(3)$ is the smallest rotation angle that aligns them.\n",
    "If $R = R_1^\\top R_2$, then:\n",
    "\n",
    "$$\n",
    "\\theta = \\cos^{-1}\\!\\left(\\frac{\\text{trace}(R) - 1}{2}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "If $R_1, R_2$ are very close, then in $R = R_1^\\top R_2$, their transpose are perpendicular, mening $R=I$, which means the $\\text{trace(R)}=3$ therefor $\\frac{\\text{trace}(R_1^\\top R_2) - 1}{2}$ is $1$, and  $\\cos^{-1}(1)=0$ \n",
    "\n",
    "\n",
    "This is the true **shortest path distance on SO(3)** (a proper Riemannian metric).\n",
    "Loss is typically defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = \\theta = \\cos^{-1}\\!\\left(\\frac{\\text{trace}(R_1^\\top R_2) - 1}{2}\\right)\n",
    "$$\n",
    "\n",
    "If using quaternions, you can avoid matrices:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = 2 \\cos^{-1} \\!\\big( |\\langle q_\\text{pred}, q_\\text{gt} \\rangle| \\big)\n",
    "$$\n",
    "\n",
    "where again we take the absolute value to fix the double-cover issue.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc43f87-347c-470c-84d7-8659de60e83f",
   "metadata": {},
   "source": [
    "### 1.1.4 Geodesic Loss Numerical Example\n",
    "\n",
    "**Ground truth rotation**: 90° around z-axis\n",
    "\n",
    "  $$\n",
    "  q_\\text{gt} = \\left[\\cos(45°), 0, 0, \\sin(45°)\\right] = [0.7071, 0, 0, 0.7071]\n",
    "  $$\n",
    "\n",
    "**Predicted rotation**: 60° around z-axis\n",
    "\n",
    "  $$\n",
    "  q_\\text{pred} = \\left[\\cos(30°), 0, 0, \\sin(30°)\\right] = [0.8660, 0, 0, 0.5]\n",
    "  $$\n",
    "\n",
    "Both are already normalized.\n",
    "\n",
    "---\n",
    "\n",
    "**Compute Dot Product**\n",
    "\n",
    "$$\n",
    "\\langle q_\\text{pred}, q_\\text{gt} \\rangle\n",
    "= (0.8660)(0.7071) + (0)(0) + (0)(0) + (0.5)(0.7071)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.6124 + 0.3536 = 0.9660\n",
    "$$\n",
    "\n",
    "Take absolute value (to handle sign ambiguity):\n",
    "\n",
    "$$\n",
    "|\\langle q_\\text{pred}, q_\\text{gt} \\rangle| = 0.9660\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Compute Geodesic Loss (Angle)**\n",
    "\n",
    "Geodesic loss (in radians):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{geo} = 2 \\cos^{-1}(0.9660)\n",
    "$$\n",
    "\n",
    "Compute step-by-step:\n",
    "\n",
    "* $\\cos^{-1}(0.9660) ≈ 0.2618 \\, \\text{rad}$\n",
    "* Multiply by 2 → $\\mathcal{L}_\\text{geo} ≈ 0.5236 \\, \\text{rad}$\n",
    "\n",
    "Convert to degrees:\n",
    "\n",
    "$$\n",
    "0.5236 \\, \\text{rad} × \\frac{180°}{\\pi} ≈ 30°\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a007e1-12be-40b5-aae3-6ee61fcebd71",
   "metadata": {},
   "source": [
    "###  1.1.5 Practical Advice\n",
    "\n",
    "* **If you only care about small orientation errors** (e.g., fine-tuning a network near correct pose):\n",
    "  Quaternion dot-product loss or L2 loss is usually fine (cheaper, smooth gradients).\n",
    "\n",
    "* **If you care about accurate global orientation** (e.g., SLAM, pose estimation, camera relocalization):\n",
    "  **Geodesic loss is strongly preferred** because it reflects the real physical difference between two orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52cded-0a15-417c-ada0-0b896a2278b6",
   "metadata": {},
   "source": [
    "## 1.2 Full Transformation Loss $SE(3)$\n",
    "\n",
    "\n",
    "To make an **SE(3) loss** you typically combine:\n",
    "\n",
    "1. a **rotation term** that measures distance on SO(3) (your geodesic loss), and\n",
    "2. a **translation term** that measures distance in $\\mathbb{R}^3$.\n",
    "\n",
    "There are two common (and solid) ways to do this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a2081-777a-4297-932d-dc15f6d7c77e",
   "metadata": {},
   "source": [
    "### 1.2.1 Option A — Simple & Effective (weighted sum)\n",
    "\n",
    "Use the geodesic **rotation angle** (in radians) plus a norm on translation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SE(3)}} \\;=\\; \\lambda_R \\,\\underbrace{\\big(2\\arccos(|\\langle q_{\\text{pred}}, q_{\\text{gt}}\\rangle|)\\big)}_{\\text{SO(3) geodesic angle}}\n",
    "\\;+\\; \\lambda_t \\,\\underbrace{\\|\\,t_{\\text{pred}}-t_{\\text{gt}}\\,\\|_2}_{\\text{meters}}\n",
    "$$\n",
    "\n",
    "* $\\lambda_R$ and $\\lambda_t$ balance **units** (radians vs meters).\n",
    "  Rules of thumb:\n",
    "\n",
    "  * If typical translation errors are \\~0.05–0.2 m and angular errors are \\~2–10°, try $\\lambda_R\\in[0.5,2.0]$, $\\lambda_t\\in[1,10]$.\n",
    "  * Tune so both terms contribute similar magnitude early in training.\n",
    "* Often use **robust norms** (e.g., Huber) on translation.\n",
    "\n",
    "This is the **go-to baseline**: simple, stable, and strong in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b551f3c-3684-48be-af3f-e4b9962d3538",
   "metadata": {},
   "source": [
    "### 1.2.2 Option B — True SE(3) geodesic via Lie Log (advanced)\n",
    "\n",
    "Compute the **relative transform** $\\Delta T = T_{\\text{gt}}^{-1}T_{\\text{pred}}$, take the **matrix logarithm** to get a 6-vector $\\xi = [\\omega, v]\\in \\mathbb{R}^6$ (rotation/translation in the tangent space), then penalize it:\n",
    "\n",
    "$$\n",
    "\\xi \\;=\\; \\log(\\Delta T) \\;=\\; \n",
    "\\begin{bmatrix} \\omega \\\\ v \\end{bmatrix},\\quad\n",
    "\\mathcal{L} \\;=\\; \\|\\; W\\,\\xi \\;\\|_2\n",
    "\\quad\\text{or}\\quad\n",
    "\\mathcal{L} \\;=\\; \\|W_\\omega \\omega\\|_2 + \\|W_v v\\|_2.\n",
    "$$\n",
    "\n",
    "* Here $W$ (or $W_\\omega, W_v$) sets the relative weighting/units.\n",
    "* This treats rotation and translation **on the same manifold footing** and is invariant to **left/right multiplication** (choose consistently).\n",
    "* Slightly more math and careful numerics (small-angle handling).\n",
    "\n",
    "\n",
    "This version gives you a **true SE(3) tangent-space error**. Use if you want strict group-theoretic consistency (e.g., in pose-graph optimization or when composition/invariance properties matter).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222b334-e085-435d-8bb9-fbba03d51440",
   "metadata": {},
   "source": [
    "### 1.2.3 Numerical Example SE(3) Option-B\n",
    "\n",
    "\n",
    "We’ll compute\n",
    "$\\Delta T = T_{\\text{gt}}^{-1}T_{\\text{pred}}$, then $\\xi=\\log(\\Delta T)=[\\omega,\\,v]\\in\\mathbb{R}^6$, and a loss $\\|\\omega\\|_2+\\|v\\|_2$.\n",
    "\n",
    "---\n",
    "\n",
    "**Poses**\n",
    "\n",
    "* Ground truth: rotation **+90° about z**, translation $t_\\text{gt}=[1,\\,0,\\,0]$\n",
    "* Prediction: rotation **+60° about z**, translation $t_\\text{pred}=[1.2,\\,0.1,\\,0]$\n",
    "\n",
    "Rotation matrices:\n",
    "\n",
    "$$\n",
    "R_z(\\phi)=\\begin{bmatrix}\\cos\\phi & -\\sin\\phi & 0\\\\ \\sin\\phi & \\cos\\phi & 0\\\\ 0&0&1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_\\text{gt}=R_z(90^\\circ),\\quad\n",
    "R_\\text{pred}=R_z(60^\\circ)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Relative transform $\\Delta T$\n",
    "\n",
    "$$\n",
    "R_{\\text{rel}} = R_\\text{gt}^\\top R_\\text{pred}\n",
    "= \\begin{bmatrix}\n",
    "0.8660254 & 0.5 & 0\\\\\n",
    "-0.5 & 0.8660254 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad(\\text{a }-30^\\circ\\text{ rotation about }z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "t_{\\text{rel}} = R_\\text{gt}^\\top (t_\\text{pred}-t_\\text{gt})\n",
    "= \\begin{bmatrix}0.1\\\\ -0.2\\\\ 0\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### $\\log(\\Delta T)\\Rightarrow [\\omega,\\,v]$\n",
    "\n",
    "**SO(3) log (rotation):**\n",
    "\n",
    "* $\\theta=\\arccos\\big((\\mathrm{tr}(R_{\\text{rel}})-1)/2\\big)=\\arccos(0.8660254)=\\;0.5235988$ rad $=30^\\circ$\n",
    "* Axis $=\\ -\\hat z$, so\n",
    "\n",
    "$$\n",
    "\\omega = \\theta\\cdot(-\\hat z) = \\begin{bmatrix}0\\\\0\\\\-0.5235988\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**SE(3) translation log:**\n",
    "\n",
    "$$\n",
    "V = I + \\frac{1-\\cos\\theta}{\\theta^2}[\\omega]_\\times\n",
    "      + \\frac{\\theta-\\sin\\theta}{\\theta^3}[\\omega]_\\times^2,\n",
    "\\qquad v = V^{-1} t_{\\text{rel}}\n",
    "$$\n",
    "\n",
    "Numerically (for $\\theta=0.5236$ rad):\n",
    "\n",
    "$$\n",
    "V \\approx\n",
    "\\begin{bmatrix}\n",
    "0.95492966 & 0.25587263 & 0\\\\\n",
    "-0.25587263 & 0.95492966 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "v \\approx\n",
    "\\begin{bmatrix}\n",
    "0.1500647\\\\\n",
    "-0.1692298\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\xi=\\log(\\Delta T)=\n",
    "\\big[\\,\\omega;\\,v\\,\\big]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "-0.5235988\\\\\n",
    "0.1500647\\\\\n",
    "-0.1692298\\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Example loss\n",
    "\n",
    "With $\\mathcal L = \\|\\omega\\|_2 + \\|v\\|_2$:\n",
    "\n",
    "* $\\|\\omega\\|_2 = 0.5235988$ (30° in radians)\n",
    "* $\\|v\\|_2 \\approx \\sqrt{0.1500647^2 + (-0.1692298)^2} \\approx 0.2261817$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal L \\approx 0.5236 + 0.2262 = 0.7498}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f7044-0fa9-426c-8800-a01ef3f977a5",
   "metadata": {},
   "source": [
    "### 1.2.3 How to pick weights (very important)\n",
    "\n",
    "* Units differ: **radians vs meters**. You must balance them.\n",
    "* Three common strategies:\n",
    "\n",
    "  1. **Manual tuning** (start with $\\lambda_R=1, \\lambda_t\\in[1,10]$).\n",
    "  2. **Normalize by dataset scale** (e.g., divide translation by scene extent).\n",
    "  3. **Learned homoscedastic uncertainty** (Kendall & Cipolla):\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L} = \\frac{1}{2\\sigma_R^2} \\, \\mathcal{L}_R + \\frac{1}{2\\sigma_t^2}\\, \\mathcal{L}_t + \\log \\sigma_R + \\log \\sigma_t\n",
    "     $$\n",
    "\n",
    "     with $\\log \\sigma_R, \\log \\sigma_t$ as learnable scalars.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2.4 Quick recommendations\n",
    "\n",
    "* Start with **Option A** (weighted sum, Huber on translation). It’s robust and easy to tune.\n",
    "* If you need **group-consistent** errors (e.g., enforcing trajectory smoothness with relative poses), use **Option B** (Lie log).\n",
    "* Always monitor **angle (deg)** and **translation (m)** separately as metrics, even if your loss is a combination.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dc364-1987-424c-b61f-29156e841050",
   "metadata": {},
   "source": [
    "## 1.3 Photometric Loss\n",
    "\n",
    "Photometric loss is commonly used in **unsupervised visual odometry** or **monocular depth estimation**.\n",
    "\n",
    "* You have a **target image** $I_t$ and a **reference image** $I_r$.\n",
    "* Using predicted **depth** $D_t$ and **camera motion** ($R, \\mathbf{t}$), you warp $I_r$ into the target frame, producing a **reconstructed image** $\\hat{I}_t$.\n",
    "* The **photometric loss** compares $I_t$ and $\\hat{I}_t$: if they are visually similar, the loss is small — meaning your depth and motion predictions are consistent.\n",
    "\n",
    "The most common version is **pixel-wise L1 loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{photo}} = \\frac{1}{N} \\sum_{i=1}^N \\big| I_t(i) - \\hat{I}_t(i) \\big|\n",
    "$$\n",
    "\n",
    "where $N$ is the number of valid pixels.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.1 Numerical Example (Pixel-wise Photometric Loss)\n",
    "\n",
    "**Target image $I_t$:**\n",
    "\n",
    "$$\n",
    "I_t =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.2 & 0.4 & 0.6 \\\\\n",
    "0.3 & 0.5 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Reconstructed image $\\hat{I}_t$:**\n",
    "\n",
    "$$\n",
    "\\hat{I}_t =\n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.1 & 0.4 \\\\\n",
    "0.3 & 0.3 & 0.5 \\\\\n",
    "0.4 & 0.4 & 0.8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Pixel-wise absolute differences:\n",
    "\n",
    "$$\n",
    "|I_t - \\hat{I}_t| =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.1 & 0.1 \\\\\n",
    "0.1 & 0.1 & 0.1 \\\\\n",
    "0.1 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Mean over all $N=9$ pixels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{photo}} = \\frac{0.9}{9} = 0.1\n",
    "$$\n",
    "\n",
    "A lower value means the reconstruction is closer to the target.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.2 Pose Representation (Rigid-Body Motion)\n",
    "\n",
    "In unsupervised VO, the network predicts the **rigid-body motion** between two frames as an SE(3) transform:\n",
    "\n",
    "$$\n",
    "T_{t \\rightarrow r} =\n",
    "\\begin{bmatrix}\n",
    "R & \\mathbf{t} \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* $R$: $3 \\times 3$ rotation matrix, usually parameterized by a **quaternion**\n",
    "* $\\mathbf{t}$: $3 \\times 1$ translation vector\n",
    "\n",
    "---\n",
    "\n",
    "**Example Transformation**\n",
    "\n",
    "* **Quaternion:** $q = (w=0.9239, x=0, y=0, z=0.3827)$ → 45° rotation about Z-axis.\n",
    "* **Translation:** $\\mathbf{t} = [1, 0, 0]^T$\n",
    "\n",
    "Rotation matrix from quaternion:\n",
    "\n",
    "$$\n",
    "R =\n",
    "\\begin{bmatrix}\n",
    "\\cos 45° & -\\sin 45° & 0 \\\\\n",
    "\\sin 45° & \\cos 45° & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.707 & -0.707 & 0 \\\\\n",
    "0.707 & 0.707 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Full transform:\n",
    "\n",
    "$$\n",
    "T_{t \\rightarrow r} =\n",
    "\\begin{bmatrix}\n",
    "0.707 & -0.707 & 0 & 1 \\\\\n",
    "0.707 & 0.707 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.3 Back-Project Pixel to 3D (Target Frame)\n",
    "\n",
    "Given intrinsics\n",
    "\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "100 & 0 & 50 \\\\\n",
    "0 & 100 & 50 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "K^{-1} =\n",
    "\\begin{bmatrix}\n",
    "0.01 & 0 & -0.5 \\\\\n",
    "0 & 0.01 & -0.5 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Take target pixel $(u_t,v_t)=(60,40)$ with predicted depth $D_t=2.0$:\n",
    "\n",
    "$$\n",
    "K^{-1} \\begin{bmatrix} 60 \\\\ 40 \\\\ 1 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ -0.1 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "\\quad\\Rightarrow\\quad\n",
    "P_t = D_t \\cdot\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ -0.1 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\ -0.2 \\\\ 2.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is the **3D point in target frame**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.4 Transform Point to Reference Frame\n",
    "\n",
    "Apply $P_r = R P_t + \\mathbf{t}$:\n",
    "\n",
    "1. Rotate:\n",
    "\n",
    "$$\n",
    "R P_t =\n",
    "\\begin{bmatrix}\n",
    "0.28284 \\\\ 0 \\\\ 2.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Translate:\n",
    "\n",
    "$$\n",
    "P_r =\n",
    "\\begin{bmatrix}\n",
    "1.28284 \\\\ 0 \\\\ 2.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.5 Project Back to Reference Image\n",
    "\n",
    "Project using intrinsics:\n",
    "\n",
    "$$\n",
    "u_r = 100 \\cdot \\frac{1.28284}{2.0} + 50 = 114.14,\n",
    "\\quad\n",
    "v_r = 100 \\cdot 0 + 50 = 50\n",
    "$$\n",
    "\n",
    "Resulting pixel in $I_r$: $(u_r,v_r)=(114.14, 50.0)$\n",
    "\n",
    "If the image is $100 \\times 100$, this lies **out of bounds** → we **mask it out** (does not contribute to loss).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.6 Sanity Check (No Translation)\n",
    "\n",
    "If $\\mathbf{t}=0$:\n",
    "\n",
    "$$\n",
    "P_r=(0.28284, 0, 2.0) \\quad\\Rightarrow\\quad\n",
    "u_r=64.14, \\; v_r=50\n",
    "$$\n",
    "\n",
    "Now the pixel is **inside the image** → it would contribute to the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.7 Scale Ambiguity in Monocular VO\n",
    "\n",
    "If we **halve depth and translation**:\n",
    "\n",
    "* $P_t'=(0.1,-0.1,1.0)$\n",
    "* $\\mathbf{t}'=[0.5,0,0]^T$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "P_r'=(0.64142, 0, 1.0)\n",
    "\\quad\\Rightarrow\\quad\n",
    "u_r'=114.14, v_r'=50\n",
    "$$\n",
    "\n",
    "The projected pixel is **identical** → photometric loss cannot recover absolute scale, only **relative geometry**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3.8 Training Loop Summary\n",
    "\n",
    "1. Predict depth $D_t$ and pose $(R,\\mathbf{t})$.\n",
    "2. Back-project pixels → 3D points $P_t$.\n",
    "3. Transform → reference frame $P_r$.\n",
    "4. Project back to image plane → $(u_r,v_r)$.\n",
    "5. Sample $I_r$ at these coordinates → reconstruct $\\hat{I}_t$.\n",
    "6. Compute photometric loss (L1 or L1+SSIM) over valid pixels.\n",
    "7. Backprop through projection, sampling, and networks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf38b0-8f8e-4853-bd5f-fc71b997f1e4",
   "metadata": {},
   "source": [
    "\n",
    "##  Convention Used in Most Papers (e.g. SfMLearner, Monodepth2)\n",
    "\n",
    "| **Frame**                    | **Role**                                                          |\n",
    "| ---------------------------- | ----------------------------------------------------------------- |\n",
    "| $I_i$                        | **Target frame** → you predict depth $D_i$ for this frame         |\n",
    "| $I_{i+1}$ (and/or $I_{i-1}$) | **Reference frame(s)** → you warp them into frame $i$’s viewpoint |\n",
    "\n",
    "---\n",
    "\n",
    "##  What Happens Step by Step\n",
    "\n",
    "1. **Depth Prediction:**\n",
    "   $D_i = \\text{DepthNet}(I_i)$\n",
    "   → per-pixel depth map **for frame $i$**.\n",
    "\n",
    "2. **Pose Prediction:**\n",
    "   $(R, \\mathbf{t}) = \\text{PoseNet}(I_i, I_{i+1})$\n",
    "   → relative transform $T_{i \\rightarrow i+1}$ (from frame $i$ to frame $i+1$).\n",
    "\n",
    "3. **Back-Project:**\n",
    "   Use $D_i$ and camera intrinsics $K$ to get 3D points $P_i$ in frame $i$.\n",
    "\n",
    "4. **Transform:**\n",
    "   Move points to frame $i+1$:\n",
    "   $P_{i+1} = T_{i \\rightarrow i+1} \\cdot P_i$.\n",
    "\n",
    "5. **Project:**\n",
    "   Project $P_{i+1}$ to 2D using intrinsics $K$ → get pixel coords $(u_{i+1}, v_{i+1})$.\n",
    "\n",
    "6. **Sample:**\n",
    "   Bilinear sample $I_{i+1}$ at these coords → reconstructed image $\\hat{I}_i$.\n",
    "\n",
    "7. **Loss:**\n",
    "   Compare $I_i$ (target) and $\\hat{I}_i$:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{photo}} = \\frac{1}{N}\\sum |I_i - \\hat{I}_i|\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "##  Intuition\n",
    "\n",
    "* **Target frame:** $I_i$ — you are trying to reproduce this image.\n",
    "* **Reference frame:** $I_{i+1}$ — you are \"borrowing\" its pixels, warping them into $i$’s viewpoint.\n",
    "* If reconstruction is good, $I_i \\approx \\hat{I}_i$.\n",
    "\n",
    "---\n",
    "\n",
    "##  You Can Also Swap\n",
    "\n",
    "You can just as well make $I_{i+1}$ the target and $I_i$ the reference — as long as you're consistent.\n",
    "But by convention:\n",
    "\n",
    "* **Depth is always predicted for the target frame.**\n",
    "* **Reference frames are the ones you warp into the target’s viewpoint.**\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR (Answer to Your Question)\n",
    "\n",
    "**With two consecutive frames $I_i, I_{i+1}$:**\n",
    "\n",
    "* **Target frame:** $I_i$ (depth $D_i$ is predicted for this one)\n",
    "* **Reference frame:** $I_{i+1}$ (warped into frame $i$’s viewpoint to create $\\hat{I}_i$)\n",
    "* **Loss computed between:** $I_i$ and $\\hat{I}_i$\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to draw a small diagram (camera frustums for $i$ and $i+1$, showing how a 3D point projects to each, and how we warp $I_{i+1}$ to reconstruct $I_i$)? It usually makes this concept stick immediately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c019d7-5e6e-456b-99a6-0d0a0587158e",
   "metadata": {},
   "source": [
    "## 1.4 Structural Similarity Index Measure (SSIM)\n",
    "\n",
    "\n",
    "The **Structural Similarity Index Measure (SSIM)** is a widely used metric for measuring the similarity between two images. Unlike simple metrics such as Mean Squared Error (MSE) or Peak Signal-to-Noise Ratio (PSNR), SSIM is designed to model the way humans perceive image quality — focusing on structural information, contrast, and luminance rather than raw pixel differences.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.1 The Idea Behind SSIM\n",
    "\n",
    "SSIM tries to answer: *“How similar are two images in terms of structure, contrast, and brightness?”*\n",
    "\n",
    "It decomposes similarity into **three components**:\n",
    "\n",
    "1. **Luminance similarity** $l(x, y)$:\n",
    "\n",
    "   Are the two images equally bright on average?\n",
    "\n",
    "   $$\n",
    "   l(x,y) = \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}\n",
    "   $$\n",
    "\n",
    "- If both images have similar brightness, this term is near 1.\n",
    "- If one image is much darker, it will drop below 1.   \n",
    "\n",
    "2. **Contrast similarity** $c(x, y)$:\n",
    "\n",
    "   Do the two images have the same amount of contrast?\n",
    "\n",
    "   $$\n",
    "   c(x,y) = \\frac{2 \\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}\n",
    "   $$\n",
    "\n",
    "- If both images have similar contrast (variability), this is near 1.\n",
    "- If one is flat (low contrast) and the other is textured (high contrast), the similarity decreases.\n",
    "\n",
    "3. **Structural similarity** $s(x, y)$:\n",
    "\n",
    "   Do the two images have the same patterns and textures?\n",
    "\n",
    "   $$\n",
    "   s(x,y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "   $$\n",
    "- If $x$ and $y$ rise and fall together (high correlation), this is near $1$.\n",
    "- If they are uncorrelated or inverted (noise, wrong edges), this value becomes smaller or even negative.   \n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\mu_x, \\mu_y$ are the mean intensities,\n",
    "* $\\sigma_x, \\sigma_y$ are standard deviations,\n",
    "* $\\sigma_{xy}$ is covariance between $x$ and $y$,\n",
    "* $C_1, C_2, C_3$ are small constants to stabilize division.\n",
    "\n",
    "The final SSIM is:\n",
    "\n",
    "$$\n",
    "SSIM(x, y) = [l(x, y)]^\\alpha \\cdot [c(x, y)]^\\beta \\cdot [s(x, y)]^\\gamma\n",
    "$$\n",
    "\n",
    "Usually $\\alpha = \\beta = \\gamma = 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6af079-e1c2-4b47-a991-631cfe2afb4d",
   "metadata": {},
   "source": [
    "### 1.4.2 Normalized Correlation\n",
    "The **normalized correlation** part of SSIM is the most “structural” component, so it’s worth understanding carefully.\n",
    "\n",
    "**Step 1: Represent the Images**\n",
    "\n",
    "Suppose you have two image patches $x$ and $y$ of size $N$ pixels (can be grayscale or single channel).\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, \\dots, x_N], \\quad y = [y_1, y_2, \\dots, y_N]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Compute the Mean**\n",
    "\n",
    "Compute the mean intensity (average brightness) of each patch:\n",
    "\n",
    "$$\n",
    "\\mu_x = \\frac{1}{N} \\sum_{i=1}^{N} x_i, \\qquad\n",
    "\\mu_y = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Compute the Standard Deviations**\n",
    "\n",
    "Compute how much pixel values vary around the mean:\n",
    "\n",
    "$$\n",
    "\\sigma_x = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu_x)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_y = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\mu_y)^2}\n",
    "$$\n",
    "\n",
    "These represent the **contrast** of each image.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Compute the Covariance**\n",
    "\n",
    "Covariance measures how much the two patches vary *together*:\n",
    "\n",
    "$$\n",
    "\\sigma_{xy} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu_x)(y_i - \\mu_y)\n",
    "$$\n",
    "\n",
    "* If $x$ and $y$ increase/decrease together → covariance is **positive**.\n",
    "* If one increases when the other decreases → covariance is **negative**.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 5: Normalize → Get the Correlation**\n",
    "\n",
    "The **Pearson correlation coefficient** is just the covariance normalized by the product of standard deviations:\n",
    "\n",
    "$$\n",
    "\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "* This value ranges between **-1 and 1**.\n",
    "\n",
    "  * $1.0 \\Rightarrow$ perfect positive linear correlation (structures align perfectly).\n",
    "  * $0 \\Rightarrow$ no linear correlation (structures unrelated).\n",
    "  * $-1.0 \\Rightarrow$ perfect negative correlation (inverted contrast).\n",
    "\n",
    "---\n",
    "\n",
    "**Step 6: Add Stabilization for SSIM**\n",
    "\n",
    "In SSIM, to avoid division by zero when contrast is very low, we use a small constant $C_3$:\n",
    "\n",
    "$$\n",
    "s(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "$$\n",
    "\n",
    "This keeps the measure well-defined even for very flat patches (e.g., almost uniform gray).\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "* **Covariance** tells you whether pixel intensities move together.\n",
    "* **Normalization** by $\\sigma_x \\sigma_y$ removes the effect of scale/contrast so you focus purely on **structure** (edges, textures, gradients).\n",
    "\n",
    "This is why SSIM can still give a high similarity score if one image is slightly brighter/darker — because the **pattern** of variations is the same.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69c051-1557-4ffc-b665-c2ae0b4170b0",
   "metadata": {},
   "source": [
    "SSIM tries to mimic the **human visual system** by:\n",
    "\n",
    "* **Normalizing for lighting** (so small brightness changes are ignored).\n",
    "* **Normalizing for contrast** (so small contrast changes are less penalized).\n",
    "* **Measuring structure** (so it cares about edges, patterns, textures).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8922ffd-8683-44e2-b112-689a9f89d734",
   "metadata": {},
   "source": [
    "### 1.4.3 Numerical SSIM Example\n",
    "Awesome—let’s do a fully worked **numerical SSIM example** with two $3\\times3$ grayscale image patches and compute every piece: mean, std, covariance, normalized correlation, the three SSIM terms (luminance/contrast/structure), and the final SSIM.\n",
    "\n",
    "**Images (grayscale, 8-bit scale assumed)**\n",
    "\n",
    "$$\n",
    "x=\\begin{bmatrix}\n",
    "10&20&30\\\\\n",
    "20&30&40\\\\\n",
    "30&40&50\n",
    "\\end{bmatrix},\\quad\n",
    "y=\\begin{bmatrix}\n",
    "12&22&32\\\\\n",
    "21&31&41\\\\\n",
    "29&39&49\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We’ll treat the whole $3\\times3$ window as one patch (i.e., a single SSIM window).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) Basic statistics\n",
    "\n",
    "Let $N=9$.\n",
    "\n",
    "**Means**\n",
    "\n",
    "$$\n",
    "\\mu_x=30.0000,\\qquad \\mu_y=30.6667\n",
    "$$\n",
    "\n",
    "**Sample standard deviations** (ddof=1)\n",
    "\n",
    "$$\n",
    "\\sigma_x=\\sqrt{150}=12.2474,\\qquad \\sigma_y=\\sqrt{129.25}=11.3688\n",
    "$$\n",
    "\n",
    "**Sample covariance**\n",
    "\n",
    "$$\n",
    "\\sigma_{xy}=\\frac{1}{N-1}\\sum (x_i-\\mu_x)(y_i-\\mu_y)=138.75\n",
    "$$\n",
    "\n",
    "**Normalized correlation (Pearson $\\rho$)**\n",
    "\n",
    "$$\n",
    "\\rho=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}=\\frac{138.75}{12.2474\\cdot 11.3688}\\approx 0.9965\n",
    "$$\n",
    "\n",
    "> Intuition: the patches vary together almost perfectly (very strong structural agreement).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) SSIM components\n",
    "\n",
    "Use the standard SSIM constants for 8-bit images:\n",
    "\n",
    "$$\n",
    "L=255,\\quad C_1=(0.01L)^2=6.5025,\\quad C_2=(0.03L)^2=58.5225,\\quad C_3=\\frac{C_2}{2}=29.26125\n",
    "$$\n",
    "\n",
    "#### (a) Luminance term $l(x,y)$\n",
    "\n",
    "$$\n",
    "l=\\frac{2\\mu_x\\mu_y+C_1}{\\mu_x^2+\\mu_y^2+C_1}\n",
    "=\\frac{2\\cdot 30\\cdot 30.6667+6.5025}{30^2+30.6667^2+6.5025}\n",
    "\\approx 0.99976\n",
    "$$\n",
    "\n",
    "#### (b) Contrast term $c(x,y)$\n",
    "\n",
    "$$\n",
    "c=\\frac{2\\sigma_x\\sigma_y+C_2}{\\sigma_x^2+\\sigma_y^2+C_2}\n",
    "=\\frac{2\\cdot 12.2474\\cdot 11.3688+58.5225}{12.2474^2+11.3688^2+58.5225}\n",
    "\\approx 0.99771\n",
    "$$\n",
    "\n",
    "#### (c) Structure term $s(x,y)$\n",
    "\n",
    "$$\n",
    "s=\\frac{\\sigma_{xy}+C_3}{\\sigma_x\\sigma_y+C_3}\n",
    "=\\frac{138.75+29.26125}{12.2474\\cdot 11.3688+29.26125}\n",
    "\\approx 0.99710\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Final SSIM\n",
    "\n",
    "$$\n",
    "SSIM=l\\cdot c\\cdot s\\approx 0.99976\\cdot 0.99771\\cdot 0.99710\\approx \\mathbf{0.99458}\n",
    "$$\n",
    "\n",
    "**Takeaway:** Despite small brightness/contrast differences, the structures match extremely well (high $\\rho$ and high SSIM ≈ **0.995**). This is exactly the kind of case where SSIM (and the structure term) shines compared to plain MSE/PSNR.\n",
    "\n",
    "If you want, I can also compute **MSE/PSNR** for the same pair so you can see how they react differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d396f6c-1e6a-4aff-a212-c17f566c38aa",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5 LPIPS\n",
    "\n",
    "So far, we talked about **SSIM** (hand-crafted metric). Now, **LPIPS (Learned Perceptual Image Patch Similarity)** goes a step further: instead of manually designing similarity measures, it uses **deep features** from pretrained networks (e.g., AlexNet, VGG, SqueezeNet) to capture perceptual similarity.\n",
    "\n",
    "\n",
    "* Proposed in **\"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\"** (Zhang et al., 2018).\n",
    "* Idea: Humans judge images by *perceptual similarity*, not pixel-wise equality.\n",
    "* LPIPS measures distance in the **feature space** of a pretrained CNN rather than raw pixels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.1 How It Works\n",
    "\n",
    "1. Take two images $x$ and $y$.\n",
    "2. Pass both through a **pretrained network** (e.g., VGG).\n",
    "3. Extract activations from multiple layers (feature maps).\n",
    "4. Normalize features and compute **L2 distance** per spatial location.\n",
    "5. Average distances across spatial positions and layers.\n",
    "6. Optionally, train small linear weights to better align with human judgments.\n",
    "\n",
    "$$\n",
    "LPIPS(x,y) = \\sum_l \\frac{1}{H_l W_l} \\sum_{h,w} w_l \\; \\| \\hat{f}_l(x)_{h,w} - \\hat{f}_l(y)_{h,w} \\|_2^2\n",
    "$$\n",
    "\n",
    "* $f_l$: feature map from layer $l$.\n",
    "* $\\hat{f}_l$: channel-wise normalized.\n",
    "* $w_l$: learned weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.2 Why LPIPS is Important\n",
    "\n",
    "* **MSE/PSNR**: pixel-wise, not perceptual.\n",
    "* **SSIM**: structural but still hand-crafted.\n",
    "* **LPIPS**: learned perceptual similarity, matches human perception much better.\n",
    "\n",
    "In practice, LPIPS is considered **state-of-the-art** for evaluating perceptual image quality (GANs, super-resolution, style transfer, inpainting).\n",
    "\n",
    "---\n",
    "\n",
    "**LPIPS in Deep Learning Workflows**\n",
    "\n",
    "* Used as an **evaluation metric** for generative models (GANs, diffusion, etc.).\n",
    "* Sometimes used as a **loss function** (LPIPS loss) for training perceptual similarity.\n",
    "* Often combined with pixel losses (L1/L2) or SSIM.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.3 Comparison: SSIM vs LPIPS\n",
    "\n",
    "| Metric       | Based on                           | Pros                                                         | Cons                                             |\n",
    "| ------------ | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------ |\n",
    "| **MSE/PSNR** | Pixel differences                  | Simple, fast                                                 | Not perceptual, sensitive to shifts              |\n",
    "| **SSIM**     | Luminance, contrast, structure     | Better perceptual alignment                                  | Hand-crafted, less robust to complex distortions |\n",
    "| **LPIPS**    | Deep features (VGG, AlexNet, etc.) | Best matches human perception, widely used in GAN evaluation | Heavier, requires pretrained nets                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b81bb-ee87-471f-9bd2-e7f17991e325",
   "metadata": {},
   "source": [
    "## 1.6 Smoothness Loss\n",
    "\n",
    "\n",
    "\n",
    "### 1.6.1 What it is (and why)\n",
    "\n",
    "Photometric+SSIM losses make the network match views, but monocular depth has many plausible solutions (esp. in textureless regions). **Smoothness loss** is a regularizer that encourages **piecewise-smooth disparity/depth** while **preserving edges** aligned with image gradients.\n",
    "\n",
    "### 1.6.2 Common formulations\n",
    "\n",
    "Let $I\\in\\mathbb{R}^{H\\times W\\times 3}$ be the target image, and $d$ the predicted **disparity** (often smoother than raw depth; $d=1/z$). Finite differences:\n",
    "$\\partial_x f_{i,j}=f_{i,j}-f_{i,j-1}$, $\\partial_y f_{i,j}=f_{i,j}-f_{i-1,j}$.\n",
    "\n",
    "### 1.6.3 First-order, edge-aware (most used)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{sm}}^{(1)}=\n",
    "\\frac{1}{HW}\\sum_{i,j}\\Big(\n",
    "\\left|\\partial_x d_{i,j}\\right|\\,e^{-\\alpha\\,\\|\\partial_x I_{i,j}\\|}\n",
    "+\n",
    "\\left|\\partial_y d_{i,j}\\right|\\,e^{-\\alpha\\,\\|\\partial_y I_{i,j}\\|}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "* The exponential **down-weights** the penalty at strong image edges so you **don’t over-smooth boundaries**.\n",
    "* Typical $\\alpha\\in[5,10]$. Use grayscale $I$ or per-channel gradient norm.\n",
    "\n",
    "### 1.6.4 Second-order (curvature) variant\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{sm}}^{(2)}=\n",
    "\\frac{1}{HW}\\sum_{i,j}\\Big(\n",
    "\\left|\\partial_{xx} d_{i,j}\\right| e^{-\\alpha \\|\\partial_x I_{i,j}\\|}\n",
    "+\n",
    "\\left|\\partial_{yy} d_{i,j}\\right| e^{-\\alpha \\|\\partial_y I_{i,j}\\|}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "* Penalizes **changes of slope**; good for avoiding “staircasing”.\n",
    "\n",
    "### 1.6.5 Robust penalty / Charbonnier\n",
    "\n",
    "Replace $|x|$ with $\\rho(x)=\\sqrt{x^2+\\epsilon^2}$ (e.g., $\\epsilon=10^{-3}$) for stability.\n",
    "\n",
    "### 1.6.6 Scale-invariant normalization\n",
    "\n",
    "Disparity amplitude can drift. Common tricks:\n",
    "\n",
    "* Use disparity $d$ instead of depth $z$.\n",
    "* Or divide by mean disparity per image: $\\tilde d = d / (\\bar d + \\varepsilon)$ before taking gradients.\n",
    "\n",
    "### 1.6.7 Multi-scale\n",
    "\n",
    "Compute $\\mathcal{L}_{\\text{sm}}$ at pyramid levels $s=0..S-1$ (coarsest $\\to$ finest). Weight by $w_s$ (e.g., $w_s=1/2^s$) and sum.\n",
    "\n",
    "### 1.6.8 How it plugs into monocular VO (with a ViT)\n",
    "\n",
    "Even if your depth/pose networks are **ViT-based**, the smoothness term is unchanged—just compute it on the **full-resolution** disparity map (after your ViT’s upsampling head).\n",
    "\n",
    "**Total loss** (typical self-supervised monocular pipeline):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \n",
    "\\lambda_{\\text{photo}} \\, \\mathcal{L}_{\\text{photo}}\n",
    "+ \\lambda_{\\text{ssim}} \\, \\mathcal{L}_{\\text{ssim}}\n",
    "+ \\lambda_{\\text{sm}} \\, \\mathcal{L}_{\\text{sm}}\n",
    "\\,(+ \\text{other terms: automask, occlusion, geometry})\n",
    "$$\n",
    "\n",
    "Reasonable starting weights (tune per dataset):\n",
    "\n",
    "* $\\lambda_{\\text{photo}}=1.0$\n",
    "* $\\lambda_{\\text{ssim}}=0.15$ (if photo is L1)\n",
    "* $\\lambda_{\\text{sm}} \\in [0.001, 0.1]$ (start small; increase if depth is noisy)\n",
    "\n",
    "**ViT-specific tips**\n",
    "\n",
    "* Upsample tokens to image space (conv+pixelshuffle or interpolation) **before** smoothness.\n",
    "* If you see block boundaries, add a tiny second-order term or anti-blocking conv in the upsampling head.\n",
    "* Detach image gradients (no backprop through $I$).\n",
    "\n",
    "\n",
    "> Notes\n",
    "\n",
    "* Provide `image` in **grayscale** or compute gradient norm channel-wise and average (as above).\n",
    "* If you want Charbonnier: replace `.abs()` with `torch.sqrt(x*x + eps*eps)`.\n",
    "\n",
    "### 1.6.9 Practical tuning & pitfalls\n",
    "\n",
    "* **Start small** $\\lambda_{\\text{sm}}$: too large $\\Rightarrow$ over-smoothed, “melted” geometry; too small $\\Rightarrow$ noisy depth.\n",
    "* Use **disparity** rather than depth; it naturally stabilizes scale.\n",
    "* **Detach** image gradients (as shown) to prevent weird coupling.\n",
    "* Consider **second-order** term if you see “staircase” artifacts.\n",
    "* Compute on **multiple scales**; strongest impact at coarse scales.\n",
    "* Dynamic objects/occlusions: combine with **auto-masking / per-pixel min reprojection** to avoid penalizing impossible warps.\n",
    "* For ViT heads, ensure good **anti-aliasing upsampling**; otherwise smoothness fights token blocking.\n",
    "\n",
    "### 1.6.10 Minimal recipe (drop-in)\n",
    "\n",
    "1. Predict multi-scale disparities $d^{(s)}$ from your ViT depth head.\n",
    "2. For each scale, compute $\\mathcal{L}_{\\text{sm}}^{(1)}$ with $\\alpha=10$, normalize by mean disparity, weight by $w_s=1/2^s$.\n",
    "3. Set $\\lambda_{\\text{sm}}=0.01$ as a starting point; tune against validation photometric error and scale drift.\n",
    "4. Keep your usual $\\mathcal{L}_{\\text{photo}} + \\mathcal{L}_{\\text{SSIM}}$ and occlusion handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41ed0d-20d6-4121-a983-68feb0c24655",
   "metadata": {},
   "source": [
    "##  Evaluation Metrics\n",
    "\n",
    "| Metric  | Tool      | Meaning                                  |\n",
    "| ------- | --------- | ---------------------------------------- |\n",
    "| **ATE** | `evo_ape` | Absolute Trajectory Error — global drift |\n",
    "| **RPE** | `evo_rpe` | Relative Pose Error — local accuracy     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d626a9-7797-4af8-8828-5671926b2878",
   "metadata": {},
   "source": [
    "## Popular Monocular Depth Datasets\n",
    "\n",
    "| Dataset          | Description                        | License / Notes                    |\n",
    "| ---------------- | ---------------------------------- | ---------------------------------- |\n",
    "| **NYU Depth V2** | Indoor scenes, Kinect RGB-D images | ✔️ Standard for indoor depth       |\n",
    "| **KITTI**        | Outdoor driving scenes (LiDAR)     | ✔️ Standard for autonomous driving |\n",
    "| **Make3D**       | Outdoor stills (Stanford)          | Older, smaller                     |\n",
    "| **DIML/CVT**     | Outdoor depth from stereo          | Large and high-resolution          |\n",
    "| **TUM RGB-D**    | Indoor SLAM dataset                | ✔️ Camera + depth                  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
