{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc54ada-c2dc-4df1-ad3d-a96c79f72872",
   "metadata": {},
   "source": [
    "A detailed **summary of everything we've discussed**, organized into **loss types**, **learning paradigms**, and **technical choices** for building a monocular visual odometry (VO) system using deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "# üß≠ Summary: Loss Functions, Learning Modes, and Model Design in Monocular Visual Odometry (VO)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ I. Types of Loss Functions Used in VO\n",
    "\n",
    "| Loss Type                       | Used For                    | Description                                                                                      | Supervised?                      | Why Use It?                                                                   |   |                                                                |\n",
    "| ------------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------ | -------------------------------- | ----------------------------------------------------------------------------- | - | -------------------------------------------------------------- |\n",
    "| **Translation Loss** (`t_loss`) | Pose regression             | Measures Euclidean distance between predicted and ground truth translation vectors               | ‚úÖ                                | Directly trains model to estimate location changes                            |   |                                                                |\n",
    "| **Rotation Loss (Euler)**       | Pose regression             | MSE over roll/pitch/yaw Euler angles                                                             | ‚úÖ                                | Simple but **discontinuous** due to angle wrapping issues                     |   |                                                                |\n",
    "| **Rotation Loss (Quaternion)**  | Pose regression             | Predicts rotation as a unit quaternion                                                           | ‚úÖ                                | Continuous and avoids gimbal lock ‚Äî preferred over Euler angles               |   |                                                                |\n",
    "| **Geodesic Loss** (\\`1 -        | q1‚ãÖq2                       | \\`)                                                                                              | Rotation comparison (quaternion) | Computes the angular distance between two rotations on the SO(3) manifold     | ‚úÖ | More mathematically sound for rotation ‚Äî used with quaternions |\n",
    "| **SE(3) Loss** (Lie algebra)    | Full pose regression        | Predicts a 6D twist vector (rotation + translation) and maps it to SE(3) via the exponential map | ‚úÖ                                | Ensures predicted poses lie on valid motion manifold                          |   |                                                                |\n",
    "| **Photometric Loss**            | Image-based motion learning | Reconstructs one frame from another using depth & pose; compares pixel intensities               | ‚úÖ/‚ùå                              | Used in both **supervised as auxiliary** and **unsupervised as primary** loss |   |                                                                |\n",
    "| **SSIM Loss**                   | Perceptual similarity       | Measures structural similarity between original and warped image                                 | ‚úÖ/‚ùå                              | Robust to lighting, noise, blurring; improves perceptual quality              |   |                                                                |\n",
    "| **Smoothness Loss**             | Depth regularization        | Encourages smooth depth while preserving edges                                                   | ‚ùå                                | Essential in unsupervised depth learning to regularize noisy predictions      |   |                                                                |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ II. Learning Paradigms\n",
    "\n",
    "### üü© **Supervised VO**\n",
    "\n",
    "* Input: Consecutive RGB frames + GT pose\n",
    "* Output: Relative pose (6-DoF)\n",
    "* Loss: `t_loss + Œª * r_loss (geodesic or quaternion)`\n",
    "* Optional: Add photometric error as an **auxiliary loss**\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* Accurate if GT is good\n",
    "  **Cons**:\n",
    "* Requires expensive pose labels (e.g., GPS, LiDAR, Vicon)\n",
    "\n",
    "---\n",
    "\n",
    "### üüß **Unsupervised / Self-Supervised VO**\n",
    "\n",
    "* Input: Only RGB image sequences (no GT)\n",
    "* Networks:\n",
    "\n",
    "  * **PoseNet**: predicts T<sub>t‚Üít+1</sub>\n",
    "  * **DepthNet**: predicts per-pixel depth D<sub>t</sub>\n",
    "* Uses photometric loss to supervise both\n",
    "\n",
    "**Core Losses**:\n",
    "\n",
    "* Photometric loss (L1 + SSIM)\n",
    "* Depth smoothness\n",
    "* Optional auto-masking\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* Needs no labels\n",
    "* Leverages massive unlabeled data\n",
    "  **Cons**:\n",
    "* Assumes static scenes\n",
    "* Can be sensitive to occlusions, lighting changes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ III. Model Design Variants\n",
    "\n",
    "| Model Type                 | Key Feature                           | Notes                             |\n",
    "| -------------------------- | ------------------------------------- | --------------------------------- |\n",
    "| ViT + Pose Head            | Transformer-based feature extraction  | We use this in your current model |\n",
    "| SE(3) Pose Regressor       | Predicts twist vector & maps to SE(3) | More geometric and principled     |\n",
    "| Multi-Frame Transformer    | Uses windowed input (e.g., 5 frames)  | More temporal context             |\n",
    "| DepthNet + PoseNet (unsup) | Joint depth and motion prediction     | Needed for self-supervised VO     |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ IV. Evaluation Metrics\n",
    "\n",
    "| Metric  | Tool      | Meaning                                  |\n",
    "| ------- | --------- | ---------------------------------------- |\n",
    "| **ATE** | `evo_ape` | Absolute Trajectory Error ‚Äî global drift |\n",
    "| **RPE** | `evo_rpe` | Relative Pose Error ‚Äî local accuracy     |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ V. Logging (wandb)\n",
    "\n",
    "| What to Log                                       | When            |\n",
    "| ------------------------------------------------- | --------------- |\n",
    "| `loss_total`, `loss_translation`, `loss_rotation` | During training |\n",
    "| `ATE`, `RPE` metrics from `evo`                   | During eval     |\n",
    "| Trajectory plot images                            | During eval     |\n",
    "| Predicted vs. GT pose tables                      | During eval     |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ VI. Camera Intrinsics\n",
    "\n",
    "* Required for **photometric loss** to work.\n",
    "* Needed to:\n",
    "\n",
    "  * Project pixels to 3D (`K‚Åª¬π[u, v, 1]`)\n",
    "  * Reproject back with depth & pose\n",
    "* Can be loaded from:\n",
    "\n",
    "  * **KITTI**: `calib.txt`\n",
    "  * **TUM**: `camera.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ VII. Remaining Deliverables (based on your ‚Äúyes‚Äù)\n",
    "\n",
    "| Task                                | Status       |\n",
    "| ----------------------------------- | ------------ |\n",
    "| ‚úÖ Multi-frame dataset + model       | Implemented  |\n",
    "| ‚úÖ SE(3) loss with Lie algebra       | Implemented  |\n",
    "| ‚úÖ `evo` trajectory export & eval    | Pending code |\n",
    "| ‚úÖ wandb integration logging metrics | Pending code |\n",
    "| ‚úÖ Photometric loss (SSIM + warp)    | To implement |\n",
    "| ‚úÖ Unsupervised pipeline design      | ‚úîÔ∏è Explained |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ What‚Äôs Next (Code Plan)\n",
    "\n",
    "Would you like me to:\n",
    "\n",
    "1. Generate a **clean unsupervised training script** (DepthNet + PoseNet)?\n",
    "2. Add **photometric + SSIM loss** with warping code?\n",
    "3. Provide a **warp module** using `grid_sample()`?\n",
    "4. Add a **KITTI calibration file loader**?\n",
    "\n",
    "This would give you a **complete self-supervised VO system** from raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb687d7-b38d-477c-adbf-ac325f484a20",
   "metadata": {},
   "source": [
    "Creating a **Transformer-based model for monocular visual odometry (VO)** in PyTorch involves combining ideas from computer vision and sequence modeling. Here's a structured roadmap and a minimal working example to help you get started.\n",
    "\n",
    "---\n",
    "\n",
    "## üöó Goal\n",
    "\n",
    "Estimate camera **ego-motion (pose)** between consecutive frames from a **monocular RGB video** using a **Transformer-based deep neural network** in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. Overview of Architecture\n",
    "\n",
    "The basic idea is:\n",
    "\n",
    "```\n",
    "Input: Two consecutive frames (I_t, I_t+1)\n",
    "‚Üì\n",
    "Backbone CNN (e.g., ResNet, ViT) ‚Üí extract features\n",
    "‚Üì\n",
    "Feature Flattening + Positional Encoding\n",
    "‚Üì\n",
    "Transformer Encoder ‚Üí capture spatial-temporal relationships\n",
    "‚Üì\n",
    "Regression Head ‚Üí predict 6-DoF pose (3 translation + 3 rotation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± 2. Key Components\n",
    "\n",
    "### (a) **Image Pair Preprocessing**\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "### (b) **Backbone (e.g., ResNet18 or ViT)**\n",
    "\n",
    "For ViT-based features:\n",
    "\n",
    "```python\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "vit.heads = torch.nn.Identity()  # Remove classification head\n",
    "```\n",
    "\n",
    "Or use CNN (e.g., ResNet) for faster experimentation:\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "resnet = resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "```\n",
    "\n",
    "### (c) **Transformer Encoder**\n",
    "\n",
    "```python\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "```\n",
    "\n",
    "### (d) **Pose Regression Head**\n",
    "\n",
    "```python\n",
    "class PoseHead(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 6)  # (x, y, z, roll, pitch, yaw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.mean(dim=1))  # mean over sequence tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 3. Full Model\n",
    "\n",
    "```python\n",
    "class MonoVOTransformer(torch.nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, pose_head):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.pose_head = pose_head\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        # img_pair: (B, 2, C, H, W)\n",
    "        B = img_pair.size(0)\n",
    "        imgs = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        feats = self.feature_extractor(imgs)           # (2B, F)\n",
    "\n",
    "        # Reshape to (B, 2, F)\n",
    "        feats = feats.view(B, 2, -1)\n",
    "        out = self.transformer(feats.permute(1, 0, 2))  # (seq_len=2, B, F)\n",
    "        out = out.permute(1, 0, 2)  # (B, seq_len, F)\n",
    "        return self.pose_head(out)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 4. Training Loop Skeleton\n",
    "\n",
    "```python\n",
    "model = MonoVOTransformer(vit, transformer, PoseHead(768))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for images, poses_gt in train_loader:\n",
    "    # images shape: (B, 2, 3, 224, 224)\n",
    "    pred_poses = model(images)\n",
    "    loss = criterion(pred_poses, poses_gt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìê 5. Pose Format\n",
    "\n",
    "* **Rotation** can be Euler angles (roll, pitch, yaw) or quaternions.\n",
    "* If using quaternions, normalize them and use **geodesic loss** or **L2** with a normalization constraint.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Optional: Loss with Scale-Invariant or Geometric Components\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, gt):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], gt[:, :3])\n",
    "    r_loss = torch.nn.functional.mse_loss(pred[:, 3:], gt[:, 3:])\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 6. Dataset Format\n",
    "\n",
    "Prepare your dataset to return:\n",
    "\n",
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    img1 = load_image(idx)\n",
    "    img2 = load_image(idx + 1)\n",
    "    pose_delta = relative_pose(gt_pose[idx], gt_pose[idx+1])  # 6-DoF\n",
    "    return torch.stack([transform(img1), transform(img2)]), torch.tensor(pose_delta)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc6fe-8c37-4c0a-b998-fe89c83af711",
   "metadata": {},
   "source": [
    "Here‚Äôs a complete and modular PyTorch training script for monocular visual odometry using a Transformer-based model, with compatibility for KITTI or TUM datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Project Structure\n",
    "\n",
    "```\n",
    "mono_vo_transformer/\n",
    "‚îú‚îÄ‚îÄ dataset/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ kitti_dataset.py\n",
    "‚îú‚îÄ‚îÄ model/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ transformer_vo.py\n",
    "‚îú‚îÄ‚îÄ train.py\n",
    "‚îú‚îÄ‚îÄ utils.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. `dataset/kitti_dataset.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_poses(file_path):\n",
    "    poses = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = list(map(float, line.strip().split()))\n",
    "            matrix = np.array(values).reshape(3, 4)\n",
    "            poses.append(matrix)\n",
    "    return poses\n",
    "\n",
    "def relative_pose(p1, p2):\n",
    "    R1, t1 = p1[:, :3], p1[:, 3]\n",
    "    R2, t2 = p2[:, :3], p2[:, 3]\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = t2 - R_rel @ t1\n",
    "    # Convert rotation to Euler\n",
    "    yaw = np.arctan2(R_rel[1, 0], R_rel[0, 0])\n",
    "    pitch = np.arcsin(-R_rel[2, 0])\n",
    "    roll = np.arctan2(R_rel[2, 1], R_rel[2, 2])\n",
    "    return np.hstack((t_rel, [roll, pitch, yaw]))\n",
    "\n",
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, image_dir, pose_file):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "        self.poses = load_poses(pose_file)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = self.transform(Image.open(self.image_paths[idx]))\n",
    "        img2 = self.transform(Image.open(self.image_paths[idx + 1]))\n",
    "        pose1 = self.poses[idx]\n",
    "        pose2 = self.poses[idx + 1]\n",
    "        rel_pose = relative_pose(pose1, pose2)\n",
    "        return torch.stack([img1, img2]), torch.tensor(rel_pose, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. `model/transformer_vo.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class PoseHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.mean(dim=1))  # (B, 6)\n",
    "\n",
    "class MonoVOTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.feature_extractor.heads = nn.Identity()  # Remove classifier\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.pose_head = PoseHead(768)\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        B = img_pair.size(0)\n",
    "        x = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        features = self.feature_extractor(x)         # (2B, 768)\n",
    "        features = features.view(B, 2, -1).permute(1, 0, 2)  # (2, B, 768)\n",
    "        encoded = self.transformer(features).permute(1, 0, 2)  # (B, 2, 768)\n",
    "        return self.pose_head(encoded)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è 3. `train.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.kitti_dataset import KITTIDataset\n",
    "from model.transformer_vo import MonoVOTransformer\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "image_dir = \"/path/to/kitti/images/\"\n",
    "pose_file = \"/path/to/kitti/poses.txt\"\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Data ---\n",
    "dataset = KITTIDataset(image_dir, pose_file)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# --- Model ---\n",
    "model = MonoVOTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img_pair, target_pose in loader:\n",
    "        img_pair = img_pair.to(device)\n",
    "        target_pose = target_pose.to(device)\n",
    "        pred_pose = model(img_pair)\n",
    "        loss = criterion(pred_pose, target_pose)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * img_pair.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vo_transformer.pth\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "\n",
    "* **KITTI poses** are in 3x4 format: `[R | t]`.\n",
    "* If using **TUM RGB-D**, similar logic applies; just adapt image loading and timestamp alignment.\n",
    "* This baseline does not include:\n",
    "\n",
    "  * Scale correction\n",
    "  * Rotation with quaternions\n",
    "  * Photometric or depth-based geometric loss\n",
    "  * Multi-frame sequences (e.g., window of 5)\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* inference + trajectory plotting code?\n",
    "* integration with tensorboard/wandb?\n",
    "* extension to 5-frame windows instead of just 2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a3e6a-ff39-48a5-a76c-9e7a3bd5080b",
   "metadata": {},
   "source": [
    "Absolutely. The line:\n",
    "\n",
    "```python\n",
    "self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "```\n",
    "\n",
    "does the following:\n",
    "\n",
    "* **`os.listdir(image_dir)`**: lists all files in the folder.\n",
    "* **`f.endswith(\".png\")`**: filters to keep only `.png` files.\n",
    "* **`os.path.join(image_dir, f)`**: builds the full path to each `.png` file.\n",
    "* **`sorted(...)`**: ensures the list is in order (which is important for consistent frame-to-frame pairing).\n",
    "\n",
    "### ‚úÖ Here's a simplified, multi-line version:\n",
    "\n",
    "```python\n",
    "all_files = os.listdir(image_dir)\n",
    "\n",
    "png_files = []\n",
    "for f in all_files:\n",
    "    if f.endswith(\".png\"):\n",
    "        full_path = os.path.join(image_dir, f)\n",
    "        png_files.append(full_path)\n",
    "\n",
    "self.image_paths = sorted(png_files)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c373c1-a9f0-41b9-9e16-7d0fe75a9086",
   "metadata": {},
   "source": [
    "## MSELoss() vs Geodesic Loss\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What We're Predicting\n",
    "\n",
    "In monocular visual odometry, the model estimates:\n",
    "\n",
    "* **Translation**: a 3D vector `t = (x, y, z)`\n",
    "* **Rotation**: either as:\n",
    "\n",
    "  * **Euler angles** (roll, pitch, yaw) ‚Äî used in the current example\n",
    "  * **Quaternions** `q = (qx, qy, qz, qw)` ‚Äî often preferred in practice\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Limitations of `torch.nn.MSELoss()` on Euler Angles\n",
    "\n",
    "Using `MSELoss()` on Euler angles has issues:\n",
    "\n",
    "### 1. **Periodicity problem**\n",
    "\n",
    "* Angles like `Œ∏ = 179¬∞` and `Œ∏ = -179¬∞` are almost identical in 3D rotation but `MSE(179, -179)` is huge.\n",
    "\n",
    "### 2. **Gimbal lock**\n",
    "\n",
    "* Euler angles can suffer from **singularities** when converting between rotation representations.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Quaternions + Geodesic Loss are Better\n",
    "\n",
    "### Quaternions:\n",
    "\n",
    "* Compact, continuous, and avoid gimbal lock.\n",
    "* Represent rotations over the 3-sphere.\n",
    "* Require normalization (unit quaternions).\n",
    "\n",
    "### Geodesic loss (angular distance):\n",
    "\n",
    "Let `q1`, `q2` be unit quaternions:\n",
    "\n",
    "```python\n",
    "loss = 1 - |‚ü®q1, q2‚ü©|\n",
    "```\n",
    "\n",
    "or use:\n",
    "\n",
    "```python\n",
    "Œ∏ = 2 * arccos(|‚ü®q1, q2‚ü©|)\n",
    "```\n",
    "\n",
    "for angular error in radians.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recommended Approach\n",
    "\n",
    "### 1. Predict `[t_x, t_y, t_z, qx, qy, qz, qw]`\n",
    "\n",
    "Normalize the quaternion output before loss:\n",
    "\n",
    "```python\n",
    "q_pred = pred[:, 3:]\n",
    "q_pred = q_pred / q_pred.norm(dim=1, keepdim=True)\n",
    "```\n",
    "\n",
    "### 2. Geodesic loss:\n",
    "\n",
    "```python\n",
    "def geodesic_loss(q_pred, q_true):\n",
    "    inner = torch.abs(torch.sum(q_pred * q_true, dim=1))\n",
    "    return torch.mean(1 - inner)  # Or 2*arccos(inner) for angle\n",
    "```\n",
    "\n",
    "### 3. Full pose loss:\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, target):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], target[:, :3])\n",
    "    q_pred = pred[:, 3:] / pred[:, 3:].norm(dim=1, keepdim=True)\n",
    "    q_true = target[:, 3:] / target[:, 3:].norm(dim=1, keepdim=True)\n",
    "    r_loss = geodesic_loss(q_pred, q_true)\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Format        | Pros                         | Cons                                |\n",
    "| ------------- | ---------------------------- | ----------------------------------- |\n",
    "| Euler Angles  | Intuitive                    | Discontinuous, gimbal lock          |\n",
    "| Quaternions   | Continuous, robust           | Requires normalization              |\n",
    "| MSE Loss      | Easy, but weak for rotations | Not rotation-aware                  |\n",
    "| Geodesic Loss | Geometry-aware rotation loss | Slightly more expensive computation |\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to rewrite your `model` and `train.py` so it predicts quaternions and uses geodesic loss properly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cdf80-8fcc-412b-95dc-8bf28a5e1176",
   "metadata": {},
   "source": [
    "## AdamW and geodesic loss\n",
    "\n",
    "Here's an updated and clean version of your visual odometry pipeline using **`AdamW`** and **geodesic loss with quaternions**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. Update `model/transformer_vo.py` to predict translation + quaternion\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class PoseHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 7)  # 3 translation + 4 quaternion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.mean(dim=1))  # mean over tokens\n",
    "\n",
    "class MonoVOTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.feature_extractor.heads = nn.Identity()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.pose_head = PoseHead(768)\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        B = img_pair.size(0)\n",
    "        x = img_pair.view(-1, *img_pair.shape[2:])\n",
    "        features = self.feature_extractor(x)          # (2B, 768)\n",
    "        features = features.view(B, 2, -1).permute(1, 0, 2)  # (2, B, 768)\n",
    "        encoded = self.transformer(features).permute(1, 0, 2)  # (B, 2, 768)\n",
    "        return self.pose_head(encoded)  # (B, 7)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. Update `dataset/kitti_dataset.py` to use quaternions\n",
    "\n",
    "Update `relative_pose()` to return `[x, y, z, qx, qy, qz, qw]`.\n",
    "\n",
    "```python\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def relative_pose(p1, p2):\n",
    "    R1, t1 = p1[:, :3], p1[:, 3]\n",
    "    R2, t2 = p2[:, :3], p2[:, 3]\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = t2 - R_rel @ t1\n",
    "    quat = R.from_matrix(R_rel).as_quat()  # (qx, qy, qz, qw)\n",
    "    return np.hstack((t_rel, quat))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. Add geodesic loss to `utils.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def geodesic_loss(q_pred, q_true):\n",
    "    # Normalize quaternions\n",
    "    q_pred = F.normalize(q_pred, dim=1)\n",
    "    q_true = F.normalize(q_true, dim=1)\n",
    "\n",
    "    dot = torch.sum(q_pred * q_true, dim=1).abs()\n",
    "    return torch.mean(1.0 - dot)  # or 2 * arccos(dot) if you want radians\n",
    "```\n",
    "\n",
    "And the full pose loss:\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, target):\n",
    "    t_loss = F.mse_loss(pred[:, :3], target[:, :3])\n",
    "    r_loss = geodesic_loss(pred[:, 3:], target[:, 3:])\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. Update `train.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from dataset.kitti_dataset import KITTIDataset\n",
    "from model.transformer_vo import MonoVOTransformer\n",
    "from utils import pose_loss\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "image_dir = \"/path/to/kitti/images/\"\n",
    "pose_file = \"/path/to/kitti/poses.txt\"\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Data ---\n",
    "dataset = KITTIDataset(image_dir, pose_file)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# --- Model ---\n",
    "model = MonoVOTransformer().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "# --- Training ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img_pair, target_pose in loader:\n",
    "        img_pair = img_pair.to(device)\n",
    "        target_pose = target_pose.to(device)\n",
    "        pred_pose = model(img_pair)\n",
    "        loss = pose_loss(pred_pose, target_pose)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * img_pair.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vo_transformer_quat.pth\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Optional Improvements Later\n",
    "\n",
    "* Use `t_loss + Œª * r_loss` with a tunable weight `Œª` if one dominates.\n",
    "* Add learning rate scheduler.\n",
    "* Predict full SE(3) transformation matrix with differentiable Lie algebra layers.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like:\n",
    "\n",
    "* Logging with `wandb` or `TensorBoard`\n",
    "* Evaluation script for computing trajectory accuracy (ATE, RPE)\n",
    "* Conversion to multi-frame input (e.g., 5-frame window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ff071-6785-4aa2-beeb-d8c775d867fe",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Use `t_loss + Œª * r_loss` with a tunable weight Œª\n",
    "\n",
    "In the combined loss:\n",
    "\n",
    "```python\n",
    "total_loss = t_loss + Œª * r_loss\n",
    "```\n",
    "\n",
    "* `t_loss` measures translation error (e.g., in meters).\n",
    "* `r_loss` measures rotation error (e.g., in radians or unit-less quaternion distance).\n",
    "* **Problem**: Their scales are different.\n",
    "\n",
    "  * `t_loss` might be ‚âà 1.0 (meters)\n",
    "  * `r_loss` might be ‚âà 0.01 (radians or normalized quaternion loss)\n",
    "* **If you just add them**, the bigger one will dominate learning, causing the model to ignore the other.\n",
    "\n",
    "### ‚úÖ Solution: introduce a balancing factor Œª (lambda)\n",
    "\n",
    "```python\n",
    "Œª = 100.0  # for example\n",
    "loss = t_loss + Œª * r_loss\n",
    "```\n",
    "\n",
    "You can:\n",
    "\n",
    "* Manually tune Œª\n",
    "* Learn Œª during training (advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Predict full SE(3) transformation matrix with differentiable Lie algebra layers\n",
    "\n",
    "The full 6-DoF pose is an **SE(3)** transformation:\n",
    "\n",
    "```math\n",
    "T = [ R | t ]\n",
    "    [ 0 | 1 ]\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `R ‚àà SO(3)` is a 3√ó3 rotation matrix\n",
    "* `t ‚àà ‚Ñù¬≥` is a translation vector\n",
    "\n",
    "Instead of predicting quaternions, you can:\n",
    "\n",
    "1. Predict a 6D twist vector: Œæ = \\[œâx, œây, œâz, vx, vy, vz]\n",
    "\n",
    "   * `œâ` = rotation in Lie algebra (so(3)),\n",
    "   * `v` = translation\n",
    "\n",
    "2. Use **exponential map** to get `R` from `œâ`:\n",
    "\n",
    "   ```math\n",
    "   R = expm([œâ]_√ó)  # Rodrigues' formula\n",
    "   ```\n",
    "\n",
    "3. Combine `R` and `t` into a transformation matrix.\n",
    "\n",
    "### ‚úÖ Why this is useful?\n",
    "\n",
    "* The prediction lies on a valid **manifold** (SE(3)).\n",
    "* You get matrix operations, composition, inversion easily.\n",
    "* Libraries like `liegroups`, `geomstats`, `pypose` help with this.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. What is ATE, RPE? (Evaluation Metrics for VO)\n",
    "\n",
    "These are **standard metrics** in visual odometry and SLAM.\n",
    "\n",
    "### üîπ Absolute Trajectory Error (ATE)\n",
    "\n",
    "* Measures **how far the predicted trajectory is from the ground truth**, globally.\n",
    "* Example:\n",
    "\n",
    "  ```math\n",
    "  ATE_i = || T_i^{gt} - T_i^{pred} ||_2\n",
    "  ```\n",
    "* Affected by drift.\n",
    "* Best for evaluating **overall consistency** of trajectory.\n",
    "\n",
    "### üîπ Relative Pose Error (RPE)\n",
    "\n",
    "* Measures **local accuracy**: how accurate the motion is over a small time interval Œît.\n",
    "* Better for short-term motion quality (e.g., 5-frame windows).\n",
    "* Computed as:\n",
    "\n",
    "  ```math\n",
    "  RPE_i = T_i^{-1} * T_{i+Œît} - T_i^{gt, -1} * T_{i+Œît}^{gt}\n",
    "  ```\n",
    "\n",
    "‚úÖ Use:\n",
    "\n",
    "* **ATE** for **global trajectory drift**\n",
    "* **RPE** for **local odometry quality**\n",
    "\n",
    "Libraries:\n",
    "\n",
    "* [evo](https://github.com/MichaelGrupp/evo) (Python tool for evaluating VO/SLAM trajectories)\n",
    "\n",
    "```bash\n",
    "evo_ape kitti groundtruth.txt predicted.txt -va --plot\n",
    "evo_rpe kitti groundtruth.txt predicted.txt -va --plot\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Conversion to Multi-Frame Input (e.g., 5-frame window)\n",
    "\n",
    "Right now:\n",
    "\n",
    "```python\n",
    "img_pair = [I_t, I_t+1]\n",
    "```\n",
    "\n",
    "To increase temporal context:\n",
    "\n",
    "```python\n",
    "img_seq = [I_t-2, I_t-1, I_t, I_t+1, I_t+2]  # window of 5 frames\n",
    "```\n",
    "\n",
    "### Model input becomes:\n",
    "\n",
    "* `img_seq.shape = (B, 5, C, H, W)`\n",
    "\n",
    "### Options:\n",
    "\n",
    "1. **Concatenate** frames and use as a single image input\n",
    "\n",
    "   * Bad for temporal reasoning\n",
    "\n",
    "2. **Extract features per frame** ‚Üí feed to **Transformer**\n",
    "\n",
    "   * You get a sequence model:\n",
    "\n",
    "     ```python\n",
    "     features = [f_{t-2}, f_{t-1}, f_t, f_{t+1}, f_{t+2}]\n",
    "     transformer(features) ‚Üí predict pose of center frame or deltas\n",
    "     ```\n",
    "\n",
    "3. **Predict multiple poses**\n",
    "\n",
    "   * Predict 4 pose deltas: `T_{t-2‚Üít-1}, T_{t-1‚Üít}, T_{t‚Üít+1}, T_{t+1‚Üít+2}`\n",
    "\n",
    "### ‚úÖ Advantages of Multi-Frame\n",
    "\n",
    "* More temporal context = better stability\n",
    "* Reduces jitter\n",
    "* Can help with disambiguating occlusion or rapid movement\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* An implementation of multi-frame support?\n",
    "* An evaluation script using `evo`?\n",
    "* An SE(3)-based model that uses Lie algebra for pose prediction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb40f15-68c0-4543-8c64-7de0e012d093",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Multi-Frame Input Support (e.g., 5 frames)\n",
    "\n",
    "### üîß Dataset Change (Sliding Window)\n",
    "\n",
    "In `kitti_dataset.py`, modify `__getitem__()` to return a sequence of 5 consecutive frames:\n",
    "\n",
    "```python\n",
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, image_dir, pose_file, window_size=5):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "        self.poses = load_poses(pose_file)\n",
    "        self.window_size = window_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - self.window_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_seq = [self.transform(Image.open(self.image_paths[idx + i])) for i in range(self.window_size)]\n",
    "        img_seq = torch.stack(img_seq)  # shape (5, C, H, W)\n",
    "\n",
    "        pose1 = self.poses[idx + self.window_size // 2]\n",
    "        pose2 = self.poses[idx + self.window_size // 2 + 1]\n",
    "        rel_pose = relative_pose(pose1, pose2)\n",
    "\n",
    "        return img_seq, torch.tensor(rel_pose, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. SE(3) Pose Regression with Lie Algebra\n",
    "\n",
    "We'll predict a 6D twist vector `Œæ = [œâ, v] ‚àà se(3)` and convert it to a full pose using the **exponential map**:\n",
    "\n",
    "```python\n",
    "# utils_lie.py\n",
    "import torch\n",
    "from liegroups.torch import SE3\n",
    "\n",
    "def se3_exponential_map(twist):\n",
    "    # twist: (B, 6), [œâx, œây, œâz, vx, vy, vz]\n",
    "    se3 = SE3.Exp(twist)\n",
    "    return se3  # returns a batch of SE3 transformation matrices (B, 4, 4)\n",
    "```\n",
    "\n",
    "In the model, the head should predict 6D vectors instead of quaternions.\n",
    "\n",
    "```python\n",
    "# transformer_vo_lie.py\n",
    "class LieHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 6)  # se(3): rotation (œâ) + translation (v)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.mean(dim=1))  # (B, 6)\n",
    "```\n",
    "\n",
    "Loss can still use:\n",
    "\n",
    "```python\n",
    "def se3_loss(pred_twist, true_twist):\n",
    "    return F.mse_loss(pred_twist, true_twist)\n",
    "```\n",
    "\n",
    "Or you can compute pose error in SE(3) by composing/inverting predicted and true poses, then measuring translational and angular error.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Evaluation Script with evo\n",
    "\n",
    "Install `evo`:\n",
    "\n",
    "```bash\n",
    "pip install evo --upgrade\n",
    "```\n",
    "\n",
    "### üîß Output Trajectories for `evo_ape` and `evo_rpe`\n",
    "\n",
    "Save your trajectory in `KITTI` format:\n",
    "\n",
    "```python\n",
    "def write_trajectory(pred_poses, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for T in pred_poses:  # T: 4x4 np.array\n",
    "            mat = T[:3].reshape(-1)\n",
    "            f.write(' '.join(map(str, mat)) + '\\n')\n",
    "```\n",
    "\n",
    "### üß™ Run Evaluation with evo:\n",
    "\n",
    "```bash\n",
    "evo_ape kitti groundtruth.txt prediction.txt -va --plot\n",
    "evo_rpe kitti groundtruth.txt prediction.txt -va --plot\n",
    "```\n",
    "\n",
    "You‚Äôll get:\n",
    "\n",
    "* **ATE RMSE**\n",
    "* **RPE translation/rotation**\n",
    "* Plots of trajectory overlap and errors\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary: What I‚Äôll deliver next\n",
    "\n",
    "I‚Äôll provide you with:\n",
    "\n",
    "1. ‚úÖ A working **multi-frame dataset + model**\n",
    "2. ‚úÖ A model that **predicts in se(3)** and uses **Lie algebra exponential map**\n",
    "3. ‚úÖ A script to **generate predicted trajectory files** for `evo`\n",
    "4. ‚úÖ (Optional) A plotting script to overlay ground truth vs. predicted poses in 3D.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff55127-62fe-4a9e-b57e-b80089a133c3",
   "metadata": {},
   "source": [
    "Great question! When using **Weights & Biases (wandb)** for a **regression-like task** such as **visual odometry**, your logging strategy is a bit different from classification (where you log TP, FP, precision, etc).\n",
    "\n",
    "Here‚Äôs what you should log and how to structure it:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **What to Log with wandb for VO**\n",
    "\n",
    "### üîπ During Training (per epoch or batch):\n",
    "\n",
    "| Metric             | What it tells you                      |\n",
    "| ------------------ | -------------------------------------- |\n",
    "| `loss_total`       | Combined translation + rotation loss   |\n",
    "| `loss_translation` | Translation loss (e.g., MSE in meters) |\n",
    "| `loss_rotation`    | Rotation loss (e.g., geodesic loss)    |\n",
    "| `lr`               | Learning rate                          |\n",
    "| `norm_grad`        | Gradient norm (optional for stability) |\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "wandb.log({\n",
    "    \"epoch\": epoch,\n",
    "    \"loss_total\": loss.item(),\n",
    "    \"loss_translation\": t_loss.item(),\n",
    "    \"loss_rotation\": r_loss.item(),\n",
    "    \"lr\": scheduler.get_last_lr()[0],\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. **After Evaluation (Whole Trajectory)**\n",
    "\n",
    "Run `evo_ape` and `evo_rpe`, then parse the results and log:\n",
    "\n",
    "| Metric           | Source    | Description                             |\n",
    "| ---------------- | --------- | --------------------------------------- |\n",
    "| `ATE_RMSE`       | `evo_ape` | Global drift over trajectory            |\n",
    "| `ATE_mean`       | `evo_ape` | Mean absolute trajectory error          |\n",
    "| `RPE_trans_rmse` | `evo_rpe` | Relative error in translation           |\n",
    "| `RPE_rot_rmse`   | `evo_rpe` | Relative error in rotation (rad or deg) |\n",
    "\n",
    "You can **log these in a `wandb.Table` or as scalars**:\n",
    "\n",
    "```python\n",
    "wandb.log({\n",
    "    \"ATE_RMSE\": ate_rmse,\n",
    "    \"RPE_trans_rmse\": rpe_trans,\n",
    "    \"RPE_rot_rmse\": rpe_rot,\n",
    "})\n",
    "```\n",
    "\n",
    "Optionally, you can upload:\n",
    "\n",
    "* üìà Plots from `evo` as images (e.g., trajectory overlay, error curves)\n",
    "* üìä Trajectory CSVs via `wandb.save()` for later inspection\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. **Visualizing Predictions**\n",
    "\n",
    "(Optional but nice!)\n",
    "\n",
    "If you reconstruct and visualize predicted camera poses:\n",
    "\n",
    "```python\n",
    "fig = visualize_trajectory(gt_poses, pred_poses)  # matplotlib or plotly\n",
    "wandb.log({\"Trajectory 3D\": wandb.Image(fig)})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Summary\n",
    "\n",
    "| When       | What to Log       | How                              |\n",
    "| ---------- | ----------------- | -------------------------------- |\n",
    "| Training   | Losses, LR        | `wandb.log()`                    |\n",
    "| Evaluation | ATE/RPE           | parse `evo`, log to `wandb`      |\n",
    "| Visuals    | Trajectory, plots | `wandb.Image()`, `wandb.Table()` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7bb01-67b4-4632-83a1-8267275ee5a5",
   "metadata": {},
   "source": [
    "Yes ‚Äî it absolutely **makes sense to include photometric error** as part of the loss in monocular visual odometry. In fact, many state-of-the-art unsupervised or self-supervised VO and depth estimation methods rely heavily on it.\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is Photometric Error?\n",
    "\n",
    "Photometric error measures **how well a predicted motion explains changes in pixel intensity** between two consecutive images.\n",
    "\n",
    "### ü§î Intuition:\n",
    "\n",
    "If your predicted pose is correct, then warping image `I_t+1` back to time `t` using the predicted depth and pose should reconstruct `I_t`.\n",
    "Photometric error = how similar the reconstructed image is to the actual one.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Mathematical Form\n",
    "\n",
    "For image $I_t$, target image $I_{t+1}$, depth $D_t$, and predicted relative pose $T_{t \\to t+1}$:\n",
    "\n",
    "1. Project 3D points from $I_t$ using $D_t$\n",
    "2. Transform to $t+1$ using predicted pose\n",
    "3. Reproject to 2D ‚Üí synthesize image $\\hat{I}_t$\n",
    "4. Compare with original $I_t$\n",
    "\n",
    "### Photometric Loss\n",
    "\n",
    "```math\n",
    "L_{photo} = \\frac{1}{N} \\sum_{i} \\| I_t(i) - \\hat{I}_t(i) \\|_1\n",
    "```\n",
    "\n",
    "Also commonly used:\n",
    "\n",
    "* SSIM loss\n",
    "* Weighted sum: `Œª1 * L1 + Œª2 * SSIM`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use It\n",
    "\n",
    "| Use Case                      | Use Photometric Loss?  |\n",
    "| ----------------------------- | ---------------------- |\n",
    "| Supervised VO (with GT poses) | ‚úÖ (as auxiliary loss)  |\n",
    "| Self-supervised VO / Depth    | ‚úÖ (essential!)         |\n",
    "| Supervised + Depth Estimation | ‚úÖ (multi-task synergy) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits\n",
    "\n",
    "* **Dense signal**: Per-pixel loss gives more supervision than sparse pose labels.\n",
    "* **Unlabeled training**: No need for ground-truth poses if depth is known or learned jointly.\n",
    "* **Helps reduce overfitting** on pose-only datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± When It Makes Less Sense\n",
    "\n",
    "* When you don‚Äôt have access to camera intrinsics + depth\n",
    "* When camera motion is large and photometric constancy breaks\n",
    "* If lighting varies too much between frames\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Implementation Outline\n",
    "\n",
    "To use photometric loss:\n",
    "\n",
    "* Need intrinsics `K`\n",
    "* Either predict or use known depth maps\n",
    "* Implement differentiable image warping (e.g., using `grid_sample` in PyTorch)\n",
    "\n",
    "### Resources:\n",
    "\n",
    "* [monodepth2](https://github.com/nianticlabs/monodepth2)\n",
    "* [SfMLearner](https://github.com/tinghuiz/SfMLearner)\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "| Should You Add Photometric Error?                  | ‚úÖ Yes, if... |\n",
    "| -------------------------------------------------- | ------------ |\n",
    "| You want more accurate, dense feedback             | ‚úÖ            |\n",
    "| You have depth or want to learn it                 | ‚úÖ            |\n",
    "| You use a self-supervised or semi-supervised setup | ‚úÖ            |\n",
    "| You only use pose GT and no image context          | ‚ùå Optional   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7922a-6c64-4ae4-9030-caeb6b912b17",
   "metadata": {},
   "source": [
    "Exactly ‚Äî you're spot on.\n",
    "\n",
    "To compute **photometric loss**, you need the camera **intrinsic calibration parameters** because you're projecting and unprojecting 3D points between views.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What Intrinsics You Need\n",
    "\n",
    "The camera intrinsic matrix $K \\in \\mathbb{R}^{3 \\times 3}$:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "f_x & 0   & c_x \\\\\n",
    "0   & f_y & c_y \\\\\n",
    "0   & 0   & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* $f_x, f_y$: focal lengths\n",
    "* $c_x, c_y$: principal point (usually image center)\n",
    "\n",
    "---\n",
    "\n",
    "## üì∏ Why You Need It\n",
    "\n",
    "### 1. **Back-project pixels to 3D**\n",
    "\n",
    "To map a pixel $(u, v)$ and depth $d$ to 3D in the camera frame:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = d \\cdot K^{-1} [u, v, 1]^T\n",
    "$$\n",
    "\n",
    "### 2. **Transform with pose**\n",
    "\n",
    "$$\n",
    "\\mathbf{X}' = T_{t \\rightarrow t+1} \\cdot \\mathbf{X}\n",
    "$$\n",
    "\n",
    "### 3. **Project back to image**\n",
    "\n",
    "$$\n",
    "[u', v', 1]^T = K \\cdot \\mathbf{X}'\n",
    "$$\n",
    "\n",
    "Then sample the target image $I_{t+1}$ at coordinates $(u', v')$ to synthesize $\\hat{I}_t$.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Where to Get Calibration Parameters\n",
    "\n",
    "### KITTI:\n",
    "\n",
    "* Calibration files: `calib.txt` in each sequence folder\n",
    "* Looks like:\n",
    "\n",
    "  ```\n",
    "  P0: fx 0 cx 0 fy cy ...\n",
    "  ```\n",
    "\n",
    "  You extract fx, fy, cx, cy from projection matrix.\n",
    "\n",
    "### TUM:\n",
    "\n",
    "* Provided as `camera.yaml` or in `*.txt`\n",
    "* Direct intrinsics + distortion\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What to Do If You Don‚Äôt Have Intrinsics\n",
    "\n",
    "You can:\n",
    "\n",
    "1. Use **default/fake intrinsics** (e.g., $f = 0.8 \\cdot W, cx = W/2, cy = H/2$)\n",
    "\n",
    "   * Works okay if normalized image coords are used\n",
    "2. Estimate intrinsics from EXIF or camera model\n",
    "\n",
    "But **real calibration helps a lot**.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Summary\n",
    "\n",
    "| Step                | Requires Intrinsics? |\n",
    "| ------------------- | -------------------- |\n",
    "| Project pixel to 3D | ‚úÖ Yes                |\n",
    "| Transform with pose | ‚ùå No (just SE(3))    |\n",
    "| Project 3D to pixel | ‚úÖ Yes                |\n",
    "| Sample warped image | ‚úÖ Yes                |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* A PyTorch implementation of photometric loss?\n",
    "* A warp module using `grid_sample` with camera intrinsics?\n",
    "* A loader that reads intrinsics from KITTI calib.txt?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d785e1-40aa-458c-89ec-b3312c068295",
   "metadata": {},
   "source": [
    "Yes ‚Äî you‚Äôre asking the exact right question.\n",
    "\n",
    "### ‚úÖ **Yes, we can train visual odometry (and even depth estimation) in an entirely unsupervised or self-supervised way using photometric loss** and a few key ingredients.\n",
    "\n",
    "This is the foundation of well-known methods like:\n",
    "\n",
    "* **SfM-Learner** (2017)\n",
    "* **Monodepth2** (2019)\n",
    "* **GeoNet**, **UnDeepVO**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How Unsupervised Learning Works with Photometric Error\n",
    "\n",
    "Instead of learning to match **ground-truth poses or depth**, we train the network to minimize **image reconstruction error**. The core idea:\n",
    "\n",
    "> *If the model predicts the right pose and depth, warping one frame into another will reconstruct the original frame.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Architecture Overview\n",
    "\n",
    "You need two learnable modules:\n",
    "\n",
    "| Module       | Output                        |\n",
    "| ------------ | ----------------------------- |\n",
    "| **DepthNet** | per-pixel depth $D_t$         |\n",
    "| **PoseNet**  | relative pose $T_{t \\to t+1}$ |\n",
    "\n",
    "Optionally, you can use a shared encoder.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Self-Supervised Learning Components\n",
    "\n",
    "### 1. ‚úÖ **Photometric Reconstruction Loss**\n",
    "\n",
    "Warp $I_{t+1} \\rightarrow I_t$ using predicted depth + pose, and minimize:\n",
    "\n",
    "```math\n",
    "L_{photo} = \\alpha \\cdot \\text{SSIM}(I_t, \\hat{I}_t) + (1 - \\alpha) \\cdot \\| I_t - \\hat{I}_t \\|_1\n",
    "```\n",
    "\n",
    "Where $\\hat{I}_t$ is the synthesized version of $I_t$ via warping.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ **Depth Smoothness Loss**\n",
    "\n",
    "Encourages spatial smoothness in the predicted depth map (especially in low-texture regions):\n",
    "\n",
    "```math\n",
    "L_{smooth} = |\\partial_x d| e^{-|\\partial_x I|} + |\\partial_y d| e^{-|\\partial_y I|}\n",
    "```\n",
    "\n",
    "This helps regularize the ill-posed nature of depth from monocular input.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ **Auto-Masking / Validity Mask**\n",
    "\n",
    "Ignore pixels that:\n",
    "\n",
    "* Are occluded\n",
    "* Are static across frames (e.g., due to dynamic objects)\n",
    "\n",
    "Popular method:\n",
    "\n",
    "```python\n",
    "mask = (|I_t - I_t+1| < |I_t - warped_I_t|)  # Use only pixels that changed due to camera motion\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ (Optional) Epipolar or Geometry Consistency Loss\n",
    "\n",
    "To ensure pose predictions satisfy geometric constraints (e.g., 8-point constraint, or triangulated consistency across multiple views).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Loss Function\n",
    "\n",
    "```python\n",
    "L = Œª1 * photometric_loss\n",
    "  + Œª2 * smoothness_loss\n",
    "  + Œª3 * geometry_consistency_loss (optional)\n",
    "```\n",
    "\n",
    "No pose or depth ground truth is used at all ‚Äî only raw video frames.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Summary: How You Can Do Unsupervised VO\n",
    "\n",
    "| Component     | Description                               |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| Inputs        | Sequence of RGB images                    |\n",
    "| Depth Network | Predicts dense depth from single frame    |\n",
    "| Pose Network  | Predicts relative pose between frames     |\n",
    "| Intrinsics    | Needed for projection/warping             |\n",
    "| Losses        | Photometric + Smoothness + Validity masks |\n",
    "| Ground Truth? | ‚ùå Not needed at all                       |\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Advantages\n",
    "\n",
    "* Works with any raw video sequence\n",
    "* Requires no labeled poses or depth\n",
    "* Jointly learns both geometry and motion\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "* Assumes mostly static scene\n",
    "* Struggles with dynamic objects\n",
    "* Sensitive to lighting changes\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to:\n",
    "\n",
    "1. Design a full unsupervised PyTorch pipeline (DepthNet + PoseNet)\n",
    "2. Implement photometric loss + warping with camera intrinsics\n",
    "3. Add smoothness + auto-masking + intrinsics loader\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99365c6f-e7b8-4f78-9372-51ef68274870",
   "metadata": {},
   "source": [
    "### ‚úÖ SSIM = **Structural Similarity Index Measure**\n",
    "\n",
    "SSIM is a **perceptual metric** that measures image similarity based on how **humans** perceive structure, contrast, and luminance ‚Äî not just pixel-by-pixel differences like L1 or L2 loss.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why SSIM?\n",
    "\n",
    "Traditional pixel-wise loss functions like **L1** or **L2** compute:\n",
    "\n",
    "$$\n",
    "L1(I_1, I_2) = \\frac{1}{N} \\sum_i |I_1(i) - I_2(i)|\n",
    "$$\n",
    "\n",
    "But they:\n",
    "\n",
    "* Treat all pixel errors equally\n",
    "* Are overly sensitive to small changes in brightness or slight misalignments\n",
    "\n",
    "### üî• SSIM focuses on **perceptual similarity**, which is more robust to:\n",
    "\n",
    "* Lighting changes\n",
    "* Blurring\n",
    "* Small pixel misalignments\n",
    "\n",
    "---\n",
    "\n",
    "## üß† SSIM Formula\n",
    "\n",
    "Given two image patches $x$ and $y$, SSIM is defined as:\n",
    "\n",
    "$$\n",
    "SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mu_x, \\mu_y$: local means\n",
    "* $\\sigma_x^2, \\sigma_y^2$: variances\n",
    "* $\\sigma_{xy}$: covariance\n",
    "* $C_1, C_2$: constants for stability\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è In PyTorch (Monodepth2-style)\n",
    "\n",
    "They define **photometric loss** as:\n",
    "\n",
    "```python\n",
    "L_photo = Œ± * (1 - SSIM(I, ƒ®)) / 2 + (1 - Œ±) * |I - ƒ®|\n",
    "```\n",
    "\n",
    "This combines **SSIM** and **L1**, where `Œ± ‚âà 0.85`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Using SSIM in VO\n",
    "\n",
    "| Feature                | SSIM        | L1 / L2 |\n",
    "| ---------------------- | ----------- | ------- |\n",
    "| Perceptually aligned   | ‚úÖ Yes       | ‚ùå No    |\n",
    "| Sensitive to structure | ‚úÖ Yes       | ‚ùå No    |\n",
    "| Lighting-robust        | ‚úÖ Better    | ‚ùå Bad   |\n",
    "| Good for warping loss  | ‚úÖ Excellent | ‚ùå Weak  |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Summary\n",
    "\n",
    "* SSIM helps the model compare **structures** between real and warped images.\n",
    "* It's standard in **self-supervised VO** and **depth estimation**.\n",
    "* Usually used in combination with L1 loss for best results.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to provide a PyTorch implementation of SSIM suitable for photometric loss?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
