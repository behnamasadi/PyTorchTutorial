{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1d6104-0a8b-4759-a4b8-547362fe0c0f",
   "metadata": {},
   "source": [
    "Creating a **Transformer-based model for monocular visual odometry (VO)** in PyTorch involves combining ideas from computer vision and sequence modeling. Here's a structured roadmap and a minimal working example to help you get started.\n",
    "\n",
    "---\n",
    "\n",
    "## üöó Goal\n",
    "\n",
    "Estimate camera **ego-motion (pose)** between consecutive frames from a **monocular RGB video** using a **Transformer-based deep neural network** in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. Overview of Architecture\n",
    "\n",
    "The basic idea is:\n",
    "\n",
    "```\n",
    "Input: Two consecutive frames (I_t, I_t+1)\n",
    "‚Üì\n",
    "Backbone CNN (e.g., ResNet, ViT) ‚Üí extract features\n",
    "‚Üì\n",
    "Feature Flattening + Positional Encoding\n",
    "‚Üì\n",
    "Transformer Encoder ‚Üí capture spatial-temporal relationships\n",
    "‚Üì\n",
    "Regression Head ‚Üí predict 6-DoF pose (3 translation + 3 rotation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± 2. Key Components\n",
    "\n",
    "### (a) **Image Pair Preprocessing**\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "### (b) **Backbone (e.g., ResNet18 or ViT)**\n",
    "\n",
    "For ViT-based features:\n",
    "\n",
    "```python\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "vit.heads = torch.nn.Identity()  # Remove classification head\n",
    "```\n",
    "\n",
    "Or use CNN (e.g., ResNet) for faster experimentation:\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "resnet = resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "```\n",
    "\n",
    "### (c) **Transformer Encoder**\n",
    "\n",
    "```python\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "```\n",
    "\n",
    "### (d) **Pose Regression Head**\n",
    "\n",
    "```python\n",
    "class PoseHead(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 6)  # (x, y, z, roll, pitch, yaw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.mean(dim=1))  # mean over sequence tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 3. Full Model\n",
    "\n",
    "```python\n",
    "class MonoVOTransformer(torch.nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, pose_head):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.pose_head = pose_head\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        # img_pair: (B, 2, C, H, W)\n",
    "        B = img_pair.size(0)\n",
    "        imgs = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        feats = self.feature_extractor(imgs)           # (2B, F)\n",
    "\n",
    "        # Reshape to (B, 2, F)\n",
    "        feats = feats.view(B, 2, -1)\n",
    "        out = self.transformer(feats.permute(1, 0, 2))  # (seq_len=2, B, F)\n",
    "        out = out.permute(1, 0, 2)  # (B, seq_len, F)\n",
    "        return self.pose_head(out)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 4. Training Loop Skeleton\n",
    "\n",
    "```python\n",
    "model = MonoVOTransformer(vit, transformer, PoseHead(768))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for images, poses_gt in train_loader:\n",
    "    # images shape: (B, 2, 3, 224, 224)\n",
    "    pred_poses = model(images)\n",
    "    loss = criterion(pred_poses, poses_gt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìê 5. Pose Format\n",
    "\n",
    "* **Rotation** can be Euler angles (roll, pitch, yaw) or quaternions.\n",
    "* If using quaternions, normalize them and use **geodesic loss** or **L2** with a normalization constraint.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Optional: Loss with Scale-Invariant or Geometric Components\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, gt):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], gt[:, :3])\n",
    "    r_loss = torch.nn.functional.mse_loss(pred[:, 3:], gt[:, 3:])\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 6. Dataset Format\n",
    "\n",
    "Prepare your dataset to return:\n",
    "\n",
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    img1 = load_image(idx)\n",
    "    img2 = load_image(idx + 1)\n",
    "    pose_delta = relative_pose(gt_pose[idx], gt_pose[idx+1])  # 6-DoF\n",
    "    return torch.stack([transform(img1), transform(img2)]), torch.tensor(pose_delta)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd763f9-f810-4d52-8678-17a694e1c29b",
   "metadata": {},
   "source": [
    "Here‚Äôs a complete and modular PyTorch training script for monocular visual odometry using a Transformer-based model, with compatibility for KITTI or TUM datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Project Structure\n",
    "\n",
    "```\n",
    "mono_vo_transformer/\n",
    "‚îú‚îÄ‚îÄ dataset/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ kitti_dataset.py\n",
    "‚îú‚îÄ‚îÄ model/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ transformer_vo.py\n",
    "‚îú‚îÄ‚îÄ train.py\n",
    "‚îú‚îÄ‚îÄ utils.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. `dataset/kitti_dataset.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_poses(file_path):\n",
    "    poses = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = list(map(float, line.strip().split()))\n",
    "            matrix = np.array(values).reshape(3, 4)\n",
    "            poses.append(matrix)\n",
    "    return poses\n",
    "\n",
    "def relative_pose(p1, p2):\n",
    "    R1, t1 = p1[:, :3], p1[:, 3]\n",
    "    R2, t2 = p2[:, :3], p2[:, 3]\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = t2 - R_rel @ t1\n",
    "    # Convert rotation to Euler\n",
    "    yaw = np.arctan2(R_rel[1, 0], R_rel[0, 0])\n",
    "    pitch = np.arcsin(-R_rel[2, 0])\n",
    "    roll = np.arctan2(R_rel[2, 1], R_rel[2, 2])\n",
    "    return np.hstack((t_rel, [roll, pitch, yaw]))\n",
    "\n",
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, image_dir, pose_file):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "        self.poses = load_poses(pose_file)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = self.transform(Image.open(self.image_paths[idx]))\n",
    "        img2 = self.transform(Image.open(self.image_paths[idx + 1]))\n",
    "        pose1 = self.poses[idx]\n",
    "        pose2 = self.poses[idx + 1]\n",
    "        rel_pose = relative_pose(pose1, pose2)\n",
    "        return torch.stack([img1, img2]), torch.tensor(rel_pose, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. `model/transformer_vo.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class PoseHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.mean(dim=1))  # (B, 6)\n",
    "\n",
    "class MonoVOTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.feature_extractor.heads = nn.Identity()  # Remove classifier\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.pose_head = PoseHead(768)\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        B = img_pair.size(0)\n",
    "        x = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        features = self.feature_extractor(x)         # (2B, 768)\n",
    "        features = features.view(B, 2, -1).permute(1, 0, 2)  # (2, B, 768)\n",
    "        encoded = self.transformer(features).permute(1, 0, 2)  # (B, 2, 768)\n",
    "        return self.pose_head(encoded)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è 3. `train.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.kitti_dataset import KITTIDataset\n",
    "from model.transformer_vo import MonoVOTransformer\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "image_dir = \"/path/to/kitti/images/\"\n",
    "pose_file = \"/path/to/kitti/poses.txt\"\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Data ---\n",
    "dataset = KITTIDataset(image_dir, pose_file)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# --- Model ---\n",
    "model = MonoVOTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img_pair, target_pose in loader:\n",
    "        img_pair = img_pair.to(device)\n",
    "        target_pose = target_pose.to(device)\n",
    "        pred_pose = model(img_pair)\n",
    "        loss = criterion(pred_pose, target_pose)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * img_pair.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vo_transformer.pth\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "\n",
    "* **KITTI poses** are in 3x4 format: `[R | t]`.\n",
    "* If using **TUM RGB-D**, similar logic applies; just adapt image loading and timestamp alignment.\n",
    "* This baseline does not include:\n",
    "\n",
    "  * Scale correction\n",
    "  * Rotation with quaternions\n",
    "  * Photometric or depth-based geometric loss\n",
    "  * Multi-frame sequences (e.g., window of 5)\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* inference + trajectory plotting code?\n",
    "* integration with tensorboard/wandb?\n",
    "* extension to 5-frame windows instead of just 2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa7dfb-b44a-4a83-857f-ceefc7483262",
   "metadata": {},
   "source": [
    "The line:\n",
    "\n",
    "```python\n",
    "self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "```\n",
    "\n",
    "does the following:\n",
    "\n",
    "* **`os.listdir(image_dir)`**: lists all files in the folder.\n",
    "* **`f.endswith(\".png\")`**: filters to keep only `.png` files.\n",
    "* **`os.path.join(image_dir, f)`**: builds the full path to each `.png` file.\n",
    "* **`sorted(...)`**: ensures the list is in order (which is important for consistent frame-to-frame pairing).\n",
    "\n",
    "### ‚úÖ Here's a simplified, multi-line version:\n",
    "\n",
    "```python\n",
    "all_files = os.listdir(image_dir)\n",
    "\n",
    "png_files = []\n",
    "for f in all_files:\n",
    "    if f.endswith(\".png\"):\n",
    "        full_path = os.path.join(image_dir, f)\n",
    "        png_files.append(full_path)\n",
    "\n",
    "self.image_paths = sorted(png_files)\n",
    "```\n",
    "\n",
    "This version is easier to read and debug, especially when you're first building or modifying datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908d464-c4e5-4a0a-b8d9-af2782b35473",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ Here's why I used `Adam` by default:\n",
    "\n",
    "The original `Adam` optimizer is a common default for most PyTorch training tasks. It performs adaptive learning rate updates and works well for a wide range of problems ‚Äî especially in vision and sequence modeling. But...\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Why **`Adam` might be suboptimal** in some Transformer models:\n",
    "\n",
    "* `Adam` applies **L2 regularization via weight decay** naively by adding it to the gradients.\n",
    "* This clashes with the **internal normalization mechanisms** in Transformers (e.g., LayerNorm, biases), which ideally should not be regularized.\n",
    "* Regularizing **all weights equally** can degrade performance in Transformer-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why **`AdamW` is better for Transformers**\n",
    "\n",
    "`AdamW` (decoupled weight decay) fixes this by **decoupling weight decay from gradient updates**, allowing:\n",
    "\n",
    "* Better handling of weight decay (especially on linear layers only).\n",
    "* Empirically better generalization for **Transformer-based architectures** (used in BERT, ViT, etc.).\n",
    "\n",
    "```python\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† When to prefer:\n",
    "\n",
    "| Scenario                      | Use                   |\n",
    "| ----------------------------- | --------------------- |\n",
    "| Transformer / ViT models      | ‚úÖ `AdamW`             |\n",
    "| CNNs or simpler MLPs          | ‚úÖ `Adam` (or `AdamW`) |\n",
    "| Want finer control over decay | ‚úÖ `AdamW`             |\n",
    "\n",
    "---\n",
    "\n",
    "If you're using a **Transformer backbone** like ViT for monocular visual odometry, then switching to `AdamW` is a better default.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca65c39-ab43-4893-bab6-5dcfb10a0e64",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üéØ What We're Predicting\n",
    "\n",
    "In monocular visual odometry, the model estimates:\n",
    "\n",
    "* **Translation**: a 3D vector `t = (x, y, z)`\n",
    "* **Rotation**: either as:\n",
    "\n",
    "  * **Euler angles** (roll, pitch, yaw) ‚Äî used in the current example\n",
    "  * **Quaternions** `q = (qx, qy, qz, qw)` ‚Äî often preferred in practice\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Limitations of `torch.nn.MSELoss()` on Euler Angles\n",
    "\n",
    "Using `MSELoss()` on Euler angles has issues:\n",
    "\n",
    "### 1. **Periodicity problem**\n",
    "\n",
    "* Angles like `Œ∏ = 179¬∞` and `Œ∏ = -179¬∞` are almost identical in 3D rotation but `MSE(179, -179)` is huge.\n",
    "\n",
    "### 2. **Gimbal lock**\n",
    "\n",
    "* Euler angles can suffer from **singularities** when converting between rotation representations.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Quaternions + Geodesic Loss are Better\n",
    "\n",
    "### Quaternions:\n",
    "\n",
    "* Compact, continuous, and avoid gimbal lock.\n",
    "* Represent rotations over the 3-sphere.\n",
    "* Require normalization (unit quaternions).\n",
    "\n",
    "### Geodesic loss (angular distance):\n",
    "\n",
    "Let `q1`, `q2` be unit quaternions:\n",
    "\n",
    "```python\n",
    "loss = 1 - |‚ü®q1, q2‚ü©|\n",
    "```\n",
    "\n",
    "or use:\n",
    "\n",
    "```python\n",
    "Œ∏ = 2 * arccos(|‚ü®q1, q2‚ü©|)\n",
    "```\n",
    "\n",
    "for angular error in radians.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recommended Approach\n",
    "\n",
    "### 1. Predict `[t_x, t_y, t_z, qx, qy, qz, qw]`\n",
    "\n",
    "Normalize the quaternion output before loss:\n",
    "\n",
    "```python\n",
    "q_pred = pred[:, 3:]\n",
    "q_pred = q_pred / q_pred.norm(dim=1, keepdim=True)\n",
    "```\n",
    "\n",
    "### 2. Geodesic loss:\n",
    "\n",
    "```python\n",
    "def geodesic_loss(q_pred, q_true):\n",
    "    inner = torch.abs(torch.sum(q_pred * q_true, dim=1))\n",
    "    return torch.mean(1 - inner)  # Or 2*arccos(inner) for angle\n",
    "```\n",
    "\n",
    "### 3. Full pose loss:\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, target):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], target[:, :3])\n",
    "    q_pred = pred[:, 3:] / pred[:, 3:].norm(dim=1, keepdim=True)\n",
    "    q_true = target[:, 3:] / target[:, 3:].norm(dim=1, keepdim=True)\n",
    "    r_loss = geodesic_loss(q_pred, q_true)\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Format        | Pros                         | Cons                                |\n",
    "| ------------- | ---------------------------- | ----------------------------------- |\n",
    "| Euler Angles  | Intuitive                    | Discontinuous, gimbal lock          |\n",
    "| Quaternions   | Continuous, robust           | Requires normalization              |\n",
    "| MSE Loss      | Easy, but weak for rotations | Not rotation-aware                  |\n",
    "| Geodesic Loss | Geometry-aware rotation loss | Slightly more expensive computation |\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to rewrite your `model` and `train.py` so it predicts quaternions and uses geodesic loss properly?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
