{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e65c52-eddb-4cea-a5f8-902b8ae0584b",
   "metadata": {},
   "source": [
    "## turn your classification-oriented ViT into a depth estimation model\n",
    "\n",
    "Great ‚Äî you're now thinking in exactly the right direction: **reusing a Vision Transformer (ViT) backbone for dense prediction**, like **monocular depth estimation**.\n",
    "\n",
    "To turn your classification-oriented ViT into a **depth estimation** model (i.e., pixel-wise regression), you'll need to modify **what you do with the output tokens**. Here‚Äôs a step-by-step guide:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ **What to change to do depth estimation with ViT**\n",
    "\n",
    "### ‚úÖ 1. **Keep all patch tokens, not just `[CLS]`**\n",
    "\n",
    "Currently:\n",
    "\n",
    "```python\n",
    "cls_out = x[:, 0]  # Only [CLS] token\n",
    "return self.mlp_head(cls_out)\n",
    "```\n",
    "\n",
    "Change to:\n",
    "\n",
    "```python\n",
    "patch_tokens = x[:, 1:, :]  # Drop CLS, keep only patches\n",
    "```\n",
    "\n",
    "We do this because:\n",
    "\n",
    "* Depth estimation is a **dense** task.\n",
    "* Each patch should predict depth for the corresponding region of the image.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **Reshape tokens into a 2D feature map**\n",
    "\n",
    "Assuming your image is split into `H x W` patches:\n",
    "\n",
    "```python\n",
    "# Suppose: x.shape = [B, N, D] with N = H * W\n",
    "H = W = int((x.size(1) - 1)**0.5)\n",
    "feat_map = patch_tokens.transpose(1, 2).reshape(B, D, H, W)  # [B, D, H, W]\n",
    "```\n",
    "\n",
    "Now you have a spatial feature map like a CNN would produce.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. **Use an upsampling head to get back to full resolution**\n",
    "\n",
    "You now need to go from `H x W` (e.g., 14x14) ‚Üí back to input resolution (e.g., 224x224).\n",
    "\n",
    "Add a **decoder** head:\n",
    "\n",
    "```python\n",
    "self.depth_head = nn.Sequential(\n",
    "    nn.ConvTranspose2d(D, D//2, kernel_size=2, stride=2),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(D//2, D//4, kernel_size=2, stride=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(D//4, 1, kernel_size=1)  # Output single-channel depth map\n",
    ")\n",
    "```\n",
    "\n",
    "In `forward()`:\n",
    "\n",
    "```python\n",
    "depth = self.depth_head(feat_map)  # [B, 1, H_out, W_out]\n",
    "```\n",
    "\n",
    "You may want to resize the output to match the original input size if needed:\n",
    "\n",
    "```python\n",
    "depth = F.interpolate(depth, size=(img_size, img_size), mode='bilinear')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. **Loss Function**\n",
    "\n",
    "Use a **regression loss**, e.g.:\n",
    "\n",
    "```python\n",
    "loss = F.l1_loss(predicted_depth, ground_truth_depth)\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "* `MSELoss`, or\n",
    "* `scale-invariant loss` (used in depth estimation literature)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary of Key Changes\n",
    "\n",
    "| Step              | Change Required                      |\n",
    "| ----------------- | ------------------------------------ |\n",
    "| Output tokens     | Use all patch tokens (`x[:, 1:, :]`) |\n",
    "| Spatial reshaping | Convert to `[B, D, H, W]`            |\n",
    "| Decoder head      | Transposed conv or upsampling layers |\n",
    "| Final output      | 1-channel image (depth map)          |\n",
    "| Loss function     | Regression (e.g., L1, MSE)           |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus Tips\n",
    "\n",
    "* For better accuracy, consider **skip connections** or **hierarchical ViT variants** (e.g., Swin Transformer).\n",
    "* Look into **MAE** or **DPT** (Dense Prediction Transformer by Intel) for more ideas.\n",
    "* Use pretrained ViT weights to improve performance.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5078f6-77e4-4ed7-b312-efc4b298d6ad",
   "metadata": {},
   "source": [
    "## What should be the data in the training set for this monocular depth estimation?\n",
    "\n",
    "Great question ‚Äî for **monocular depth estimation**, your training data needs to include:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. **Input**: RGB Image\n",
    "\n",
    "* Shape: `[3, H, W]`\n",
    "* Example: a regular color photo from a monocular camera.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. **Target**: Ground Truth Depth Map\n",
    "\n",
    "* Shape: `[1, H, W]` ‚Äî single channel.\n",
    "* Each pixel contains the depth value (e.g., in meters or normalized units).\n",
    "* Usually stored as:\n",
    "\n",
    "  * 16-bit grayscale images (`.png`, `.tiff`, etc.), or\n",
    "  * Floating-point `.npy` arrays.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Example dataset entry:\n",
    "\n",
    "| Item        | Format                  | Description                        |\n",
    "| ----------- | ----------------------- | ---------------------------------- |\n",
    "| `image.png` | `[3, 224, 224]` (float) | RGB image                          |\n",
    "| `depth.png` | `[1, 224, 224]` (float) | Ground-truth depth map (same size) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. **Normalization and Preprocessing**\n",
    "\n",
    "### For RGB image:\n",
    "\n",
    "* Convert to float, normalize to `[0, 1]` or ImageNet stats:\n",
    "\n",
    "  ```python\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "  ```\n",
    "\n",
    "### For depth map:\n",
    "\n",
    "* Convert to float32\n",
    "* Normalize optionally (depends on your loss function and depth range)\n",
    "\n",
    "  ```python\n",
    "  depth = depth / 1000.0  # if in millimeters ‚Üí meters\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. **Popular Monocular Depth Datasets**\n",
    "\n",
    "| Dataset          | Description                        | License / Notes                    |\n",
    "| ---------------- | ---------------------------------- | ---------------------------------- |\n",
    "| **NYU Depth V2** | Indoor scenes, Kinect RGB-D images | ‚úîÔ∏è Standard for indoor depth       |\n",
    "| **KITTI**        | Outdoor driving scenes (LiDAR)     | ‚úîÔ∏è Standard for autonomous driving |\n",
    "| **Make3D**       | Outdoor stills (Stanford)          | Older, smaller                     |\n",
    "| **DIML/CVT**     | Outdoor depth from stereo          | Large and high-resolution          |\n",
    "| **TUM RGB-D**    | Indoor SLAM dataset                | ‚úîÔ∏è Camera + depth                  |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. **Custom Dataset Structure Example**\n",
    "\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00001.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00002.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ depths/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00001.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 00002.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "Each `image` should match its corresponding `depth` map by filename.\n",
    "\n",
    "You can implement a PyTorch `Dataset` like this:\n",
    "\n",
    "```python\n",
    "class DepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, depth_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.depth_paths = depth_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        depth = Image.open(self.depth_paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        depth = torch.tensor(np.array(depth), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return img, depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary\n",
    "\n",
    "| Requirement     | Description                             |\n",
    "| --------------- | --------------------------------------- |\n",
    "| Input           | RGB image `[3, H, W]`                   |\n",
    "| Target          | Depth map `[1, H, W]`                   |\n",
    "| Same size?      | ‚úÖ Yes ‚Äî input and depth must be aligned |\n",
    "| Normalization   | Yes ‚Äî for both image and depth          |\n",
    "| Output of model | `[B, 1, H, W]`                          |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d6104-0a8b-4759-a4b8-547362fe0c0f",
   "metadata": {},
   "source": [
    "Creating a **Transformer-based model for monocular visual odometry (VO)** in PyTorch involves combining ideas from computer vision and sequence modeling. Here's a structured roadmap and a minimal working example to help you get started.\n",
    "\n",
    "---\n",
    "\n",
    "## üöó Goal\n",
    "\n",
    "Estimate camera **ego-motion (pose)** between consecutive frames from a **monocular RGB video** using a **Transformer-based deep neural network** in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. Overview of Architecture\n",
    "\n",
    "The basic idea is:\n",
    "\n",
    "```\n",
    "Input: Two consecutive frames (I_t, I_t+1)\n",
    "‚Üì\n",
    "Backbone CNN (e.g., ResNet, ViT) ‚Üí extract features\n",
    "‚Üì\n",
    "Feature Flattening + Positional Encoding\n",
    "‚Üì\n",
    "Transformer Encoder ‚Üí capture spatial-temporal relationships\n",
    "‚Üì\n",
    "Regression Head ‚Üí predict 6-DoF pose (3 translation + 3 rotation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± 2. Key Components\n",
    "\n",
    "### (a) **Image Pair Preprocessing**\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "### (b) **Backbone (e.g., ResNet18 or ViT)**\n",
    "\n",
    "For ViT-based features:\n",
    "\n",
    "```python\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "vit.heads = torch.nn.Identity()  # Remove classification head\n",
    "```\n",
    "\n",
    "Or use CNN (e.g., ResNet) for faster experimentation:\n",
    "\n",
    "```python\n",
    "from torchvision.models import resnet18\n",
    "resnet = resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "```\n",
    "\n",
    "### (c) **Transformer Encoder**\n",
    "\n",
    "```python\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "```\n",
    "\n",
    "### (d) **Pose Regression Head**\n",
    "\n",
    "```python\n",
    "class PoseHead(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 6)  # (x, y, z, roll, pitch, yaw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.mean(dim=1))  # mean over sequence tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 3. Full Model\n",
    "\n",
    "```python\n",
    "class MonoVOTransformer(torch.nn.Module):\n",
    "    def __init__(self, feature_extractor, transformer, pose_head):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        self.pose_head = pose_head\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        # img_pair: (B, 2, C, H, W)\n",
    "        B = img_pair.size(0)\n",
    "        imgs = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        feats = self.feature_extractor(imgs)           # (2B, F)\n",
    "\n",
    "        # Reshape to (B, 2, F)\n",
    "        feats = feats.view(B, 2, -1)\n",
    "        out = self.transformer(feats.permute(1, 0, 2))  # (seq_len=2, B, F)\n",
    "        out = out.permute(1, 0, 2)  # (B, seq_len, F)\n",
    "        return self.pose_head(out)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 4. Training Loop Skeleton\n",
    "\n",
    "```python\n",
    "model = MonoVOTransformer(vit, transformer, PoseHead(768))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for images, poses_gt in train_loader:\n",
    "    # images shape: (B, 2, 3, 224, 224)\n",
    "    pred_poses = model(images)\n",
    "    loss = criterion(pred_poses, poses_gt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìê 5. Pose Format\n",
    "\n",
    "* **Rotation** can be Euler angles (roll, pitch, yaw) or quaternions.\n",
    "* If using quaternions, normalize them and use **geodesic loss** or **L2** with a normalization constraint.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Optional: Loss with Scale-Invariant or Geometric Components\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, gt):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], gt[:, :3])\n",
    "    r_loss = torch.nn.functional.mse_loss(pred[:, 3:], gt[:, 3:])\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 6. Dataset Format\n",
    "\n",
    "Prepare your dataset to return:\n",
    "\n",
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    img1 = load_image(idx)\n",
    "    img2 = load_image(idx + 1)\n",
    "    pose_delta = relative_pose(gt_pose[idx], gt_pose[idx+1])  # 6-DoF\n",
    "    return torch.stack([transform(img1), transform(img2)]), torch.tensor(pose_delta)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b01ba8-1ae0-4b96-9b24-9b51e8916fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd763f9-f810-4d52-8678-17a694e1c29b",
   "metadata": {},
   "source": [
    "Here‚Äôs a complete and modular PyTorch training script for monocular visual odometry using a Transformer-based model, with compatibility for KITTI or TUM datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Project Structure\n",
    "\n",
    "```\n",
    "mono_vo_transformer/\n",
    "‚îú‚îÄ‚îÄ dataset/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ kitti_dataset.py\n",
    "‚îú‚îÄ‚îÄ model/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ transformer_vo.py\n",
    "‚îú‚îÄ‚îÄ train.py\n",
    "‚îú‚îÄ‚îÄ utils.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. `dataset/kitti_dataset.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_poses(file_path):\n",
    "    poses = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = list(map(float, line.strip().split()))\n",
    "            matrix = np.array(values).reshape(3, 4)\n",
    "            poses.append(matrix)\n",
    "    return poses\n",
    "\n",
    "def relative_pose(p1, p2):\n",
    "    R1, t1 = p1[:, :3], p1[:, 3]\n",
    "    R2, t2 = p2[:, :3], p2[:, 3]\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = t2 - R_rel @ t1\n",
    "    # Convert rotation to Euler\n",
    "    yaw = np.arctan2(R_rel[1, 0], R_rel[0, 0])\n",
    "    pitch = np.arcsin(-R_rel[2, 0])\n",
    "    roll = np.arctan2(R_rel[2, 1], R_rel[2, 2])\n",
    "    return np.hstack((t_rel, [roll, pitch, yaw]))\n",
    "\n",
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, image_dir, pose_file):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "        self.poses = load_poses(pose_file)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = self.transform(Image.open(self.image_paths[idx]))\n",
    "        img2 = self.transform(Image.open(self.image_paths[idx + 1]))\n",
    "        pose1 = self.poses[idx]\n",
    "        pose2 = self.poses[idx + 1]\n",
    "        rel_pose = relative_pose(pose1, pose2)\n",
    "        return torch.stack([img1, img2]), torch.tensor(rel_pose, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. `model/transformer_vo.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class PoseHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.mean(dim=1))  # (B, 6)\n",
    "\n",
    "class MonoVOTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        self.feature_extractor.heads = nn.Identity()  # Remove classifier\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.pose_head = PoseHead(768)\n",
    "\n",
    "    def forward(self, img_pair):\n",
    "        B = img_pair.size(0)\n",
    "        x = img_pair.view(-1, *img_pair.shape[2:])  # (2B, C, H, W)\n",
    "        features = self.feature_extractor(x)         # (2B, 768)\n",
    "        features = features.view(B, 2, -1).permute(1, 0, 2)  # (2, B, 768)\n",
    "        encoded = self.transformer(features).permute(1, 0, 2)  # (B, 2, 768)\n",
    "        return self.pose_head(encoded)  # (B, 6)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è 3. `train.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.kitti_dataset import KITTIDataset\n",
    "from model.transformer_vo import MonoVOTransformer\n",
    "import os\n",
    "\n",
    "# --- Config ---\n",
    "image_dir = \"/path/to/kitti/images/\"\n",
    "pose_file = \"/path/to/kitti/poses.txt\"\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Data ---\n",
    "dataset = KITTIDataset(image_dir, pose_file)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# --- Model ---\n",
    "model = MonoVOTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img_pair, target_pose in loader:\n",
    "        img_pair = img_pair.to(device)\n",
    "        target_pose = target_pose.to(device)\n",
    "        pred_pose = model(img_pair)\n",
    "        loss = criterion(pred_pose, target_pose)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * img_pair.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"vo_transformer.pth\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notes\n",
    "\n",
    "* **KITTI poses** are in 3x4 format: `[R | t]`.\n",
    "* If using **TUM RGB-D**, similar logic applies; just adapt image loading and timestamp alignment.\n",
    "* This baseline does not include:\n",
    "\n",
    "  * Scale correction\n",
    "  * Rotation with quaternions\n",
    "  * Photometric or depth-based geometric loss\n",
    "  * Multi-frame sequences (e.g., window of 5)\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* inference + trajectory plotting code?\n",
    "* integration with tensorboard/wandb?\n",
    "* extension to 5-frame windows instead of just 2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa7dfb-b44a-4a83-857f-ceefc7483262",
   "metadata": {},
   "source": [
    "The line:\n",
    "\n",
    "```python\n",
    "self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "```\n",
    "\n",
    "does the following:\n",
    "\n",
    "* **`os.listdir(image_dir)`**: lists all files in the folder.\n",
    "* **`f.endswith(\".png\")`**: filters to keep only `.png` files.\n",
    "* **`os.path.join(image_dir, f)`**: builds the full path to each `.png` file.\n",
    "* **`sorted(...)`**: ensures the list is in order (which is important for consistent frame-to-frame pairing).\n",
    "\n",
    "### ‚úÖ Here's a simplified, multi-line version:\n",
    "\n",
    "```python\n",
    "all_files = os.listdir(image_dir)\n",
    "\n",
    "png_files = []\n",
    "for f in all_files:\n",
    "    if f.endswith(\".png\"):\n",
    "        full_path = os.path.join(image_dir, f)\n",
    "        png_files.append(full_path)\n",
    "\n",
    "self.image_paths = sorted(png_files)\n",
    "```\n",
    "\n",
    "This version is easier to read and debug, especially when you're first building or modifying datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908d464-c4e5-4a0a-b8d9-af2782b35473",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ Here's why I used `Adam` by default:\n",
    "\n",
    "The original `Adam` optimizer is a common default for most PyTorch training tasks. It performs adaptive learning rate updates and works well for a wide range of problems ‚Äî especially in vision and sequence modeling. But...\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ Why **`Adam` might be suboptimal** in some Transformer models:\n",
    "\n",
    "* `Adam` applies **L2 regularization via weight decay** naively by adding it to the gradients.\n",
    "* This clashes with the **internal normalization mechanisms** in Transformers (e.g., LayerNorm, biases), which ideally should not be regularized.\n",
    "* Regularizing **all weights equally** can degrade performance in Transformer-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why **`AdamW` is better for Transformers**\n",
    "\n",
    "`AdamW` (decoupled weight decay) fixes this by **decoupling weight decay from gradient updates**, allowing:\n",
    "\n",
    "* Better handling of weight decay (especially on linear layers only).\n",
    "* Empirically better generalization for **Transformer-based architectures** (used in BERT, ViT, etc.).\n",
    "\n",
    "```python\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† When to prefer:\n",
    "\n",
    "| Scenario                      | Use                   |\n",
    "| ----------------------------- | --------------------- |\n",
    "| Transformer / ViT models      | ‚úÖ `AdamW`             |\n",
    "| CNNs or simpler MLPs          | ‚úÖ `Adam` (or `AdamW`) |\n",
    "| Want finer control over decay | ‚úÖ `AdamW`             |\n",
    "\n",
    "---\n",
    "\n",
    "If you're using a **Transformer backbone** like ViT for monocular visual odometry, then switching to `AdamW` is a better default.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca65c39-ab43-4893-bab6-5dcfb10a0e64",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üéØ What We're Predicting\n",
    "\n",
    "In monocular visual odometry, the model estimates:\n",
    "\n",
    "* **Translation**: a 3D vector `t = (x, y, z)`\n",
    "* **Rotation**: either as:\n",
    "\n",
    "  * **Euler angles** (roll, pitch, yaw) ‚Äî used in the current example\n",
    "  * **Quaternions** `q = (qx, qy, qz, qw)` ‚Äî often preferred in practice\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Limitations of `torch.nn.MSELoss()` on Euler Angles\n",
    "\n",
    "Using `MSELoss()` on Euler angles has issues:\n",
    "\n",
    "### 1. **Periodicity problem**\n",
    "\n",
    "* Angles like `Œ∏ = 179¬∞` and `Œ∏ = -179¬∞` are almost identical in 3D rotation but `MSE(179, -179)` is huge.\n",
    "\n",
    "### 2. **Gimbal lock**\n",
    "\n",
    "* Euler angles can suffer from **singularities** when converting between rotation representations.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Quaternions + Geodesic Loss are Better\n",
    "\n",
    "### Quaternions:\n",
    "\n",
    "* Compact, continuous, and avoid gimbal lock.\n",
    "* Represent rotations over the 3-sphere.\n",
    "* Require normalization (unit quaternions).\n",
    "\n",
    "### Geodesic loss (angular distance):\n",
    "\n",
    "Let `q1`, `q2` be unit quaternions:\n",
    "\n",
    "```python\n",
    "loss = 1 - |‚ü®q1, q2‚ü©|\n",
    "```\n",
    "\n",
    "or use:\n",
    "\n",
    "```python\n",
    "Œ∏ = 2 * arccos(|‚ü®q1, q2‚ü©|)\n",
    "```\n",
    "\n",
    "for angular error in radians.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recommended Approach\n",
    "\n",
    "### 1. Predict `[t_x, t_y, t_z, qx, qy, qz, qw]`\n",
    "\n",
    "Normalize the quaternion output before loss:\n",
    "\n",
    "```python\n",
    "q_pred = pred[:, 3:]\n",
    "q_pred = q_pred / q_pred.norm(dim=1, keepdim=True)\n",
    "```\n",
    "\n",
    "### 2. Geodesic loss:\n",
    "\n",
    "```python\n",
    "def geodesic_loss(q_pred, q_true):\n",
    "    inner = torch.abs(torch.sum(q_pred * q_true, dim=1))\n",
    "    return torch.mean(1 - inner)  # Or 2*arccos(inner) for angle\n",
    "```\n",
    "\n",
    "### 3. Full pose loss:\n",
    "\n",
    "```python\n",
    "def pose_loss(pred, target):\n",
    "    t_loss = torch.nn.functional.mse_loss(pred[:, :3], target[:, :3])\n",
    "    q_pred = pred[:, 3:] / pred[:, 3:].norm(dim=1, keepdim=True)\n",
    "    q_true = target[:, 3:] / target[:, 3:].norm(dim=1, keepdim=True)\n",
    "    r_loss = geodesic_loss(q_pred, q_true)\n",
    "    return t_loss + r_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Format        | Pros                         | Cons                                |\n",
    "| ------------- | ---------------------------- | ----------------------------------- |\n",
    "| Euler Angles  | Intuitive                    | Discontinuous, gimbal lock          |\n",
    "| Quaternions   | Continuous, robust           | Requires normalization              |\n",
    "| MSE Loss      | Easy, but weak for rotations | Not rotation-aware                  |\n",
    "| Geodesic Loss | Geometry-aware rotation loss | Slightly more expensive computation |\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to rewrite your `model` and `train.py` so it predicts quaternions and uses geodesic loss properly?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
