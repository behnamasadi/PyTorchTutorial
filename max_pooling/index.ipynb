{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f63c9f8-c573-436e-adc6-87c42ac684e1",
   "metadata": {},
   "source": [
    "#  What is Max Pooling?\n",
    "Max pooling is a key concept in **deep learning**, especially in **Convolutional Neural Networks (CNNs)** used for image processing and computer vision. \n",
    "\n",
    "\n",
    "**Max pooling** is a **downsampling** operation that reduces the spatial dimensions (width and height) of an input feature map while retaining the most important information.\n",
    "\n",
    "It works by sliding a window (typically 2×2) over the input and **taking the maximum value** in each region.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you have a 4×4 feature map:\n",
    "\n",
    "```\n",
    "1  3  2  4  \n",
    "5  6  1  2  \n",
    "3  2  0  1  \n",
    "1  2  4  3  \n",
    "```\n",
    "\n",
    "Applying 2×2 max pooling with stride 2 gives:\n",
    "\n",
    "```\n",
    "6  4  \n",
    "3  4  \n",
    "```\n",
    "\n",
    "We took the **maximum** from each 2×2 block:\n",
    "- max(1, 3, 5, 6) = 6\n",
    "- max(2, 4, 1, 2) = 4\n",
    "- etc.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Do We Use Max Pooling?\n",
    "\n",
    "1. **Dimensionality Reduction**\n",
    "   - Reduces the number of computations in later layers.\n",
    "   - Helps with overfitting by summarizing regions.\n",
    "\n",
    "2. **Translation Invariance**\n",
    "   - Small shifts or movements in the image don’t change the pooled value.\n",
    "   - Useful for recognizing features regardless of their exact position.\n",
    "\n",
    "3. **Highlighting Strong Features**\n",
    "   - Max pooling keeps only the **strongest activation** (most important signal) in each region.\n",
    "\n",
    "---\n",
    "\n",
    "###  Common Pooling Types\n",
    "\n",
    "| Type           | What it does                         |\n",
    "|----------------|--------------------------------------|\n",
    "| Max Pooling    | Takes the **maximum** value          |\n",
    "| Average Pooling| Takes the **average** value          |\n",
    "| Global Pooling | Takes max/average over entire map    |\n",
    "\n",
    "---\n",
    "\n",
    "###  Typical Parameters\n",
    "\n",
    "- **Kernel Size**: Size of the window (e.g., 2×2)\n",
    "- **Stride**: How far the window moves (e.g., 2 skips every other pixel)\n",
    "- **Padding**: Whether to pad the input to keep the same size (usually not used in pooling)\n",
    "\n",
    "---\n",
    "\n",
    "###  Is Max Pooling Always Good?\n",
    "\n",
    "- **Pros**:\n",
    "  - Reduces memory and computation\n",
    "  - Adds robustness to small changes\n",
    "  - Helps generalization\n",
    "\n",
    "- **Cons**:\n",
    "  - Can lose spatial precision\n",
    "  - Not learnable (fixed operation)\n",
    "\n",
    "---\n",
    "\n",
    "###  Alternatives to Max Pooling\n",
    "\n",
    "- **Strided Convolutions**: Learnable and can replace pooling\n",
    "- **Global Average Pooling**: Often used before fully connected layers\n",
    "- **Attention Mechanisms**: Learn what to focus on instead of blindly pooling\n",
    "\n",
    "---\n",
    "\n",
    "###  Intuition\n",
    "\n",
    "Imagine scanning a patch of an image: max pooling keeps **only the strongest signal** (like the brightest pixel or most confident feature), making the model focus on **what matters most** while ignoring noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf0747-c107-4705-8c59-30fbb5a3a2e4",
   "metadata": {},
   "source": [
    "# The Order of Relu and Max Pooling \n",
    "\n",
    "\n",
    "The order **does matter**, and typically we use **ReLU → MaxPooling (most common)**, not the other way around.\n",
    "\n",
    "\n",
    "With **ReLU** and **MaxPool**, the **forward result cannot differ**—for any pooling window $S$,\n",
    "\n",
    "$$\n",
    "\\max(\\operatorname{ReLU}(S)) \\;=\\; \\operatorname{ReLU}(\\max(S)).\n",
    "$$\n",
    "\n",
    "ReLU is monotone, and max is monotone, so the two orders give the same pooled value.\n",
    "\n",
    "But you *can* get **different arg-max indices** (and thus different backprop routes / unpooling behavior) even though the numeric output matches. Here’s a concrete numeric case:\n",
    "\n",
    "### Example (2×2 window)\n",
    "\n",
    "Let the conv output in a pooling window be\n",
    "\n",
    "$$\n",
    "W=\\begin{bmatrix}\n",
    "-4 & -4\\\\\n",
    "-4 & \\mathbf{-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Order A: Conv → ReLU → MaxPool**\n",
    "  After ReLU:\n",
    "\n",
    "  $$\n",
    "  \\operatorname{ReLU}(W)=\\begin{bmatrix}\n",
    "  0 & 0\\\\\n",
    "  0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  All entries tie at 0. Many libraries break ties by picking the **first** index (e.g., top-left).\n",
    "  **Pooled value:** $0$. **Index chosen:** (top-left).\n",
    "\n",
    "* **Order B: Conv → MaxPool → ReLU**\n",
    "  MaxPool (pre-ReLU) picks the **least negative** (i.e., the maximum) which is $-1$ at **bottom-right**.\n",
    "  After ReLU: $\\operatorname{ReLU}(-1)=0$.\n",
    "  **Pooled value:** $0$. **Index chosen:** (bottom-right).\n",
    "\n",
    "So:\n",
    "\n",
    "* **Forward pooled value** is $0$ in both orders (identical).\n",
    "* **Selected index differs** (top-left vs bottom-right).\n",
    "  This can change **which spatial location receives gradient** (or the stored indices used by MaxUnpool), even though the scalar output is the same. (In this specific all-nonpositive case, the gradient magnitude still becomes 0 due to ReLU, but the *index* recorded by pooling differs.)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Learnable parameters\n",
    "\n",
    "* **Conv layers** have learnable parameters:\n",
    "\n",
    "  * **weights** (the filter kernels)\n",
    "  * **biases** (if enabled)\n",
    "* **ReLU** has no parameters (it’s just a fixed non-linearity).\n",
    "* **MaxPool** has no parameters either — it’s just an operation that selects the maximum in each window.\n",
    "\n",
    " So the *only* learnable parameters are in the convolution. Pooling never has learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### What MaxPool does “remember”\n",
    "\n",
    "MaxPool does not learn, but it **remembers the index of the max element** in each pooling window during the forward pass.\n",
    "\n",
    "* In the **backward pass**, the gradient is sent only to that max location, all other positions get zero gradient.\n",
    "* This is what we saw in the PyTorch experiment: the gradient routes depend on which index was picked.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why order matters\n",
    "\n",
    "Even though forward outputs from `Conv→ReLU→MaxPool` and `Conv→MaxPool→ReLU` are the same, the **chosen indices can differ** (especially in windows with negatives).\n",
    "\n",
    "That means:\n",
    "\n",
    "* **Gradient routing can differ**:\n",
    "\n",
    "  * `Conv→ReLU→MaxPool`: negatives are zeroed first, so only positive activations can receive gradient.\n",
    "  * `Conv→MaxPool→ReLU`: MaxPool might pick a negative as the “max” (if all are negative). After ReLU, the output becomes 0, but the index is still recorded → during backprop, the gradient will flow to that negative conv output before being squashed.\n",
    "* This affects how the optimizer updates the conv **weights**, since gradients are computed with respect to those weights.\n",
    "\n",
    "So, **the difference is not because MaxPool has learnable parameters** (it doesn’t).\n",
    "The difference is in **which conv weights get updated**, because gradient flow is determined by the pooling index selection.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "* The learnable parameters are **conv weights & biases only**.\n",
    "* Order matters because **gradient paths differ** due to how MaxPool selects indices **before or after ReLU**.\n",
    "* Over many updates, this can slightly change how the network learns — which is why almost all architectures standardize on **Conv → ReLU → MaxPool**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81e37e-5af6-4777-9059-003d44899eb9",
   "metadata": {},
   "source": [
    "a **step-by-step backprop** with tiny 2×2 windows to show exactly what happens. We’ll use a single conv feature map (so we can ignore multi-channel complications), a **2×2 MaxPool** (so it reduces to a single scalar), and the loss $L$ is just the pooled output (so $\\partial L/\\partial(\\text{pooled})=1$).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) All-negative window → gradients die in both orders\n",
    "\n",
    "Let the **conv output** (pre-ReLU) be\n",
    "\n",
    "$$\n",
    "Z=\\begin{bmatrix}\n",
    "-4 & -2\\\\\n",
    "-3 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Order A: Conv → ReLU → MaxPool**\n",
    "\n",
    "1. **ReLU**: $A=\\operatorname{ReLU}(Z)=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}$\n",
    "2. **MaxPool** over $A$: pooled value $y = \\max(A)=0$.\n",
    "   (There’s a tie; suppose the pool **stores** top-left index, $(0,0)$, by convention.)\n",
    "3. **Loss**: $L = y$ ⇒ $\\frac{\\partial L}{\\partial y}=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* Through MaxPool: gradient goes to the stored index in $A$:\n",
    "  $\\frac{\\partial L}{\\partial A}=\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}$.\n",
    "* Through ReLU: $A=\\operatorname{ReLU}(Z)\\Rightarrow \\operatorname{ReLU}'(Z)=0$ element-wise (since all $Z\\le 0$).\n",
    "  $\\frac{\\partial L}{\\partial Z}=\\frac{\\partial L}{\\partial A}\\odot \\operatorname{ReLU}'(Z)=\\mathbf{0}$.\n",
    "\n",
    "**Result:** $\\frac{\\partial L}{\\partial Z}=0$ everywhere ⇒ **no weight updates** from this window.\n",
    "\n",
    "#### Order B: Conv → MaxPool → ReLU\n",
    "\n",
    "1. **MaxPool** over $Z$: pooled pre-ReLU value $y'=\\max(Z)=-1$ at index $(1,1)$ (bottom-right).\n",
    "2. **ReLU**: $y=\\operatorname{ReLU}(y')=\\operatorname{ReLU}(-1)=0$.\n",
    "3. **Loss**: $L=y$ ⇒ $\\frac{\\partial L}{\\partial y}=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* Through ReLU at $y'=-1$: $\\operatorname{ReLU}'(y')=0$ ⇒ $\\frac{\\partial L}{\\partial y'}=0$.\n",
    "* Through MaxPool: the gradient to $Z$ at the max index is $0$, others $0$ too.\n",
    "  $\\frac{\\partial L}{\\partial Z}=\\mathbf{0}$.\n",
    "\n",
    "**Result:** again **no weight updates**.\n",
    "\n",
    "**Conclusion (all-negative case):** Picking $-1$ (Order B) vs a top-left tie at 0 (Order A) **does not help** — ReLU’s derivative is 0 at negatives, so gradients die either way.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Mixed-sign window → gradients flow (and are the same)\n",
    "\n",
    "Now let\n",
    "\n",
    "$$\n",
    "Z=\\begin{bmatrix}\n",
    "-4 & \\mathbf{2}\\\\\n",
    "1 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Order A (ReLU first):** $A=\\begin{bmatrix}0&2\\\\1&0\\end{bmatrix}$, MaxPool picks $2$ at $(0,1)$.\n",
    "* **Order B (Pool first):** Max of $Z$ is $2$ at $(0,1)$; ReLU keeps it $2$.\n",
    "\n",
    "Both orders: $y=2$, $L=y\\Rightarrow \\partial L/\\partial y=1$.\n",
    "\n",
    "**Backward:**\n",
    "\n",
    "* The pool **stores the same index** $(0,1)$.\n",
    "* ReLU derivative at that location is **1** (since $Z_{0,1}=2>0$).\n",
    "* So $\\frac{\\partial L}{\\partial Z}$ is **1 at $(0,1)$** and **0 elsewhere** — **for both orders**.\n",
    "\n",
    "**Result:** **Same forward, same gradient flow** when there’s a strictly positive max.\n",
    "\n",
    "---\n",
    "\n",
    "#### Takeaways\n",
    "\n",
    "* **MaxPool has no learnable parameters.** It only stores **indices** of maxima.\n",
    "* With **all negatives**, both orders yield **zero gradient** due to $\\operatorname{ReLU}'=0$ at negatives.\n",
    "* With **positives present**, both orders pick the same (strict) max and route **the same gradient**.\n",
    "* Differences can occur in **stored indices** for the all-nonpositive case (tie vs least negative), but **gradients still end up zero**.\n",
    "* That’s why the community standardizes on **Conv → ReLU → MaxPool**: cleaner semantics (pool only over active features) without risking odd index choices that don’t help learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
