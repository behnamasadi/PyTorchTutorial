{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1fa753-88f5-49c8-90d2-66e3b931940f",
   "metadata": {},
   "source": [
    "The **Universal Approximation Theorem (UAT)** is a foundational result in neural network theory. It states that a sufficiently large neural network can approximate **any continuous function** to **any desired accuracy**, under certain conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Intuitive idea\n",
    "\n",
    "A neural network with **at least one hidden layer** and **nonlinear activation functions** can represent almost any mapping between inputs and outputs.\n",
    "In other words, if you give it enough neurons, it can approximate any function\n",
    "$$\n",
    "f: \\mathbb{R}^n \\to \\mathbb{R}^m\n",
    "$$\n",
    "as closely as you want.\n",
    "\n",
    "Think of the neurons as *building blocks* — by combining many simple nonlinear transformations, you can shape them to approximate complex curves or surfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The formal statement\n",
    "\n",
    "One of the classical formulations (Cybenko, 1989) says:\n",
    "\n",
    "Let $\\sigma$ be a continuous, bounded, and non-constant activation function.\n",
    "Then, for any continuous function $f$ defined on a compact subset $K \\subset \\mathbb{R}^n$,\n",
    "and for any $\\varepsilon > 0$,\n",
    "there exists a neural network of the form\n",
    "$$\n",
    "F(x) = \\sum_{i=1}^{N} \\alpha_i , \\sigma(w_i^T x + b_i)\n",
    "$$\n",
    "such that\n",
    "$$\n",
    "|F(x) - f(x)| < \\varepsilon, \\quad \\forall x \\in K\n",
    "$$\n",
    " i.e. $F$ approximates $f$ uniformly on $K$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What this means\n",
    "\n",
    "* **Single hidden layer is enough** (in theory).\n",
    "  You don’t need a deep network to approximate any function — one hidden layer with enough neurons can do it.\n",
    "\n",
    "* **The activation function must be nonlinear.**\n",
    "  If $\\sigma$ is linear (e.g., $\\sigma(x)=x$), the network can only represent linear functions — not universal.\n",
    "\n",
    "* **\"Approximation\" means closeness, not exact equality.**\n",
    "  The theorem guarantees the *existence* of weights and biases that approximate $f$, not that training will find them efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example: intuition in 1D\n",
    "\n",
    "Suppose you want to approximate a function $f(x)$ on $[0, 1]$.\n",
    "\n",
    "Each neuron $\\sigma(w_i x + b_i)$ is like a *bump* or *step* (depending on $\\sigma$).\n",
    "By summing many of them, you can construct a function that follows the shape of $f(x)$ very closely — much like approximating a curve with many small rectangles or triangles.\n",
    "\n",
    "For example, with ReLU activations:\n",
    "$$\n",
    "F(x) = \\sum_i \\alpha_i \\max(0, w_i x + b_i)\n",
    "$$\n",
    "forms a **piecewise-linear** approximation of $f(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Variants and extensions\n",
    "\n",
    "* **Hornik (1991):** generalized the theorem — the specific form of $\\sigma$ doesn’t matter much; what matters is that it’s nonlinear and measurable.\n",
    "* **ReLU networks:** though ReLU is unbounded, they also satisfy UAT on compact domains.\n",
    "* **Deep networks:** multiple hidden layers don’t increase the *theoretical expressiveness*, but they dramatically improve **efficiency** (fewer neurons needed for the same approximation).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key takeaway\n",
    "\n",
    "✅ **Universal Approximation Theorem:**\n",
    "A feedforward neural network with at least one hidden layer and a nonlinear activation function can approximate any continuous function on a compact domain to arbitrary precision.\n",
    "\n",
    "But:\n",
    "\n",
    "❌ It says nothing about:\n",
    "\n",
    "* how *efficiently* the approximation can be achieved,\n",
    "* whether *training* will find such parameters,\n",
    "* or how *generalizable* the model will be.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9364d-7ca2-428e-a227-967b55c7a97a",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=qx7hirqgfuU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
