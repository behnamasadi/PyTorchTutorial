{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f58e6d3-3c29-484b-bc78-57b3dd8bf42d",
   "metadata": {},
   "source": [
    "##  1. Shapley Value\n",
    "\n",
    "The **Shapley value** comes from cooperative game theory.\n",
    "It is a fair way to distribute the \"payout\" (or contribution) among players depending on how much each one contributes to the group.\n",
    "\n",
    "* We have a set of players (features, workers, companies…).\n",
    "* They can cooperate in groups (called **coalitions**).\n",
    "* Each coalition has a \"value\" (e.g., revenue, accuracy, utility).\n",
    "* The **Shapley value** tells us:\n",
    "   If everyone works together, how much of the total value should each player get, based on their **average marginal contribution** across all possible coalition orders.\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Formula\n",
    "\n",
    "For player $i$:\n",
    "\n",
    "$$\n",
    "\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} \\; \\big[ v(S \\cup \\{i\\}) - v(S) \\big]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "#### 1. $N$ \n",
    "the set of **all players** (e.g. $N=\\{A,B,C\\}$).\n",
    "#### 2. $\\{i\\}$ \n",
    "the set containing just the player we’re focusing on (e.g. $\\{A\\}$).\n",
    "#### 3. $N \\setminus \\{i\\}$ \n",
    "“all players except $i$”  example  $N \\setminus \\{A\\} = \\{B,C\\}$.\n",
    "    \n",
    "\n",
    "\n",
    "#### 4. $S \\subseteq N \\setminus \\{i\\}$ \n",
    "a subset $S$ taken from that set of “other players.”\n",
    "    * Now, $S$ must be a subset of $\\{B,C\\}$.\n",
    "      So the possible $S$ are:\n",
    "$$\n",
    "\\varnothing, \\{B\\}, \\{C\\}, \\{B,C\\}\n",
    "$$\n",
    "#### 5. $v(S)$ \n",
    "value of coalition $S$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6. $v(S)$ \n",
    "the value of coalition *before* player $i$ joined.\n",
    "\n",
    "#### 7. $v(S \\cup \\{i\\})$ \n",
    "the value *after* player $i$ joined.\n",
    "\n",
    "So if we already allowed $S$ to include $i$, that difference would always be zero.\n",
    "\n",
    "\n",
    "\n",
    "#### 8. $\\frac{|S|!(|N|-|S|-1)!}{|N|!}$ \n",
    "**weight** that tells us how often this coalition size appears across all permutations\n",
    "\n",
    "\n",
    "Think of the **permutation definition**:\n",
    "\n",
    "* The Shapley value of player $i$ is their **average marginal contribution across all possible orders of players**.\n",
    "* There are $|N|!$ total permutations (ways to order all players).\n",
    "* For a given subset $S$, how many permutations put exactly the set $S$ before player $i$, and the rest after?\n",
    "\n",
    "\n",
    "\n",
    "**Step 1: Fix a subset $S$ before $i$**\n",
    "\n",
    "Suppose we want player $i$ to appear in an order such that:\n",
    "\n",
    "* All players in $S$ come before $i$.\n",
    "* All players not in $S \\cup \\{i\\}$ come after $i$.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Count permutations with that structure**\n",
    "\n",
    "1. The players in $S$ can be arranged among themselves in $|S|!$ ways.\n",
    "2. The players after $i$ (the remaining $|N|-|S|-1$) can be arranged in $(|N|-|S|-1)!$ ways.\n",
    "3. Player $i$ is fixed in the middle (between $S$ and the rest).\n",
    "\n",
    "So the total number of permutations where exactly $S$ comes before $i$ is:\n",
    "\n",
    "$$\n",
    "|S|!\\,(|N|-|S|-1)!\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Convert to a probability (weight)**\n",
    "\n",
    "Since there are $|N|!$ total permutations, the **probability (weight)** of seeing $S$ just before $i$ is:\n",
    "\n",
    "$$\n",
    "w(S) = \\frac{|S|!\\,(|N|-|S|-1)!}{|N|!}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d535bbd-ae13-4ee7-91b4-d5c32ea7cce5",
   "metadata": {},
   "source": [
    "## 3. Example: 3 Players\n",
    "\n",
    "Imagine **3 friends** (A, B, C) working on a project.\n",
    "They generate revenue depending on who works together:\n",
    "\n",
    "| Coalition   | Value $v(S)$ |\n",
    "| ----------- | ------------ |\n",
    "| {} (nobody) | 0            |\n",
    "| {A}         | 100          |\n",
    "| {B}         | 80           |\n",
    "| {C}         | 60           |\n",
    "| {A,B}       | 180          |\n",
    "| {A,C}       | 160          |\n",
    "| {B,C}       | 140          |\n",
    "| {A,B,C}     | 200          |\n",
    "\n",
    "The question: **How should the total 200 be fairly divided?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15113abf-f14e-49cd-a89c-9a25b7b26252",
   "metadata": {},
   "source": [
    "#### Shapley formula for $i=A$ and $i=B$.\n",
    "\n",
    "$$\n",
    "\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} \\; \\big[ v(S \\cup \\{i\\}) - v(S) \\big]\n",
    "$$\n",
    "\n",
    "\n",
    "We have $N=\\{A,B,C\\}$ (so $|N|=3$).\n",
    "Coalition values:\n",
    "\n",
    "* $v(\\varnothing)=0,\\; v(\\{A\\})=100,\\; v(\\{B\\})=80,\\; v(\\{C\\})=60$\n",
    "* $v(\\{A,B\\})=180,\\; v(\\{A,C\\})=160,\\; v(\\{B,C\\})=140,\\; v(\\{A,B,C\\})=200$\n",
    "\n",
    "Weight for each subset $S\\subseteq N\\setminus\\{i\\}$:\n",
    "\n",
    "$$\n",
    "w(S)=\\frac{|S|!(|N|-|S|-1)!}{|N|!}\n",
    "=\\begin{cases}\n",
    "\\frac{1}{3}, & |S|=0 \\text{ or } 2\\\\[2pt]\n",
    "\\frac{1}{6}, & |S|=1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### For $i=A$\n",
    "\n",
    "Subsets $S\\subseteq\\{B,C\\}$: $\\varnothing,\\{B\\},\\{C\\},\\{B,C\\}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi_A\n",
    "&=\\tfrac{1}{3}\\big[v(\\{A\\})-v(\\varnothing)\\big]\n",
    "+\\tfrac{1}{6}\\big[v(\\{A,B\\})-v(\\{B\\})\\big]\\\\\n",
    "&\\quad+\\tfrac{1}{6}\\big[v(\\{A,C\\})-v(\\{C\\})\\big]\n",
    "+\\tfrac{1}{3}\\big[v(\\{A,B,C\\})-v(\\{B,C\\})\\big]\\\\[6pt]\n",
    "&=\\tfrac{1}{3}(100-0)+\\tfrac{1}{6}(180-80)\n",
    "+\\tfrac{1}{6}(160-60)+\\tfrac{1}{3}(200-140)\\\\[6pt]\n",
    "&=\\tfrac{1}{3}\\cdot100+\\tfrac{1}{6}\\cdot100+\\tfrac{1}{6}\\cdot100+\\tfrac{1}{3}\\cdot60\\\\[6pt]\n",
    "&=100\\!\\left(\\tfrac{1}{3}+\\tfrac{1}{6}+\\tfrac{1}{6}\\right)+60\\!\\left(\\tfrac{1}{3}\\right)\n",
    "=100\\!\\left(\\tfrac{2}{3}\\right)+60\\!\\left(\\tfrac{1}{3}\\right)\\\\[4pt]\n",
    "&=\\tfrac{200}{3}+\\tfrac{60}{3}=\\tfrac{260}{3}=86.\\overline{6}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Result:** $\\boxed{\\phi_A = 86.\\overline{6}}$\n",
    "\n",
    "#### For $i=B$\n",
    "\n",
    "Subsets $S\\subseteq\\{A,C\\}$: $\\varnothing,\\{A\\},\\{C\\},\\{A,C\\}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi_B\n",
    "&=\\tfrac{1}{3}\\big[v(\\{B\\})-v(\\varnothing)\\big]\n",
    "+\\tfrac{1}{6}\\big[v(\\{A,B\\})-v(\\{A\\})\\big]\\\\\n",
    "&\\quad+\\tfrac{1}{6}\\big[v(\\{B,C\\})-v(\\{C\\})\\big]\n",
    "+\\tfrac{1}{3}\\big[v(\\{A,B,C\\})-v(\\{A,C\\})\\big]\\\\[6pt]\n",
    "&=\\tfrac{1}{3}(80-0)+\\tfrac{1}{6}(180-100)\n",
    "+\\tfrac{1}{6}(140-60)+\\tfrac{1}{3}(200-160)\\\\[6pt]\n",
    "&=\\tfrac{1}{3}\\cdot80+\\tfrac{1}{6}\\cdot80+\\tfrac{1}{6}\\cdot80+\\tfrac{1}{3}\\cdot40\\\\[6pt]\n",
    "&=80\\!\\left(\\tfrac{1}{3}+\\tfrac{1}{6}+\\tfrac{1}{6}\\right)+40\\!\\left(\\tfrac{1}{3}\\right)\n",
    "=80\\!\\left(\\tfrac{2}{3}\\right)+40\\!\\left(\\tfrac{1}{3}\\right)\\\\[4pt]\n",
    "&=\\tfrac{160}{3}+\\tfrac{40}{3}=\\tfrac{200}{3}=66.\\overline{6}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Result:** $\\boxed{\\phi_B = 66.\\overline{6}}$\n",
    "\n",
    "(And for completeness, $\\phi_C = 46.\\overline{6}$, so the three sum to $200$.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1456a5-55e6-46c8-b43f-310b85f06a27",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "* Even though A alone was strongest, the Shapley value accounts for **synergies and cooperation**.\n",
    "* Each player gets credit for their **average marginal contribution** over all possible collaboration orders.\n",
    "* This is why Shapley values are used in **Explainable AI (XAI)** to fairly distribute model predictions among input features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a57de-3268-40ec-b04b-c67bedf94de0",
   "metadata": {},
   "source": [
    "## 3. Shapley in Deep Learning\n",
    "\n",
    "In cooperative game theory:\n",
    "\n",
    "* **Players** = people joining a team.\n",
    "* **Value function** = revenue of the team.\n",
    "* **Shapley value** = fair distribution of the revenue.\n",
    "\n",
    "In deep learning:\n",
    "\n",
    "* **Players** = input features (pixels, words, genes, …).\n",
    "* **Value function** = the model’s prediction or score for a given input.\n",
    "* **Shapley value** = fair attribution of how much each feature contributed to the prediction.\n",
    "\n",
    " So Shapley gives us a **principled way to explain model predictions feature-by-feature**.\n",
    " \n",
    "---\n",
    "\n",
    "#### Shapley Values in XAI\n",
    "\n",
    "For a model $f(x)$, input $x = (x_1, x_2, ..., x_n)$:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \n",
    "\\frac{|S|! (n-|S|-1)!}{n!} \\Big( f(S \\cup \\{i\\}) - f(S) \\Big)\n",
    "$$\n",
    "\n",
    "* $N = \\{1,2,\\dots,n\\}$ = all features.\n",
    "* $S$ = subset of features not including feature $i$.\n",
    "* $f(S)$ = model prediction using only features in $S$, with others “missing” (replaced with baseline / sampled values).\n",
    "* $\\phi_i$ = Shapley value for feature $i$, i.e. contribution to the prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883d94c-1c61-4315-a733-0878a68ce666",
   "metadata": {},
   "source": [
    "####  Mini Numerical Example\n",
    "\n",
    "Imagine a trained neural net that predicts whether a patient has a disease.\n",
    "Input features:\n",
    "\n",
    "* $x_1$ = Age\n",
    "* $x_2$ = Blood Pressure\n",
    "\n",
    "Say for one patient:\n",
    "\n",
    "* Age = 60\n",
    "* BP = High\n",
    "\n",
    "And model output: $f(x_1=60,x_2=\\text{High}) = 0.9$ (90% probability).\n",
    "\n",
    "### Step 1: Define coalition values\n",
    "\n",
    "We need model outputs with subsets of features:\n",
    "* In our little Age/Blood Pressure example, the neural net (or any model) expects *all features* as input.\n",
    "* But in the Shapley formula we need values like $f(\\{\\})$, $f(\\{Age\\})$, $f(\\{BP\\})$, i.e. predictions with **some features “missing.”**\n",
    "\n",
    "So the big question:\n",
    " *How do we define “the model prediction if only some features are known”?*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Handling “missing” features in Shapley for ML\n",
    "\n",
    "There are a few common strategies:\n",
    "\n",
    "#### 1. **Marginal expectation (classic game-theory definition)**\n",
    "\n",
    "For a subset $S$, we compute:\n",
    "\n",
    "$$\n",
    "f(S) = \\mathbb{E}[f(x) \\mid x_S]\n",
    "$$\n",
    "\n",
    "That is:\n",
    "\n",
    "* Fix the known features in $S$ (e.g., Age=60).\n",
    "* Average the model’s predictions over the distribution of the unknown features (e.g., average over many possible Blood Pressure values from training data).\n",
    "\n",
    "This is the mathematically correct version, but often expensive.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Conditioning on data samples (SHAP approximation)**\n",
    "\n",
    "Instead of integrating over distributions, we approximate using real training samples:\n",
    "\n",
    "* To compute $f(\\{Age=60\\})$, we take the test input (Age=60) and then “fill in” BP with background values sampled from the training set, average the predictions.\n",
    "* Similarly for $f(\\{BP\\})$.\n",
    "* For $f(\\{\\})$, we just average the model prediction over background data (that’s the baseline expectation).\n",
    "\n",
    "This is what the **SHAP library** does in practice with `KernelExplainer` and `DeepExplainer`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Set missing features to a baseline constant**\n",
    "\n",
    "Simplest but less principled. For example:\n",
    "\n",
    "* If Age is missing, replace it with the training mean (say 45).\n",
    "* If BP is missing, replace it with “Normal” (encoded value).\n",
    "\n",
    "This can bias explanations, so it’s usually avoided unless distribution info is missing.\n",
    "\n",
    "---\n",
    "\n",
    "#### Back to our toy Age/BP example\n",
    "\n",
    "Let’s assume we had training data to estimate distributions. Then:\n",
    "\n",
    "* **$f(\\{\\})$** = expected prediction if Age and BP are unknown → mean prediction over dataset. (I set it to 0.2 as a baseline earlier.)\n",
    "* **$f(\\{Age=60\\})$** = average prediction across all possible BP values for Age=60. (I set it to 0.6.)\n",
    "* **$f(\\{BP=High\\})$** = average prediction across all possible Ages for BP=High. (I set it to 0.7.)\n",
    "* **$f(\\{Age=60, BP=High\\})$** = the actual prediction for this full input = 0.9.\n",
    "\n",
    "These numbers were just illustrative, but in practice you’d **estimate them by sampling from your training data**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "* The model always takes full input, but Shapley requires reasoning about *subsets*.\n",
    "* We simulate “missing features” by **averaging over a background distribution** (marginal expectation).\n",
    "* That’s why the SHAP library requires you to provide `background_data` → it’s the reference distribution used when features are missing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* With none (baseline): $f(\\{\\}) = 0.2$\n",
    "* With only Age: $f(\\{x_1=60\\}) = 0.6$\n",
    "* With only BP: $f(\\{x_2=\\text{High}\\}) = 0.7$\n",
    "* With both: $f(\\{x_1=60, x_2=\\text{High}\\}) = 0.9$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Step 2: Compute Shapley values\n",
    "\n",
    "For **Age**:\n",
    "\n",
    "* Contribution when joining ∅: $0.6 - 0.2 = 0.4$\n",
    "* Contribution when joining {BP}: $0.9 - 0.7 = 0.2$\n",
    "  Weights (for 2 features): both are 0.5.\n",
    "  So: $\\phi_{\\text{Age}} = 0.5(0.4 + 0.2) = 0.3$\n",
    "\n",
    "For **BP**:\n",
    "\n",
    "* Contribution when joining ∅: $0.7 - 0.2 = 0.5$\n",
    "* Contribution when joining {Age}: $0.9 - 0.6 = 0.3$\n",
    "  Weights = 0.5.\n",
    "  So: $\\phi_{\\text{BP}} = 0.5(0.5 + 0.3) = 0.4$\n",
    "\n",
    "Check: baseline + Age + BP = $0.2 + 0.3 + 0.4 = 0.9$, matches model \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba4f5a-3da9-4d83-be82-6a450adff1a7",
   "metadata": {},
   "source": [
    "## Pytoch Example\n",
    "Here’s a **minimal, concrete PyTorch + SHAP demo** that shows exactly how the “missing features” $f(S)$ are computed by **averaging over background data** (marginal expectation), both **manually** and via **`shap.KernelExplainer`**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65340520-2b5a-4275-93ca-401533576cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x* (raw features)  Age= 39.44011032348155 BP= 0.0\n",
      "Background size: 200\n",
      "\n",
      "Manual marginal expectations:\n",
      "f(∅)                 = 0.5390608\n",
      "f({Age=a*})          = 0.37930885\n",
      "f({BP=b*})           = 0.4741207\n",
      "f({Age=a*, BP=b*})   = 0.2591433\n",
      "\n",
      "Manual Shapley (2-feature case):\n",
      "φ_Age = -0.18736467\n",
      "φ_BP  = -0.09255281\n",
      "Check: baseline + sum φ ≈ f(x*): 0.2591433 vs 0.2591433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 492.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAP KernelExplainer:\n",
      "φ_Age (SHAP) = -0.1873646941734476\n",
      "φ_BP  (SHAP) = -0.09255279297940455\n",
      "SHAP expected value (baseline) = 0.5390607800567523\n",
      "Check: baseline + sum φ ≈ f(x*): 0.25914329290390015 vs 0.2591433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Two features: Age (continuous), BP (binary: 0=Normal, 1=High)\n",
    "n = 3000\n",
    "age = rng.normal(45, 12, size=n)          # years\n",
    "bp  = rng.integers(0, 2, size=n).astype(float)\n",
    "\n",
    "# Nonlinear target: prob(disease) grows with age^2 (scaled) and high BP\n",
    "logit =  -6.0 + 0.04*age + 0.002*(age**2) + 1.2*bp\n",
    "p = 1/(1+np.exp(-logit))\n",
    "y = rng.binomial(1, p).astype(float)\n",
    "\n",
    "X = np.stack([age, bp], axis=1)\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Scale only the continuous feature (Age). Keep BP as-is (0/1).\n",
    "scaler = StandardScaler().fit(X_train[:, [0]])\n",
    "def transform(X):\n",
    "    X2 = X.copy()\n",
    "    X2[:, 0] = scaler.transform(X[:, [0]]).ravel()\n",
    "    return X2\n",
    "\n",
    "X_train_t = torch.tensor(transform(X_train), dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_t  = torch.tensor(transform(X_test),  dtype=torch.float32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_t = X_train_t.to(device); y_train_t = y_train_t.to(device)\n",
    "\n",
    "# Small classifier (binary)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "model = MLP().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train quickly\n",
    "model.train()\n",
    "for epoch in range(300):\n",
    "    opt.zero_grad()\n",
    "    logits = model(X_train_t)\n",
    "    loss = loss_fn(logits, y_train_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Helper: probability prediction on (numpy) inputs\n",
    "def predict_proba(x_np):\n",
    "    x_t = torch.tensor(transform(x_np), dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        logit = model(x_t).squeeze(1)\n",
    "        prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
    "    return prob\n",
    "\n",
    "# Pick ONE test instance to explain (Age= a*, BP= b*)\n",
    "x_star = X_test[0:1].copy()       # shape (1,2)\n",
    "a_star, b_star = x_star[0,0], x_star[0,1]\n",
    "print(\"x* (raw features)  Age=\", a_star, \"BP=\", b_star)\n",
    "\n",
    "# Build a background dataset (reference distribution used for “missing” features)\n",
    "# Here we’ll use 200 random points from TRAIN as background\n",
    "bg_idx = rng.choice(len(X_train), size=200, replace=False)\n",
    "B = X_train[bg_idx].copy()   # shape (B, 2)\n",
    "print(\"Background size:\", len(B))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) MANUAL Shapley terms via marginal expectations (sampling)\n",
    "# ============================================================\n",
    "\n",
    "# f(∅): baseline = E_X[f(X)] ≈ average over background\n",
    "f_empty = predict_proba(B).mean()\n",
    "\n",
    "# f({Age=a*}): fix Age=a*, sample BP from background\n",
    "B_age = B.copy()\n",
    "B_age[:, 0] = a_star       # fix Age\n",
    "f_age = predict_proba(B_age).mean()\n",
    "\n",
    "# f({BP=b*}): fix BP=b*, sample Age from background\n",
    "B_bp = B.copy()\n",
    "B_bp[:, 1] = b_star        # fix BP\n",
    "f_bp = predict_proba(B_bp).mean()\n",
    "\n",
    "# f({Age=a*, BP=b*}) = model’s prediction for x*\n",
    "f_both = predict_proba(x_star)[0]\n",
    "\n",
    "print(\"\\nManual marginal expectations:\")\n",
    "print(\"f(∅)                 =\", f_empty)\n",
    "print(\"f({Age=a*})          =\", f_age)\n",
    "print(\"f({BP=b*})           =\", f_bp)\n",
    "print(\"f({Age=a*, BP=b*})   =\", f_both)\n",
    "\n",
    "# Two features → Shapley weights are 1/2 for both terms of each feature\n",
    "# φ_Age = 1/2*( f({Age}) - f(∅) ) + 1/2*( f({Age,BP}) - f({BP}) )\n",
    "phi_age = 0.5 * (f_age - f_empty) + 0.5 * (f_both - f_bp)\n",
    "\n",
    "# φ_BP  = 1/2*( f({BP}) - f(∅) ) + 1/2*( f({Age,BP}) - f({Age}) )\n",
    "phi_bp  = 0.5 * (f_bp - f_empty) + 0.5 * (f_both - f_age)\n",
    "\n",
    "print(\"\\nManual Shapley (2-feature case):\")\n",
    "print(\"φ_Age =\", phi_age)\n",
    "print(\"φ_BP  =\", phi_bp)\n",
    "print(\"Check: baseline + sum φ ≈ f(x*):\", f_empty + phi_age + phi_bp, \"vs\", f_both)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) SHAP KernelExplainer (does the same averaging for you)\n",
    "# ============================================================\n",
    "\n",
    "# SHAP expects: predict_fn(np.array) -> np.array of outputs\n",
    "def predict_fn_for_shap(x_np):\n",
    "    return predict_proba(x_np)\n",
    "\n",
    "# KernelExplainer uses B as the background (the “missing feature” reference)\n",
    "kexpl = shap.KernelExplainer(predict_fn_for_shap, B)\n",
    "\n",
    "# Explain x_star; nsamples=\"auto\" is fine (or set an int for speed/accuracy tradeoff)\n",
    "shap_vals = kexpl.shap_values(x_star, nsamples=\"auto\")  # shape (1,2)\n",
    "shap_vals = np.asarray(shap_vals)  # ensure np array\n",
    "print(\"\\nSHAP KernelExplainer:\")\n",
    "print(\"φ_Age (SHAP) =\", shap_vals[0,0])\n",
    "print(\"φ_BP  (SHAP) =\", shap_vals[0,1])\n",
    "\n",
    "# Baseline that SHAP used (expected value over background)\n",
    "print(\"SHAP expected value (baseline) =\", kexpl.expected_value)\n",
    "print(\"Check: baseline + sum φ ≈ f(x*):\",\n",
    "      kexpl.expected_value + shap_vals.sum(), \"vs\", f_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cebcd0-8c0f-40ea-b2df-7026f8821e72",
   "metadata": {},
   "source": [
    "\n",
    "#### What this shows\n",
    "\n",
    "* We **manually** compute:\n",
    "\n",
    "  * $f(\\varnothing)$ = average prediction over background\n",
    "  * $f(\\{Age=a^*\\})$ = average prediction when we **fix Age** to the instance’s value and **sample BP** from the background\n",
    "  * $f(\\{BP=b^*\\})$ = average prediction when we **fix BP** and **sample Age**\n",
    "  * $f(\\{Age=a^*, BP=b^*\\})$ = the model’s actual prediction for the full input\n",
    "* Then we plug those into the **2-feature Shapley formula** (weights $=\\frac12$ each).\n",
    "* **`KernelExplainer`** does the *same* marginal-expectation averaging internally, using the background `B`. You should see:\n",
    "\n",
    "  * `φ_Age (manual)` ≈ `φ_Age (SHAP)`\n",
    "  * `φ_BP  (manual)` ≈ `φ_BP  (SHAP)`\n",
    "  * `baseline + φ_Age + φ_BP ≈ f(x*)`\n",
    "\n",
    "---\n",
    "\n",
    "If you’d like, I can tweak this to:\n",
    "\n",
    "* explain a **batch** of test points and plot a SHAP **beeswarm/bar**,\n",
    "* switch to a **regression** target,\n",
    "* or show a **conditioning-on-class** variant (e.g., class-1 probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cd51a-a934-4ebe-86e8-2f874dda7440",
   "metadata": {},
   "source": [
    "##  Common use cases for SHAP in deep learning\n",
    "\n",
    "### 1. **Tabular data (structured features)**\n",
    "\n",
    "* Example: predicting **health outcomes from patient features** (age, blood pressure, lab tests).\n",
    "* SHAP tells you: *which features most influenced a prediction*.\n",
    "* This is the **most natural and common use case** for SHAP.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Natural Language Processing (NLP)**\n",
    "\n",
    "* Example: sentiment classification.\n",
    "* SHAP highlights which **words** pushed the prediction positive vs negative.\n",
    "* Works well when input = sequence of discrete tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Small-dimensional scientific/engineering inputs**\n",
    "\n",
    "* Example: physics-informed models, finance models, genomics.\n",
    "* When features have clear meaning, SHAP helps interpret what drives predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Less common / not ideal for computer vision\n",
    "\n",
    "In **computer vision**, the input is usually thousands of pixels.\n",
    "\n",
    "* Each pixel is a “feature,” but **Shapley values per pixel** are not very useful (too many, hard to interpret).\n",
    "* Computing exact/marginal expectations for millions of pixels is **computationally intractable**.\n",
    "\n",
    "That’s why **SHAP is rarely used directly for CNNs**. Instead, people prefer:\n",
    "\n",
    "* **Grad-CAM** (gradient-weighted activation maps) → highlights important regions in feature maps.\n",
    "* **Integrated Gradients** → accumulates gradients from baseline to input.\n",
    "* **SmoothGrad** → averages gradients over noisy inputs.\n",
    "\n",
    "---\n",
    "\n",
    "##  But SHAP *can* be used in vision, with tricks\n",
    "\n",
    "* Use **superpixels** (segments of image regions) instead of raw pixels.\n",
    "\n",
    "  * Each “player” = a superpixel.\n",
    "  * SHAP can then tell you which *regions* of the image matter.\n",
    "* This is implemented in `shap.ImageExplainer` and `shap.maskers.Image`.\n",
    "* Example: explaining why a CNN classified an image as “dog” by showing which patches (ear, tail, etc.) had positive contributions.\n",
    "\n",
    "Still, **Grad-CAM is much more standard** for CNN interpretability.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
