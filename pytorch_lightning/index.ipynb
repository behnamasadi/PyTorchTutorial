{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975f0eca-03ae-4a40-a7bd-8ccbc628b3de",
   "metadata": {},
   "source": [
    "# PyTorch Lightning\n",
    "PyTorch Lightning is a lightweight framework built on top of PyTorch that **organizes your deep-learning code**, removes boilerplate, and gives you **production-ready training features** with almost no extra work.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Structure your training code cleanly\n",
    "\n",
    "Lightning forces you to separate your model, data, and training logic into clean functions:\n",
    "\n",
    "* `training_step`\n",
    "* `validation_step`\n",
    "* `configure_optimizers`\n",
    "* `train_dataloader`\n",
    "\n",
    "This removes boilerplate like:\n",
    "\n",
    "* manual GPU placement\n",
    "* writing loops for epochs, batches, metrics\n",
    "* managing gradient accumulation\n",
    "* writing `model.train()` / `model.eval()`\n",
    "\n",
    "Result: your code becomes **shorter, more readable, and easier to debug**.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Use multiple GPUs easily (DDP)\n",
    "\n",
    "Without Lightning, distributed training requires a lot of code.\n",
    "With Lightning:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(devices=4, accelerator=\"gpu\", strategy=\"ddp\")\n",
    "```\n",
    "\n",
    "Lightning automatically:\n",
    "\n",
    "* launches processes\n",
    "* syncs gradients\n",
    "* handles multiprocessing issues\n",
    "* avoids deadlocks\n",
    "\n",
    "You get **linear scaling** with almost zero effort.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Mixed precision training (AMP/bf16/fp16)\n",
    "\n",
    "Instead of manually writing autocast/scaler logic:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(precision=\"16-mixed\")\n",
    "```\n",
    "\n",
    "Lightning automatically uses AMP to:\n",
    "\n",
    "* reduce VRAM usage\n",
    "* speed up training\n",
    "* avoid underflows\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Gradient accumulation and clipping\n",
    "\n",
    "Need larger batch sizes than your GPU memory allows?\n",
    "\n",
    "```python\n",
    "trainer = Trainer(accumulate_grad_batches=4)\n",
    "```\n",
    "\n",
    "Need stable gradients?\n",
    "\n",
    "```python\n",
    "trainer = Trainer(gradient_clip_val=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Built-in callbacks and checkpoints\n",
    "\n",
    "Lightning has powerful built-in systems for:\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "```python\n",
    "ModelCheckpoint(monitor=\"val_loss\", save_top_k=3)\n",
    "```\n",
    "\n",
    "### Early stopping\n",
    "\n",
    "```python\n",
    "EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "```\n",
    "\n",
    "### Learning rate monitoring\n",
    "\n",
    "```python\n",
    "LearningRateMonitor()\n",
    "```\n",
    "\n",
    "No need to manually implement these.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Log everything automatically (TensorBoard, wandb, MLflow)\n",
    "\n",
    "Lightning integrates with all major loggers:\n",
    "\n",
    "* TensorBoard\n",
    "* WandB\n",
    "* MLflow\n",
    "* CSV logger\n",
    "* Neptune\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(logger=WandbLogger(project=\"my_project\"))\n",
    "```\n",
    "\n",
    "All metrics from `self.log(...)` appear in your dashboard.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Train on CPU, GPU, TPU, multi-node clusters\n",
    "\n",
    "Lightning abstracts hardware, so your code doesn't change:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=8)\n",
    "```\n",
    "\n",
    "Same code runs on:\n",
    "\n",
    "* local machine\n",
    "* multiple GPUs\n",
    "* SLURM or Kubernetes clusters\n",
    "* TPUs (Google Cloud)\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Automatic optimization loop\n",
    "\n",
    "Lightning handles:\n",
    "\n",
    "* optimizer.zero_grad()\n",
    "* loss.backward()\n",
    "* optimizer.step()\n",
    "\n",
    "This is called **automatic optimization**, and it cleans up your loop significantly.\n",
    "You can disable it for custom training loops (GANs, RL, contrastive methods, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Reproducible experiments\n",
    "\n",
    "Lightning automatically:\n",
    "\n",
    "* seeds all random generators\n",
    "* tracks hyperparameters\n",
    "* restores complete training state from checkpoints\n",
    "\n",
    "Reproducibility becomes trivial.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Model deployment and exporting\n",
    "\n",
    "Lightning supports exporting models:\n",
    "\n",
    "* TorchScript\n",
    "* ONNX\n",
    "* Serving via Lightning’s own \"TorchServe-like\" system (Lightning Apps)\n",
    "* Integration with HuggingFace pipelines\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model = lit_model.to_torchscript()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 11. Data pipelines (LightningDataModule)\n",
    "\n",
    "A `LightningDataModule` organizes data loading:\n",
    "\n",
    "* train/val/test splits\n",
    "* transforms\n",
    "* loaders\n",
    "\n",
    "Keeps your training script clean and reusable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d316d3f-9f5e-4caf-8d62-8b89d8b317c2",
   "metadata": {},
   "source": [
    "# **PyTorch Lightning Tutorial (Step-by-Step)**\n",
    "\n",
    "## **Step 1 — Install PyTorch Lightning**\n",
    "\n",
    "```bash\n",
    "pip install pytorch-lightning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2 — Understand the Lightning architecture**\n",
    "\n",
    "Lightning separates your code into **three parts**:\n",
    "\n",
    "1. **LightningModule** → model + training logic\n",
    "2. **LightningDataModule (optional)** → dataloaders\n",
    "3. **Trainer** → hardware, training strategy, logging, callbacks\n",
    "\n",
    "This separation keeps the code clean.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3 — Create your LightningModule**\n",
    "\n",
    "Inside this module you implement:\n",
    "\n",
    "1. `__init__` → define model, loss, hyperparameters\n",
    "2. `forward` → inference\n",
    "3. `training_step` → compute loss\n",
    "4. `validation_step` → compute val metrics\n",
    "5. `configure_optimizers` → define optimizer & scheduler\n",
    "\n",
    "Minimal example:\n",
    "\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(10, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4 — Create your DataLoaders**\n",
    "\n",
    "Three ways:\n",
    "\n",
    "### A. Write them normally (simplest)\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "```\n",
    "\n",
    "### B. Or create a LightningDataModule (cleaner for big projects)\n",
    "\n",
    "```python\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = ...\n",
    "        self.val_ds = ...\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5 — Instantiate the model**\n",
    "\n",
    "```python\n",
    "model = LitModel(lr=1e-3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6 — Configure the Trainer**\n",
    "\n",
    "This controls training, GPUs, logging, precision, etc.\n",
    "\n",
    "Examples:\n",
    "\n",
    "### Single GPU\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1)\n",
    "```\n",
    "\n",
    "### Mixed precision (recommended)\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(precision=\"16-mixed\")\n",
    "```\n",
    "\n",
    "### Multi-GPU\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\n",
    "```\n",
    "\n",
    "### Enable checkpoints & early stopping\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\", save_top_k=1),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7 — Train the model**\n",
    "\n",
    "If using dataloaders:\n",
    "\n",
    "```python\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "```\n",
    "\n",
    "If using a DataModule:\n",
    "\n",
    "```python\n",
    "dm = MyDataModule()\n",
    "trainer.fit(model, dm)\n",
    "```\n",
    "\n",
    "Lightning handles:\n",
    "\n",
    "* device placement\n",
    "* gradient scaling\n",
    "* backprop\n",
    "* optimization\n",
    "* epoch loops\n",
    "* validation loops\n",
    "\n",
    "You only write the high-level logic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8 — Test (optional)**\n",
    "\n",
    "Add a `test_step` and then:\n",
    "\n",
    "```python\n",
    "trainer.test(model, datamodule=dm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 9 — Load a checkpoint**\n",
    "\n",
    "```python\n",
    "model = LitModel.load_from_checkpoint(\"path/to/check.ckpt\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 10 — Use for inference**\n",
    "\n",
    "LightningModule works like a normal PyTorch model:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Extra (Optional but Useful)\n",
    "\n",
    "## **A. Add WandB logging**\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "trainer = pl.Trainer(logger=WandbLogger(project=\"my_project\"))\n",
    "```\n",
    "\n",
    "## **B. Freeze/unfreeze backbone**\n",
    "\n",
    "Lightning supports manual `.requires_grad = False` or callbacks for staged training.\n",
    "\n",
    "## **C. Custom training loops**\n",
    "\n",
    "If you want full control (GANs, contrastive learning):\n",
    "\n",
    "```python\n",
    "self.automatic_optimization = False\n",
    "```\n",
    "\n",
    "Then you manually call `optimizer.step()` etc.\n",
    "\n",
    "---\n",
    "\n",
    "# Complete Step Summary (Short Version)\n",
    "\n",
    "1. Install Lightning\n",
    "2. Create `LightningModule`\n",
    "3. Implement forward, training_step, validation_step\n",
    "4. Implement optimizer in configure_optimizers\n",
    "5. Create dataloaders or DataModule\n",
    "6. Create Trainer (GPU, precision, callbacks)\n",
    "7. Call trainer.fit\n",
    "8. (Optional) trainer.test\n",
    "9. Load checkpoint for inference\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can now generate:\n",
    "\n",
    "1. A complete ready-to-run **template project**\n",
    "2. A tutorial specifically for **timm + Lightning**\n",
    "3. A tutorial specifically for **MONAI + Lightning**\n",
    "4. A template integrated with **wandb + MLflow**\n",
    "5. A **Lightning + KITTI** or **Lightning + Lung_Disease_Dataset** pipeline\n",
    "\n",
    "Tell me which one you prefer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
