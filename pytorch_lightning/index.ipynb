{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d316d3f-9f5e-4caf-8d62-8b89d8b317c2",
   "metadata": {},
   "source": [
    "# **PyTorch Lightning**\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install pytorch-lightning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Understand the Lightning architecture**\n",
    "\n",
    "Lightning separates your code into **three parts**:\n",
    "\n",
    "1. **LightningModule** → model + training logic\n",
    "2. **LightningDataModule (optional)** → dataloaders\n",
    "3. **Trainer** → hardware, training strategy, logging, callbacks\n",
    "\n",
    "This separation keeps the code clean.\n",
    "\n",
    "---\n",
    "\n",
    "## **Create LightningModule**\n",
    "\n",
    "Inside this module you implement:\n",
    "\n",
    "1. `__init__` → define model, loss, hyperparameters\n",
    "2. `forward` → inference\n",
    "3. `training_step` → compute loss\n",
    "4. `validation_step` → compute val metrics\n",
    "5. `configure_optimizers` → define optimizer & scheduler\n",
    "\n",
    "Minimal example:\n",
    "\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(10, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        # Used mainly for visualization.\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        # Used for:\n",
    "        #   - model selection\n",
    "        #   - early stopping\n",
    "        #   - monitoring generalization\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Create DataLoaders**\n",
    "\n",
    "Three ways:\n",
    "\n",
    "### A. Write them normally (simplest)\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "```\n",
    "\n",
    "### B. Or create a LightningDataModule (cleaner for big projects)\n",
    "\n",
    "## LightningDataModule\n",
    "\n",
    "\n",
    "A **LightningDataModule** is a standardized place to put:\n",
    "\n",
    "* dataset download\n",
    "* dataset preprocessing\n",
    "* train/val/test splits\n",
    "* augmentations\n",
    "* dataloader creation\n",
    "* batch size configuration\n",
    "* multiple workers, pin_memory, shuffling\n",
    "\n",
    "Instead of spreading your data logic across different files, everything is put into **one clean class**.\n",
    "\n",
    "It replaces messy code like this scattered everywhere:\n",
    "\n",
    "```python\n",
    "train_dataset = ...\n",
    "train_loader = DataLoader(...)\n",
    "val_loader = DataLoader(...)\n",
    "test_loader = DataLoader(...)\n",
    "```\n",
    "\n",
    "with a **well-organized object**.\n",
    "\n",
    "---\n",
    "\n",
    "## **What does a DataModule contain? (5 lifecycle hooks)**\n",
    "\n",
    "LightningDataModule has **five** common methods you implement:\n",
    "\n",
    "#### **1. `__init__`**\n",
    "\n",
    "Store configuration like:\n",
    "\n",
    "* paths\n",
    "* batch size\n",
    "* augmentations\n",
    "* num_workers\n",
    "\n",
    "#### **2. `prepare_data()`**\n",
    "\n",
    "Used only **once** on one GPU:\n",
    "\n",
    "* download dataset\n",
    "* tokenize\n",
    "* heavy preprocessing\n",
    "\n",
    "#### **3. `setup(stage)`**\n",
    "\n",
    "Called on **every GPU**:\n",
    "\n",
    "* create train/val/test splits\n",
    "* set transforms\n",
    "* load datasets into memory if needed\n",
    "\n",
    "#### **4. `train_dataloader()`**\n",
    "\n",
    "Return a PyTorch DataLoader for training.\n",
    "\n",
    "#### **5. `val_dataloader()`**\n",
    "\n",
    "Return DataLoader for validation.\n",
    "\n",
    "#### **6. `test_dataloader()` (optional)**\n",
    "\n",
    "#### **7. `predict_dataloader()` (optional)**\n",
    "\n",
    "---\n",
    "\n",
    "## **Minimal Example**\n",
    "\n",
    "```python\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"./data\", batch_size=32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # downloads only once\n",
    "        torchvision.datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        torchvision.datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # transformations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            full = torchvision.datasets.CIFAR10(self.data_dir, train=True, transform=transform)\n",
    "            self.train_ds, self.val_ds = random_split(full, [45000, 5000])\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_ds = torchvision.datasets.CIFAR10(self.data_dir, train=False, transform=transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321853e6-d201-4b85-82de-a253ad5033af",
   "metadata": {},
   "source": [
    "## **Configure the Trainer**\n",
    "\n",
    "This controls training, GPUs, logging, precision, etc.\n",
    "\n",
    "Examples:\n",
    "\n",
    "### Single GPU\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1)\n",
    "```\n",
    "\n",
    "### Mixed precision (recommended)\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(precision=\"16-mixed\")\n",
    "```\n",
    "\n",
    "### Multi-GPU\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\n",
    "```\n",
    "\n",
    "### Enable checkpoints & early stopping\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\", save_top_k=1),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Train the model**\n",
    "\n",
    "If using dataloaders:\n",
    "\n",
    "```python\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "```\n",
    "\n",
    "If using a DataModule:\n",
    "\n",
    "```python\n",
    "dm = MyDataModule()\n",
    "trainer.fit(model, dm)\n",
    "```\n",
    "\n",
    "\n",
    "## **Test (optional)**\n",
    "\n",
    "Add a `test_step` and then:\n",
    "\n",
    "```python\n",
    "trainer.test(model, datamodule=dm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Load a checkpoint for inference**\n",
    "\n",
    "```python\n",
    "model = LitModel.load_from_checkpoint(\"path/to/check.ckpt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(x)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c3a90-51a6-447b-939a-b6d5d2194296",
   "metadata": {},
   "source": [
    "## **PyTorch Lightning callbacks**\n",
    "---\n",
    "\n",
    "## **1. BatchSizeFinder**\n",
    "\n",
    "Automatically finds the **largest batch size** that fits in GPU memory.\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(auto_scale_batch_size=\"power\")\n",
    "```\n",
    "\n",
    "It increases batch size until out-of-memory occurs, then backs off.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. BackboneFinetuning**\n",
    "\n",
    "For transfer learning or timm/MONAI models.\n",
    "\n",
    "It:\n",
    "\n",
    "* freezes backbone layers at the start\n",
    "* unfreezes them after a number of epochs\n",
    "* optionally sets different learning rates per layer\n",
    "\n",
    "Use case:\n",
    "\n",
    "* Fine-tuning ResNet\n",
    "* ViT two-stage training\n",
    "* MONAI UNet encoder freezing\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "BackboneFinetuning(unfreeze_backbone_at_epoch=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **3. ModelCheckpoint**\n",
    "\n",
    "Automatically saves model checkpoints based on a metric.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* saving best model\n",
    "* saving last model\n",
    "* resuming training\n",
    "* model selection\n",
    "\n",
    "---\n",
    "\n",
    "## **4. DeviceStatsMonitor**\n",
    "\n",
    "Logs:\n",
    "\n",
    "* GPU memory\n",
    "* GPU utilization\n",
    "* CPU usage\n",
    "\n",
    "Very useful with WandB or TensorBoard.\n",
    "\n",
    "```python\n",
    "trainer = Trainer(callbacks=[DeviceStatsMonitor()])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. EarlyStopping**\n",
    "\n",
    "Stops training when a metric stops improving.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\"\n",
    ")\n",
    "```\n",
    "\n",
    "Prevents overfitting and wasted GPU hours.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. GradientAccumulationScheduler**\n",
    "\n",
    "Changes gradient accumulation behavior at specific epochs.\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* warmup of effective batch size\n",
    "* dynamic training strategies\n",
    "* stabilizing early epochs\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "GradientAccumulationScheduler({\n",
    "    0: 1,  # epoch 0: accumulate 1 batch\n",
    "    5: 4,  # epoch 5+: accumulate 4 batches\n",
    "})\n",
    "```\n",
    "\n",
    "This gradually increases effective batch size.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **7. LearningRateMonitor**\n",
    "\n",
    "Logs learning rate each step or epoch.\n",
    "\n",
    "```python\n",
    "LearningRateMonitor(logging_interval=\"epoch\")\n",
    "```\n",
    "\n",
    "Essential for debugging schedulers.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. RichProgressBar**\n",
    "\n",
    "Replaces default TQDM bar with a beautiful rich-based display.\n",
    "\n",
    "```python\n",
    "trainer = Trainer(callbacks=[RichProgressBar()])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. StochasticWeightAveraging (SWA)**\n",
    "\n",
    "Improves generalization by averaging weights over multiple checkpoints.\n",
    "\n",
    "```python\n",
    "StochasticWeightAveraging(swa_lrs=1e-3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10. ModelPruning**\n",
    "\n",
    "Automatically prunes weights during training.\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import ModelPruning\n",
    "```\n",
    "\n",
    "Sparsity improves model size and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. ModelSummary**\n",
    "\n",
    "Shows a clean summary of your architecture at the start of training.\n",
    "\n",
    "```python\n",
    "ModelSummary(max_depth=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **12. QuantizationAwareTraining**\n",
    "\n",
    "Integrates PyTorch quantization with Lightning.\n",
    "\n",
    "---\n",
    "\n",
    "## **13. Timer**\n",
    "\n",
    "Allows you to stop training based on total training time.\n",
    "\n",
    "```python\n",
    "Timer(duration=\"00:30:00\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14. FaultTolerantTraining**\n",
    "\n",
    "Restart training automatically after crashes.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **15. LambdaCallback**\n",
    "\n",
    "A simple callback where you define your own functions without writing a class.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "callback = LambdaCallback(\n",
    "    on_train_start=lambda trainer, pl_module: print(\"Training started\"),\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **4. When do callbacks run?**\n",
    "\n",
    "Callbacks have dozens of hooks:\n",
    "\n",
    "* `on_train_start`\n",
    "* `on_train_end`\n",
    "* `on_train_batch_start`\n",
    "* `on_train_batch_end`\n",
    "* `on_validation_start`\n",
    "* `on_validation_end`\n",
    "* `on_validation_batch_end`\n",
    "* `on_fit_end`\n",
    "* `on_exception`\n",
    "* `on_before_optimizer_step`\n",
    "* `on_save_checkpoint`\n",
    "* `on_load_checkpoint`\n",
    "\n",
    "This makes callbacks extremely powerful for custom logic.\n",
    "\n",
    "---\n",
    "\n",
    "# **5. How to use callbacks**\n",
    "\n",
    "Just pass a list to Trainer:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\"),\n",
    "        EarlyStopping(monitor=\"val_loss\"),\n",
    "        LearningRateMonitor(),\n",
    "        DeviceStatsMonitor(),\n",
    "        RichProgressBar(),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Callbacks do not require modifying your LightningModule.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bce35-122c-449d-8b00-f30d7165620f",
   "metadata": {},
   "source": [
    "# **Safely Combining Gradient Accumulation, Batch Size Finder, and Model Pruning in PyTorch Lightning**\n",
    "\n",
    "When training deep learning models, it’s common to optimize:\n",
    "\n",
    "* **Memory usage** (bigger batches, mixed precision)\n",
    "* **Training stability** (gradient accumulation)\n",
    "* **Model efficiency** (pruning)\n",
    "* **Throughput** (optimal batch size)\n",
    "\n",
    "However, these features interact in complex ways inside PyTorch Lightning.\n",
    "This tutorial shows the **correct and safe workflow** for combining:\n",
    "\n",
    "* `GradientAccumulationScheduler`\n",
    "* `BatchSizeFinder`\n",
    "* `ModelPruning`\n",
    "* Standard callbacks (checkpointing, early stopping, logging)\n",
    "\n",
    "We explain why certain combinations must be avoided and how to structure your training code properly.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Why Batch Size Finder Cannot Run Together with Gradient Accumulation or Model Pruning**\n",
    "\n",
    "### **BatchSizeFinder determines the maximum batch size that fits in GPU memory.**\n",
    "\n",
    "It does that by testing multiple forward/backward passes:\n",
    "\n",
    "* With accumulation **disabled**\n",
    "* With pruning **not yet applied**\n",
    "\n",
    "### But gradient accumulation affects memory:\n",
    "\n",
    "* Gradients remain in memory across several backward passes\n",
    "* This increases peak memory later in training\n",
    "\n",
    "### And model pruning affects memory too:\n",
    "\n",
    "* Pruning changes which parameters exist\n",
    "* It changes activation shapes\n",
    "* It changes memory usage dynamically\n",
    "\n",
    "Because of that:\n",
    "\n",
    "### **BatchSizeFinder’s batch-size probe is NOT valid once gradient accumulation or pruning is active.**\n",
    "\n",
    "This is why:\n",
    "\n",
    "#### ❌ You must NOT include BatchSizeFinder in the same trainer that uses:\n",
    "\n",
    "* `GradientAccumulationScheduler`\n",
    "* `ModelPruning`\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Correct Workflow (Two-Stage Training Strategy)**\n",
    "\n",
    "## **Stage 1 — (Optional) Tune batch size in a clean environment**\n",
    "\n",
    "Use a small, dedicated trainer:\n",
    "\n",
    "```python\n",
    "batch_size_finder = BatchSizeFinder(mode=\"power\", max_val=max_batch_size)\n",
    "\n",
    "tune_trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    "    callbacks=[batch_size_finder]\n",
    ")\n",
    "\n",
    "tune_trainer.tune(model, datamodule=data_module)\n",
    "```\n",
    "\n",
    "After this:\n",
    "\n",
    "1. Read the suggested batch size\n",
    "2. Update your `LitDataModule(batch_size=...)`\n",
    "3. Remove BatchSizeFinder entirely\n",
    "\n",
    "Now you have a safe per-step batch size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Stage 2 — Train normally with gradient accumulation + pruning**\n",
    "\n",
    "Once the batch size is known, you can add your full callback stack:\n",
    "\n",
    "* Gradient accumulation scheduler\n",
    "* Pruning\n",
    "* Checkpoint\n",
    "* Early stopping\n",
    "* LR monitor\n",
    "* Device stats\n",
    "* Model summary\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=100,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        DebugCallback(),\n",
    "        checkpoint,\n",
    "        early_stopping,\n",
    "        learning_rate_monitor,\n",
    "        device_stats_monitor,\n",
    "        model_summary,\n",
    "        gradient_accumulation_scheduler,\n",
    "        model_pruning,\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Why does this work?\n",
    "\n",
    "Because now:\n",
    "\n",
    "* Batch size is fixed\n",
    "* Gradient accumulation is allowed\n",
    "* Pruning no longer interferes with batch-size discovery\n",
    "* Training is stable\n",
    "* Mixed precision reduces memory and improves speed\n",
    "\n",
    "---\n",
    "\n",
    "# **3. How GradientAccumulationScheduler Works**\n",
    "\n",
    "You defined:\n",
    "\n",
    "```python\n",
    "GradientAccumulationScheduler({\n",
    "    0: 1,\n",
    "    5: 4,\n",
    "})\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Epochs 0–4 → accumulate 1 batch (normal training)\n",
    "* Epochs 5+ → accumulate 4 batches before optimizer step\n",
    "\n",
    "This increases **effective batch size**:\n",
    "\n",
    "If per-step batch size = 32:\n",
    "\n",
    "Epochs 0–4:\n",
    "$$\n",
    "32 \\times 1 = 32\n",
    "$$\n",
    "\n",
    "Epochs 5+:\n",
    "$$\n",
    "32 \\times 4 = 128\n",
    "$$\n",
    "\n",
    "This is stable because batch size was determined *before* enabling accumulation.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Why accumulate_grad_batches must NOT be set manually**\n",
    "\n",
    "Lightning requires:\n",
    "\n",
    "```python\n",
    "accumulate_grad_batches = 1\n",
    "```\n",
    "\n",
    "when using `GradientAccumulationScheduler`.\n",
    "\n",
    "The callback mutates this value **during training**, so you must not override it manually in `Trainer()`.\n",
    "\n",
    "You correctly kept it default:\n",
    "\n",
    "```python\n",
    "# Do NOT set accumulate_grad_batches manually\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Why pruning is safe only during normal training**\n",
    "\n",
    "Your pruning callback:\n",
    "\n",
    "```python\n",
    "model_pruning = ModelPruning(\n",
    "    pruning_fn=\"l1_unstructured\",\n",
    "    amount=0.5\n",
    ")\n",
    "```\n",
    "\n",
    "Pruning happens during training, so the graph and memory footprint change progressively.\n",
    "\n",
    "This is why pruning cannot run during the BatchSizeFinder stage.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Putting It All Together**\n",
    "\n",
    "Your final trainer:\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=100,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        DebugCallback(),\n",
    "        checkpoint,\n",
    "        early_stopping,\n",
    "        learning_rate_monitor,\n",
    "        device_stats_monitor,\n",
    "        model_summary,\n",
    "        gradient_accumulation_scheduler,\n",
    "        model_pruning\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### This setup is now correct because:\n",
    "\n",
    "* Batch size is fixed and known to be safe\n",
    "* Gradient accumulation dynamically increases effective batch size\n",
    "* Mixed precision reduces memory\n",
    "* Pruning modifies layers safely during real training\n",
    "* All logging + monitoring callbacks work together\n",
    "* BatchSizeFinder is isolated in a separate tuning step\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Final Recommended Workflow Diagram**\n",
    "\n",
    "```\n",
    "[ Stage 1: Batch Size Tuning ]\n",
    "---------------------------------------\n",
    "Trainer(auto_scale_batch_size=True)\n",
    "     ↓\n",
    "Find max batch size that fits in memory\n",
    "     ↓\n",
    "Update LitDataModule(batch_size=X)\n",
    "     ↓\n",
    "Remove BatchSizeFinder\n",
    "\n",
    "\n",
    "[ Stage 2: Real Training ]\n",
    "---------------------------------------\n",
    "Trainer(\n",
    "    callbacks = [\n",
    "        ModelCheckpoint,\n",
    "        EarlyStopping,\n",
    "        DeviceStatsMonitor,\n",
    "        GradientAccumulationScheduler,\n",
    "        ModelPruning,\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "trainer.fit(...)\n",
    "```\n",
    "\n",
    "This tutorial captures the **correct best-practice workflow** for:\n",
    "\n",
    "* batch-size tuning\n",
    "* gradient accumulation\n",
    "* model pruning\n",
    "* safe memory usage\n",
    "* stable mixed-precision training\n",
    "* multi-callback integration\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also write:\n",
    "\n",
    "* A Markdown version of this tutorial\n",
    "* A minimal working example (MWE) script\n",
    "* A version formatted for GitHub README\n",
    "* A diagram explaining memory flow with accumulation vs non-accumulation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
