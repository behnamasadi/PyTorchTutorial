{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee83880-9395-4f4b-bda0-fe4fa365f00f",
   "metadata": {},
   "source": [
    "# 1. Weights & Biases (WandB)\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Use `config` to store run settings (they version well and show up in filters).\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"demo-proj\",\n",
    "    name=\"exp-resnet50-lr1e-3\",\n",
    "    config={\n",
    "        \"seed\": 42,\n",
    "        \"model\": \"resnet50\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 20,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"num_layers\": 50,\n",
    "        \"dataset\": \"CIFAR10\",\n",
    "        \"img_size\": 224,\n",
    "    },\n",
    ")\n",
    "cfg = wandb.config\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics (changing over time)\n",
    "\n",
    "Log scalars per **step** or **epoch**. Use hierarchical keys to keep dashboards tidy.\n",
    "\n",
    "```python\n",
    "global_step = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    # ... compute loss, acc ...\n",
    "    train_loss, train_acc = 0.42, 0.91\n",
    "    val_loss, val_acc = 0.38, 0.93\n",
    "\n",
    "    wandb.log({\n",
    "        \"global_step\": global_step,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/acc\":  train_acc,\n",
    "        \"val/loss\":   val_loss,\n",
    "        \"val/acc\":    val_acc,\n",
    "        \"epoch\":      epoch,\n",
    "    }, step=global_step)\n",
    "\n",
    "    global_step += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Model Gradients (optional)\n",
    "\n",
    "Let W\\&B watch the model to capture gradients/weights histograms every N steps.\n",
    "\n",
    "```python\n",
    "# after you create your model:\n",
    "# model = ...\n",
    "wandb.watch(models=model, log=\"gradients\", log_freq=100)  # or log=\"all\"\n",
    "```\n",
    "\n",
    "If you want manual control, log specific grad histograms:\n",
    "\n",
    "```python\n",
    "for name, p in model.named_parameters():\n",
    "    if p.grad is not None and name.endswith(\"weight\"):\n",
    "        wandb.log({f\"grad/{name}\": wandb.Histogram(p.grad.detach().cpu().numpy())}, step=global_step)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Model Weights & Checkpoints (optional)\n",
    "\n",
    "Save checkpoints locally **and** version them with **Artifacts**:\n",
    "\n",
    "```python\n",
    "import torch, os\n",
    "\n",
    "ckpt_path = f\"checkpoints/epoch{epoch:03d}_acc{val_acc:.3f}.pt\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save({\"epoch\": epoch, \"model\": model.state_dict()}, ckpt_path)\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"{wandb.run.project}-model\",\n",
    "    type=\"model\",\n",
    "    metadata={\"epoch\": epoch, \"val_acc\": val_acc, \"model\": cfg.model}\n",
    ")\n",
    "artifact.add_file(ckpt_path)\n",
    "wandb.log_artifact(artifact)\n",
    "```\n",
    "\n",
    "Later, in another run, you can **restore** a specific version:\n",
    "\n",
    "```python\n",
    "model_art = wandb.use_artifact(f\"{wandb.run.entity}/{wandb.run.project}-model:latest\")\n",
    "model_dir = model_art.download()\n",
    "# load from `model_dir/...pt`\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Artifacts (datasets, predictions, eval results)\n",
    "\n",
    "Version non-model files: raw or processed datasets, eval JSON, etc.\n",
    "\n",
    "```python\n",
    "# 1) Log a dataset folder (e.g., the exact split you trained on)\n",
    "ds_art = wandb.Artifact(\"cifar10-split-v1\", type=\"dataset\", metadata={\"split_seed\": 42})\n",
    "ds_art.add_dir(\"data/cifar10_split\")    # reproducible split\n",
    "wandb.log_artifact(ds_art)\n",
    "\n",
    "# 2) Log evaluation results as a structured file\n",
    "import json\n",
    "eval_payload = {\"epoch\": epoch, \"val_acc\": val_acc, \"per_class\": {\"cat\":0.95, \"dog\":0.91}}\n",
    "os.makedirs(\"eval\", exist_ok=True)\n",
    "with open(\"eval/metrics.json\", \"w\") as f:\n",
    "    json.dump(eval_payload, f, indent=2)\n",
    "\n",
    "eval_art = wandb.Artifact(\"eval-epoch-%03d\" % epoch, type=\"evaluation\")\n",
    "eval_art.add_file(\"eval/metrics.json\")\n",
    "wandb.log_artifact(eval_art)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Custom Visualizations\n",
    "\n",
    "### Images (e.g., predictions vs ground truth)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# imgs: (B,H,W,3) uint8 or file paths; preds, labels: lists\n",
    "samples = []\n",
    "for i in range(8):\n",
    "    img = np.random.randint(0, 255, size=(224,224,3), dtype=np.uint8)\n",
    "    pred, label = \"dog\", \"cat\"\n",
    "    samples.append(wandb.Image(img, caption=f\"true={label} pred={pred}\"))\n",
    "\n",
    "wandb.log({\"val/examples\": samples}, step=global_step)\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([0,1,2,1,0,2,2,1,0])\n",
    "y_pred = np.array([0,2,2,1,0,2,1,1,0])\n",
    "class_names = [\"cat\", \"dog\", \"car\"]\n",
    "\n",
    "cm_plot = wandb.plot.confusion_matrix(\n",
    "    probs=None,\n",
    "    y_true=y_true,\n",
    "    preds=y_pred,\n",
    "    class_names=class_names\n",
    ")\n",
    "wandb.log({\"val/confusion_matrix\": cm_plot}, step=global_step)\n",
    "```\n",
    "\n",
    "### Tables (inspectable rows with media)\n",
    "\n",
    "```python\n",
    "table = wandb.Table(columns=[\"id\", \"y_true\", \"y_pred\", \"confidence\", \"image\"])\n",
    "for i in range(5):\n",
    "    img = np.random.randint(0, 255, (128,128,3), dtype=np.uint8)\n",
    "    table.add_data(f\"img_{i}\", \"cat\", \"dog\", 0.63, wandb.Image(img))\n",
    "wandb.log({\"val/table_samples\": table}, step=global_step)\n",
    "```\n",
    "\n",
    "### Videos (mp4 or numpy tensor)\n",
    "\n",
    "```python\n",
    "# From file:\n",
    "wandb.log({\"demo/video\": wandb.Video(\"samples/clip.mp4\", fps=24, format=\"mp4\")}, step=global_step)\n",
    "\n",
    "# Or from a numpy tensor (T,H,W,C), uint8:\n",
    "vid = np.random.randint(0,255,(60,128,128,3), dtype=np.uint8)\n",
    "wandb.log({\"demo/sim_rollout\": wandb.Video(vid, fps=10, format=\"mp4\")}, step=global_step)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Putting it together (tiny end-to-end sketch)\n",
    "\n",
    "```python\n",
    "import wandb, torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "run = wandb.init(project=\"demo-proj\", config={\"lr\":1e-3, \"epochs\":3, \"batch_size\":32})\n",
    "cfg = run.config\n",
    "\n",
    "model = nn.Sequential(nn.Flatten(), nn.Linear(224*224*3, 10))\n",
    "opt = optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "wandb.watch(model, log=\"gradients\", log_freq=50)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    # ... your dataloader here ...\n",
    "    loss = torch.tensor(0.123)  # pretend\n",
    "    acc  = 0.91\n",
    "\n",
    "    wandb.log({\"train/loss\": loss.item(), \"train/acc\": acc, \"epoch\": epoch}, step=global_step)\n",
    "\n",
    "    # save checkpoint + artifact\n",
    "    ckpt = f\"checkpoints/epoch{epoch:03d}.pt\"\n",
    "    torch.save({\"model\": model.state_dict(), \"epoch\": epoch}, ckpt)\n",
    "    art = wandb.Artifact(\"demo-model\", type=\"model\", metadata={\"epoch\": epoch})\n",
    "    art.add_file(ckpt)\n",
    "    wandb.log_artifact(art)\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "run.finish()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can refactor these into a **drop-in `logger.py`** for your PyTorch template (with CLI flags like `--log online|offline|disabled`, automatic artifact versioning for checkpoints, and helper methods for images/tables/CM).\n",
    "\n",
    "  \n",
    "---\n",
    "\n",
    "##  **1.1 How is it Logged (Online Mode**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527ced4-d0d7-43bf-8eac-2f904dd8d107",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import wandb\n",
    "import os\n",
    "import math\n",
    "\n",
    "# wandb.require(\"core\")\n",
    "wandb.login()\n",
    "project = \"simulated-experiment\"\n",
    "config = {\n",
    "    \"lr\": 0.001,\n",
    "    \"model\": \"CNN\",\n",
    "    \"weight\": True\n",
    "}\n",
    "with wandb.init(project=project, config=config, name=\"\") as run:\n",
    "    epochs = 10\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = 1/(epoch)\n",
    "        acc = 1 - 2/(epoch*epoch)\n",
    "\n",
    "        run.log({\"acc\": acc, \"loss\": loss})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcda7c-73fb-47fd-8318-cdcc52cc04d9",
   "metadata": {},
   "source": [
    "\n",
    "#### **Log hyperparameters**\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.init( project=\"my-project\", entity=\"behnamasadi\", config={\"learning_rate\": 0.01, \"epochs\": 5,  \"batch_size\": 64})\n",
    "```\n",
    "\n",
    "- The `entity` parameter refers to the username or team name that owns the project. It is useful when you want to organize projects under different teams or users within your wandb workspace.\n",
    "When you omit the `entity` parameter, wandb defaults to using your personal account (the one you're logged in with)\n",
    "\n",
    "- The `config` parameter is used to track and version your hyperparameters, model configuration, and other experiment settings. It's a dictionary that gets stored with your run and can be used to:\n",
    "\n",
    "1. Track experiment parameters (like learning rate, batch size, epochs)\n",
    "2. Compare different configurations across runs\n",
    "3. Version your experiments\n",
    "4. Reproduce experiments later\n",
    "\n",
    "In the **wandb** dashboard, if you click on `project>my-project`, then select your run under `files` you can see `config.yaml`\n",
    "\n",
    "\n",
    "You can access these config values during your run using:\n",
    "```python\n",
    "wandb.config.learning_rate  # returns 0.01\n",
    "wandb.config.epochs         # returns 5\n",
    "wandb.config.batch_size     # returns 64\n",
    "```\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6e2fd-5308-49f6-9597-9a3cee8f063c",
   "metadata": {},
   "source": [
    "#### **Log metrics**\n",
    "\n",
    "\n",
    "```python\n",
    "    for epoch in range(10):\n",
    "\n",
    "    train_loss = random.random()\n",
    "    val_loss = random.random()\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed1f80-b430-4c71-8f74-c685210a69ef",
   "metadata": {},
   "source": [
    "#### **Logging Model Gradients, Weights, and Checkpoints**\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"simple-example\", name=\"gradients-weights-checkpoints\")\n",
    "\n",
    "# Simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 1)\n",
    "\n",
    "# Training\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(x)\n",
    "    loss = loss_fn(preds, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    #  Log gradients and weights\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        wandb.log({\n",
    "            f\"gradients/{name}\": wandb.Histogram(param.grad.detach().cpu().numpy()),\n",
    "            f\"weights/{name}\": wandb.Histogram(param.detach().cpu().numpy())\n",
    "        })\n",
    "\n",
    "#  Save and log model checkpoint\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "wandb.save(\"model.pth\")\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b5fba-6023-4d92-875d-724652fe8e2f",
   "metadata": {},
   "source": [
    "#### **Logging an Artifact**\n",
    "\n",
    "\n",
    "```python\n",
    "# Create an artifact\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('model.pth')\n",
    "wandb.log_artifact(artifact)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a76585-5b7e-4336-8ddc-18a30894d9b7",
   "metadata": {},
   "source": [
    "#### **Logging Custom Visualizations**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Dummy labels\n",
    "y_true = np.random.randint(0, 3, size=(100,))\n",
    "y_pred = np.random.randint(0, 3, size=(100,))\n",
    "\n",
    "#  Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
    "\n",
    "# Log some sample images\n",
    "for i in range(5):\n",
    "    random_image = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)\n",
    "    wandb.log({f\"sample_image_{i}\": [wandb.Image(random_image, caption=f\"Random {i}\")]})\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47533559-a313-460b-b59c-b9cbf13280b3",
   "metadata": {},
   "source": [
    "##  **1.2 Offline Mode (local logging, sync later)**\n",
    "You can **run Weights & Biases (WandB) locally** without sending data to the WandB cloud server ( **\"offline mode\"** or **\"local mode\"**).\n",
    "\n",
    "Logs data to your local machine first, and you can choose to sync to the server later.\n",
    "\n",
    "```bash\n",
    "export WANDB_MODE=offline\n",
    "```\n",
    "\n",
    "Or in Python:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.init(mode=\"offline\", project=\"my-project\", config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 64\n",
    "})\n",
    "```\n",
    "\n",
    "This creates a local folder `wandb/` with logs.\n",
    "\n",
    "1. **Default Location**: By default, wandb creates a `wandb` directory in your current working directory. This is where it stores all the run data, including logs, configuration, and model checkpoints.\n",
    "\n",
    "2. **Environment Variables**: You can override the default location by setting the `WANDB_DIR` environment variable. This allows you to specify a custom directory for wandb to store its data.\n",
    "\n",
    "3. **Configuration in `wandb.init()`**: When you initialize wandb with `wandb.init()`, you can specify the `dir` parameter to set a custom directory for the current run. This is useful if you want to change the location for a specific run without affecting others.\n",
    "\n",
    "4. **Run ID**: Each run is assigned a unique ID, which is used to create a subdirectory within the `wandb` directory. This subdirectory contains all the data related to that specific run, including logs.\n",
    "\n",
    "\n",
    "\n",
    "### Browse Data\n",
    "\n",
    "Now you can visualize your logged data using WandB's built-in UI:\n",
    "\n",
    "#### Option A: Sync and View in Browser (but still local)\n",
    "1. Run this to convert offline logs into viewable runs:\n",
    "\n",
    "\n",
    "```bash\n",
    "wandb server start\n",
    "```\n",
    "\n",
    "```bash\n",
    "wandb server stop\n",
    "```\n",
    "\n",
    "```bash\n",
    "wandb status\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "wandb sync wandb/offline-run-*\n",
    "```\n",
    "\n",
    "2. Then open the local dashboard:\n",
    "```bash\n",
    "wandb local\n",
    "```\n",
    "\n",
    "This launches a local server at:\n",
    "```\n",
    "http://localhost:8080\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Later, if you want to sync to the cloud:\n",
    "\n",
    "```bash\n",
    "wandb sync wandb/offline-run-*\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9d4f-272c-4909-97de-f21be1473054",
   "metadata": {},
   "source": [
    "# **2. WandB Configuration**\n",
    "\n",
    "## 2.1. Settings\n",
    "\n",
    "```bash\n",
    "wandb status\n",
    "```\n",
    "\n",
    "give status about  your settings:\n",
    "\n",
    "```bash\n",
    "Current Settings\n",
    "{\n",
    "  \"_extra_http_headers\": null,\n",
    "  \"_proxies\": null,\n",
    "  \"api_key\": null,\n",
    "  \"base_url\": \"https://api.wandb.ai\",\n",
    "  \"entity\": null,\n",
    "  \"git_remote\": \"origin\",\n",
    "  \"ignore_globs\": [],\n",
    "  \"organization\": null,\n",
    "  \"project\": null,\n",
    "  \"root_dir\": null,\n",
    "  \"section\": \"default\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "WandB uses a file called `.wandb/settings` to store configuration.  \n",
    "It can be in two places:\n",
    "- **Local project**: inside your current folder, like `./wandb/settings`\n",
    "- **Global user**: inside your home directory `~/.config/wandb/settings`\n",
    "\n",
    "which might be like:\n",
    "\n",
    "```\n",
    "[default]\n",
    "base_url = https://api.wandb.ai\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Password and API key\n",
    "\n",
    "The file is in `~/.netrc` (permissions must be 600), so fix it by `chmod 600 ~/.netrc`\n",
    "\n",
    "\n",
    "```\n",
    "machine api.wandb.ai\n",
    "  login behnamasadi\n",
    "  password <API-KEY>\n",
    "```\n",
    "\n",
    "Now run:\n",
    "\n",
    "```bash\n",
    "wandb login --relogin\n",
    "```\n",
    "\n",
    "Paste your **API Key** from [https://wandb.ai/settings](https://wandb.ai/settings).\n",
    "\n",
    "This creates a fresh clean setup that points **only to the cloud**.\n",
    "\n",
    "The API key should go to:\n",
    "\n",
    "\n",
    "```\n",
    "~/.netrc\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079f553-42f9-4ed3-83e2-7ec86a11bce8",
   "metadata": {},
   "source": [
    "## **2.3. Remove any Docker containers/images/servers related to WandB**\n",
    "\n",
    "#### 1. **Check all running Docker containers**\n",
    "\n",
    "First, list any running containers:\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "If you see containers like `wandb-local`, `wandb-server`, `wandb-postgres`, etc â€”  \n",
    " they are still running.\n",
    "\n",
    " Stop all WandB-related containers:\n",
    "\n",
    "```bash\n",
    "docker stop $(docker ps -q --filter \"ancestor=wandb/local\")\n",
    "```\n",
    "or more generally:\n",
    "\n",
    "```bash\n",
    "docker ps | grep wandb\n",
    "docker stop <container_id>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Remove WandB containers**\n",
    "\n",
    "List **all containers** (including stopped ones):\n",
    "\n",
    "```bash\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "If you see wandb-related ones (names like `wandb-local`, `wandb-server`),  \n",
    "then remove them:\n",
    "\n",
    "```bash\n",
    "docker rm <container_id>\n",
    "```\n",
    "or if you want to **force remove all stopped containers**:\n",
    "\n",
    "```bash\n",
    "docker container prune\n",
    "```\n",
    "( Caution: this removes **all** stopped containers.)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Remove WandB Docker images**\n",
    "\n",
    "Now remove WandB docker images to free space.\n",
    "\n",
    "List docker images:\n",
    "\n",
    "```bash\n",
    "docker images\n",
    "```\n",
    "\n",
    "Look for images named like:\n",
    "- `wandb/local`\n",
    "- `wandb/server`\n",
    "- or anything with `wandb`\n",
    "\n",
    "Then remove them:\n",
    "\n",
    "```bash\n",
    "docker rmi <image_id>\n",
    "```\n",
    "\n",
    "Or force remove **all** unused images:\n",
    "\n",
    "```bash\n",
    "docker image prune -a\n",
    "```\n",
    "( Careful: this removes all images you aren't actively using.)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **(Optional) Remove Docker volumes**\n",
    "\n",
    "Sometimes WandB also creates **Docker volumes** (for database, storage).\n",
    "\n",
    "List volumes:\n",
    "\n",
    "```bash\n",
    "docker volume ls\n",
    "```\n",
    "\n",
    "If you see wandb-related volumes (names like `wandb-db`, `wandb-storage`), remove them:\n",
    "\n",
    "```bash\n",
    "docker volume rm <volume_name>\n",
    "```\n",
    "\n",
    "Or prune all unused volumes:\n",
    "\n",
    "```bash\n",
    "docker volume prune\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **(Optional) Remove WandB server install files**\n",
    "\n",
    "If you previously downloaded a WandB `docker-compose.yml` or setup folder for self-hosted server,  \n",
    "manually delete it:\n",
    "\n",
    "```bash\n",
    "rm -rf /path/to/your/wandb-server-folder\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
