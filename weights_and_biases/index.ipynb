{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc4d0d2-f34a-4bd9-b26b-b02900625eef",
   "metadata": {},
   "source": [
    "# Weights & Biases (WandB)\n",
    "\n",
    "## 1. Login\n",
    "\n",
    "Weights & Biases (wandb) Login Guide:\n",
    "\n",
    "WHERE TO GET YOUR API KEY:\n",
    "1. Go to https://wandb.ai/authorize (or https://wandb.ai/settings)\n",
    "2. Sign up/Login to your wandb account\n",
    "3. Copy your API key from the settings page\n",
    "\n",
    "HOW TO LOGIN (choose one method):\n",
    "\n",
    "**Method 1** \n",
    "- Environment Variable (Recommended for production):\n",
    "```bash\n",
    "    export WANDB_API_KEY=\"your-api-key-here\"\n",
    "```\n",
    "\n",
    "\n",
    "This ensures your script never silently trains without `wandb` logging.\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "key = os.environ.get(\"WANDB_API_KEY\")\n",
    "\n",
    "if key is None:\n",
    "    raise RuntimeError(\"WANDB_API_KEY not set in environment\")\n",
    "\n",
    "wandb.login(key=key)\n",
    "```\n",
    "\n",
    "\n",
    "For Jupyter notebooks: Notebook magic (no need to paste the key):\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "wandb.login()\n",
    "```\n",
    "\n",
    "This will open a browser and authenticate using OAuth instead of API keys.\n",
    "\n",
    "\n",
    "\n",
    "**Method 2** \n",
    "- Command Line Login (Easiest for first time):\n",
    "    Run in terminal:\n",
    "```bash\n",
    "wandb login\n",
    "```\n",
    "It will prompt you to paste your API key and save it locally\n",
    "\n",
    "**Method 3**\n",
    "- Pass key directly in code:\n",
    "```python \n",
    "    wandb.login(key=\"your-api-key-here\")\n",
    "```\n",
    "\n",
    "**Method 4**\n",
    "- Interactive prompt:\n",
    "    Just call\n",
    "```python\n",
    "    wandb.login() \n",
    "```\n",
    "\n",
    "without any arguments, It will prompt you to enter the key or open a browser\n",
    "\n",
    "\n",
    "#### Authentication\n",
    "wandb will automatically use credentials in this order:\n",
    "1. `WANDB_API_KEY` environment variable (if set)\n",
    "2. Previously saved credentials from 'wandb login' command\n",
    "3. Prompt interactively if neither is available\n",
    "You can also explicitly login with: wandb.login(key=\"your-api-key\")\n",
    "\n",
    "WHERE CREDENTIALS ARE STORED:\n",
    "\n",
    "When you run 'wandb login', your API key is saved in:\n",
    "\n",
    "```bash\n",
    "  ~/.netrc (or ~/_netrc on Windows)\n",
    "```\n",
    "The `.netrc` file format:\n",
    "\n",
    "```bash\n",
    "  machine api.wandb.ai\n",
    "    login user\n",
    "    password <your-api-key>\n",
    "```\n",
    "Additional settings may be stored in:\n",
    "\n",
    "- `~/.config/wandb/settings`\n",
    "- `~/.config/wandb/credentials.json` (for OIDC tokens)\n",
    "\n",
    "#### How WANDB Reads Credentials\n",
    "\n",
    "1. Checks `WANDB_API_KEY` environment variable\n",
    "2. Reads from `~/.netrc` file using `requests.utils.get_netrc_auth()`\n",
    "3. Checks for credentials.json in `~/.config/wandb/`\n",
    "4. Prompts interactively if none found\n",
    "\n",
    "To view your stored credentials location:\n",
    "```bash\n",
    "  cat ~/.netrc | grep -A 2 wandb\n",
    "```\n",
    "To logout/remove credentials:\n",
    "```bash\n",
    "  wandb logout\n",
    "```\n",
    "\n",
    "(or manually edit/delete `~/.netrc`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d817-9ca9-4bd8-91e1-4d8fcd5c0d7e",
   "metadata": {},
   "source": [
    "## 2. Production-Style Way to Handle Weights & Biases (wandb) API key\n",
    "Below is the cleanest, safest, production-style way to handle your Weights & Biases (wandb) API key when using PyTorch projects. This avoids hard-coding secrets, keeps your code portable, and works in local, Docker, and cloud training environments.\n",
    "\n",
    "\n",
    "\n",
    "#### In Docker containers\n",
    "\n",
    "Pass the environment variable:\n",
    "\n",
    "```bash\n",
    "docker run --env WANDB_API_KEY=$WANDB_API_KEY my-training-image\n",
    "```\n",
    "\n",
    "\n",
    "#### In GitHub Actions (CI/CD)\n",
    "\n",
    "Store the key as a GitHub Secret:\n",
    "\n",
    "Settings → Secrets → Actions → `WANDB_API_KEY`\n",
    "\n",
    "Workflow:\n",
    "\n",
    "```yaml\n",
    "env:\n",
    "  WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}\n",
    "\n",
    "steps:\n",
    "  - name: Login to wandb\n",
    "    run: |\n",
    "      python - <<'EOF'\n",
    "      import os, wandb\n",
    "      wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "      EOF\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02083643-13a5-4e7b-a196-eab507cdc555",
   "metadata": {},
   "source": [
    "## 3. W&B In PyTorch Projects\n",
    "\n",
    "A practical guide to:\n",
    "- hyperparameters\n",
    "- metrics\n",
    "- artifacts\n",
    "- images\n",
    "- videos\n",
    "- model checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "#### Project Setup and Initialization\n",
    "\n",
    "Every experiment in W&B begins with:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"demo-proj\",\n",
    "    name=\"exp-resnet50-lr1e-3\",\n",
    "    config={\n",
    "        \"seed\": 42,\n",
    "        \"model\": \"resnet50\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 20,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"num_layers\": 50,\n",
    "        \"dataset\": \"CIFAR10\",\n",
    "        \"img_size\": 224,\n",
    "    },\n",
    ")\n",
    "\n",
    "cfg = wandb.config\n",
    "```\n",
    "\n",
    "**Why use `config`?**\n",
    "\n",
    "* Keeps hyperparameters versioned\n",
    "* Makes comparisons easy\n",
    "* Exposes filters on the W&B dashboard\n",
    "* Lets you run sweeps with zero code changes\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Creating Multiple Experiments Inside The Same Project\n",
    "\n",
    "#### Core principle\n",
    "\n",
    "A **project** groups runs.\n",
    "A **run name** identifies one specific experiment. **Every new experiment/run requires a new `wandb.init()` call.**\n",
    "\n",
    "So you can keep:\n",
    "\n",
    "```python\n",
    "run = wandb.init(project=\"foo_test\", name=\"something\", config={...})\n",
    "```\n",
    "\n",
    "And make the name change automatically on each run.\n",
    "\n",
    "Below are the best patterns used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Let wandb auto-generate run names (simplest)\n",
    "\n",
    "This is surprisingly good and widely used:\n",
    "\n",
    "```python\n",
    "run = wandb.init(project=\"foo_test\", config=config_dict)\n",
    "```\n",
    "\n",
    "wandb will generate names like:\n",
    "\n",
    "```\n",
    "wonderful-thunder-17\n",
    "bright-sun-42\n",
    "```\n",
    "\n",
    "All unique. No manual work.\n",
    "Great for development phases.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Add a human prefix + auto-generated suffix\n",
    "\n",
    "Better for organized experiments:\n",
    "\n",
    "```python\n",
    "run = wandb.init(\n",
    "    project=\"foo_test\",\n",
    "    name=f\"fee-{wandb.util.generate_id()}\",\n",
    "    config=config_dict\n",
    ")\n",
    "```\n",
    "\n",
    "Result example:\n",
    "\n",
    "```\n",
    "fee-x8375a\n",
    "fee-lks992\n",
    "```\n",
    "\n",
    "Clean and traceable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Add timestamps (common in research code)\n",
    "\n",
    "This is deterministic and avoids name collisions:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"foo_test\",\n",
    "    name=f\"fee-{time.strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config=config_dict\n",
    ")\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "fee-20251208-152532\n",
    "```\n",
    "\n",
    "This method is excellent when sweeping through many configs.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Auto-increment run index (perfect for sequential experiments)\n",
    "\n",
    "Store a counter in a file `.run_counter`:\n",
    "\n",
    "Python:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "counter_file = Path(\"run_counter.txt\")\n",
    "if counter_file.exists():\n",
    "    run_id = int(counter_file.read_text()) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "\n",
    "counter_file.write_text(str(run_id))\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"foo_test\",\n",
    "    name=f\"fee-{run_id}\",\n",
    "    config=config_dict,\n",
    ")\n",
    "```\n",
    "\n",
    "Run names become:\n",
    "\n",
    "```\n",
    "fee-1\n",
    "fee-2\n",
    "fee-3\n",
    "...\n",
    "```\n",
    "\n",
    "This is great if you want human numbering.\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "import wandb\n",
    "import uuid\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"foo_test\",\n",
    "    name=f\"fee-{uuid.uuid4().hex[:6]}\",\n",
    "    config=config_dict\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Example names:\n",
    "\n",
    "```\n",
    "fee-a93f11\n",
    "fee-884bc2\n",
    "fee-41e23d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Use wandb group and job_type for structure\n",
    "\n",
    "If your experiments have variations:\n",
    "\n",
    "```python\n",
    "run = wandb.init(\n",
    "    project=\"foo_test\",\n",
    "    name=f\"fee-lr{lr}-bs{batch_size}\",\n",
    "    group=\"baseline\",\n",
    "    job_type=\"training\",\n",
    "    config=config_dict\n",
    ")\n",
    "```\n",
    "\n",
    "This allows grouped comparison in UI.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea8444-16e8-4049-8ab3-49f8625ff4b5",
   "metadata": {},
   "source": [
    "## 5. Logging Metrics During Training\n",
    "\n",
    "W&B captures evolving metrics over time.\n",
    "Use hierarchical names (`train/loss`, `val/loss`) to keep dashboards tidy.\n",
    "\n",
    "```python\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    # assume you compute these:\n",
    "    train_loss, train_acc = 0.42, 0.91\n",
    "    val_loss, val_acc     = 0.38, 0.93\n",
    "\n",
    "    wandb.log({\n",
    "        \"global_step\": global_step,\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/acc\":  train_acc,\n",
    "        \"val/loss\":   val_loss,\n",
    "        \"val/acc\":    val_acc,\n",
    "    }, step=global_step)\n",
    "\n",
    "    global_step += 1\n",
    "```\n",
    "\n",
    "You can call `wandb.log()` every batch or every epoch.\n",
    "\n",
    "---\n",
    "\n",
    "#### `train/loss` is **always a y-axis metric**\n",
    "\n",
    "Every key you log (loss, accuracy, lr, etc.) becomes a **y-value** in a time series.\n",
    "\n",
    "So W&B interprets:\n",
    "\n",
    "```\n",
    "train/loss → y value\n",
    "val/loss   → y value\n",
    "lr         → y value\n",
    "```\n",
    "\n",
    "Always.\n",
    "\n",
    "---\n",
    "\n",
    "#### What is the x-axis? It depends on what you provide.\n",
    "\n",
    "W&B needs a **step** value for the x-axis.\n",
    "There are **three ways** it gets that.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case A — You call `wandb.log({...})` with no `step=` argument\n",
    "\n",
    "W&B uses an **internal counter**:\n",
    "\n",
    "```\n",
    "step = 0\n",
    "step = 1\n",
    "step = 2\n",
    "...\n",
    "```\n",
    "\n",
    "Every time you call `wandb.log()`, W&B increments a global step value.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "wandb.log({\"train/loss\": 0.3})  # step = 0\n",
    "wandb.log({\"train/loss\": 0.2})  # step = 1\n",
    "wandb.log({\"train/loss\": 0.1})  # step = 2\n",
    "```\n",
    "\n",
    "Here the x-axis is **W&B's auto-step**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case B — You explicitly set the step:\n",
    "\n",
    "This overrides W&B:\n",
    "\n",
    "```python\n",
    "wandb.log({\"train/loss\": 0.42}, step=global_step)\n",
    "```\n",
    "\n",
    "Now the x-axis is **your own variable**, such as:\n",
    "\n",
    "* global_step\n",
    "* iteration\n",
    "* epoch\n",
    "* batch_index\n",
    "\n",
    "Whatever you choose.\n",
    "\n",
    "This is the **recommended way**, because it gives you full control.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case C — You define a metric to use another metric as x-axis\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "wandb.define_metric(\"epoch\")\n",
    "wandb.define_metric(\"train/loss\", step_metric=\"epoch\")\n",
    "```\n",
    "\n",
    "Then the x-axis of `train/loss` will be `epoch`, not step number.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc75382-e9f9-4c9d-83dc-14bee114765d",
   "metadata": {},
   "source": [
    "## 6. Logging Model Checkpoints/ Artifacts\n",
    "\n",
    "You can store checkpoints locally, but **Artifacts** make them reproducible and shareable.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ckpt_path = f\"checkpoints/epoch{epoch:03d}_acc{val_acc:.3f}.pt\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model\": model.state_dict(),\n",
    "}, ckpt_path)\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"{wandb.run.project}-model\",\n",
    "    type=\"model\",\n",
    "    metadata={\"epoch\": epoch, \"val_acc\": val_acc, \"model\": cfg.model}\n",
    ")\n",
    "\n",
    "artifact.add_file(ckpt_path)\n",
    "wandb.log_artifact(artifact)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35b8d5-8796-4113-bc14-76df4f341c07",
   "metadata": {},
   "source": [
    "## 7. Logging Images\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "samples = []\n",
    "for i in range(8):\n",
    "    img = np.random.randint(0,255,(224,224,3),dtype=np.uint8)\n",
    "    pred, label = \"dog\", \"cat\"\n",
    "    samples.append(wandb.Image(img, caption=f\"true={label} pred={pred}\"))\n",
    "\n",
    "wandb.log({ \"val/examples\": samples }, step=global_step)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396631c4-82c1-4897-93fe-82123368e1f1",
   "metadata": {},
   "source": [
    "## 8. Logging Plots/Confusion Matrix\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([0,1,2,1,0,2,2,1,0])\n",
    "y_pred = np.array([0,2,2,1,0,2,1,1,0])\n",
    "class_names = [\"cat\", \"dog\", \"car\"]\n",
    "\n",
    "#wandb.plot.bar\n",
    "#wandb.plot.roc_curve\n",
    "\n",
    "cm_plot = wandb.plot.confusion_matrix(\n",
    "    y_true=y_true,\n",
    "    preds=y_pred,\n",
    "    probs=None,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "wandb.log({\"val/confusion_matrix\": cm_plot}, step=global_step)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2dead-acc6-4081-b1b8-b0ba8f82405c",
   "metadata": {},
   "source": [
    "## 9. Logging Gradients and Weights (Optional)\n",
    "\n",
    "W&B can automatically track:\n",
    "\n",
    "* gradient distributions\n",
    "* parameter histograms\n",
    "* model topology\n",
    "\n",
    "```python\n",
    "model = nn.Linear(10, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "wandb.watch(model, log=\"gradients\", log_freq=1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    x = torch.randn(4, 10)\n",
    "    y = torch.tensor([0, 1, 2, 1])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()          # GRADIENTS GENERATED HERE\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({\n",
    "        \"train/loss\": loss.item(),\n",
    "        \"train/acc\": torch.rand(1).item()\n",
    "    })\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
