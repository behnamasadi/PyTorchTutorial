{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d185a7-82f2-4168-a9d0-f1ea0a67852f",
   "metadata": {},
   "source": [
    "# 1. What gradient clipping is\n",
    "\n",
    "**Gradient clipping limits the *magnitude* of the gradients** during backprop to prevent sudden exploding updates.\n",
    "\n",
    "The most common form is **norm clipping**:\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_{\\text{clipped}} = \\mathbf{g} \\cdot \\min\\left(1, \\frac{\\tau}{\\|\\mathbf{g}\\|}\\right)\n",
    "$$\n",
    "\n",
    "If the gradient norm is larger than `clip_norm`, it is scaled down proportionally.\n",
    "\n",
    "This prevents:\n",
    "\n",
    "* exploding gradients\n",
    "* unstable parameter updates\n",
    "* extremely large steps that break training (common in RNNs, Transformers, and reinforcement learning)\n",
    "\n",
    "### Example in PyTorch Lightning\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n",
    "```\n",
    "\n",
    "### Pure PyTorch\n",
    "\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2. What weight decay / L2 regularization does\n",
    "\n",
    "**Weight decay penalizes large weights**, not large gradients.\n",
    "\n",
    "Classic L2 regularization adds\n",
    "\n",
    "$$\n",
    "\\lambda \\lVert w \\rVert^2\n",
    "$$\n",
    "\n",
    "to the loss, which modifies the gradient update by shrinking weights:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta (\\nabla L + \\lambda w)\n",
    "$$\n",
    "\n",
    "This improves:\n",
    "\n",
    "* generalization\n",
    "* overfitting control\n",
    "* smoother weight distributions\n",
    "\n",
    "**It does NOT prevent exploding gradients.**\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Why gradient clipping ≠ weight decay\n",
    "\n",
    "| Mechanism             | Modifies  | Purpose            | Prevents exploding gradients? |\n",
    "| --------------------- | --------- | ------------------ | ----------------------------- |\n",
    "| **Gradient clipping** | gradients | training stability | **Yes**                       |\n",
    "| **Weight decay / L2** | weights   | regularization     | ❌ No                          |\n",
    "\n",
    "Even with strong L2 regularization:\n",
    "\n",
    "* gradients can still blow up\n",
    "* weights can change too quickly\n",
    "* training can diverge in one update step\n",
    "\n",
    "Weight decay shrinks the *weights slowly*.\n",
    "Gradient clipping prevents *one bad update*.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. When clipping is essential\n",
    "\n",
    "Used almost always in:\n",
    "\n",
    "* RNNs (LSTMs, GRUs)\n",
    "* Transformers\n",
    "* GANs\n",
    "* Reinforcement learning\n",
    "* Very deep networks\n",
    "\n",
    "Because their gradients can explode unpredictably.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Why clipping is NOT a substitute for L2 / weight decay\n",
    "\n",
    "You still want regularization to prevent overfitting.\n",
    "Clipping does not help generalization; it only prevents training from blowing up.\n",
    "\n",
    "You still want clipping to avoid catastrophic steps in training.\n",
    "Weight decay cannot do that.\n",
    "\n",
    "They complement each other.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fe0d2-90e3-4f57-8f8a-9f0c28872ea0",
   "metadata": {},
   "source": [
    "Gradient clipping is a **stabilization technique** used during training to prevent gradients from becoming **too large** (exploding gradients).\n",
    "PyTorch Lightning exposes this as:\n",
    "\n",
    "```python\n",
    "Trainer(gradient_clip_val=1.0)\n",
    "```\n",
    "\n",
    "Here is the clear explanation.\n",
    "\n",
    "---\n",
    "\n",
    "# **What is `gradient_clip_val`?**\n",
    "\n",
    "It sets a **maximum allowed norm** (or value) for your gradients.\n",
    "During backpropagation, if the gradients exceed this threshold, Lightning **scales them down**.\n",
    "\n",
    "So:\n",
    "\n",
    "* If gradients are small → do nothing\n",
    "* If gradients are huge → shrink them to a safe range\n",
    "\n",
    "This prevents updates that would **blow up your weights** or destabilize training.\n",
    "\n",
    "---\n",
    "\n",
    "# **Why exploding gradients are bad**\n",
    "\n",
    "Exploding gradients cause:\n",
    "\n",
    "* very large weight updates\n",
    "* unstable loss\n",
    "* NaNs during backprop\n",
    "* divergence instead of convergence\n",
    "\n",
    "This commonly happens with:\n",
    "\n",
    "* recurrent networks (LSTMs, GRUs)\n",
    "* transformers with high learning rates\n",
    "* very deep networks\n",
    "* unstable tasks (e.g., RL, GANs)\n",
    "\n",
    "---\n",
    "\n",
    "# **How clipping works mathematically**\n",
    "\n",
    "If you set:\n",
    "\n",
    "```python\n",
    "gradient_clip_val = c\n",
    "```\n",
    "\n",
    "Then Lightning enforces:\n",
    "\n",
    "$$\n",
    "\\Vert g \\Vert_2 \\le c\n",
    "$$\n",
    "\n",
    "If the norm of gradients is larger than (c), they are scaled:\n",
    "\n",
    "$$\n",
    "g := g \\cdot \\frac{c}{\\Vert g \\Vert_2}\n",
    "$$\n",
    "\n",
    "So gradients stay inside a safe range.\n",
    "\n",
    "---\n",
    "\n",
    "# **Type of clipping used by default**\n",
    "\n",
    "Lightning uses **gradient norm clipping** (L2 norm) by default:\n",
    "\n",
    "```python\n",
    "Trainer(gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\")\n",
    "```\n",
    "\n",
    "You can also clip by value:\n",
    "\n",
    "```python\n",
    "Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n",
    "```\n",
    "\n",
    "Value clipping simply does:\n",
    "\n",
    "$$\n",
    "g := \\text{clip}(g, -c, c)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **Typical values that people use**\n",
    "\n",
    "Different for different models:\n",
    "\n",
    "| Model              | Typical clip value |\n",
    "| ------------------ | ------------------ |\n",
    "| RNN / LSTM / GRU   | 0.1 to 1.0         |\n",
    "| Transformers       | 0.5 to 1.0         |\n",
    "| GANs               | 0.5 to 5.0         |\n",
    "| CNNs               | often not needed   |\n",
    "| Very deep networks | 0.25 to 1.0        |\n",
    "\n",
    "---\n",
    "\n",
    "# **What happens if you set gradient_clip_val too high or too low?**\n",
    "\n",
    "### If too low (e.g., 0.01):\n",
    "\n",
    "* model trains very slowly\n",
    "* gradients always clipped\n",
    "* underfitting\n",
    "\n",
    "### If too high (e.g., 10000):\n",
    "\n",
    "* effectively no clipping\n",
    "* exploding gradients still happen\n",
    "\n",
    "**Typical safe default:**\n",
    "\n",
    "```python\n",
    "gradient_clip_val = 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **Example in PyTorch Lightning**\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    gradient_clip_algorithm=\"norm\"\n",
    ")\n",
    "```\n",
    "\n",
    "Lightning performs clipping automatically **after backward()** and **before optimizer.step()**.\n",
    "\n",
    "---\n",
    "\n",
    "# **Intuition summary**\n",
    "\n",
    "* The goal is to prevent gradients from blowing up.\n",
    "* We restrict their magnitude.\n",
    "* This keeps training stable, especially for deep or sensitive architectures.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can show you:\n",
    "\n",
    "* A visualization of gradient clipping\n",
    "* A minimal PyTorch example showing exploding gradients without clipping\n",
    "* A recommended clip value for your current models (ConvNeXt, ViT, etc.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
