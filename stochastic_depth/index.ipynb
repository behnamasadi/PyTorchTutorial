{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27788ad7-2afc-4d7b-9770-5916c1a1cb18",
   "metadata": {},
   "source": [
    "## **Stochastic Depth (DropPath)**\n",
    "**Stochastic Depth** is still **relevant and widely used** today, especially in **deep convolutional** and **transformer-based architectures** (e.g., **ResNet**, **EfficientNet**, **Vision Transformers**, **Swin Transformer**, **ConvNeXt**, etc.).\n",
    "\n",
    "Let’s break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "When training **very deep networks**, such as ResNet-152 or beyond, the following problems occur:\n",
    "\n",
    "* **Overfitting** — too many layers memorize the training data.\n",
    "* **Vanishing gradients** — especially in early layers.\n",
    "* **High training time** — every layer always participates in every iteration.\n",
    "\n",
    "To combat this, **Huang et al. (2016)** introduced **Stochastic Depth**, also known as **Layer Dropout**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Core Idea\n",
    "\n",
    "Instead of using all residual blocks during training, you **randomly drop entire residual blocks** (i.e., skip connections) with some probability.\n",
    "At inference time, **all layers are active**, but their outputs are **rescaled** to match the expected value during training.\n",
    "\n",
    "So, during training, some layers are **bypassed entirely**, forcing the network to learn to **be robust to missing layers**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example (Residual Block)\n",
    "\n",
    "A standard residual block:\n",
    "\n",
    "$$\n",
    "x_{l+1} = x_l + f_l(x_l)\n",
    "$$\n",
    "\n",
    "With **stochastic depth**, you modify it as:\n",
    "\n",
    "$$\n",
    "x_{l+1} =\n",
    "\\begin{cases}\n",
    "x_l + f_l(x_l), & \\text{with probability } (1 - p_l) \\\n",
    "x_l, & \\text{with probability } p_l\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where ( p_l ) is the **drop probability** for layer ( l ).\n",
    "\n",
    "At inference time, you use all layers but **scale** the residual:\n",
    "\n",
    "$$\n",
    "x_{l+1} = x_l + (1 - p_l) f_l(x_l)\n",
    "$$\n",
    "\n",
    "This ensures that the **expected output** is consistent between training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implementation Example (PyTorch)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p  # probability of dropping the layer\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return x + residual\n",
    "        if torch.rand(1) < self.p:\n",
    "            return x  # drop layer (skip)\n",
    "        else:\n",
    "            return x + residual / (1 - self.p)  # scale to preserve expectation\n",
    "```\n",
    "\n",
    "You can use this inside a **ResNet block** or **Transformer block**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why It Helps\n",
    "\n",
    "✅ **Regularization effect:**\n",
    "Each mini-batch effectively trains a slightly shallower network — like an ensemble of subnetworks.\n",
    "\n",
    "✅ **Better gradient flow:**\n",
    "Fewer active layers per iteration → shallower path for gradient propagation.\n",
    "\n",
    "✅ **Less overfitting:**\n",
    "Especially helpful when you have limited data or very deep networks.\n",
    "\n",
    "✅ **Faster training:**\n",
    "Some layers are skipped, reducing computation.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Modern Usage\n",
    "\n",
    "Still very relevant:\n",
    "\n",
    "* **ResNet variants** — often implemented as `DropPath` in modern libraries.\n",
    "* **Vision Transformers (ViT, DeiT, Swin, ConvNeXt, EfficientNetV2)** — use **DropPath** or **Stochastic Depth** for regularization.\n",
    "* **Transformers** — similar idea applied to **residual branches**, sometimes called **LayerDrop**.\n",
    "\n",
    "In PyTorch/Timm, it’s implemented as:\n",
    "\n",
    "```python\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "x = x + DropPath(prob=0.1)(residual)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Relation to Other Techniques\n",
    "\n",
    "| Technique                   | Drops                 | Level         | Goal                     |\n",
    "| --------------------------- | --------------------- | ------------- | ------------------------ |\n",
    "| Dropout                     | Random neurons        | Within layer  | Prevent co-adaptation    |\n",
    "| DropConnect                 | Random weights        | Within layer  | Regularize connections   |\n",
    "| Stochastic Depth (DropPath) | Entire residual block | Across layers | Regularize deep networks |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Is It Still Relevant?\n",
    "\n",
    "✅ **Yes**, very much.\n",
    "\n",
    "Especially in **large ViT-based models**, stochastic depth (DropPath) is standard.\n",
    "For example:\n",
    "\n",
    "* **DeiT** uses stochastic depth = 0.1\n",
    "* **Swin Transformer** uses stochastic depth up to 0.3\n",
    "* **ConvNeXt** uses stochastic depth proportional to layer depth.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Stochastic Depth** randomly skips entire residual blocks during training:\n",
    "\n",
    "$$\n",
    "x_{l+1} =\n",
    "\\begin{cases}\n",
    "x_l + f_l(x_l) & \\text{w.p. } 1 - p_l \\\n",
    "x_l & \\text{w.p. } p_l\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It acts like **dropout at the layer level**, improving generalization, stability, and efficiency — and remains **a key component** in most modern deep networks today.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show how **DropPath** is implemented in a **Transformer block** (e.g., ViT or Swin)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f4aa7-ccc4-4494-b233-64236deb1df2",
   "metadata": {},
   "source": [
    "## 1. Reminder: Standard Transformer Block (ViT-style)\n",
    "\n",
    "Each block typically looks like this:\n",
    "\n",
    "$$\n",
    "x' = x + \\text{DropPath}(\\text{MSA}(\\text{Norm}(x))) \n",
    "$$\n",
    "\n",
    "$$\n",
    "y = x' + \\text{DropPath}(\\text{MLP}(\\text{Norm}(x')))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **MSA** = Multi-Head Self-Attention\n",
    "* **MLP** = Feed-forward network\n",
    "* **DropPath** randomly drops the *residual branch*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Minimal PyTorch Implementation\n",
    "\n",
    "Here’s a simplified Vision Transformer block with **Stochastic Depth (DropPath)**.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# ------------------------------------------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        # shape: [batch_size, 1, 1, ...] to broadcast across features\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        binary_mask = torch.floor(random_tensor)\n",
    "        return x / keep_prob * binary_mask\n",
    "```\n",
    "\n",
    "This is the **layer-level dropout** mechanism.\n",
    "If the random mask = 0, the entire residual branch is **skipped**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Simplified Transformer Block with DropPath\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.drop_path1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        self.drop_path2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attention + DropPath\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self.drop_path1(attn_out)\n",
    "\n",
    "        # Feed Forward + DropPath\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x = x + self.drop_path2(mlp_out)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example Run\n",
    "\n",
    "```python\n",
    "B, N, D = 2, 8, 64  # batch, tokens, embedding dim\n",
    "x = torch.randn(B, N, D)\n",
    "\n",
    "block = TransformerBlock(dim=D, num_heads=4, drop_path=0.2)\n",
    "block.train()  # stochastic depth only during training\n",
    "\n",
    "y = block(x)\n",
    "print(y.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "torch.Size([2, 8, 64])\n",
    "```\n",
    "\n",
    "If you run this multiple times during training,\n",
    "you’ll see that **some residual paths are skipped randomly** due to DropPath.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How It’s Used in Practice\n",
    "\n",
    "In **modern ViT or Swin**, the drop probability often **increases with layer depth**:\n",
    "\n",
    "$$\n",
    "p_l = p_{max} \\cdot \\frac{l}{L}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $ L $ = total number of layers\n",
    "* $ l $ = current layer index\n",
    "* $ p_{max} $ = max drop probability (e.g., 0.1–0.3)\n",
    "\n",
    "This means **deeper layers** have higher chance to be dropped — helping regularize the model more strongly in deeper parts.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Code Snippet (Progressive DropPath)\n",
    "\n",
    "```python\n",
    "def build_transformer(depth, dim, num_heads, drop_path_rate=0.3):\n",
    "    blocks = []\n",
    "    for i in range(depth):\n",
    "        drop_prob = drop_path_rate * (i / (depth - 1))\n",
    "        blocks.append(TransformerBlock(dim, num_heads, drop_path=drop_prob))\n",
    "    return nn.Sequential(*blocks)\n",
    "```\n",
    "\n",
    "This is exactly how **Swin Transformer**, **DeiT**, and **ConvNeXt** handle it.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why DropPath Is Effective in Transformers\n",
    "\n",
    "✅ **Stabilizes training:** Helps prevent overfitting and reduces gradient variance.\n",
    "✅ **Acts as layer-level regularization:** Similar to dropout but across residual branches.\n",
    "✅ **Improves generalization:** Especially in large ViT and hierarchical models (Swin, ConvNeXt).\n",
    "✅ **Adds robustness:** The model doesn’t depend on any single residual branch.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "| Component                       | Effect                           | Notes                             |\n",
    "| ------------------------------- | -------------------------------- | --------------------------------- |\n",
    "| **Dropout**                     | Randomly zeros neurons           | Inside feed-forward or attention  |\n",
    "| **DropPath / Stochastic Depth** | Randomly skips residual branches | Applied at layer level            |\n",
    "| **Used in**                     | ResNet, DeiT, Swin, ConvNeXt     | In all major modern architectures |\n",
    "\n",
    "During training:\n",
    "\n",
    "$$\n",
    "x_{l+1} =\n",
    "\\begin{cases}\n",
    "x_l + f_l(x_l), & \\text{w.p. } 1 - p_l \\\n",
    "x_l, & \\text{w.p. } p_l\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "During inference:\n",
    "\n",
    "$$\n",
    "x_{l+1} = x_l + (1 - p_l) f_l(x_l)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080be2c7-f087-4136-92d2-f39eb74940c3",
   "metadata": {},
   "source": [
    "Ref: [1](https://www.youtube.com/watch?v=0KtoTnogk5A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
