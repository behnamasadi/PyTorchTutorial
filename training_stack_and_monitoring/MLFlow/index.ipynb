{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bee8e6b-1295-4f32-9930-fdf414ae1cac",
   "metadata": {},
   "source": [
    "model metrics f1 score, accuracy\n",
    "model parameters -> dataset version, solver i.e. adamw, activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb12107-7ddc-42a6-b609-a61268ebfb4a",
   "metadata": {},
   "source": [
    "# 1) Quick path (works if your model‚Äôs `forward` accepts a NumPy array / tensor directly)\n",
    "\n",
    "**A. Start MLflow tracking server (optional but recommended)**\n",
    "\n",
    "```bash\n",
    "mlflow server \\\n",
    "  --host 127.0.0.1 --port 5000 \\\n",
    "  --backend-store-uri sqlite:///mlflow.db \\\n",
    "  --artifacts-destination ./artifacts\n",
    "```\n",
    "\n",
    "**B. Log + register the PyTorch model**\n",
    "\n",
    "```python\n",
    "import torch, mlflow, mlflow.pytorch\n",
    "import numpy as np\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# 1) Recreate your model and load weights\n",
    "model = YourModelClass(...)\n",
    "state = torch.load(\"your_model.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "# 2) Create a sample input matching your real inference input\n",
    "# Example: (N, C, H, W) = (1,1,224,224)\n",
    "x_example = np.random.rand(1, 1, 224, 224).astype(\"float32\")\n",
    "\n",
    "# 3) Infer signature (optional but strongly recommended)\n",
    "with torch.no_grad():\n",
    "    y_example = model(torch.from_numpy(x_example)).numpy()\n",
    "signature = infer_signature(x_example, y_example)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")  # if running a server\n",
    "registered_name = \"brain_mri_classifier\"          # pick your name\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_arch\", \"YourModelClass\")\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=registered_name,\n",
    "        signature=signature,\n",
    "        input_example=x_example,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==2.14.1\",        # use your versions\n",
    "            \"torch==2.3.1\",\n",
    "            \"numpy\",\n",
    "            \"pandas\",\n",
    "            \"pillow\"\n",
    "        ],\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4448e71-3bbc-4ea5-bbe9-a16a9afb0c83",
   "metadata": {},
   "source": [
    "\n",
    "**D. Serve the Production model**\n",
    "\n",
    "```bash\n",
    "mlflow models serve \\\n",
    "  -m \"models:/brain_mri_classifier/Production\" \\\n",
    "  -h 0.0.0.0 -p 8080\n",
    "```\n",
    "\n",
    "**E. Call the REST API**\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:8080/invocations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"inputs\": [[[ [/* your 224x224 */] ]]] }'\n",
    "```\n",
    "\n",
    "* The payload shape must match your signature (here `NCHW` as `float32`).\n",
    "\n",
    "**Docker (optional)**\n",
    "\n",
    "```bash\n",
    "mlflow models build-docker -m \"models:/brain_mri_classifier/Production\" -n mri:latest\n",
    "docker run -p 8080:8080 mri:latest\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadd9df-664c-4913-a1f7-d407d8aea5d1",
   "metadata": {},
   "source": [
    "\n",
    "### üîπ How versions are created\n",
    "\n",
    "* A **registered model** is like a ‚Äúfolder‚Äù (e.g., `brain_mri_classifier`).\n",
    "* Every time you **register/log a new model** to that name:\n",
    "\n",
    "  ```python\n",
    "  mlflow.pytorch.log_model(..., registered_model_name=\"brain_mri_classifier\")\n",
    "  ```\n",
    "\n",
    "  MLflow **automatically increments the version**:\n",
    "\n",
    "  * First one ‚Üí `Version 1`\n",
    "  * Next one ‚Üí `Version 2`\n",
    "  * etc.\n",
    "* **You cannot manually set the version number**. The registry handles that.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üîπ Stages\n",
    "\n",
    "Each version starts in stage **\"None\"**.\n",
    "You can promote them:\n",
    "\n",
    "* `\"Staging\"` ‚Üí testing environment\n",
    "* `\"Production\"` ‚Üí live\n",
    "* `\"Archived\"` ‚Üí old models you don‚Äôt want used anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb38d2b-2a39-44e3-a7fe-1506995c9d1a",
   "metadata": {},
   "source": [
    "## 1) Stages are deprecated ‚Üí use **aliases** (or explicit versions)\n",
    "\n",
    "* The new Model Registry UI hides Stages (‚ÄúProduction‚Äù, ‚ÄúStaging‚Äù). MLflow recommends **model version aliases** instead (e.g., `champion`, `production`, `staging`). You then reference models as:\n",
    "\n",
    "  * By **alias**: `models:/brain_mri_classifier@production`\n",
    "  * By **version**: `models:/brain_mri_classifier/2`\n",
    "  * By **latest**: `models:/brain_mri_classifier/latest` (per flavor support)\n",
    "    Docs & migration notes: ([mlflow.org][1])\n",
    "\n",
    "### Set an alias (once per version)\n",
    "\n",
    "```python\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# point alias \"production\" to version 2\n",
    "client.set_registered_model_alias(\"brain_mri_classifier\", \"production\", \"2\")\n",
    "\n",
    "# (later) roll forward: repoint alias to version 3\n",
    "client.set_registered_model_alias(\"brain_mri_classifier\", \"production\", \"3\")\n",
    "```\n",
    "\n",
    "\n",
    "### Serve using the alias (or a version)\n",
    "\n",
    "```bash\n",
    "# make sure the client knows your tracking/registry server\n",
    "export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n",
    "\n",
    "# serve via alias (recommended)\n",
    "mlflow models serve -m \"models:/brain_mri_classifier@production\" -h 0.0.0.0 -p 8080\n",
    "\n",
    "# or serve a specific version\n",
    "mlflow models serve -m \"models:/brain_mri_classifier/2\" -h 0.0.0.0 -p 8080\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e69b2-05b0-40c5-848e-88966ef989ef",
   "metadata": {},
   "source": [
    "most teams **don‚Äôt** ship `mlflow models serve` to customers. They use it for **local testing / quick internal demos**, then deploy a **FastAPI (or Flask) microservice** that *loads the model from the MLflow Registry* (by **alias**), containerize it, and run it behind an API gateway with logging, auth, and autoscaling.\n",
    "\n",
    "Here‚Äôs the landscape + a practical template you can drop in:\n",
    "\n",
    "# What‚Äôs common in MLOps\n",
    "\n",
    "**Good for local & CI smoke tests**\n",
    "\n",
    "* `mlflow models serve` (or `build-docker`)\n",
    "  Pros: 1-command, schema validation, works with any MLflow flavor.\n",
    "  Cons: limited control over auth, routing, logging/metrics, batching, GPU pinning, multi-model, and custom business logic. Not ideal for production APIs.\n",
    "\n",
    "**Most common for production**\n",
    "\n",
    "* **FastAPI** (+ Uvicorn/Gunicorn) service that:\n",
    "\n",
    "  1. Resolves your model from **MLflow Registry by alias** (e.g., `@production`).\n",
    "  2. Does custom **pre/post-processing** (images, normalization, thresholds).\n",
    "  3. Emits **structured logs/metrics**; supports **auth**, **tracing**, **health checks**.\n",
    "  4. Runs in Docker/Kubernetes with autoscaling.\n",
    "\n",
    "**Platformy options (when you need fleet scale)**\n",
    "\n",
    "* KServe/Seldon, BentoML, TorchServe, vLLM/TF-Serving, Triton.\n",
    "  MLflow remains your **registry of truth**; serving is handled by the platform.\n",
    "\n",
    "---\n",
    "\n",
    "# A production-ready FastAPI template (loads from MLflow Registry alias)\n",
    "\n",
    "> Works with your earlier **pyfunc** wrapper or a pure PyTorch flavor. Uses aliasing (stages deprecated). GPU optional.\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image\n",
    "import mlflow\n",
    "\n",
    "# 1) Point to your tracking/registry\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://127.0.0.1:5000\")\n",
    "MODEL_URI = os.getenv(\"MODEL_URI\", \"models:/brain_mri_classifier@production\")  # alias!\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# 2) Load once at startup (pyfunc recommended)\n",
    "app = FastAPI(title=\"MRI Classifier Service\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    global model\n",
    "    model = mlflow.pyfunc.load_model(MODEL_URI)  # loads your PythonModel wrapper\n",
    "    # If you logged a torch flavor instead:\n",
    "    # import mlflow.pytorch as mpt\n",
    "    # model = mpt.load_model(MODEL_URI).eval()\n",
    "\n",
    "class PredictB64(BaseModel):\n",
    "    image_b64: str\n",
    "\n",
    "def pil_from_b64(b64: str) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(base64.b64decode(b64)))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\", \"model_uri\": MODEL_URI}\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(req: PredictB64):\n",
    "    # pyfunc expects a DataFrame by design\n",
    "    df = pd.DataFrame({\"image_b64\": [req.image_b64]})\n",
    "    preds = model.predict(df)  # np.ndarray or DataFrame (your wrapper decides)\n",
    "    # convert to python types for JSON\n",
    "    return {\"predictions\": preds.tolist()}\n",
    "```\n",
    "\n",
    "**Run locally**\n",
    "\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n",
    "export MODEL_URI=\"models:/brain_mri_classifier@production\"\n",
    "uvicorn app:app --host 0.0.0.0 --port 8080\n",
    "```\n",
    "\n",
    "**Dockerfile**\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "# Pin the same deps you logged with the model\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY app.py .\n",
    "ENV MLFLOW_TRACKING_URI=http://mlflow:5000\n",
    "ENV MODEL_URI=models:/brain_mri_classifier@production\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "```\n",
    "\n",
    "**requirements.txt (align with what you logged)**\n",
    "\n",
    "```\n",
    "mlflow==2.14.1\n",
    "torch==2.3.1\n",
    "torchvision\n",
    "pillow\n",
    "pandas\n",
    "fastapi\n",
    "uvicorn\n",
    "```\n",
    "\n",
    "> **Rollouts:** when you train a better model, register it ‚Üí **set alias**:\n",
    ">\n",
    "> ```python\n",
    "> from mlflow import MlflowClient\n",
    "> MlflowClient().set_registered_model_alias(\"brain_mri_classifier\", \"production\", \"3\")\n",
    "> ```\n",
    ">\n",
    "> Your service can either:\n",
    ">\n",
    "> * **Re-deploy** (cleanest), or\n",
    "> * Expose a `/reload` admin endpoint that calls `mlflow.pyfunc.load_model(MODEL_URI)` again, or\n",
    "> * Periodically re-resolve the alias on a timer.\n",
    "\n",
    "---\n",
    "\n",
    "# Why this beats `mlflow models serve` in prod\n",
    "\n",
    "* **Pre/Post-processing**: MRI normalization, resizing, ensembles, thresholds.\n",
    "* **Infra hooks**: request IDs, auth (JWT/API keys), rate limits, CORS.\n",
    "* **Observability**: Prometheus/OTel metrics & traces, structured logs.\n",
    "* **GPU control**: explicit `.to(\"cuda\")` paths if you load a torch flavor.\n",
    "* **Multi-endpoint**: `/predict`, `/metadata`, `/explain`, `/health`.\n",
    "* **Safe rollouts**: alias-switch + blue/green or canary at the API gateway.\n",
    "\n",
    "---\n",
    "\n",
    "# When to still use `mlflow models serve`\n",
    "\n",
    "* Quick **manual QA** of a just-logged model.\n",
    "* **CI smoke tests** to ensure the model artifact is runnable.\n",
    "* Prototyping the input schema (`signature` / `input_example`).\n",
    "\n",
    "---\n",
    "\n",
    "# Minimal checklist for your MRI classifier\n",
    "\n",
    "1. **Registry**\n",
    "\n",
    "   * Log model with `registered_model_name=...`, **signature**, **input\\_example**, **pinned requirements**.\n",
    "   * Use **aliases** (`production`, `staging`).\n",
    "\n",
    "2. **Service**\n",
    "\n",
    "   * FastAPI + pyfunc wrapper (you already have it).\n",
    "   * Health check, error handling, request validation.\n",
    "\n",
    "3. **Container & Run**\n",
    "\n",
    "   * Docker, Uvicorn/Gunicorn, env vars for `MLFLOW_TRACKING_URI` and `MODEL_URI`.\n",
    "   * GPU node? Add `nvidia/cuda` base + `--gpus` in Docker run / k8s device plugin.\n",
    "\n",
    "4. **Ops**\n",
    "\n",
    "   * Logs/metrics/traces, autoscaling (HPA), gateway (auth, TLS), alerts.\n",
    "   * Rollout via **alias switch**; rollback = point alias back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc05c20-7728-48a8-9332-a47ee233ef3d",
   "metadata": {},
   "source": [
    "## What `mlflow models serve` actually does\n",
    "\n",
    "When you run:\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m \"<MODEL_URI>\" -p 8080\n",
    "```\n",
    "\n",
    "MLflow:\n",
    "\n",
    "1. **Resolves your model URI** (run path, registry version/alias, local folder).\n",
    "2. **Loads the model‚Äôs ‚Äúflavor‚Äù** (most commonly `pyfunc`, but could be `pytorch`, `sklearn`, etc.; all are exposed via the `pyfunc` interface). ([mlflow.org][1])\n",
    "3. **Restores the model‚Äôs environment** if requested (based on the saved `conda.yaml` / pip requirements). For non-local env managers, MLflow will **spin up a separate ‚ÄúModel Scoring Server‚Äù process** in that environment and proxy requests to it. ([mlflow.org][2])\n",
    "4. **Starts a local HTTP scoring server** implementing MLflow‚Äôs standard REST API (not your app server):\n",
    "\n",
    "   * `GET /ping` for health\n",
    "   * `POST /invocations` for predictions (accepts JSON or CSV in the MLflow scoring schema). ([mlserver.readthedocs.io][3], [mlflow.org][1])\n",
    "\n",
    "This CLI is ideal for **local testing / smoke tests** and ‚Äúdoes what it says on the tin‚Äù: runs a small scoring server bound to your model and its saved dependencies. ([mlflow.org][4])\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Request/response format follows the **pyfunc protocol** (pandas-style JSON like `dataframe_split`, or CSV). Your `pyfunc` wrapper‚Äôs `predict()` is what gets called. ([mlflow.org][5])\n",
    "* If you logged an **input example**, MLflow even generates a **serving payload example** (`serving_input_example.json`). ([mlflow.org][1])\n",
    "\n",
    "---\n",
    "\n",
    "## Is there a ‚Äúbuild-docker‚Äù option?\n",
    "\n",
    "Yes. MLflow ships a **Docker image builder** for models:\n",
    "\n",
    "```bash\n",
    "mlflow models build-docker \\\n",
    "  -m \"<MODEL_URI>\" \\\n",
    "  -n mymodel:latest \\\n",
    "  --enable-mlserver   # optional: use Seldon‚Äôs MLServer runtime\n",
    "```\n",
    "\n",
    "* The image‚Äôs entrypoint serves your model on port **8080** using the **python\\_function (pyfunc) scoring server** (or **MLServer** if you pass `--enable-mlserver`). ([mlflow.org][6], [mlserver.readthedocs.io][3])\n",
    "* You can then run: `docker run -p 8080:8080 mymodel:latest` and hit `/invocations`. ([mlflow.org][6])\n",
    "\n",
    "MLflow also documents broader deployment targets (local server, Kubernetes with the built image, cloud endpoints, etc.). ([mlflow.org][7])\n",
    "\n",
    "---\n",
    "\n",
    "## When to use which\n",
    "\n",
    "* **`mlflow models serve`**: quick local runs, CI smoke tests, ad-hoc validation. ([mlflow.org][4])\n",
    "* **`mlflow models build-docker`**: package the exact scoring server + deps as a portable image (great for K8s). ([mlflow.org][7])\n",
    "* **Custom FastAPI/Flask service**: production APIs needing auth, custom pre/post-processing, observability, batching, GPU control (most teams do this; the service *loads from the MLflow Registry by alias*). MLflow remains your **model packaging + registry**, not necessarily your prod web stack.\n",
    "\n",
    "If you want, I can give you a tiny `Makefile` + `Dockerfile` pairing both workflows (plain `serve` vs image via `build-docker`) wired to your current registry alias.\n",
    "\n",
    "[1]: https://mlflow.org/docs/latest/ml/model/?utm_source=chatgpt.com \"MLflow Models\"\n",
    "[2]: https://www.mlflow.org/docs/2.17.2/_modules/mlflow/pyfunc.html?utm_source=chatgpt.com \"mlflow.pyfunc\"\n",
    "[3]: https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html?utm_source=chatgpt.com \"Serving MLflow models ‚Äî MLServer Documentation\"\n",
    "[4]: https://mlflow.org/docs/latest/ml/deployment/deploy-model-locally/?utm_source=chatgpt.com \"Deploy MLflow Model as a Local Inference Server\"\n",
    "[5]: https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html?utm_source=chatgpt.com \"mlflow.pyfunc\"\n",
    "[6]: https://mlflow.org/docs/latest/cli.html?utm_source=chatgpt.com \"Command-Line Interface\"\n",
    "[7]: https://mlflow.org/docs/latest/ml/deployment/deploy-model-to-kubernetes/?utm_source=chatgpt.com \"Deploy MLflow Model to Kubernetes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c7f7e-6a69-404a-890c-dc6192078b88",
   "metadata": {},
   "source": [
    "Great question. Two parts:\n",
    "\n",
    "# 1) Auto-capturing `pip_requirements` when you call `mlflow.pytorch.log_model`\n",
    "\n",
    "* **Do nothing**: if you *don‚Äôt* pass `pip_requirements` / `conda_env` / `python_env`, MLflow will **infer** the minimal requirements for the model flavor and write them into the model‚Äôs `requirements.txt`, `python_env.yaml`, and `conda.yaml`. Adding an `input_example` improves this inference because MLflow performs a dry-run predict to see imports in action. ([mlflow.org][1])\n",
    "\n",
    "* **Add to what MLflow inferred**: pass `extra_pip_requirements=[...]` to append packages (e.g., your preprocessing libs) without overriding the inferred set. ([mlflow.org][1])\n",
    "\n",
    "* **Fully control deps**: pass `pip_requirements=[...]` (explicit list or path to a `requirements.txt`) **or** `conda_env=...` / `python_env=...` to take full control. (Don‚Äôt pass these together; they‚Äôre mutually exclusive.) Many flavors‚Äîincluding PyTorch and PyFunc‚Äîsupport this pattern. ([mlflow.org][2])\n",
    "\n",
    "* **From ‚Äúcurrent project‚Äù explicitly**: you can precompute with:\n",
    "\n",
    "  ```python\n",
    "  from mlflow.models import infer_pip_requirements\n",
    "  reqs = infer_pip_requirements(model_uri=\"/path/to/temp_model_dir\", flavor=\"pytorch\")\n",
    "  mlflow.pytorch.log_model(model, artifact_path=\"model\", pip_requirements=reqs)\n",
    "  ```\n",
    "\n",
    "  (If inference hits something unusual, MLflow falls back to flavor defaults; you‚Äôll see a warning.) ([mlflow.org][1], [community.databricks.com][3])\n",
    "\n",
    "* **Lock all transitive deps** (for reproducibility): set\n",
    "\n",
    "  ```bash\n",
    "  export MLFLOW_LOCK_MODEL_DEPENDENCIES=true\n",
    "  ```\n",
    "\n",
    "  MLflow will resolve and pin the full tree (uses `uv` if present) when logging the model. ([mlflow.org][1])\n",
    "\n",
    "Quick example you can reuse:\n",
    "\n",
    "```python\n",
    "mlflow.pytorch.log_model(\n",
    "    pytorch_model=model,\n",
    "    artifact_path=\"model\",\n",
    "    # leave pip_requirements unset to let MLflow infer,\n",
    "    extra_pip_requirements=[\"pillow==10.4.0\"],  # add your extras\n",
    "    input_example=x_example,                     # helps inference accuracy\n",
    "    signature=signature,\n",
    ")\n",
    "```\n",
    "\n",
    "# 2) ‚ÄúDocker option‚Äù for models\n",
    "\n",
    "There isn‚Äôt a `docker=` switch in `log_model`. Docker packaging is a **separate step**:\n",
    "\n",
    "* **CLI**:\n",
    "\n",
    "  ```bash\n",
    "  mlflow models build-docker -m \"models:/brain_mri_classifier@production\" -n mri:latest\n",
    "  docker run -p 8080:8080 mri:latest\n",
    "  ```\n",
    "\n",
    "  Builds an image whose entrypoint serves your model on port 8080 (pyfunc server by default; add `--enable-mlserver` to use MLServer). ([mlflow.org][4])\n",
    "\n",
    "* **Python API**:\n",
    "\n",
    "  ```python\n",
    "  import mlflow\n",
    "  mlflow.models.build_docker(\n",
    "      model_uri=\"models:/brain_mri_classifier@production\",\n",
    "      name=\"mri:latest\",\n",
    "      enable_mlserver=False,  # True to use MLServer\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  (Handy for CI/CD pipelines and k8s deploys.) ([mlflow.org][5])\n",
    "\n",
    "# TL;DR\n",
    "\n",
    "* **Easiest**: let MLflow infer deps; add `input_example`; optionally append `extra_pip_requirements`. ([mlflow.org][1])\n",
    "* **Strict reproducibility**: flip `MLFLOW_LOCK_MODEL_DEPENDENCIES=true`. ([mlflow.org][1])\n",
    "* **Docker** is done **after** logging via `mlflow models build-docker` (CLI or Python). ([mlflow.org][4])\n",
    "\n",
    "If you want, I can generate a tiny script that logs your current model, prints the inferred requirements, and builds the Docker image in one go.\n",
    "\n",
    "[1]: https://mlflow.org/docs/latest/ml/model/dependencies/ \"Managing Dependencies in MLflow Models | MLflow\"\n",
    "[2]: https://mlflow.org/docs/2.3.0/python_api/mlflow.pytorch.html?utm_source=chatgpt.com \"mlflow.pytorch ‚Äî MLflow 2.3.0 documentation\"\n",
    "[3]: https://community.databricks.com/t5/machine-learning/how-to-fix-quot-warning-mlflow-utils-environment-quot-when-run/td-p/15930?utm_source=chatgpt.com \"How to fix \\\"WARNING mlflow.utils.environment\\\" when...\"\n",
    "[4]: https://mlflow.org/docs/latest/cli.html?utm_source=chatgpt.com \"Command-Line Interface\"\n",
    "[5]: https://mlflow.org/docs/latest/ml/deployment/deploy-model-to-kubernetes/?utm_source=chatgpt.com \"Deploy MLflow Model to Kubernetes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd5aff-9023-45a1-b189-9b3caa076705",
   "metadata": {},
   "source": [
    "That‚Äôs a very sharp question üôÇ ‚Äî it gets right at how **MLflow controls Docker images**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How `mlflow models build-docker` works\n",
    "\n",
    "* You **don‚Äôt pass your own Dockerfile** directly.\n",
    "* MLflow auto-generates a **standard Dockerfile** for you, based on:\n",
    "\n",
    "  * The **model flavor** (e.g. `pyfunc`, `pytorch`, `sklearn`).\n",
    "  * The **environment spec** that was saved when you logged the model (`conda.yaml`, `python_env.yaml`, or `requirements.txt`).\n",
    "* That Dockerfile installs:\n",
    "\n",
    "  1. Base image (`python:3.11-slim` or similar).\n",
    "  2. Your pinned dependencies from the model‚Äôs environment files.\n",
    "  3. MLflow itself + the scoring server.\n",
    "  4. Entrypoint: runs `mlflow models serve` (or `mlserver` if you pass `--enable-mlserver`).\n",
    "\n",
    "So when you run:\n",
    "\n",
    "```bash\n",
    "mlflow models build-docker \\\n",
    "    -m models:/brain_mri_classifier@production \\\n",
    "    -n brain_mri:latest\n",
    "```\n",
    "\n",
    "MLflow generates:\n",
    "\n",
    "* `Dockerfile` (with the base + env).\n",
    "* `requirements.txt` / `conda.yaml` from the model artifact.\n",
    "* Then runs `docker build` behind the scenes.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Can you customize it?\n",
    "\n",
    "Yes, but **not inline**. Options:\n",
    "\n",
    "1. **Edit the logged model‚Äôs environment**\n",
    "\n",
    "   * When you `log_model`, supply your own `pip_requirements` or `conda_env`.\n",
    "   * Those files are baked into the auto-Dockerfile.\n",
    "   * This is the standard way to say ‚Äúwhat goes into the Docker image.‚Äù\n",
    "\n",
    "2. **Extend the generated image**\n",
    "\n",
    "   * Let MLflow build the image (`mlflow models build-docker -n base_model:latest`).\n",
    "   * Then write your own `Dockerfile` that uses it as the base:\n",
    "\n",
    "     ```dockerfile\n",
    "     FROM base_model:latest\n",
    "     RUN apt-get update && apt-get install -y ffmpeg\n",
    "     COPY extras/ /app/extras/\n",
    "     ```\n",
    "   * Build your extended image with `docker build`.\n",
    "   * This is the common pattern if you need system packages (e.g., OpenCV libs, CUDA, ffmpeg).\n",
    "\n",
    "3. **Programmatic control**\n",
    "\n",
    "   * MLflow exposes a `build_docker()` Python API (same as CLI).\n",
    "   * You still don‚Äôt give it a custom Dockerfile, but you can control some knobs (image name, whether to use MLServer).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Best practice in MLOps\n",
    "\n",
    "* **Reproducibility**: lock your environment (`MLFLOW_LOCK_MODEL_DEPENDENCIES=true`) at log time ‚Üí guarantees the same deps in the Docker image.\n",
    "* **Custom runtime needs** (system libs, configs, GPU base images): extend MLflow‚Äôs generated image with your own Dockerfile (pattern #2).\n",
    "* **Production serving**: many teams only use `mlflow models build-docker` to generate a **base scoring image**, then wrap it in a FastAPI/Flask microservice that does auth/logging, and extend the Dockerfile accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary:**\n",
    "\n",
    "* You don‚Äôt supply a Dockerfile in `mlflow models build-docker`.\n",
    "* It‚Äôs generated automatically from the model‚Äôs saved env (`conda.yaml`, `requirements.txt`, etc.).\n",
    "* To customize: either (a) set `pip_requirements`/`conda_env` at log time, or (b) extend the generated image with your own Dockerfile.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you **what the generated Dockerfile looks like** for a PyTorch model, so you know exactly what MLflow puts in there before you extend it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f2aa2-ccd6-4930-b6ad-4d5ca7eb0ab5",
   "metadata": {},
   "source": [
    "Perfect question üëç ‚Äî this is one of the least obvious parts of MLflow.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What happens when you run\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m <MODEL_URI> -p 8080\n",
    "```\n",
    "\n",
    "### Step 1. Resolve the model\n",
    "\n",
    "* MLflow resolves `<MODEL_URI>` to a local path (download from registry or use run artifact).\n",
    "\n",
    "### Step 2. Inspect environment files\n",
    "\n",
    "* Every logged MLflow model has an **environment spec** saved under its model directory (`MLmodel` file references it):\n",
    "\n",
    "  * `conda.yaml`\n",
    "  * or `python_env.yaml`\n",
    "  * or `requirements.txt`\n",
    "\n",
    "This was created when you logged the model (either inferred or user-provided).\n",
    "That spec tells MLflow **which packages** to install for serving.\n",
    "\n",
    "### Step 3. Launch a scoring process\n",
    "\n",
    "* By default MLflow **does not change your current shell environment**.\n",
    "* Instead, it starts a **new subprocess** called the **MLflow Scoring Server** in the right environment.\n",
    "* The environment is prepared by:\n",
    "\n",
    "  * **pip** if you used `python_env.yaml` or `requirements.txt` (virtualenv backend).\n",
    "  * **conda** if you used `conda.yaml` (conda backend).\n",
    "\n",
    "Which backend gets used depends on what‚Äôs available and how the model was logged:\n",
    "\n",
    "* MLflow checks environment files in this order: `python_env.yaml` ‚Üí `conda.yaml` ‚Üí `requirements.txt`.\n",
    "* If it finds `conda.yaml`, it tries to use conda.\n",
    "* If `python_env.yaml`, it uses MLflow‚Äôs internal **virtualenv backend** (pip-based).\n",
    "* If none, it falls back to your current Python env.\n",
    "\n",
    "So **yes**: it may create a conda env, or a virtualenv, and then `pip install -r` inside it.\n",
    "\n",
    "### Step 4. Run the scoring server\n",
    "\n",
    "* Inside that isolated env, MLflow starts a small REST server:\n",
    "\n",
    "  * `GET /ping` ‚Üí health check\n",
    "  * `POST /invocations` ‚Üí runs your model‚Äôs `pyfunc.predict()` on the payload\n",
    "* It communicates with your outer process through HTTP (on port 8080).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What‚Äôs installed in that env\n",
    "\n",
    "* Exactly the packages listed in the env files (`conda.yaml` / `requirements.txt` / `python_env.yaml`).\n",
    "* Always includes `mlflow` itself and whatever flavor library your model needs (e.g. `torch`, `sklearn`).\n",
    "* If you had `pip_requirements` pinned when logging ‚Üí those are pip installed.\n",
    "* If you set `MLFLOW_LOCK_MODEL_DEPENDENCIES=true` ‚Üí it resolves and locks versions, then uses those in serving.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Where it runs\n",
    "\n",
    "* By default, the envs are created under MLflow‚Äôs cache directory (`~/.mlflow/envs/...`).\n",
    "* Each unique set of requirements creates its own cached environment.\n",
    "* Next time you `mlflow models serve` the same model, MLflow reuses that env.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ TL;DR\n",
    "\n",
    "* **Yes, MLflow installs requirements automatically** ‚Äî pip or conda depending on what env file was saved.\n",
    "* It runs the model inside an **isolated scoring process** with that env, not in your current Python session.\n",
    "* That‚Äôs why `mlflow models serve` ‚Äújust works‚Äù even if your shell env doesn‚Äôt have the right deps.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Want me to show you a real example of the **generated `conda.yaml`** and the **subprocess command** MLflow runs when serving a PyTorch model? That way you can see exactly how your `.pth` gets wrapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d33751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffea2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, \n",
    "                         n_informative=5, n_redundant=10, \n",
    "                         n_clusters_per_class=2, random_state=1, \n",
    "                         weights=[0.5, 0.40, 0.1])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model parameters\n",
    "lr_params = {\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"random_state\": 8888,\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"random_state\": 8888,\n",
    "    \"max_depth\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e393f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50e9a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, params, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(run_name=f\"{model_name}\"):\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log overall metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"weighted_precision\", report_dict['weighted avg']['precision'])\n",
    "        mlflow.log_metric(\"weighted_recall\", report_dict['weighted avg']['recall'])\n",
    "        mlflow.log_metric(\"weighted_f1\", report_dict['weighted avg']['f1-score'])\n",
    "        mlflow.log_metric(\"macro_avg_precision\", report_dict['macro avg']['precision'])\n",
    "        mlflow.log_metric(\"macro_avg_recall\", report_dict['macro avg']['recall'])\n",
    "        mlflow.log_metric(\"macro_avg_f1\", report_dict['macro avg']['f1-score'])\n",
    "        \n",
    "        # Log per-class metrics\n",
    "        for class_idx in range(len(report_dict) - 3):  # -3 to exclude the averages\n",
    "            class_name = str(class_idx)\n",
    "            if class_name in report_dict:\n",
    "                mlflow.log_metric(f\"class_{class_name}_precision\", report_dict[class_name]['precision'])\n",
    "                mlflow.log_metric(f\"class_{class_name}_recall\", report_dict[class_name]['recall'])\n",
    "                mlflow.log_metric(f\"class_{class_name}_f1\", report_dict[class_name]['f1-score'])\n",
    "                mlflow.log_metric(f\"class_{class_name}_support\", report_dict[class_name]['support'])\n",
    "        \n",
    "        # Log the classification report as a text artifact\n",
    "        # with open(\"classification_report.txt\", \"w\") as f:\n",
    "        #     f.write(classification_report(y_test, y_pred))\n",
    "        mlflow.log_artifact(\"classification_report.txt\")\n",
    "        \n",
    "        # Log the model\n",
    "        signature = infer_signature(X_train, model.predict(X_train))\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=f\"{model_name.lower()}_model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train,\n",
    "            registered_model_name=f\"{model_name.lower()}-model\",\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550fe2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI set to: http://127.0.0.1:8080\n",
      "Error setting up experiment: API request to http://127.0.0.1:8080/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Model+Comparison+New (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c5baff0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "API request to http://127.0.0.1:8080/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c8933e0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:445\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1326\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1085\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1088\u001b[39m \n\u001b[32m   1089\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1029\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:276\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    278\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:213\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    217\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x763d5c5baff0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 871 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Model+Comparison+New (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c5baff0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:181\u001b[39m, in \u001b[36mhttp_request\u001b[39m\u001b[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_creds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/request_utils.py:237\u001b[39m, in \u001b[36m_get_http_response_with_retries\u001b[39m\u001b[34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m allow_redirects = env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Model+Comparison+New (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c5baff0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Try to get the experiment by name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     experiment = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# If experiment doesn't exist, create it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/fluent.py:1678\u001b[39m, in \u001b[36mget_experiment_by_name\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1646\u001b[39m \u001b[33;03mRetrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[32m   1647\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1676\u001b[39m \u001b[33;03m    Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[32m   1677\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/client.py:1698\u001b[39m, in \u001b[36mMlflowClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[32m   1668\u001b[39m \n\u001b[32m   1669\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1696\u001b[39m \u001b[33;03m    Lifecycle_stage: active\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py:586\u001b[39m, in \u001b[36mTrackingServiceClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[33;03m    name: The experiment name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    584\u001b[39m \u001b[33;03m    :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:641\u001b[39m, in \u001b[36mRestStore.get_experiment_by_name\u001b[39m\u001b[34m(self, experiment_name)\u001b[39m\n\u001b[32m    640\u001b[39m req_body = message_to_json(GetExperimentByName(experiment_name=experiment_name))\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Experiment.from_proto(response_proto.experiment)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint)\u001b[39m\n\u001b[32m     89\u001b[39m response_proto = api.Response()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:387\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[39m\n\u001b[32m    386\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     response = \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:204\u001b[39m, in \u001b[36mhttp_request\u001b[39m\u001b[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed with exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mMlflowException\u001b[39m: API request to http://127.0.0.1:8080/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Model+Comparison+New (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c5baff0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:445\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1326\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1085\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1088\u001b[39m \n\u001b[32m   1089\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/http/client.py:1029\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:276\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    278\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connection.py:213\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    217\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x763d5c8933e0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 871 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c8933e0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:181\u001b[39m, in \u001b[36mhttp_request\u001b[39m\u001b[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost_creds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/request_utils.py:237\u001b[39m, in \u001b[36m_get_http_response_with_retries\u001b[39m\u001b[34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m allow_redirects = env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/requests/adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c8933e0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError setting up experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Fallback to using the default experiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train and evaluate LogisticRegression\u001b[39;00m\n\u001b[32m     29\u001b[39m lr = LogisticRegression(**lr_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/fluent.py:157\u001b[39m, in \u001b[36mset_experiment\u001b[39m\u001b[34m(experiment_name, experiment_id)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         experiment = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[32m    159\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/client.py:1698\u001b[39m, in \u001b[36mMlflowClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Optional[Experiment]:\n\u001b[32m   1667\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[32m   1668\u001b[39m \n\u001b[32m   1669\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1696\u001b[39m \u001b[33;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py:586\u001b[39m, in \u001b[36mTrackingServiceClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m    579\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[33;03m        name: The experiment name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    584\u001b[39m \u001b[33;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:641\u001b[39m, in \u001b[36mRestStore.get_experiment_by_name\u001b[39m\u001b[34m(self, experiment_name)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    640\u001b[39m     req_body = message_to_json(GetExperimentByName(experiment_name=experiment_name))\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment.from_proto(response_proto.experiment)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint)\u001b[39m\n\u001b[32m     88\u001b[39m     endpoint, method = _METHOD_TO_INFO[api]\n\u001b[32m     89\u001b[39m response_proto = api.Response()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:387\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    386\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     response = \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    389\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PyTorchTutorial/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:204\u001b[39m, in \u001b[36mhttp_request\u001b[39m\u001b[34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUrlException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01miu\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed with exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mMlflowException\u001b[39m: API request to http://127.0.0.1:8080/api/2.0/mlflow/experiments/get-by-name failed with exception HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=Default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x763d5c8933e0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# Set MLflow tracking URI\n",
    "host = \"127.0.0.1\"\n",
    "port = 8080\n",
    "\n",
    "mlflow.set_tracking_uri(uri=f\"http://{host}:{port}\")\n",
    "print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Create a new MLflow Experiment with a unique name\n",
    "experiment_name = \"Model Comparison New\"\n",
    "try:\n",
    "    # Try to get the experiment by name\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    # If experiment doesn't exist, create it\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        print(f\"Created new experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment_name}\")\n",
    "    \n",
    "    # Set the experiment as active\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up experiment: {e}\")\n",
    "    # Fallback to using the default experiment\n",
    "    mlflow.set_experiment(\"Default\")\n",
    "\n",
    "# Train and evaluate LogisticRegression\n",
    "lr = LogisticRegression(**lr_params)\n",
    "lr_model_info = train_and_evaluate_model(lr, \"LogisticRegression\", lr_params, \n",
    "                                      X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train and evaluate GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(**gb_params)\n",
    "gb_model_info = train_and_evaluate_model(gb, \"GradientBoosting\", gb_params, \n",
    "                                      X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e24a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions\n",
    "lr_predictions = lr.predict(X_test)\n",
    "gb_predictions = gb.predict(X_test)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'actual_class': y_test,\n",
    "    'lr_predicted': lr_predictions,\n",
    "    'gb_predicted': gb_predictions\n",
    "})\n",
    "\n",
    "print(\"\\nPrediction Comparison (first 5 samples):\")\n",
    "print(comparison_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
