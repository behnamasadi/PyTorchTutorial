{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17520a3-c7c8-43d9-b996-1388a343f9af",
   "metadata": {},
   "source": [
    "# 1) Training stack & monitoring (PyTorch-first)\n",
    "\n",
    "**Project structure**\n",
    "\n",
    "* `src/` (datasets, models, train.py, eval.py), `configs/` (Hydra/YAML), `scripts/`, `tests/`, `data/` (read-only!), `artifacts/` (checkpoints, logs), `docker/`.\n",
    "* Config via **Hydra** (or plain YAML + argparse). Keep all hyperparams in config, not code.\n",
    "\n",
    "**Data**\n",
    "\n",
    "* Version datasets with **DVC** or **lakeFS**; keep metadata (schema, label map, splits).\n",
    "* Data validation with **Great Expectations** (nulls, ranges, class balance, leakage checks).\n",
    "* Reproducible splits: fixed seed, `StratifiedKFold` for class problems.\n",
    "\n",
    "**Training**\n",
    "\n",
    "* Use **PyTorch Lightning**/**Lightning Fabric** or a light custom loop with:\n",
    "\n",
    "  * AMP: `torch.cuda.amp.autocast()` + `GradScaler` (or bfloat16 on newer GPUs).\n",
    "  * **DDP** or **FSDP** (and/or **DeepSpeed ZeRO**) for multi-GPU.\n",
    "  * Gradient clipping + gradient accumulation.\n",
    "  * LR schedulers (Cosine, OneCycle) and **AdamW** (default), weight decay set.\n",
    "  * Determinism toggles for reproducibility when needed.\n",
    "\n",
    "**Experiment tracking**\n",
    "\n",
    "* Pick one: **Weights & Biases**, **MLflow**, **Neptune**, or **ClearML**.\n",
    "\n",
    "  * Track: run config, code commit, dataset hash, metrics, loss curves, LR, grad norms, GPU util, confusion matrices/PR curves, examples.\n",
    "  * Log artifacts: checkpoints, TensorBoard logs, plots, ONNX/TorchScript exports.\n",
    "\n",
    "**Profiling & performance**\n",
    "\n",
    "* **torch.profiler** (schedule + trace to TensorBoard), `torch.backends.cudnn.benchmark=True` for speed (turn off for determinism), watch dataloader bottlenecks (`num_workers`, `pin_memory`).\n",
    "* Memory: `torch.cuda.memory_summary()`; optimize with channels-last, fused ops where possible.\n",
    "\n",
    "**Example: training loop essentials**\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, (x, y) in enumerate(loader):\n",
    "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "    scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "# 2) Quality gates (before you even think “release”)\n",
    "\n",
    "**Offline evaluation**\n",
    "\n",
    "* Hold-out test set untouched by tuning.\n",
    "* Cross-validation when data is small/imbalanced.\n",
    "* Threshold-free metrics (AUC, log loss) + business metrics (e.g., precision\\@target-recall).\n",
    "* **Stat sig** on deltas (bootstrap CIs), not single-run noise.\n",
    "\n",
    "**Robustness & safety**\n",
    "\n",
    "* Slice metrics (per class/segment/lighting/device).\n",
    "* Adversarial/noise tests, distribution shift tests, ood detectors if relevant.\n",
    "* Bias/fairness checks where applicable.\n",
    "* Model size/latency checks against SLOs (p95/p99) on target hardware.\n",
    "\n",
    "**Promotion criteria (write them down)**\n",
    "\n",
    "* “Promote to Staging if: AUC ≥ X on hold-out; p95 latency ≤ Y ms on T4; passes data validation; reproducible environment.”\n",
    "\n",
    "# 3) Packaging & inference optimization\n",
    "\n",
    "**Artifacts**\n",
    "\n",
    "* Save both: (a) training **state\\_dict** and (b) an **inference graph**:\n",
    "\n",
    "  * TorchScript: `torch.jit.script`/`trace`\n",
    "  * **ONNX** for cross-runtime; optionally convert to **TensorRT** on NVIDIA.\n",
    "  * For PyTorch 2.x, try `torch.compile(model)` for CPU/GPU speedups.\n",
    "\n",
    "**Inference optimizations**\n",
    "\n",
    "* `torch.inference_mode()` (faster than `no_grad()`), batch where possible, static shapes if you can.\n",
    "* Half/bfloat16 on GPU; channels-last for conv nets.\n",
    "* Operator fusion (PyTorch 2 compiler), quantization:\n",
    "\n",
    "  * **PTQ** (post-training quant) to int8 for CPU/edge.\n",
    "  * **QAT** (quant-aware training) if accuracy loss is too high.\n",
    "\n",
    "**Model registry**\n",
    "\n",
    "* Use **MLflow Model Registry**, **W\\&B Artifacts**, or **S3+manifest** with semantic versioning `model: 1.4.2`, stage = `Staging/Production/Archived`.\n",
    "\n",
    "# 4) Release & rollout to production\n",
    "\n",
    "**Serving options (common picks)**\n",
    "\n",
    "* **FastAPI** + PyTorch (flexible, easy A/B and business logic).\n",
    "* **TorchServe** (model store + handlers).\n",
    "* **BentoML** (nice bundling), or **NVIDIA Triton** (multi-framework, dynamic batching).\n",
    "* Batch vs. online: Use batch for offline scoring; online with autoscaling.\n",
    "\n",
    "**Infra**\n",
    "\n",
    "* Containerize (Docker): pin CUDA/cuDNN + PyTorch versions; non-root user; healthcheck.\n",
    "* K8s deploy: readiness/liveness probes, resource requests/limits, HPA for autoscaling, node affinity if you need GPUs. Use **Knative** for scale-to-zero.\n",
    "* Secrets & config: **K8s Secrets**, **Vault**; never bake creds into images.\n",
    "\n",
    "**Release strategies**\n",
    "\n",
    "* **Shadow** (mirror traffic; no user impact) → **Canary** (1–5–25–50–100%) → **Blue/Green** (instant switch with rollback).\n",
    "* A/B testing for business metrics (conversion, attach rate, etc.) when user-visible.\n",
    "* Feature flags (e.g., Unleash/LaunchDarkly) for controlled exposure.\n",
    "* Rollback plan: one command to revert to previous image/model version.\n",
    "\n",
    "**CI/CD for ML (“MLOps”)**\n",
    "\n",
    "* **CI** (GitHub Actions/GitLab CI):\n",
    "\n",
    "  * Lint/format (ruff/black, isort), type check (mypy), unit tests (pytest), small data smoke tests, export test (TorchScript/ONNX), reproducibility check, security scan (bandit/trivy).\n",
    "* **CD**:\n",
    "\n",
    "  * Build image with run ID + git SHA + model version label.\n",
    "  * Push to registry, update Helm chart/kustomize; deploy to **Staging**; run synthetic and A/B tests; promote to **Prod** on pass.\n",
    "* **Orchestration** (pipelines): **Airflow**, **Prefect**, or **Dagster** for retraining, evaluation, and automated promotions on schedule or data drift.\n",
    "\n",
    "# 5) Production monitoring & operations\n",
    "\n",
    "**System (golden signals)**\n",
    "\n",
    "* Latency (p50/p95/p99), throughput (RPS), error rate, saturation (GPU/CPU/mem).\n",
    "* Export Prometheus metrics; dashboards in Grafana. Trace with **OpenTelemetry**.\n",
    "\n",
    "**Model telemetry**\n",
    "\n",
    "* Request mix, input schema checks, outlier rate, drift:\n",
    "\n",
    "  * Data drift (e.g., PSI/JS divergence per feature), prediction drift vs. training.\n",
    "  * Quality proxy when labels are delayed (leading indicators).\n",
    "  * If you get labels later: continuous evaluation (lagged accuracy/AUC, calibration).\n",
    "* Log samples (privacy-safe, sampled & hashed); enable replay for debugging.\n",
    "\n",
    "**Alerting & SLOs**\n",
    "\n",
    "* SLOs (e.g., p95 < 60 ms; 99.9% availability; drift PSI < 0.2).\n",
    "* Alerts: divergence (nan %, loss spikes), drift exceedance, latency/error budget burn, GPU OOM.\n",
    "\n",
    "**Model lifecycle**\n",
    "\n",
    "* Scheduled retraining or event-based (data volume/quality change).\n",
    "* Auto-promotion only with strict gates + human approval.\n",
    "* Decommission old models (archive artifacts + model card + changelog).\n",
    "\n",
    "# 6) Reference checklists + mini-snippets\n",
    "\n",
    "## Training-time checklist\n",
    "\n",
    "* [ ] Data versioned; validation suite green\n",
    "* [ ] Repro seed set; env pinned (PyTorch/CUDA, Python, OS)\n",
    "* [ ] AMP on; DDP/FSDP if multi-GPU\n",
    "* [ ] Tracking & artifacts logged (W\\&B/MLflow)\n",
    "* [ ] Profiler run shows no dataloader bottleneck\n",
    "* [ ] Best-epoch checkpoint + exported TorchScript/ONNX\n",
    "\n",
    "## Pre-release gates\n",
    "\n",
    "* [ ] Test set metrics meet thresholds (with CIs)\n",
    "* [ ] Slice metrics OK; latency budget OK on target HW\n",
    "* [ ] Bias/safety checks OK\n",
    "* [ ] Model card updated; changelog written\n",
    "\n",
    "## Deploy checklist\n",
    "\n",
    "* [ ] Image built (git SHA + model ver tags)\n",
    "* [ ] Health endpoints added; readiness passes\n",
    "* [ ] Canary plan & rollback ready\n",
    "* [ ] Dashboards + alerts wired\n",
    "\n",
    "**FastAPI serving skeleton**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "model = torch.jit.load(\"model.ts\").eval()\n",
    "@torch.inference_mode()\n",
    "@app.post(\"/predict\")\n",
    "async def predict(payload: dict):\n",
    "    x = preprocess(payload)                # tensor on device\n",
    "    y = model(x)                           # batched inference\n",
    "    return postprocess(y)\n",
    "```\n",
    "\n",
    "**Prometheus metrics (example using fastapi-instrumentator)**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram\n",
    "REQS = Counter(\"inference_requests_total\", \"Total requests\")\n",
    "LAT = Histogram(\"inference_latency_seconds\", \"Latency\")\n",
    "```\n",
    "\n",
    "**MLflow quick logging**\n",
    "\n",
    "```python\n",
    "import mlflow, mlflow.pytorch\n",
    "mlflow.set_experiment(\"my_model\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(cfg_as_dict)\n",
    "    mlflow.log_metric(\"val_auc\", auc)\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "```\n",
    "\n",
    "**Export to ONNX**\n",
    "\n",
    "```python\n",
    "torch.onnx.export(model, example_input, \"model.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "                  opset_version=17)\n",
    "```\n",
    "\n",
    "**Quantization (PTQ) sketch**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
    "prepared = torch.ao.quantization.prepare(model)\n",
    "# calibrate with a few hundred real samples\n",
    "quantized = torch.ao.quantization.convert(prepared)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended tool picks (opinionated)\n",
    "\n",
    "* **Config & structure:** Hydra + Lightning (or clean custom loops)\n",
    "* **Tracking:** W\\&B *or* MLflow (pick one)\n",
    "* **Data versioning:** DVC\n",
    "* **Validation:** Great Expectations\n",
    "* **Serving:** FastAPI (flexibility) or Triton (throughput) or TorchServe (PyTorch-native)\n",
    "* **Pipelines:** Prefect (DX) or Airflow (enterprise)\n",
    "* **Deploy:** Docker + K8s + Helm; Prometheus/Grafana + OpenTelemetry\n",
    "\n",
    "If you want, tell me your target hardware (GPU/CPU/edge), latency/throughput goals, and the problem type (classification/detection/etc.). I can turn this into a concrete template repo (Dockerfile, GH Actions CI, FastAPI server, Helm chart, and minimal training loop) tailored to your setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
